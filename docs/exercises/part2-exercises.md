# ç¬¬2éƒ¨ æ¼”ç¿’å•é¡Œ

## æ¼”ç¿’ 2.1: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨åŸ‹ã‚è¾¼ã¿

### å•é¡Œ 1
ç°¡å˜ãªBPEï¼ˆByte Pair Encodingï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã«å¯¾ã—ã¦ã€3å›ã®ãƒãƒ¼ã‚¸æ“ä½œã‚’è¡Œã„ã¾ã™ã€‚

```python
corpus = ["low", "lower", "newest", "widest"]
```

??? è§£ç­”
    ```python
    from collections import defaultdict, Counter
    
    def get_vocab(corpus):
        """å˜èªã‚’æ–‡å­—å˜ä½ã«åˆ†è§£ã—ã€é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        vocab = defaultdict(int)
        for word in corpus:
            word_tokens = ' '.join(list(word)) + ' </w>'
            vocab[word_tokens] += 1
        return vocab
    
    def get_stats(vocab):
        """éš£æ¥ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã®é »åº¦ã‚’è¨ˆç®—"""
        pairs = defaultdict(int)
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[symbols[i], symbols[i + 1]] += freq
        return pairs
    
    def merge_vocab(pair, vocab):
        """æœ€é »å‡ºãƒšã‚¢ã‚’ãƒãƒ¼ã‚¸"""
        out = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word in vocab:
            new_word = word.replace(bigram, replacement)
            out[new_word] = vocab[word]
        return out
    
    # BPEå®Ÿè¡Œ
    corpus = ["low", "lower", "newest", "widest"]
    vocab = get_vocab(corpus)
    print("åˆæœŸèªå½™:", vocab)
    
    for i in range(3):
        pairs = get_stats(vocab)
        if not pairs:
            break
            
        best = max(pairs, key=pairs.get)
        vocab = merge_vocab(best, vocab)
        print(f"\nãƒãƒ¼ã‚¸ {i+1}: {best} -> {''.join(best)}")
        print("æ›´æ–°å¾Œã®èªå½™:", vocab)
    
    # å‡ºåŠ›ä¾‹:
    # ãƒãƒ¼ã‚¸ 1: ('e', 's') -> 'es'
    # ãƒãƒ¼ã‚¸ 2: ('es', 't') -> 'est'
    # ãƒãƒ¼ã‚¸ 3: ('l', 'o') -> 'lo'
    ```

### å•é¡Œ 2
å˜èªåŸ‹ã‚è¾¼ã¿ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

??? è§£ç­”
    ```python
    import numpy as np
    
    def cosine_similarity(vec1, vec2):
        """2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    
    # ãƒ†ã‚¹ãƒˆ
    # ä»®æƒ³çš„ãªå˜èªåŸ‹ã‚è¾¼ã¿
    embeddings = {
        "cat": np.array([0.2, 0.8, 0.1]),
        "dog": np.array([0.3, 0.7, 0.2]),
        "car": np.array([0.9, 0.1, 0.3]),
        "truck": np.array([0.8, 0.2, 0.4])
    }
    
    # é¡ä¼¼åº¦è¨ˆç®—
    print(f"cat vs dog: {cosine_similarity(embeddings['cat'], embeddings['dog']):.3f}")
    print(f"cat vs car: {cosine_similarity(embeddings['cat'], embeddings['car']):.3f}")
    print(f"car vs truck: {cosine_similarity(embeddings['car'], embeddings['truck']):.3f}")
    
    # æœ€ã‚‚é¡ä¼¼ã—ãŸå˜èªã‚’è¦‹ã¤ã‘ã‚‹
    def find_most_similar(word, embeddings, top_k=2):
        target_vec = embeddings[word]
        similarities = []
        
        for other_word, other_vec in embeddings.items():
            if other_word != word:
                sim = cosine_similarity(target_vec, other_vec)
                similarities.append((other_word, sim))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    print(f"\n'cat'ã«æœ€ã‚‚é¡ä¼¼: {find_most_similar('cat', embeddings)}")
    ```

## æ¼”ç¿’ 2.2: æ³¨æ„æ©Ÿæ§‹

### å•é¡Œ 3
ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©æ³¨æ„ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
ãƒã‚¹ã‚¯ã®ã‚µãƒãƒ¼ãƒˆã‚‚å«ã‚ã¾ã™ã€‚

??? è§£ç­”
    ```python
    import torch
    import torch.nn.functional as F
    import math
    
    def scaled_dot_product_attention(Q, K, V, mask=None):
        """
        ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©æ³¨æ„ã®å®Ÿè£…
        
        Args:
            Q: ã‚¯ã‚¨ãƒª [batch_size, seq_len, d_k]
            K: ã‚­ãƒ¼ [batch_size, seq_len, d_k]
            V: ãƒãƒªãƒ¥ãƒ¼ [batch_size, seq_len, d_v]
            mask: ãƒã‚¹ã‚¯ [batch_size, seq_len, seq_len] or None
        
        Returns:
            output: æ³¨æ„ã®å‡ºåŠ› [batch_size, seq_len, d_v]
            attention_weights: æ³¨æ„ã®é‡ã¿ [batch_size, seq_len, seq_len]
        """
        d_k = Q.size(-1)
        
        # ã‚¹ã‚³ã‚¢ã®è¨ˆç®—: QK^T / âˆšd_k
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # ãƒã‚¹ã‚¯ã®é©ç”¨
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§æ³¨æ„ã®é‡ã¿ã‚’è¨ˆç®—
        attention_weights = F.softmax(scores, dim=-1)
        
        # é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    # ãƒ†ã‚¹ãƒˆ
    batch_size, seq_len, d_k = 2, 4, 8
    Q = torch.randn(batch_size, seq_len, d_k)
    K = torch.randn(batch_size, seq_len, d_k)
    V = torch.randn(batch_size, seq_len, d_k)
    
    # ãƒã‚¹ã‚¯ãªã—
    output, weights = scaled_dot_product_attention(Q, K, V)
    print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„é‡ã¿å½¢çŠ¶: {weights.shape}")
    
    # å› æœçš„ãƒã‚¹ã‚¯ï¼ˆä¸‹ä¸‰è§’è¡Œåˆ—ï¼‰
    mask = torch.tril(torch.ones(seq_len, seq_len))
    mask = mask.unsqueeze(0).expand(batch_size, -1, -1)
    
    output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask)
    print(f"\nãƒã‚¹ã‚¯é©ç”¨å¾Œã®æ³¨æ„é‡ã¿ï¼ˆæœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
    print(weights_masked[0])
    ```

### å•é¡Œ 4
ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã‚’å®Ÿè£…ã—ã€ä½ç½®ã«ã‚ˆã‚‹æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é•ã„ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    class SelfAttention(nn.Module):
        def __init__(self, d_model, d_k=None):
            super().__init__()
            if d_k is None:
                d_k = d_model
                
            self.d_k = d_k
            self.W_q = nn.Linear(d_model, d_k, bias=False)
            self.W_k = nn.Linear(d_model, d_k, bias=False)
            self.W_v = nn.Linear(d_model, d_k, bias=False)
            self.W_o = nn.Linear(d_k, d_model, bias=False)
            
        def forward(self, x, mask=None):
            Q = self.W_q(x)
            K = self.W_k(x)
            V = self.W_v(x)
            
            # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆç©æ³¨æ„
            d_k = self.d_k
            scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)
            
            if mask is not None:
                scores = scores.masked_fill(mask == 0, -1e9)
                
            attn_weights = F.softmax(scores, dim=-1)
            context = torch.matmul(attn_weights, V)
            
            output = self.W_o(context)
            
            return output, attn_weights
    
    # ãƒ†ã‚¹ãƒˆã¨å¯è¦–åŒ–
    torch.manual_seed(42)
    d_model = 64
    seq_len = 8
    
    # ãƒ€ãƒŸãƒ¼ã®å…¥åŠ›ï¼ˆä½ç½®ã«ã‚ˆã£ã¦ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    x = torch.randn(1, seq_len, d_model)
    
    # ä½ç½®æƒ…å ±ã‚’è¿½åŠ ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    for i in range(seq_len):
        x[0, i, :] += torch.sin(torch.arange(d_model) * i / 10)
    
    # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é©ç”¨
    self_attn = SelfAttention(d_model)
    output, attn_weights = self_attn(x)
    
    # æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–
    plt.figure(figsize=(8, 6))
    sns.heatmap(attn_weights[0].detach().numpy(), 
                annot=True, fmt='.2f', cmap='Blues',
                xticklabels=[f'Pos{i}' for i in range(seq_len)],
                yticklabels=[f'Pos{i}' for i in range(seq_len)])
    plt.title('Self-Attention Pattern')
    plt.xlabel('Keys')
    plt.ylabel('Queries')
    plt.tight_layout()
    plt.show()
    
    # å„ä½ç½®ãŒã©ã“ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹åˆ†æ
    for i in range(min(4, seq_len)):
        top_3 = torch.topk(attn_weights[0, i], 3)
        print(f"ä½ç½®{i}ãŒæœ€ã‚‚æ³¨ç›®ã—ã¦ã„ã‚‹ä½ç½®: {top_3.indices.tolist()}")
    ```

## æ¼”ç¿’ 2.3: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

### å•é¡Œ 5
æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè£…ã—ã€ç•°ãªã‚‹æ¬¡å…ƒã§ã®å‘¨æœŸæ€§ã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    
    def positional_encoding(max_len, d_model):
        """æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ç”Ÿæˆ"""
        pe = np.zeros((max_len, d_model))
        position = np.arange(0, max_len).reshape(-1, 1)
        
        # å‘¨æ³¢æ•°é …ã®è¨ˆç®—
        div_term = np.exp(np.arange(0, d_model, 2) * 
                          -(np.log(10000.0) / d_model))
        
        # sin ã¨ cos ã‚’é©ç”¨
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        return pe
    
    # ç”Ÿæˆã¨å¯è¦–åŒ–
    max_len = 100
    d_model = 64
    pe = positional_encoding(max_len, d_model)
    
    # ç•°ãªã‚‹æ¬¡å…ƒã§ã®å‘¨æœŸæ€§ã‚’å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # æœ€åˆã®4æ¬¡å…ƒ
    for i, ax in enumerate(axes.flat):
        ax.plot(pe[:, i], label=f'dim={i}')
        ax.set_xlabel('Position')
        ax.set_ylabel('Value')
        ax.set_title(f'Positional Encoding - Dimension {i}')
        ax.grid(True, alpha=0.3)
        ax.legend()
    
    plt.tight_layout()
    plt.show()
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å…¨ä½“åƒã‚’è¡¨ç¤º
    plt.figure(figsize=(12, 4))
    plt.imshow(pe.T, aspect='auto', cmap='RdBu', 
               extent=[0, max_len, d_model, 0])
    plt.colorbar()
    plt.xlabel('Position')
    plt.ylabel('Dimension')
    plt.title('Positional Encoding Heatmap')
    plt.show()
    
    # ç›¸å¯¾ä½ç½®ã®æ€§è³ªã‚’ç¢ºèª
    def check_relative_position_property(pe, pos1, pos2, k):
        """ç›¸å¯¾ä½ç½®ké›¢ã‚ŒãŸä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å†…ç©"""
        vec1 = pe[pos1]
        vec2 = pe[pos2]
        vec1_k = pe[pos1 + k] if pos1 + k < len(pe) else None
        vec2_k = pe[pos2 + k] if pos2 + k < len(pe) else None
        
        if vec1_k is not None and vec2_k is not None:
            dot1 = np.dot(vec1, vec2)
            dot2 = np.dot(vec1_k, vec2_k)
            print(f"ä½ç½®{pos1}ã¨{pos2}ã®å†…ç©: {dot1:.3f}")
            print(f"ä½ç½®{pos1+k}ã¨{pos2+k}ã®å†…ç©: {dot2:.3f}")
            print(f"å·®: {abs(dot1 - dot2):.3f}")
    
    print("\nç›¸å¯¾ä½ç½®ã®æ€§è³ª:")
    check_relative_position_property(pe, 10, 15, 5)
    ```

### å•é¡Œ 6
å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ã¨æ­£å¼¦æ³¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¯”è¼ƒã™ã‚‹å®Ÿé¨“ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class LearnablePositionalEmbedding(nn.Module):
        """å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿"""
        def __init__(self, max_len, d_model):
            super().__init__()
            self.pos_embedding = nn.Embedding(max_len, d_model)
            
        def forward(self, x):
            seq_len = x.size(1)
            positions = torch.arange(seq_len, device=x.device)
            return x + self.pos_embedding(positions)
    
    class SinusoidalPositionalEncoding(nn.Module):
        """æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        def __init__(self, max_len, d_model):
            super().__init__()
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len).unsqueeze(1).float()
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                               -(np.log(10000.0) / d_model))
            
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            self.register_buffer('pe', pe.unsqueeze(0))
            
        def forward(self, x):
            return x + self.pe[:, :x.size(1)]
    
    # ç°¡å˜ãªç³»åˆ—ã‚¿ã‚¹ã‚¯ã§æ¯”è¼ƒ
    class PositionAwareModel(nn.Module):
        def __init__(self, vocab_size, d_model, pos_encoding_type='learnable'):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, d_model)
            
            if pos_encoding_type == 'learnable':
                self.pos_encoding = LearnablePositionalEmbedding(100, d_model)
            else:
                self.pos_encoding = SinusoidalPositionalEncoding(100, d_model)
                
            self.lstm = nn.LSTM(d_model, d_model, batch_first=True)
            self.output = nn.Linear(d_model, vocab_size)
            
        def forward(self, x):
            x = self.embedding(x)
            x = self.pos_encoding(x)
            x, _ = self.lstm(x)
            return self.output(x)
    
    # ä½ç½®ä¾å­˜ã‚¿ã‚¹ã‚¯ã®ä½œæˆï¼ˆä½ç½®ã‚’åè»¢ã™ã‚‹ï¼‰
    def create_position_task(seq_len=10, vocab_size=20, n_samples=100):
        data = []
        for _ in range(n_samples):
            # ãƒ©ãƒ³ãƒ€ãƒ ãªç³»åˆ—ã‚’ç”Ÿæˆ
            seq = torch.randint(3, vocab_size, (seq_len,))
            # åè»¢ã—ãŸç³»åˆ—ãŒç›®æ¨™
            target = torch.flip(seq, [0])
            data.append((seq, target))
        return data
    
    # è¨“ç·´ã¨è©•ä¾¡
    def train_and_evaluate(model, data, epochs=50):
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        losses = []
        
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for seq, target in data:
                seq = seq.unsqueeze(0)
                target = target.unsqueeze(0)
                
                output = model(seq)
                loss = F.cross_entropy(output.reshape(-1, output.size(-1)), 
                                     target.reshape(-1))
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                # ç²¾åº¦è¨ˆç®—
                _, predicted = output.max(-1)
                correct += (predicted == target).sum().item()
                total += target.numel()
            
            accuracy = correct / total
            losses.append(total_loss / len(data))
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Loss={losses[-1]:.3f}, Acc={accuracy:.3f}")
        
        return losses
    
    # å®Ÿé¨“å®Ÿè¡Œ
    print("ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®æ¯”è¼ƒå®Ÿé¨“")
    print("ã‚¿ã‚¹ã‚¯: å…¥åŠ›ç³»åˆ—ã‚’åè»¢ã™ã‚‹\n")
    
    vocab_size = 20
    d_model = 32
    data = create_position_task()
    
    # å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿
    print("1. å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿:")
    model_learnable = PositionAwareModel(vocab_size, d_model, 'learnable')
    losses_learnable = train_and_evaluate(model_learnable, data)
    
    print("\n2. æ­£å¼¦æ³¢ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°:")
    model_sinusoidal = PositionAwareModel(vocab_size, d_model, 'sinusoidal')
    losses_sinusoidal = train_and_evaluate(model_sinusoidal, data)
    
    # çµæœã®å¯è¦–åŒ–
    plt.figure(figsize=(8, 6))
    plt.plot(losses_learnable, label='Learnable')
    plt.plot(losses_sinusoidal, label='Sinusoidal')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Position Encoding Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    ```

## æ¼”ç¿’ 2.4: æ·±å±¤å­¦ç¿’ã®åŸºç¤

### å•é¡Œ 7
æ®‹å·®æ¥ç¶šã‚’å«ã‚€ç°¡å˜ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã—ã€å‹¾é…ã®æµã‚Œã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import matplotlib.pyplot as plt
    
    class ResidualBlock(nn.Module):
        def __init__(self, dim):
            super().__init__()
            self.fc1 = nn.Linear(dim, dim)
            self.fc2 = nn.Linear(dim, dim)
            self.relu = nn.ReLU()
            
        def forward(self, x):
            residual = x
            out = self.fc1(x)
            out = self.relu(out)
            out = self.fc2(out)
            out = out + residual  # æ®‹å·®æ¥ç¶š
            return out
    
    class DeepNetwork(nn.Module):
        def __init__(self, input_dim, n_blocks, use_residual=True):
            super().__init__()
            self.use_residual = use_residual
            self.blocks = nn.ModuleList()
            
            for _ in range(n_blocks):
                if use_residual:
                    self.blocks.append(ResidualBlock(input_dim))
                else:
                    self.blocks.append(nn.Sequential(
                        nn.Linear(input_dim, input_dim),
                        nn.ReLU(),
                        nn.Linear(input_dim, input_dim)
                    ))
                    
        def forward(self, x):
            activations = [x]
            for block in self.blocks:
                x = block(x)
                activations.append(x)
            return x, activations
    
    # å‹¾é…ã®æµã‚Œã‚’è¨˜éŒ²
    def record_gradients(model, input_data, target):
        gradients = []
        
        def hook_fn(module, grad_input, grad_output):
            gradients.append(grad_output[0].norm().item())
        
        # ãƒ•ãƒƒã‚¯ã‚’ç™»éŒ²
        handles = []
        for block in model.blocks:
            handle = block.register_backward_hook(hook_fn)
            handles.append(handle)
        
        # é †ä¼æ’­ã¨é€†ä¼æ’­
        output, _ = model(input_data)
        loss = nn.MSELoss()(output, target)
        loss.backward()
        
        # ãƒ•ãƒƒã‚¯ã‚’å‰Šé™¤
        for handle in handles:
            handle.remove()
            
        return gradients[::-1]  # é€†é †ã«ã—ã¦å…¥åŠ›å´ã‹ã‚‰ä¸¦ã¹ã‚‹
    
    # å®Ÿé¨“
    input_dim = 64
    n_blocks = 10
    batch_size = 32
    
    # ãƒ‡ãƒ¼ã‚¿
    x = torch.randn(batch_size, input_dim)
    target = torch.randn(batch_size, input_dim)
    
    # æ®‹å·®æ¥ç¶šã‚ã‚Šã¨ãªã—ã§æ¯”è¼ƒ
    model_with_residual = DeepNetwork(input_dim, n_blocks, use_residual=True)
    model_without_residual = DeepNetwork(input_dim, n_blocks, use_residual=False)
    
    grad_with = record_gradients(model_with_residual, x.clone(), target)
    grad_without = record_gradients(model_without_residual, x.clone(), target)
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(10, 6))
    blocks = list(range(1, n_blocks + 1))
    
    plt.semilogy(blocks, grad_with, 'bo-', label='With Residual', linewidth=2)
    plt.semilogy(blocks, grad_without, 'ro-', label='Without Residual', linewidth=2)
    
    plt.xlabel('Block Number (from input)')
    plt.ylabel('Gradient Norm (log scale)')
    plt.title('Gradient Flow in Deep Networks')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print(f"æœ€åˆã®å±¤ã®å‹¾é…ãƒãƒ«ãƒ :")
    print(f"  æ®‹å·®æ¥ç¶šã‚ã‚Š: {grad_with[0]:.6f}")
    print(f"  æ®‹å·®æ¥ç¶šãªã—: {grad_without[0]:.6f}")
    print(f"  æ¯”ç‡: {grad_with[0] / grad_without[0]:.2f}x")
    ```

## ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œ

### å•é¡Œ 8 ğŸŒŸ
ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç°¡æ˜“å®Ÿè£…ã—ã€å„ãƒ˜ãƒƒãƒ‰ãŒç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    class MultiHeadAttention(nn.Module):
        def __init__(self, d_model, n_heads):
            super().__init__()
            assert d_model % n_heads == 0
            
            self.d_model = d_model
            self.n_heads = n_heads
            self.d_k = d_model // n_heads
            
            self.W_q = nn.Linear(d_model, d_model)
            self.W_k = nn.Linear(d_model, d_model)
            self.W_v = nn.Linear(d_model, d_model)
            self.W_o = nn.Linear(d_model, d_model)
            
        def forward(self, query, key, value, mask=None):
            batch_size, seq_len, _ = query.shape
            
            # ç·šå½¢å¤‰æ›ã¨å½¢çŠ¶å¤‰æ›´
            Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k)
            K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k)
            V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k)
            
            # è»¢ç½®ã—ã¦ãƒ˜ãƒƒãƒ‰ã‚’åˆ¥æ¬¡å…ƒã«
            Q = Q.transpose(1, 2)  # [batch, heads, seq_len, d_k]
            K = K.transpose(1, 2)
            V = V.transpose(1, 2)
            
            # å„ãƒ˜ãƒƒãƒ‰ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—
            scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
            
            if mask is not None:
                scores = scores.masked_fill(mask == 0, -1e9)
            
            attn_weights = F.softmax(scores, dim=-1)
            context = torch.matmul(attn_weights, V)
            
            # ãƒ˜ãƒƒãƒ‰ã‚’çµåˆ
            context = context.transpose(1, 2).contiguous()
            context = context.view(batch_size, seq_len, self.d_model)
            
            output = self.W_o(context)
            
            return output, attn_weights
    
    # ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã•ã›ã‚‹å®Ÿé¨“
    def create_pattern_data():
        """ç•°ãªã‚‹ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        seq_len = 8
        d_model = 64
        batch_size = 100
        
        data = []
        
        for _ in range(batch_size):
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: éš£æ¥ä¾å­˜
            x1 = torch.randn(seq_len, d_model)
            for i in range(1, seq_len):
                x1[i] += 0.5 * x1[i-1]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: é•·è·é›¢ä¾å­˜
            x2 = torch.randn(seq_len, d_model)
            x2[seq_len//2:] += x2[:seq_len//2]
            
            # æ··åˆ
            x = x1 + x2
            x = x / x.norm(dim=-1, keepdim=True)
            
            data.append(x)
            
        return torch.stack(data)
    
    # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
    def train_multihead_attention():
        d_model = 64
        n_heads = 4
        mha = MultiHeadAttention(d_model, n_heads)
        
        # ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        data = create_pattern_data()
        
        # è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆå…¥åŠ›ã‚’å†æ§‹æˆï¼‰
        optimizer = torch.optim.Adam(mha.parameters(), lr=0.001)
        
        for epoch in range(100):
            # ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸå…¥åŠ›
            noisy_input = data + 0.1 * torch.randn_like(data)
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é©ç”¨
            output, attn_weights = mha(noisy_input, noisy_input, noisy_input)
            
            # å†æ§‹æˆèª¤å·®
            loss = F.mse_loss(output, data)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
        
        return mha, data, attn_weights
    
    # å®Ÿè¡Œã¨å¯è¦–åŒ–
    mha, data, attn_weights = train_multihead_attention()
    
    # å„ãƒ˜ãƒƒãƒ‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()
    
    sample_idx = 0  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«
    
    for head in range(4):
        ax = axes[head]
        attn = attn_weights[sample_idx, head].detach().numpy()
        
        sns.heatmap(attn, ax=ax, cmap='Blues', cbar=True)
        ax.set_title(f'Head {head + 1} Attention Pattern')
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')
        
        # ä¸»è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        avg_attn = attn.mean(axis=0)
        main_focus = avg_attn.argmax()
        print(f"Head {head + 1} - å¹³å‡çš„ãªç„¦ç‚¹ä½ç½®: {main_focus}")
    
    plt.tight_layout()
    plt.show()
    
    # ãƒ˜ãƒƒãƒ‰é–“ã®å¤šæ§˜æ€§ã‚’å®šé‡åŒ–
    def attention_diversity(attn_weights):
        """ãƒ˜ãƒƒãƒ‰é–“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤šæ§˜æ€§ã‚’è¨ˆç®—"""
        n_heads = attn_weights.shape[1]
        
        # å„ãƒ˜ãƒƒãƒ‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¹³å¦åŒ–
        flattened = attn_weights.reshape(attn_weights.shape[0], n_heads, -1)
        
        # ãƒ˜ãƒƒãƒ‰é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        similarities = []
        for i in range(n_heads):
            for j in range(i + 1, n_heads):
                sim = F.cosine_similarity(
                    flattened[:, i], 
                    flattened[:, j], 
                    dim=-1
                ).mean().item()
                similarities.append(sim)
        
        # å¤šæ§˜æ€§ = 1 - å¹³å‡é¡ä¼¼åº¦
        diversity = 1 - np.mean(similarities)
        return diversity
    
    diversity = attention_diversity(attn_weights)
    print(f"\nã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®å¤šæ§˜æ€§: {diversity:.3f}")
    print("(1ã«è¿‘ã„ã»ã©å¤šæ§˜ã€0ã«è¿‘ã„ã»ã©é¡ä¼¼)")
    ```

## å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ğŸš€

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: ãƒŸãƒ‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
ç¬¬2éƒ¨ã§å­¦ã‚“ã ã™ã¹ã¦ã®è¦ç´ ã‚’çµ„ã¿åˆã‚ã›ã¦ã€å°ã•ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

è¦ä»¶ï¼š
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡å˜ãªç©ºç™½åŒºåˆ‡ã‚Šï¼‰
- åŸ‹ã‚è¾¼ã¿å±¤
- ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆ1å±¤ï¼‰
- å‡ºåŠ›å±¤

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class MiniLanguageModel(nn.Module):
        def __init__(self, vocab_size, d_model=128, max_len=100):
            super().__init__()
            
            # åŸ‹ã‚è¾¼ã¿å±¤
            self.token_embedding = nn.Embedding(vocab_size, d_model)
            
            # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            self.position_embedding = nn.Embedding(max_len, d_model)
            
            # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
            self.attention = nn.MultiheadAttention(d_model, num_heads=4, 
                                                  batch_first=True)
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.ReLU(),
                nn.Linear(d_model * 4, d_model)
            )
            
            # å±¤æ­£è¦åŒ–
            self.norm1 = nn.LayerNorm(d_model)
            self.norm2 = nn.LayerNorm(d_model)
            
            # å‡ºåŠ›å±¤
            self.output = nn.Linear(d_model, vocab_size)
            
        def forward(self, x, mask=None):
            seq_len = x.size(1)
            
            # åŸ‹ã‚è¾¼ã¿
            token_emb = self.token_embedding(x)
            pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)
            pos_emb = self.position_embedding(pos_ids)
            
            x = token_emb + pos_emb
            
            # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆæ®‹å·®æ¥ç¶šä»˜ãï¼‰
            attn_output, _ = self.attention(x, x, x, attn_mask=mask)
            x = self.norm1(x + attn_output)
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ®‹å·®æ¥ç¶šä»˜ãï¼‰
            ff_output = self.ffn(x)
            x = self.norm2(x + ff_output)
            
            # å‡ºåŠ›
            return self.output(x)
        
        def generate(self, start_tokens, max_length=50):
            """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
            self.eval()
            generated = start_tokens.clone()
            
            with torch.no_grad():
                for _ in range(max_length):
                    # ç¾åœ¨ã®ç³»åˆ—ã§äºˆæ¸¬
                    outputs = self(generated)
                    
                    # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ã‚’å–å¾—
                    next_token_logits = outputs[:, -1, :]
                    
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, 1)
                    
                    # è¿½åŠ 
                    generated = torch.cat([generated, next_token], dim=1)
                    
                    # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿè£…ã«ã‚ˆã‚‹ï¼‰
                    
            return generated
    
    # ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    class SimpleTokenizer:
        def __init__(self, texts):
            self.word_to_id = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}
            self.id_to_word = {v: k for k, v in self.word_to_id.items()}
            
            # èªå½™æ§‹ç¯‰
            for text in texts:
                for word in text.lower().split():
                    if word not in self.word_to_id:
                        idx = len(self.word_to_id)
                        self.word_to_id[word] = idx
                        self.id_to_word[idx] = word
                        
        def encode(self, text):
            tokens = []
            for word in text.lower().split():
                tokens.append(self.word_to_id.get(word, 1))  # 1 = <unk>
            return tokens
        
        def decode(self, tokens):
            words = []
            for token in tokens:
                if token in self.id_to_word:
                    words.append(self.id_to_word[token])
            return ' '.join(words)
    
    # ä½¿ç”¨ä¾‹
    texts = [
        "the cat sat on the mat",
        "the dog played in the park",
        "cats and dogs are pets",
        "the sun shines bright",
        "birds fly in the sky"
    ]
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½œæˆ
    tokenizer = SimpleTokenizer(texts)
    print(f"èªå½™ã‚µã‚¤ã‚º: {len(tokenizer.word_to_id)}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = MiniLanguageModel(len(tokenizer.word_to_id))
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    encoded_texts = [torch.tensor(tokenizer.encode(text)) for text in texts]
    
    # ç°¡å˜ãªè¨“ç·´ãƒ«ãƒ¼ãƒ—
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(50):
        total_loss = 0
        
        for encoded in encoded_texts:
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
            x = encoded[:-1].unsqueeze(0)
            y = encoded[1:].unsqueeze(0)
            
            # äºˆæ¸¬
            outputs = model(x)
            loss = F.cross_entropy(outputs.reshape(-1, outputs.size(-1)), 
                                 y.reshape(-1))
            
            # æœ€é©åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {total_loss / len(texts):.4f}")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    start = torch.tensor([[tokenizer.word_to_id['the']]])
    generated = model.generate(start, max_length=10)
    print(f"\nç”Ÿæˆ: {tokenizer.decode(generated[0].tolist())}")
    ```

ã“ã‚Œã§ç¬¬2éƒ¨ã®æ¼”ç¿’ã¯å®Œäº†ã§ã™ï¼æ¬¡ã¯ç¬¬3éƒ¨ã§Transformerã®è©³ç´°ãªæ§‹æˆè¦ç´ ã‚’å­¦ã³ã¾ã—ã‚‡ã†ã€‚