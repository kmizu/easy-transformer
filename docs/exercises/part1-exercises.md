# ç¬¬1éƒ¨ æ¼”ç¿’å•é¡Œ

## æ¼”ç¿’ 1.1: Transformerã®é‡è¦æ€§

### å•é¡Œ 1
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®ã†ã¡ã€TransformerãŒç‰¹ã«å„ªã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿãã®ç†ç”±ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

1. ç”»åƒã®ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã§ã®åˆ†é¡
2. é•·æ–‡ã®è¦ç´„
3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®éŸ³å£°èªè­˜
4. æ•°å€¤è¨ˆç®—ã®æœ€é©åŒ–

??? è§£ç­”
    **ç­”ãˆ: 2. é•·æ–‡ã®è¦ç´„**
    
    ç†ç”±ï¼š
    - Transformerã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã«ã‚ˆã‚Šã€æ–‡æ›¸å…¨ä½“ã®æ–‡è„ˆã‚’åŠ¹ç‡çš„ã«æŠŠæ¡ã§ãã‚‹
    - é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã‚‹
    - ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚Šã€é•·æ–‡ã§ã‚‚é«˜é€Ÿã«å‡¦ç†å¯èƒ½
    
    ä»–ã®é¸æŠè‚¢ã«ã¤ã„ã¦ï¼š
    - 1: CNNã®æ–¹ãŒå±€æ‰€çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã«é©ã—ã¦ã„ã‚‹
    - 3: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’è€ƒãˆã‚‹ã¨RNNãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚‚æ¤œè¨ã•ã‚Œã‚‹
    - 4: æ•°å€¤è¨ˆç®—ã¯å¾“æ¥ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ–¹ãŒåŠ¹ç‡çš„

### å•é¡Œ 2
ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã®æœ€é©åŒ–ãƒ‘ã‚¹ã¨Transformerã®å±¤ã®é¡ä¼¼ç‚¹ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    1. **æ®µéšçš„ãªæŠ½è±¡åŒ–**
       - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ â†’ AST â†’ IR â†’ æ©Ÿæ¢°èª
       - Transformer: ãƒˆãƒ¼ã‚¯ãƒ³ â†’ åŸ‹ã‚è¾¼ã¿ â†’ æ–‡è„ˆè¡¨ç¾ â†’ å‡ºåŠ›
    
    2. **æƒ…å ±ã®ä¿æŒã¨å¤‰æ›**
       - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©: å„ãƒ‘ã‚¹ã§å¿…è¦ãªæƒ…å ±ã‚’ä¿æŒã—ã¤ã¤å¤‰æ›
       - Transformer: æ®‹å·®æ¥ç¶šã«ã‚ˆã‚Šå…ƒã®æƒ…å ±ã‚’ä¿æŒã—ã¤ã¤å¤‰æ›
    
    3. **ä¸¦åˆ—å‡¦ç†ã®å¯èƒ½æ€§**
       - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©: ç‹¬ç«‹ã—ãŸæœ€é©åŒ–ã¯ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½
       - Transformer: è‡ªå·±æ³¨æ„ã¯å…¨ä½ç½®ã§ä¸¦åˆ—è¨ˆç®—å¯èƒ½

## æ¼”ç¿’ 1.2: æ•°å­¦çš„åŸºç¤

### å•é¡Œ 3
ä»¥ä¸‹ã®è¡Œåˆ—ã®ç©ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ï¼š

```
A = [[1, 2],    B = [[5, 6],
     [3, 4]]         [7, 8]]
```

??? è§£ç­”
    ```python
    import numpy as np
    
    A = np.array([[1, 2], [3, 4]])
    B = np.array([[5, 6], [7, 8]])
    
    C = A @ B
    # C = [[1*5 + 2*7, 1*6 + 2*8],
    #      [3*5 + 4*7, 3*6 + 4*8]]
    #   = [[19, 22],
    #      [43, 50]]
    ```

### å•é¡Œ 4
ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’å®Ÿè£…ã—ã€ä»¥ä¸‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã«é©ç”¨ã—ã¦ãã ã•ã„ï¼š
`x = [2.0, 1.0, 0.1]`

??? è§£ç­”
    ```python
    import numpy as np
    
    def softmax(x):
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ã®ãŸã‚æœ€å¤§å€¤ã‚’å¼•ã
        x_shifted = x - np.max(x)
        exp_x = np.exp(x_shifted)
        return exp_x / np.sum(exp_x)
    
    x = np.array([2.0, 1.0, 0.1])
    result = softmax(x)
    print(result)
    # [0.6590, 0.2424, 0.0986]
    
    # ç¢ºèªï¼šåˆè¨ˆãŒ1ã«ãªã‚‹
    print(np.sum(result))  # 1.0
    ```

## æ¼”ç¿’ 1.3: PyTorchå®Ÿè·µ

### å•é¡Œ 5
PyTorchã§ç°¡å˜ãª2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
- å…¥åŠ›æ¬¡å…ƒ: 10
- éš ã‚Œå±¤: 20ãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆReLUæ´»æ€§åŒ–ï¼‰
- å‡ºåŠ›æ¬¡å…ƒ: 3

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    
    class SimpleNN(nn.Module):
        def __init__(self, input_dim=10, hidden_dim=20, output_dim=3):
            super(SimpleNN, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(hidden_dim, output_dim)
            
        def forward(self, x):
            x = self.fc1(x)
            x = self.relu(x)
            x = self.fc2(x)
            return x
    
    # ãƒ†ã‚¹ãƒˆ
    model = SimpleNN()
    x = torch.randn(32, 10)  # ãƒãƒƒãƒã‚µã‚¤ã‚º32
    output = model(x)
    print(output.shape)  # torch.Size([32, 3])
    ```

### å•é¡Œ 6
å‹¾é…é™ä¸‹æ³•ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ‰‹å‹•ã§å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

??? è§£ç­”
    ```python
    import torch
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å‹¾é…
    w = torch.tensor([1.0, 2.0], requires_grad=True)
    
    # ç°¡å˜ãªæå¤±é–¢æ•°: L = w[0]^2 + w[1]^2
    loss = w[0]**2 + w[1]**2
    
    # å‹¾é…è¨ˆç®—
    loss.backward()
    
    # æ‰‹å‹•ã§å‹¾é…é™ä¸‹
    learning_rate = 0.1
    with torch.no_grad():
        # w = w - lr * gradient
        w_new = w - learning_rate * w.grad
        
    print(f"å…ƒã®é‡ã¿: {w.data}")
    print(f"å‹¾é…: {w.grad}")
    print(f"æ›´æ–°å¾Œã®é‡ã¿: {w_new}")
    
    # æœŸå¾…ã•ã‚Œã‚‹çµæœ:
    # å‹¾é…: [2.0, 4.0]
    # æ›´æ–°: [1.0 - 0.1*2.0, 2.0 - 0.1*4.0] = [0.8, 1.6]
    ```

## æ¼”ç¿’ 1.4: ç·åˆå•é¡Œ

### å•é¡Œ 7
ç°¡å˜ãªæ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
å…¥åŠ›: "hello world"
ç›®æ¨™: å„æ–‡å­—ã‹ã‚‰æ¬¡ã®æ–‡å­—ã‚’äºˆæ¸¬

??? è§£ç­”
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    text = "hello world"
    chars = sorted(list(set(text)))
    char_to_idx = {ch: i for i, ch in enumerate(chars)}
    idx_to_char = {i: ch for i, ch in enumerate(chars)}
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    data = [char_to_idx[ch] for ch in text]
    x = torch.tensor(data[:-1])
    y = torch.tensor(data[1:])
    
    # ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«
    class CharModel(nn.Module):
        def __init__(self, vocab_size, hidden_size=16):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, hidden_size)
            self.fc = nn.Linear(hidden_size, vocab_size)
            
        def forward(self, x):
            x = self.embedding(x)
            x = self.fc(x)
            return x
    
    # è¨“ç·´
    model = CharModel(len(chars))
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    for epoch in range(100):
        # é †ä¼æ’­
        logits = model(x)
        loss = F.cross_entropy(logits, y)
        
        # é€†ä¼æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    with torch.no_grad():
        # "h"ã‹ã‚‰é–‹å§‹
        idx = char_to_idx['h']
        result = 'h'
        
        for _ in range(10):
            x_test = torch.tensor([idx])
            logits = model(x_test)
            probs = F.softmax(logits, dim=-1)
            idx = torch.argmax(probs, dim=-1).item()
            result += idx_to_char[idx]
            
        print(f"ç”Ÿæˆçµæœ: {result}")
    ```

## ãƒãƒ£ãƒ¬ãƒ³ã‚¸å•é¡Œ

### å•é¡Œ 8 ğŸŒŸ
ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã®å­—å¥è§£æå™¨ã®ã‚ˆã†ã«ã€ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®è¦å‰‡ã«å¾“ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ï¼š
- ç©ºç™½ã§å˜èªã‚’åˆ†å‰²
- å¥èª­ç‚¹ã¯ç‹¬ç«‹ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã†
- æ•°å­—ã¯1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ã¾ã¨ã‚ã‚‹

å…¥åŠ›ä¾‹: "Hello, world! 123 test."
æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: ["Hello", ",", "world", "!", "123", "test", "."]

??? è§£ç­”
    ```python
    import re
    
    class SimpleTokenizer:
        def __init__(self):
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            self.patterns = [
                (r'\d+', 'NUMBER'),           # æ•°å­—
                (r'[a-zA-Z]+', 'WORD'),       # å˜èª
                (r'[.,!?;:]', 'PUNCTUATION'), # å¥èª­ç‚¹
                (r'\s+', 'SPACE'),            # ç©ºç™½ï¼ˆã‚¹ã‚­ãƒƒãƒ—ç”¨ï¼‰
            ]
            self.regex = '|'.join(f'({pattern})' for pattern, _ in self.patterns)
            
        def tokenize(self, text):
            tokens = []
            
            for match in re.finditer(self.regex, text):
                token = match.group()
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®š
                for i, (pattern, token_type) in enumerate(self.patterns):
                    if match.group(i + 1):  # ã‚°ãƒ«ãƒ¼ãƒ—ãŒãƒãƒƒãƒã—ãŸ
                        if token_type != 'SPACE':  # ç©ºç™½ã¯ã‚¹ã‚­ãƒƒãƒ—
                            tokens.append(token)
                        break
                        
            return tokens
    
    # ãƒ†ã‚¹ãƒˆ
    tokenizer = SimpleTokenizer()
    text = "Hello, world! 123 test."
    tokens = tokenizer.tokenize(text)
    print(tokens)
    # ['Hello', ',', 'world', '!', '123', 'test', '.']
    
    # ã‚ˆã‚Šè¤‡é›‘ãªä¾‹
    text2 = "The price is $99.99, isn't it?"
    tokens2 = tokenizer.tokenize(text2)
    print(tokens2)
    # ['The', 'price', 'is', '99', '.', '99', ',', 'isn', 't', 'it', '?']
    ```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã“ã‚Œã‚‰ã®æ¼”ç¿’ã‚’å®Œäº†ã—ãŸã‚‰ã€ç¬¬2éƒ¨ã«é€²ã‚“ã§Transformerã®æ ¸å¿ƒçš„ãªä»•çµ„ã¿ã‚’å­¦ã³ã¾ã—ã‚‡ã†ï¼

ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: è§£ç­”ã‚’è¦‹ã‚‹å‰ã«ã€ã¾ãšè‡ªåˆ†ã§å®Ÿè£…ã—ã¦ã¿ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ã€ãã‚ŒãŒå­¦ç¿’ã®ä¸€éƒ¨ã§ã™ï¼