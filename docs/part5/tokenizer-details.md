# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©³ç´°

## ã¯ã˜ã‚ã«ï¼šè¨€èªã®åŸå­

ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã®å­—å¥è§£æå™¨ï¼ˆãƒ¬ã‚­ã‚µãƒ¼ï¼‰ã‚’ä½œã£ãŸã“ã¨ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’æ„å‘³ã®ã‚ã‚‹æœ€å°å˜ä½ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ†å‰²ã™ã‚‹ä½œæ¥­ã§ã™ã€‚`if (x > 10) { return true; }` ã‚’ `IF`, `LPAREN`, `IDENT(x)`, `GT`, `NUMBER(10)`, ... ã«åˆ†è§£ã—ã¾ã™ã€‚

è‡ªç„¶è¨€èªã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚åŒã˜å½¹å‰²ã‚’æœãŸã—ã¾ã™ãŒã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã¨é•ã£ã¦æ˜ç¢ºãªè¦å‰‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å˜èªï¼Ÿæ–‡å­—ï¼Ÿãã‚Œã¨ã‚‚ä½•ã‹åˆ¥ã®å˜ä½ï¼Ÿã“ã®ç« ã§ã¯ã€ç¾ä»£ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒã©ã®ã‚ˆã†ã«è¨€èªã‚’ã€ŒåŸå­ã€ã«åˆ†è§£ã—ã€ãªãœãã‚ŒãŒé‡è¦ãªã®ã‹ã‚’æ¢ã‚Šã¾ã™ã€‚

## 19.1 ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®åŸºç¤

### ãªãœãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒé‡è¦ã‹

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter, defaultdict
import regex as re  # Unicodeã‚µãƒãƒ¼ãƒˆãŒå„ªã‚Œã¦ã„ã‚‹
import json
from dataclasses import dataclass
import heapq
from tqdm import tqdm

class TokenizationBasics:
    """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®åŸºç¤æ¦‚å¿µ"""
    
    def explain_tokenization_challenges(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®èª²é¡Œã‚’èª¬æ˜"""
        print("=== ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®èª²é¡Œ ===\n")
        
        # æ§˜ã€…ãªè¨€èªã§ã®ä¾‹
        examples = {
            "è‹±èª": {
                "text": "I don't think it's working.",
                "word_tokens": ["I", "don't", "think", "it's", "working", "."],
                "char_tokens": list("I don't think it's working."),
                "challenges": "ç¸®ç´„å½¢ï¼ˆdon't, it'sï¼‰ã®æ‰±ã„"
            },
            
            "æ—¥æœ¬èª": {
                "text": "ç§ã¯çŒ«ãŒå¥½ãã§ã™ã€‚",
                "word_tokens": ["ç§", "ã¯", "çŒ«", "ãŒ", "å¥½ã", "ã§ã™", "ã€‚"],
                "char_tokens": list("ç§ã¯çŒ«ãŒå¥½ãã§ã™ã€‚"),
                "challenges": "å˜èªå¢ƒç•ŒãŒä¸æ˜ç¢º"
            },
            
            "ãƒ‰ã‚¤ãƒ„èª": {
                "text": "Donaudampfschifffahrtsgesellschaft",
                "word_tokens": ["Donaudampfschifffahrtsgesellschaft"],
                "char_tokens": list("Donaudampfschifffahrtsgesellschaft"),
                "challenges": "è¤‡åˆèªãŒéå¸¸ã«é•·ã„"
            },
            
            "ä¸­å›½èª": {
                "text": "æˆ‘å–œæ¬¢åƒè‹¹æœ",
                "word_tokens": ["æˆ‘", "å–œæ¬¢", "åƒ", "è‹¹æœ"],
                "char_tokens": list("æˆ‘å–œæ¬¢åƒè‹¹æœ"),
                "challenges": "ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„"
            }
        }
        
        for lang, info in examples.items():
            print(f"{lang}:")
            print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {info['text']}")
            print(f"  å˜èªãƒˆãƒ¼ã‚¯ãƒ³: {info['word_tokens'][:5]}...")
            print(f"  æ–‡å­—ãƒˆãƒ¼ã‚¯ãƒ³: {info['char_tokens'][:10]}...")
            print(f"  èª²é¡Œ: {info['challenges']}\n")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ
        self._compare_tokenization_methods()
    
    def _compare_tokenization_methods(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ"""
        print("=== ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ ===\n")
        
        methods = {
            "Word-level": {
                "vocab_size": "50,000-200,000",
                "OOV_handling": "Poor",
                "morphology": "No",
                "efficiency": "Low"
            },
            "Character-level": {
                "vocab_size": "100-1,000",
                "OOV_handling": "Perfect",
                "morphology": "Implicit",
                "efficiency": "Very Low"
            },
            "Subword (BPE/WordPiece)": {
                "vocab_size": "10,000-100,000",
                "OOV_handling": "Good",
                "morphology": "Partial",
                "efficiency": "High"
            },
            "SentencePiece": {
                "vocab_size": "10,000-100,000",
                "OOV_handling": "Good",
                "morphology": "Partial",
                "efficiency": "High"
            }
        }
        
        # è¡¨å½¢å¼ã§è¡¨ç¤º
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.axis('tight')
        ax.axis('off')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = ["Method", "Vocab Size", "OOV Handling", "Morphology", "Efficiency"]
        cell_data = []
        
        for method, props in methods.items():
            row = [method] + list(props.values())
            cell_data.append(row)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        table = ax.table(cellText=cell_data, colLabels=headers,
                        cellLoc='center', loc='center',
                        colWidths=[0.25, 0.2, 0.15, 0.15, 0.15])
        
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)
        
        # ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
        for i in range(len(headers)):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        # è‰²åˆ†ã‘
        colors = ['#ffebee', '#e8f5e9', '#fff3e0', '#e3f2fd']
        for i, color in enumerate(colors):
            for j in range(len(headers)):
                table[(i+1, j)].set_facecolor(color)
        
        plt.title('Comparison of Tokenization Methods', fontsize=14, weight='bold', pad=20)
        plt.show()

## 19.2 Byte Pair Encoding (BPE)

class BytePairEncoding:
    """BPEãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…"""
    
    def __init__(self):
        self.vocab = {}
        self.merges = []
        
    def train(self, texts: List[str], vocab_size: int = 1000):
        """BPEã®å­¦ç¿’"""
        print("=== BPEå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ ===\n")
        
        # åˆæœŸèªå½™ï¼ˆæ–‡å­—ãƒ¬ãƒ™ãƒ«ï¼‰
        word_freqs = defaultdict(int)
        for text in texts:
            words = text.split()
            for word in words:
                word_freqs[' '.join(list(word) + ['</w>'])] += 1
        
        # åˆæœŸèªå½™ã‚’ä½œæˆ
        self.vocab = {}
        for word, freq in word_freqs.items():
            for char in word.split():
                if char not in self.vocab:
                    self.vocab[char] = len(self.vocab)
        
        print(f"åˆæœŸèªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        print(f"åˆæœŸèªå½™ä¾‹: {list(self.vocab.keys())[:10]}\n")
        
        # ãƒãƒ¼ã‚¸æ“ä½œ
        num_merges = vocab_size - len(self.vocab)
        
        for i in tqdm(range(num_merges), desc="Learning merges"):
            # ãƒšã‚¢ã®é »åº¦ã‚’è¨ˆç®—
            pair_freqs = self._get_pair_frequencies(word_freqs)
            
            if not pair_freqs:
                break
            
            # æœ€é »å‡ºãƒšã‚¢ã‚’é¸æŠ
            best_pair = max(pair_freqs, key=pair_freqs.get)
            self.merges.append(best_pair)
            
            # èªå½™ã‚’æ›´æ–°
            new_token = ''.join(best_pair)
            self.vocab[new_token] = len(self.vocab)
            
            # ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ›´æ–°
            word_freqs = self._merge_pair(word_freqs, best_pair)
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 100 == 0:
                print(f"ãƒãƒ¼ã‚¸ {i+1}: {best_pair} â†’ {new_token}")
        
        print(f"\næœ€çµ‚èªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        
        # å­¦ç¿’çµæœã®å¯è¦–åŒ–
        self._visualize_vocabulary()
    
    def _get_pair_frequencies(self, word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], int]:
        """ãƒšã‚¢ã®é »åº¦ã‚’è¨ˆç®—"""
        pair_freqs = defaultdict(int)
        
        for word, freq in word_freqs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pair_freqs[(symbols[i], symbols[i + 1])] += freq
        
        return pair_freqs
    
    def _merge_pair(self, word_freqs: Dict[str, int], 
                    pair: Tuple[str, str]) -> Dict[str, int]:
        """ãƒšã‚¢ã‚’ãƒãƒ¼ã‚¸"""
        new_word_freqs = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word, freq in word_freqs.items():
            new_word = word.replace(bigram, replacement)
            new_word_freqs[new_word] = freq
        
        return new_word_freqs
    
    def encode(self, text: str) -> List[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        words = text.split()
        tokens = []
        
        for word in words:
            # å˜èªã‚’æ–‡å­—ã«åˆ†å‰²
            word_tokens = list(word) + ['</w>']
            
            # ãƒãƒ¼ã‚¸ã‚’é©ç”¨
            for merge in self.merges:
                i = 0
                while i < len(word_tokens) - 1:
                    if (word_tokens[i], word_tokens[i + 1]) == merge:
                        word_tokens = word_tokens[:i] + [''.join(merge)] + word_tokens[i + 2:]
                    else:
                        i += 1
            
            # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
            for token in word_tokens:
                if token in self.vocab:
                    tokens.append(self.vocab[token])
                else:
                    # Unknown token
                    tokens.append(self.vocab.get('<unk>', 0))
        
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        # é€†å¼•ãè¾æ›¸
        id_to_token = {v: k for k, v in self.vocab.items()}
        
        tokens = [id_to_token.get(id, '<unk>') for id in token_ids]
        text = ' '.join(tokens).replace('</w> ', ' ').replace('</w>', '')
        
        return text
    
    def _visualize_vocabulary(self):
        """èªå½™ã®å¯è¦–åŒ–"""
        # ãƒˆãƒ¼ã‚¯ãƒ³é•·ã®åˆ†å¸ƒ
        token_lengths = [len(token.replace('</w>', '')) for token in self.vocab.keys()]
        
        plt.figure(figsize=(10, 6))
        plt.hist(token_lengths, bins=range(1, max(token_lengths) + 2), 
                alpha=0.7, edgecolor='black')
        plt.xlabel('Token Length (characters)')
        plt.ylabel('Count')
        plt.title('Distribution of Token Lengths in BPE Vocabulary')
        plt.grid(True, alpha=0.3)
        
        # çµ±è¨ˆæƒ…å ±
        avg_length = np.mean(token_lengths)
        plt.axvline(avg_length, color='red', linestyle='--', 
                   label=f'Average: {avg_length:.2f}')
        plt.legend()
        
        plt.tight_layout()
        plt.show()

class BPEDemo:
    """BPEã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def demonstrate_bpe_process(self):
        """BPEãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ‡ãƒ¢"""
        print("=== BPEãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ– ===\n")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        corpus = [
            "the cat sat on the mat",
            "the dog sat on the log",
            "cats and dogs are pets"
        ]
        
        # åˆæœŸçŠ¶æ…‹
        words = []
        for text in corpus:
            words.extend(text.split())
        
        # å˜èªã‚’æ–‡å­—ã«åˆ†å‰²
        word_splits = {}
        for word in set(words):
            word_splits[word] = list(word) + ['</w>']
        
        print("åˆæœŸçŠ¶æ…‹ï¼ˆæ–‡å­—åˆ†å‰²ï¼‰:")
        for word, splits in list(word_splits.items())[:5]:
            print(f"  {word}: {' '.join(splits)}")
        
        # ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        merges = [
            ('t', 'h'),      # th
            ('th', 'e'),     # the
            ('a', 't'),      # at
            ('s', 'at'),     # sat
            ('o', 'n'),      # on
        ]
        
        print("\n\nãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹:")
        current_splits = word_splits.copy()
        
        for i, (a, b) in enumerate(merges):
            print(f"\nã‚¹ãƒ†ãƒƒãƒ— {i+1}: '{a}' + '{b}' â†’ '{a+b}'")
            
            # ãƒãƒ¼ã‚¸ã‚’é©ç”¨
            for word, splits in current_splits.items():
                new_splits = []
                j = 0
                while j < len(splits):
                    if j < len(splits) - 1 and splits[j] == a and splits[j+1] == b:
                        new_splits.append(a + b)
                        j += 2
                    else:
                        new_splits.append(splits[j])
                        j += 1
                current_splits[word] = new_splits
            
            # å¤‰æ›´ã•ã‚ŒãŸå˜èªã‚’è¡¨ç¤º
            for word, splits in list(current_splits.items())[:3]:
                print(f"    {word}: {' '.join(splits)}")
        
        # æœ€çµ‚çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        self._visualize_tokenization_result(current_splits)
    
    def _visualize_tokenization_result(self, word_splits: Dict[str, List[str]]):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–çµæœã®å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # ã‚µãƒ³ãƒ—ãƒ«æ–‡
        sentence = "the cat sat on the mat"
        words = sentence.split()
        
        y_pos = 0.5
        x_pos = 0
        
        colors = plt.cm.Set3(np.linspace(0, 1, 20))
        color_idx = 0
        
        for word in words:
            tokens = word_splits.get(word, list(word) + ['</w>'])
            
            for token in tokens:
                if token == '</w>':
                    # å˜èªå¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼
                    width = 0.3
                    rect = plt.Rectangle((x_pos, y_pos), width, 0.3,
                                       facecolor='lightgray', 
                                       edgecolor='black', linewidth=1)
                    ax.add_patch(rect)
                    ax.text(x_pos + width/2, y_pos + 0.15, '</w>',
                           ha='center', va='center', fontsize=8)
                else:
                    # é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³
                    width = len(token) * 0.15
                    rect = plt.Rectangle((x_pos, y_pos), width, 0.3,
                                       facecolor=colors[color_idx % len(colors)],
                                       edgecolor='black', linewidth=1)
                    ax.add_patch(rect)
                    ax.text(x_pos + width/2, y_pos + 0.15, token,
                           ha='center', va='center', fontsize=10)
                    color_idx += 1
                
                x_pos += width + 0.05
            
            x_pos += 0.2  # å˜èªé–“ã®ã‚¹ãƒšãƒ¼ã‚¹
        
        ax.set_xlim(-0.1, x_pos)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('BPE Tokenization Result', fontsize=14, weight='bold')
        
        # å…ƒã®æ–‡ã‚’è¡¨ç¤º
        ax.text(x_pos/2, 0.9, f'Original: "{sentence}"', 
               ha='center', va='center', fontsize=12, style='italic')
        
        plt.tight_layout()
        plt.show()

## 19.3 WordPieceã¨SentencePiece

class WordPieceTokenizer:
    """WordPieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…"""
    
    def __init__(self):
        self.vocab = {}
        self.unk_token = '[UNK]'
        self.max_input_chars_per_word = 100
        
    def train(self, texts: List[str], vocab_size: int = 1000):
        """WordPieceã®å­¦ç¿’ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        print("=== WordPieceå­¦ç¿’ ===\n")
        
        # åˆæœŸèªå½™ã®æ§‹ç¯‰
        char_counts = Counter()
        word_counts = Counter()
        
        for text in texts:
            words = text.lower().split()
            for word in words:
                word_counts[word] += 1
                for char in word:
                    char_counts[char] += 1
        
        # åŸºæœ¬èªå½™
        self.vocab = {
            '[PAD]': 0,
            '[UNK]': 1,
            '[CLS]': 2,
            '[SEP]': 3,
            '[MASK]': 4
        }
        
        # æ–‡å­—ã‚’è¿½åŠ 
        for char, count in char_counts.most_common():
            if len(self.vocab) < 100:  # æœ€åˆã®100ã¯æ–‡å­—ç”¨
                self.vocab[char] = len(self.vocab)
        
        # WordPieceã®è¿½åŠ 
        print(f"åˆæœŸèªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        
        # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å€™è£œã®ç”Ÿæˆã¨è©•ä¾¡
        while len(self.vocab) < vocab_size:
            # å€™è£œã‚’ç”Ÿæˆ
            candidates = self._generate_candidates(word_counts)
            
            if not candidates:
                break
            
            # ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            best_candidate = max(candidates, 
                               key=lambda x: self._score_candidate(x, word_counts))
            
            # èªå½™ã«è¿½åŠ 
            if best_candidate.startswith('##'):
                self.vocab[best_candidate] = len(self.vocab)
            else:
                self.vocab['##' + best_candidate] = len(self.vocab)
            
            # é€²æ—
            if len(self.vocab) % 100 == 0:
                print(f"èªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        
        print(f"\næœ€çµ‚èªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
        
        # WordPieceã®ç‰¹å¾´ã‚’å¯è¦–åŒ–
        self._visualize_wordpiece_features()
    
    def _generate_candidates(self, word_counts: Counter) -> List[str]:
        """ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å€™è£œã‚’ç”Ÿæˆ"""
        candidates = set()
        
        for word in word_counts:
            for i in range(len(word)):
                for j in range(i + 1, min(i + 10, len(word) + 1)):
                    subword = word[i:j]
                    if len(subword) > 1:
                        candidates.add(subword)
        
        return list(candidates)
    
    def _score_candidate(self, candidate: str, word_counts: Counter) -> float:
        """å€™è£œã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0
        for word, count in word_counts.items():
            if candidate in word:
                score += count
        return score
    
    def tokenize(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        output_tokens = []
        
        for word in text.lower().split():
            if len(word) > self.max_input_chars_per_word:
                output_tokens.append(self.unk_token)
                continue
            
            is_bad = False
            sub_tokens = []
            start = 0
            
            while start < len(word):
                end = len(word)
                cur_substr = None
                
                while start < end:
                    substr = word[start:end]
                    if start > 0:
                        substr = '##' + substr
                    
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    
                    end -= 1
                
                if cur_substr is None:
                    is_bad = True
                    break
                
                sub_tokens.append(cur_substr)
                start = end
            
            if is_bad:
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        
        return output_tokens
    
    def _visualize_wordpiece_features(self):
        """WordPieceã®ç‰¹å¾´ã‚’å¯è¦–åŒ–"""
        # ##ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®çµ±è¨ˆ
        prefix_tokens = [token for token in self.vocab.keys() 
                        if token.startswith('##')]
        
        print(f"\n##ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ããƒˆãƒ¼ã‚¯ãƒ³: {len(prefix_tokens)}")
        print(f"ä¾‹: {prefix_tokens[:10]}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¾‹
        examples = [
            "playing",
            "unbelievable",
            "internationalization"
        ]
        
        print("\n\nãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®ä¾‹:")
        for word in examples:
            tokens = self.tokenize(word)
            print(f"  {word} â†’ {tokens}")

class SentencePieceDemo:
    """SentencePieceã®ãƒ‡ãƒ¢"""
    
    def explain_sentencepiece(self):
        """SentencePieceã®èª¬æ˜"""
        print("=== SentencePiece ===\n")
        
        print("ç‰¹å¾´:")
        print("1. è¨€èªç‹¬ç«‹:")
        print("   - å‰å‡¦ç†ä¸è¦ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãªã—ï¼‰")
        print("   - ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥å­¦ç¿’")
        print("   - ã‚¹ãƒšãƒ¼ã‚¹ã‚‚é€šå¸¸ã®æ–‡å­—ã¨ã—ã¦æ‰±ã†\n")
        
        print("2. å¯é€†çš„:")
        print("   - ãƒ‡ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å®Œå…¨å¾©å…ƒ")
        print("   - æƒ…å ±ã®æå¤±ãªã—\n")
        
        print("3. ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®æ­£è¦åŒ–:")
        print("   - ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°")
        print("   - è¤‡æ•°ã®åˆ†å‰²å€™è£œã‹ã‚‰é¸æŠ\n")
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ
        self._compare_subword_algorithms()
    
    def _compare_subword_algorithms(self):
        """ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 6))
        
        # BPE
        ax = axes[0]
        ax.set_title('BPE', fontsize=12, weight='bold')
        
        # BPEã®ãƒãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ã‚¹
        bpe_steps = [
            "c a t s </w>",
            "ca t s </w>",
            "ca ts </w>",
            "cats </w>"
        ]
        
        for i, step in enumerate(bpe_steps):
            y = 0.8 - i * 0.2
            ax.text(0.5, y, step, ha='center', va='center',
                   fontsize=10, family='monospace',
                   bbox=dict(boxstyle="round,pad=0.3", 
                           facecolor='lightblue', alpha=0.7))
            
            if i < len(bpe_steps) - 1:
                ax.arrow(0.5, y - 0.05, 0, -0.1, 
                        head_width=0.05, head_length=0.02,
                        fc='black', ec='black')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.text(0.5, 0.95, 'Bottom-up Merging', ha='center', 
               fontsize=10, style='italic')
        
        # WordPiece
        ax = axes[1]
        ax.set_title('WordPiece', fontsize=12, weight='bold')
        
        # WordPieceã®åˆ†å‰²
        word = "playing"
        tokens = ["play", "##ing"]
        
        # å˜èªå…¨ä½“
        rect = plt.Rectangle((0.2, 0.5), 0.6, 0.2,
                           facecolor='lightgreen', edgecolor='black')
        ax.add_patch(rect)
        ax.text(0.5, 0.6, word, ha='center', va='center', fontsize=12)
        
        # åˆ†å‰²å¾Œ
        x_pos = 0.2
        for token in tokens:
            width = 0.25
            rect = plt.Rectangle((x_pos, 0.2), width, 0.15,
                               facecolor='lightyellow', edgecolor='black')
            ax.add_patch(rect)
            ax.text(x_pos + width/2, 0.275, token, 
                   ha='center', va='center', fontsize=10)
            x_pos += width + 0.1
        
        # çŸ¢å°
        ax.arrow(0.5, 0.48, 0, -0.1, head_width=0.05, head_length=0.02,
                fc='black', ec='black')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.text(0.5, 0.95, 'Likelihood Maximization', ha='center',
               fontsize=10, style='italic')
        
        # SentencePiece
        ax = axes[2]
        ax.set_title('SentencePiece', fontsize=12, weight='bold')
        
        # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
        text = "â–theâ–cat"
        ax.text(0.5, 0.7, text, ha='center', va='center',
               fontsize=12, family='monospace',
               bbox=dict(boxstyle="round,pad=0.3", 
                       facecolor='lightcoral', alpha=0.7))
        
        # è¤‡æ•°ã®åˆ†å‰²å€™è£œ
        candidates = [
            ["â–the", "â–cat"],
            ["â–th", "e", "â–cat"],
            ["â–the", "â–c", "at"]
        ]
        
        y_start = 0.4
        for i, cand in enumerate(candidates):
            y = y_start - i * 0.15
            text = ' '.join(cand)
            ax.text(0.5, y, text, ha='center', va='center',
                   fontsize=9, family='monospace',
                   bbox=dict(boxstyle="round,pad=0.2",
                           facecolor='lightyellow', alpha=0.5))
        
        ax.text(0.8, 0.25, 'Sample', ha='center', fontsize=8,
               style='italic', color='red')
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.text(0.5, 0.95, 'Unigram LM / BPE', ha='center',
               fontsize=10, style='italic')
        
        plt.tight_layout()
        plt.show()

## 19.4 ç¾ä»£çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

class ModernTokenizers:
    """ç¾ä»£çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…ã¨æ¯”è¼ƒ"""
    
    def compare_modern_tokenizers(self):
        """ç¾ä»£çš„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ¯”è¼ƒ"""
        print("=== ç¾ä»£çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ===\n")
        
        tokenizers = {
            "GPT-2/GPT-3": {
                "type": "BPE",
                "vocab_size": "50,257",
                "special": "Byte-level BPE",
                "features": "ã‚¹ãƒšãƒ¼ã‚¹å‡¦ç†ã®æ”¹å–„"
            },
            
            "BERT": {
                "type": "WordPiece",
                "vocab_size": "30,522",
                "special": "##ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹",
                "features": "äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å¿…è¦"
            },
            
            "T5/mT5": {
                "type": "SentencePiece",
                "vocab_size": "32,000",
                "special": "â–(ã‚¹ãƒšãƒ¼ã‚¹ãƒãƒ¼ã‚«ãƒ¼)",
                "features": "è¨€èªç‹¬ç«‹"
            },
            
            "LLaMA": {
                "type": "SentencePiece (BPE)",
                "vocab_size": "32,000",
                "special": "Byte fallback",
                "features": "æœªçŸ¥æ–‡å­—ã®å‡¦ç†"
            },
            
            "ChatGPT": {
                "type": "cl100k_base (tiktoken)",
                "vocab_size": "100,277",
                "special": "æ”¹è‰¯ã•ã‚ŒãŸBPE",
                "features": "åŠ¹ç‡çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
            }
        }
        
        # æ¯”è¼ƒè¡¨ç¤º
        self._visualize_tokenizer_comparison(tokenizers)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åŠ¹ç‡ã®æ¯”è¼ƒ
        self._compare_encoding_efficiency()
    
    def _visualize_tokenizer_comparison(self, tokenizers: Dict[str, Dict[str, str]]):
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ¯”è¼ƒã‚’å¯è¦–åŒ–"""
        # èªå½™ã‚µã‚¤ã‚ºã®æ¯”è¼ƒ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # èªå½™ã‚µã‚¤ã‚º
        names = list(tokenizers.keys())
        vocab_sizes = []
        for name, info in tokenizers.items():
            size_str = info["vocab_size"].replace(",", "")
            vocab_sizes.append(int(size_str))
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(names)))
        bars = ax1.bar(names, vocab_sizes, color=colors)
        
        # å€¤ã‚’è¡¨ç¤º
        for bar, size in zip(bars, vocab_sizes):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{size:,}', ha='center', va='bottom')
        
        ax1.set_ylabel('Vocabulary Size')
        ax1.set_title('Vocabulary Sizes of Modern Tokenizers')
        ax1.tick_params(axis='x', rotation=45)
        
        # ã‚¿ã‚¤ãƒ—åˆ¥åˆ†é¡
        type_counts = Counter(info["type"].split()[0] for info in tokenizers.values())
        
        ax2.pie(type_counts.values(), labels=type_counts.keys(),
               autopct='%1.1f%%', startangle=90)
        ax2.set_title('Distribution of Tokenizer Types')
        
        plt.tight_layout()
        plt.show()
    
    def _compare_encoding_efficiency(self):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åŠ¹ç‡ã®æ¯”è¼ƒ"""
        print("\n=== ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åŠ¹ç‡ã®æ¯”è¼ƒ ===")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        samples = {
            "English": "The quick brown fox jumps over the lazy dog.",
            "Code": "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
            "Mixed": "Helloä¸–ç•Œ! ğŸŒ This is a test â†’ Î»x.x+1",
            "URL": "https://github.com/openai/gpt-3/blob/main/model.py"
        }
        
        # ä»®æƒ³çš„ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå®Ÿéš›ã®æ¯”ç‡ã«åŸºã¥ãï¼‰
        tokenizer_efficiency = {
            "GPT-2": {"English": 11, "Code": 24, "Mixed": 18, "URL": 35},
            "BERT": {"English": 12, "Code": 28, "Mixed": 22, "URL": 40},
            "T5": {"English": 10, "Code": 25, "Mixed": 16, "URL": 38},
            "ChatGPT": {"English": 9, "Code": 20, "Mixed": 14, "URL": 25}
        }
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤º
        fig, ax = plt.subplots(figsize=(10, 6))
        
        tokenizers = list(tokenizer_efficiency.keys())
        text_types = list(samples.keys())
        
        efficiency_matrix = np.array([
            [tokenizer_efficiency[tok][txt] for txt in text_types]
            for tok in tokenizers
        ])
        
        im = ax.imshow(efficiency_matrix, cmap='RdYlGn_r', aspect='auto')
        
        # ãƒ©ãƒ™ãƒ«
        ax.set_xticks(np.arange(len(text_types)))
        ax.set_yticks(np.arange(len(tokenizers)))
        ax.set_xticklabels(text_types)
        ax.set_yticklabels(tokenizers)
        
        # å€¤ã‚’è¡¨ç¤º
        for i in range(len(tokenizers)):
            for j in range(len(text_types)):
                text = ax.text(j, i, efficiency_matrix[i, j],
                             ha="center", va="center", color="black")
        
        ax.set_title('Token Count Comparison (Lower is Better)')
        plt.colorbar(im, ax=ax, label='Number of Tokens')
        
        plt.tight_layout()
        plt.show()

class TokenizerImplementationTips:
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Ÿè£…ã®ãƒ’ãƒ³ãƒˆ"""
    
    def share_best_practices(self):
        """ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®å…±æœ‰"""
        print("=== ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ ===\n")
        
        practices = {
            "1. å‰å‡¦ç†": [
                "Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰",
                "ç©ºç™½æ–‡å­—ã®çµ±ä¸€",
                "ç‰¹æ®Šæ–‡å­—ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—",
                "å¤§æ–‡å­—å°æ–‡å­—ã®æ‰±ã„ã‚’æ±ºå®š"
            ],
            
            "2. ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³": [
                "[PAD], [UNK], [CLS], [SEP]ã®è¿½åŠ ",
                "ã‚¿ã‚¹ã‚¯å›ºæœ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­è¨ˆ",
                "äºˆç´„é ˜åŸŸã®ç¢ºä¿",
                "ãƒˆãƒ¼ã‚¯ãƒ³IDã®å›ºå®šåŒ–"
            ],
            
            "3. åŠ¹ç‡åŒ–": [
                "ãƒˆãƒ©ã‚¤æœ¨ã§ã®é«˜é€Ÿæ¤œç´¢",
                "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨",
                "ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…",
                "ä¸¦åˆ—åŒ–å¯èƒ½ãªè¨­è¨ˆ"
            ],
            
            "4. å …ç‰¢æ€§": [
                "æœªçŸ¥æ–‡å­—ã®é©åˆ‡ãªå‡¦ç†",
                "æœ€å¤§é•·ã®åˆ¶é™",
                "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°",
                "ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›"
            ]
        }
        
        for category, items in practices.items():
            print(f"{category}:")
            for item in items:
                print(f"  â€¢ {item}")
            print()
        
        # å®Ÿè£…ä¾‹
        self._show_implementation_example()
    
    def _show_implementation_example(self):
        """å®Ÿè£…ä¾‹ã‚’è¡¨ç¤º"""
        print("\n=== åŠ¹ç‡çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…ä¾‹ ===\n")
        
        code = '''
class EfficientTokenizer:
    """åŠ¹ç‡çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…"""
    
    def __init__(self, vocab: Dict[str, int]):
        self.vocab = vocab
        self.trie = self._build_trie(vocab)
        self.cache = {}
        
    def _build_trie(self, vocab: Dict[str, int]) -> Dict:
        """ãƒˆãƒ©ã‚¤æœ¨ã®æ§‹ç¯‰"""
        trie = {}
        for token, token_id in vocab.items():
            node = trie
            for char in token:
                if char not in node:
                    node[char] = {}
                node = node[char]
            node['<END>'] = token_id
        return trie
    
    def encode(self, text: str) -> List[int]:
        """é«˜é€Ÿã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if text in self.cache:
            return self.cache[text]
        
        tokens = []
        i = 0
        
        while i < len(text):
            # æœ€é•·ä¸€è‡´
            node = self.trie
            longest_token_id = None
            longest_end = i
            
            for j in range(i, len(text)):
                if text[j] not in node:
                    break
                node = node[text[j]]
                if '<END>' in node:
                    longest_token_id = node['<END>']
                    longest_end = j + 1
            
            if longest_token_id is not None:
                tokens.append(longest_token_id)
                i = longest_end
            else:
                # Unknown token
                tokens.append(self.vocab.get('<UNK>', 0))
                i += 1
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache[text] = tokens
        return tokens
'''
        
        print(code)

# å®Ÿè¡Œã¨ãƒ‡ãƒ¢
def run_tokenizer_demo():
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
    print("=" * 70)
    print("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©³ç´°")
    print("=" * 70 + "\n")
    
    # 1. åŸºç¤æ¦‚å¿µ
    basics = TokenizationBasics()
    basics.explain_tokenization_challenges()
    
    # 2. BPE
    print("\n")
    bpe = BytePairEncoding()
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹ã§BPEã‚’å­¦ç¿’
    sample_corpus = [
        "the cat sat on the mat",
        "the dog sat on the log", 
        "cats and dogs are pets",
        "the quick brown fox jumps"
    ]
    
    bpe.train(sample_corpus, vocab_size=50)
    
    # BPEã®ãƒ‡ãƒ¢
    print("\n")
    bpe_demo = BPEDemo()
    bpe_demo.demonstrate_bpe_process()
    
    # 3. WordPiece
    print("\n")
    wp = WordPieceTokenizer()
    wp.train(sample_corpus, vocab_size=100)
    
    # 4. SentencePiece
    print("\n")
    sp_demo = SentencePieceDemo()
    sp_demo.explain_sentencepiece()
    
    # 5. ç¾ä»£çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    print("\n")
    modern = ModernTokenizers()
    modern.compare_modern_tokenizers()
    
    # 6. å®Ÿè£…ã®ãƒ’ãƒ³ãƒˆ
    print("\n")
    tips = TokenizerImplementationTips()
    tips.share_best_practices()
    
    print("\n" + "=" * 70)
    print("ã¾ã¨ã‚")
    print("=" * 70)
    print("\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¦ç‚¹:")
    print("â€¢ è¨€èªã®å¤šæ§˜æ€§ã«å¯¾å¿œã™ã‚‹æŸ”è»Ÿæ€§")
    print("â€¢ è¨ˆç®—åŠ¹ç‡ã¨è¡¨ç¾åŠ›ã®ãƒãƒ©ãƒ³ã‚¹")
    print("â€¢ ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã«ã‚ˆã‚‹æœªçŸ¥èªå¯¾å¿œ")
    print("â€¢ ã‚¿ã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼é¸æŠ")
    print("\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã€Œç›®ã€ã§ã‚ã‚Šã€")
    print("ãã®è¨­è¨ˆãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    run_tokenizer_demo()