# å˜èªã®æ•°å€¤è¡¨ç¾

## ã¯ã˜ã‚ã«ï¼šè¨€èªã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹æŒ‘æˆ¦

ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã‚’ä½œã‚‹éš›ã€æœ€åˆã«è¡Œã†ã®ã¯å­—å¥è§£æï¼ˆãƒ¬ã‚­ã‚·ãƒ³ã‚°ï¼‰ã§ã™ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¨ã„ã†æ–‡å­—åˆ—ã‚’ã€æ„å‘³ã®ã‚ã‚‹å˜ä½ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ†å‰²ã—ã¾ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã§ã‚‚åŒã˜ã“ã¨ã‚’è¡Œã„ã¾ã™ãŒã€ãã“ã«ã¯å¤§ããªé•ã„ãŒã‚ã‚Šã¾ã™ã€‚

ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã¯**å½¢å¼è¨€èª**ã§ã™ã€‚å³å¯†ãªæ–‡æ³•è¦å‰‡ãŒã‚ã‚Šã€æ›–æ˜§ã•ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸€æ–¹ã€è‡ªç„¶è¨€èªã¯**æ›–æ˜§ã•ã®å¡Š**ã§ã™ã€‚åŒã˜å˜èªãŒæ–‡è„ˆã«ã‚ˆã£ã¦å…¨ãç•°ãªã‚‹æ„å‘³ã‚’æŒã¡ã€æ–°ã—ã„å˜èªãŒæ—¥ã€…ç”Ÿã¾ã‚Œã€æ–‡æ³•è¦å‰‡ã«ã¯ç„¡æ•°ã®ä¾‹å¤–ãŒã‚ã‚Šã¾ã™ã€‚

ã“ã®ç« ã§ã¯ã€ã“ã®æŒ‘æˆ¦çš„ãªå•é¡Œã«å¯¾ã™ã‚‹Transformerã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªå‡¦ç†ã®çŸ¥è­˜ã‚’æ´»ã‹ã—ãªãŒã‚‰ç†è§£ã—ã¦ã„ãã¾ã™ã€‚

## 5.1 ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼šè¨€èªã®åŸå­ã‚’è¦‹ã¤ã‘ã‚‹

### ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª vs è‡ªç„¶è¨€èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

```python
import re
from typing import List, Dict, Tuple, Optional, Union
from collections import Counter, defaultdict
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass, field

class TokenizationComparison:
    """ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã¨è‡ªç„¶è¨€èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ¯”è¼ƒ"""
    
    def programming_language_tokenizer(self, code: str) -> List[Tuple[str, str]]:
        """ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®å­—å¥è§£æå™¨"""
        # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©
        token_specification = [
            ('NUMBER',    r'\d+(\.\d*)?'),                    # æ•°å€¤
            ('IDENT',     r'[a-zA-Z_]\w*'),                  # è­˜åˆ¥å­
            ('STRING',    r'"[^"]*"'),                        # æ–‡å­—åˆ—
            ('COMMENT',   r'//[^\n]*'),                       # ã‚³ãƒ¡ãƒ³ãƒˆ
            ('ASSIGN',    r'='),                              # ä»£å…¥
            ('END',       r';'),                              # æ–‡æœ«
            ('OP',        r'[+\-*/]'),                        # æ¼”ç®—å­
            ('LPAREN',    r'\('),                             # å·¦æ‹¬å¼§
            ('RPAREN',    r'\)'),                             # å³æ‹¬å¼§
            ('LBRACE',    r'\{'),                             # å·¦æ³¢æ‹¬å¼§
            ('RBRACE',    r'\}'),                             # å³æ³¢æ‹¬å¼§
            ('SKIP',      r'[ \t]+'),                         # ã‚¹ãƒšãƒ¼ã‚¹
            ('NEWLINE',   r'\n'),                             # æ”¹è¡Œ
            ('MISMATCH',  r'.'),                              # ã‚¨ãƒ©ãƒ¼
        ]
        
        tok_regex = '|'.join(f'(?P<{name}>{pattern})' for name, pattern in token_specification)
        tokens = []
        
        for match in re.finditer(tok_regex, code):
            kind = match.lastgroup
            value = match.group()
            if kind not in ['SKIP', 'NEWLINE', 'COMMENT']:
                tokens.append((kind, value))
        
        return tokens
    
    def natural_language_challenges(self) -> None:
        """è‡ªç„¶è¨€èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®èª²é¡Œã‚’å®Ÿæ¼”"""
        print("=== è‡ªç„¶è¨€èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®èª²é¡Œ ===")
        
        # 1. å˜èªå¢ƒç•Œã®æ›–æ˜§ã•
        examples = {
            "è¤‡åˆèª": "New York Times",  # 1ãƒˆãƒ¼ã‚¯ãƒ³ï¼Ÿ3ãƒˆãƒ¼ã‚¯ãƒ³ï¼Ÿ
            "ç¸®ç´„": "don't",             # "do not"ï¼Ÿ1ãƒˆãƒ¼ã‚¯ãƒ³ï¼Ÿ
            "ãƒã‚¤ãƒ•ãƒ³": "state-of-the-art",  # ã©ã†åˆ†å‰²ï¼Ÿ
            "æ•°å€¤": "$1,234.56",         # é€šè²¨è¨˜å·ã¨æ•°å€¤ã‚’åˆ†ã‘ã‚‹ï¼Ÿ
            "çµµæ–‡å­—": "Hello ğŸ‘‹ World",   # çµµæ–‡å­—ã®æ‰±ã„
            "æ—¥æœ¬èª": "ç§ã¯å­¦ç”Ÿã§ã™",      # ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„è¨€èª
        }
        
        for category, text in examples.items():
            print(f"\n{category}: '{text}'")
            
            # å˜ç´”ãªã‚¹ãƒšãƒ¼ã‚¹åˆ†å‰²
            simple_tokens = text.split()
            print(f"  ã‚¹ãƒšãƒ¼ã‚¹åˆ†å‰²: {simple_tokens}")
            
            # ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸåˆ†å‰²ï¼ˆå¾Œè¿°ï¼‰
    
    def why_subword_tokenization(self) -> None:
        """ãªãœã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒå¿…è¦ã‹"""
        
        # å•é¡Œ1: èªå½™çˆ†ç™º
        print("\n=== èªå½™çˆ†ç™ºã®å•é¡Œ ===")
        
        # è‹±èªã®å˜èªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        word_variations = [
            "run", "runs", "running", "ran", "runner", "runners",
            "runnable", "rerun", "overrun", "outrun"
        ]
        
        print("'run'ã®å¤‰åŒ–å½¢:", word_variations)
        print(f"å˜èªãƒ¬ãƒ™ãƒ«ã§ã¯{len(word_variations)}å€‹ã®ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦")
        
        # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã®ä¾‹
        subword_splits = {
            "running": ["run", "##ning"],
            "runners": ["run", "##ner", "##s"],
            "unrunnable": ["un", "##run", "##nable"],
        }
        
        print("\nã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²:")
        for word, subwords in subword_splits.items():
            print(f"  {word} â†’ {subwords}")
        
        # å•é¡Œ2: æœªçŸ¥èªï¼ˆOOV: Out-of-Vocabularyï¼‰
        print("\n=== æœªçŸ¥èªã®å•é¡Œ ===")
        
        # è¨“ç·´æ™‚ã«è¦‹ãŸã“ã¨ãŒãªã„å˜èª
        unknown_words = [
            "COVID-19",      # æ–°ã—ã„ç”¨èª
            "Pneumonoultramicroscopicsilicovolcanoconiosis",  # é•·ã„å°‚é–€ç”¨èª
            "ğŸš€ğŸŒŸ",          # çµµæ–‡å­—ã®çµ„ã¿åˆã‚ã›
            "supercalifragilisticexpialidocious",  # é€ èª
        ]
        
        print("æœªçŸ¥èªã®ä¾‹:", unknown_words)
        print("å˜èªãƒ¬ãƒ™ãƒ«ã§ã¯å…¨ã¦[UNK]ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã£ã¦ã—ã¾ã†")
```

### Byte-Pair Encoding (BPE) ã®å®Ÿè£…

```python
@dataclass
class BPEToken:
    """BPEãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    text: str
    frequency: int = 0
    
class BytePairEncoding:
    """BPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…"""
    
    def __init__(self, vocab_size: int = 1000):
        self.vocab_size = vocab_size
        self.word_freq = Counter()
        self.vocab = {}
        self.merges = []
    
    def train(self, corpus: List[str], verbose: bool = True) -> None:
        """ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰BPEãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        # ã‚¹ãƒ†ãƒƒãƒ—1: å˜èªé »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for text in corpus:
            words = text.lower().split()
            for word in words:
                # å˜èªã‚’æ–‡å­—å˜ä½ã«åˆ†å‰²ï¼ˆç‰¹æ®Šãªçµ‚ç«¯è¨˜å·ã‚’è¿½åŠ ï¼‰
                word_tokens = list(word) + ['</w>']
                self.word_freq[tuple(word_tokens)] += 1
        
        # åˆæœŸèªå½™ï¼ˆå…¨ã¦ã®æ–‡å­—ï¼‰
        vocab = set()
        for word_tokens, freq in self.word_freq.items():
            for token in word_tokens:
                vocab.add(token)
        
        if verbose:
            print(f"åˆæœŸèªå½™ã‚µã‚¤ã‚º: {len(vocab)}")
            print(f"åˆæœŸèªå½™: {sorted(list(vocab))[:20]}...")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒ¼ã‚¸ã‚’ç¹°ã‚Šè¿”ã™
        num_merges = self.vocab_size - len(vocab)
        
        for i in range(num_merges):
            # æœ€ã‚‚é »åº¦ã®é«˜ã„ãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
            pair_freq = self._get_pair_frequencies()
            
            if not pair_freq:
                break
            
            best_pair = max(pair_freq, key=pair_freq.get)
            self.merges.append(best_pair)
            
            if verbose and i % 100 == 0:
                print(f"\nãƒãƒ¼ã‚¸ {i+1}: {best_pair} (é »åº¦: {pair_freq[best_pair]})")
            
            # èªå½™ã‚’æ›´æ–°
            self.word_freq = self._merge_pair(best_pair)
            
            # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èªå½™ã«è¿½åŠ 
            new_token = ''.join(best_pair)
            vocab.add(new_token)
        
        # æœ€çµ‚èªå½™ã‚’ä½œæˆ
        self.vocab = {token: idx for idx, token in enumerate(sorted(vocab))}
        
        if verbose:
            print(f"\næœ€çµ‚èªå½™ã‚µã‚¤ã‚º: {len(self.vocab)}")
            print(f"å­¦ç¿’ã•ã‚ŒãŸãƒãƒ¼ã‚¸æ•°: {len(self.merges)}")
    
    def _get_pair_frequencies(self) -> Dict[Tuple[str, str], int]:
        """éš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã®é »åº¦ã‚’è¨ˆç®—"""
        pair_freq = defaultdict(int)
        
        for word_tokens, freq in self.word_freq.items():
            for i in range(len(word_tokens) - 1):
                pair = (word_tokens[i], word_tokens[i + 1])
                pair_freq[pair] += freq
        
        return pair_freq
    
    def _merge_pair(self, pair: Tuple[str, str]) -> Counter:
        """æŒ‡å®šã•ã‚ŒãŸãƒšã‚¢ã‚’ãƒãƒ¼ã‚¸"""
        new_word_freq = Counter()
        
        for word_tokens, freq in self.word_freq.items():
            new_word_tokens = []
            i = 0
            
            while i < len(word_tokens):
                # ãƒšã‚¢ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ãƒãƒ¼ã‚¸
                if i < len(word_tokens) - 1 and \
                   word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:
                    new_word_tokens.append(pair[0] + pair[1])
                    i += 2
                else:
                    new_word_tokens.append(word_tokens[i])
                    i += 1
            
            new_word_freq[tuple(new_word_tokens)] = freq
        
        return new_word_freq
    
    def tokenize(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’BPEãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²"""
        words = text.lower().split()
        tokens = []
        
        for word in words:
            word_tokens = list(word) + ['</w>']
            
            # å­¦ç¿’ã—ãŸãƒãƒ¼ã‚¸ã‚’é©ç”¨
            for pair in self.merges:
                i = 0
                new_word_tokens = []
                
                while i < len(word_tokens):
                    if i < len(word_tokens) - 1 and \
                       word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:
                        new_word_tokens.append(pair[0] + pair[1])
                        i += 2
                    else:
                        new_word_tokens.append(word_tokens[i])
                        i += 1
                
                word_tokens = new_word_tokens
            
            tokens.extend(word_tokens)
        
        return tokens
    
    def visualize_tokenization(self, text: str) -> None:
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®éç¨‹ã‚’å¯è¦–åŒ–"""
        words = text.lower().split()
        
        fig, axes = plt.subplots(len(words), 1, figsize=(12, 3 * len(words)))
        if len(words) == 1:
            axes = [axes]
        
        for idx, word in enumerate(words):
            ax = axes[idx]
            word_tokens = list(word) + ['</w>']
            
            # å„ãƒãƒ¼ã‚¸ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜éŒ²
            steps = [word_tokens.copy()]
            
            for pair in self.merges:
                i = 0
                new_word_tokens = []
                
                while i < len(word_tokens):
                    if i < len(word_tokens) - 1 and \
                       word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:
                        new_word_tokens.append(pair[0] + pair[1])
                        i += 2
                    else:
                        new_word_tokens.append(word_tokens[i])
                        i += 1
                
                if new_word_tokens != word_tokens:
                    word_tokens = new_word_tokens
                    steps.append(word_tokens.copy())
            
            # å¯è¦–åŒ–
            y_labels = [f"Step {i}" for i in range(len(steps))]
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            max_len = max(len(step) for step in steps)
            heatmap_data = np.zeros((len(steps), max_len))
            
            for i, step in enumerate(steps):
                for j, token in enumerate(step):
                    heatmap_data[i, j] = len(token)
            
            im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¡¨ç¤º
            for i, step in enumerate(steps):
                for j, token in enumerate(step):
                    ax.text(j, i, token, ha='center', va='center', fontsize=10)
            
            ax.set_yticks(range(len(steps)))
            ax.set_yticklabels(y_labels)
            ax.set_xticks([])
            ax.set_title(f"'{word}'ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–éç¨‹")
        
        plt.tight_layout()
        plt.show()
```

### WordPieceã¨SentencePieceã®æ¯”è¼ƒ

```python
class ModernTokenizers:
    """ç¾ä»£çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ¯”è¼ƒ"""
    
    def __init__(self):
        self.tokenizers = {}
    
    def compare_tokenization_methods(self, text: str) -> None:
        """ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ"""
        print(f"=== ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒ ===")
        print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: '{text}'")
        
        # 1. å˜èªãƒ¬ãƒ™ãƒ«
        word_tokens = text.split()
        print(f"\n1. å˜èªãƒ¬ãƒ™ãƒ«: {word_tokens}")
        print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(word_tokens)}")
        
        # 2. æ–‡å­—ãƒ¬ãƒ™ãƒ«
        char_tokens = list(text)
        print(f"\n2. æ–‡å­—ãƒ¬ãƒ™ãƒ«: {char_tokens[:50]}...")
        print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(char_tokens)}")
        
        # 3. BPEï¼ˆç°¡æ˜“ç‰ˆï¼‰
        bpe_tokens = self._simple_bpe_tokenize(text)
        print(f"\n3. BPE: {bpe_tokens}")
        print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(bpe_tokens)}")
        
        # 4. WordPieceï¼ˆç°¡æ˜“ç‰ˆï¼‰
        wordpiece_tokens = self._simple_wordpiece_tokenize(text)
        print(f"\n4. WordPiece: {wordpiece_tokens}")
        print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(wordpiece_tokens)}")
        
        # å¯è¦–åŒ–
        self._visualize_tokenization_comparison(text, {
            'Word': word_tokens,
            'Character': char_tokens[:30],  # è¡¨ç¤ºç”¨ã«åˆ¶é™
            'BPE': bpe_tokens,
            'WordPiece': wordpiece_tokens
        })
    
    def _simple_bpe_tokenize(self, text: str) -> List[str]:
        """ç°¡æ˜“BPEãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        # å®Ÿéš›ã®BPEã¯å­¦ç¿’ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ç°¡æ˜“ç‰ˆ
        tokens = []
        for word in text.split():
            if len(word) > 6:
                # é•·ã„å˜èªã¯åˆ†å‰²
                tokens.extend([word[:3], '##' + word[3:]])
            else:
                tokens.append(word)
        return tokens
    
    def _simple_wordpiece_tokenize(self, text: str) -> List[str]:
        """ç°¡æ˜“WordPieceãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        # å®Ÿéš›ã®WordPieceã‚‚å­¦ç¿’ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ç°¡æ˜“ç‰ˆ
        common_prefixes = ['un', 're', 'pre', 'post', 'sub', 'over']
        common_suffixes = ['ing', 'ed', 'er', 'est', 'ly', 'tion', 'ness']
        
        tokens = []
        for word in text.split():
            tokenized = False
            
            # æ¥é ­è¾ãƒã‚§ãƒƒã‚¯
            for prefix in common_prefixes:
                if word.startswith(prefix) and len(word) > len(prefix) + 2:
                    tokens.extend([prefix, '##' + word[len(prefix):]])
                    tokenized = True
                    break
            
            # æ¥å°¾è¾ãƒã‚§ãƒƒã‚¯
            if not tokenized:
                for suffix in common_suffixes:
                    if word.endswith(suffix) and len(word) > len(suffix) + 2:
                        tokens.extend([word[:-len(suffix)], '##' + suffix])
                        tokenized = True
                        break
            
            if not tokenized:
                tokens.append(word)
        
        return tokens
    
    def _visualize_tokenization_comparison(self, text: str, tokenizations: Dict[str, List[str]]):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®æ¯”è¼ƒã‚’å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        methods = list(tokenizations.keys())
        token_counts = [len(tokens) for tokens in tokenizations.values()]
        
        bars = ax.bar(methods, token_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        
        # å„ãƒãƒ¼ã®ä¸Šã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¡¨ç¤º
        for bar, count in zip(bars, token_counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{count}', ha='center', va='bottom')
        
        ax.set_xlabel('ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•')
        ax.set_ylabel('ãƒˆãƒ¼ã‚¯ãƒ³æ•°')
        ax.set_title('ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ‰‹æ³•ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¯”è¼ƒ')
        
        # ç†æƒ³çš„ãªç¯„å›²ã‚’è¡¨ç¤º
        ax.axhspan(10, 30, alpha=0.2, color='green', label='ç†æƒ³çš„ãªç¯„å›²')
        ax.legend()
        
        plt.tight_layout()
        plt.show()
```

## 5.2 å˜èªåŸ‹ã‚è¾¼ã¿ï¼šæ„å‘³ã‚’æ‰ãˆã‚‹ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾

### ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®é™ç•Œ

```python
class WordEmbeddings:
    """å˜èªåŸ‹ã‚è¾¼ã¿ã®æ¦‚å¿µã¨å®Ÿè£…"""
    
    def __init__(self, vocab_size: int = 10000, embedding_dim: int = 300):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.embeddings = None
    
    def one_hot_problems(self) -> None:
        """ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œç‚¹"""
        print("=== ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œ ===")
        
        # å°ã•ãªèªå½™ã§ã®ä¾‹
        words = ["cat", "dog", "animal", "car", "vehicle"]
        vocab_size = len(words)
        
        # ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«ä½œæˆ
        one_hot_vectors = {}
        for idx, word in enumerate(words):
            vector = np.zeros(vocab_size)
            vector[idx] = 1
            one_hot_vectors[word] = vector
        
        print("ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«:")
        for word, vector in one_hot_vectors.items():
            print(f"{word}: {vector}")
        
        # å•é¡Œ1: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§
        print(f"\nå•é¡Œ1: ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§")
        print(f"èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
        print(f"éã‚¼ãƒ­è¦ç´ : 1/{vocab_size} = {1/vocab_size:.1%}")
        
        # å•é¡Œ2: æ„å‘³çš„é–¢ä¿‚ã®æ¬ å¦‚
        print(f"\nå•é¡Œ2: æ„å‘³çš„é–¢ä¿‚ã®æ¬ å¦‚")
        
        def cosine_similarity(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        
        similarities = {
            "cat vs dog": cosine_similarity(one_hot_vectors["cat"], one_hot_vectors["dog"]),
            "cat vs animal": cosine_similarity(one_hot_vectors["cat"], one_hot_vectors["animal"]),
            "car vs vehicle": cosine_similarity(one_hot_vectors["car"], one_hot_vectors["vehicle"]),
        }
        
        for pair, sim in similarities.items():
            print(f"{pair}: {sim}")
        print("â†’ å…¨ã¦ã®å˜èªé–“ã®é¡ä¼¼åº¦ãŒ0ï¼ˆç›´äº¤ï¼‰")
        
        # å•é¡Œ3: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
        print(f"\nå•é¡Œ3: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡")
        real_vocab_size = 50000
        print(f"å®Ÿéš›ã®èªå½™ã‚µã‚¤ã‚º: {real_vocab_size:,}")
        memory_one_hot = real_vocab_size * real_vocab_size * 4  # float32
        print(f"å¿…è¦ãƒ¡ãƒ¢ãƒªï¼ˆå…¨å˜èªï¼‰: {memory_one_hot / 1e9:.1f} GB")
    
    def dense_embeddings_intuition(self) -> None:
        """å¯†ãªåŸ‹ã‚è¾¼ã¿ã®ç›´æ„Ÿçš„ç†è§£"""
        print("\n=== å¯†ãªåŸ‹ã‚è¾¼ã¿ã®åˆ©ç‚¹ ===")
        
        # ä»®æƒ³çš„ãªåŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã¯å­¦ç¿’ã§ç²å¾—ï¼‰
        embeddings = {
            "cat": np.array([0.2, 0.8, -0.1, 0.3]),
            "dog": np.array([0.3, 0.7, -0.2, 0.4]),
            "animal": np.array([0.25, 0.75, -0.15, 0.35]),
            "car": np.array([-0.5, -0.3, 0.8, -0.2]),
            "vehicle": np.array([-0.4, -0.35, 0.75, -0.15]),
        }
        
        embedding_dim = 4
        
        print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {embedding_dim}")
        print("\nåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«:")
        for word, vec in embeddings.items():
            print(f"{word}: {vec}")
        
        # æ„å‘³çš„é¡ä¼¼åº¦
        print("\næ„å‘³çš„é¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰:")
        
        def cosine_similarity(v1, v2):
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        
        pairs = [
            ("cat", "dog"),
            ("cat", "animal"),
            ("car", "vehicle"),
            ("cat", "car"),
        ]
        
        for w1, w2 in pairs:
            sim = cosine_similarity(embeddings[w1], embeddings[w2])
            print(f"{w1} vs {w2}: {sim:.3f}")
        
        # å¯è¦–åŒ–
        self._visualize_embeddings(embeddings)
    
    def _visualize_embeddings(self, embeddings: Dict[str, np.ndarray]):
        """åŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–ï¼ˆ2DæŠ•å½±ï¼‰"""
        from sklearn.decomposition import PCA
        
        words = list(embeddings.keys())
        vectors = np.array(list(embeddings.values()))
        
        # PCAã§2æ¬¡å…ƒã«å‰Šæ¸›
        if vectors.shape[1] > 2:
            pca = PCA(n_components=2)
            vectors_2d = pca.fit_transform(vectors)
        else:
            vectors_2d = vectors
        
        plt.figure(figsize=(10, 8))
        
        # å˜èªã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        for i, word in enumerate(words):
            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1], s=200)
            plt.annotate(word, 
                        xy=(vectors_2d[i, 0], vectors_2d[i, 1]),
                        xytext=(5, 5), 
                        textcoords='offset points',
                        fontsize=12)
        
        # é¡ä¼¼ã™ã‚‹å˜èªã‚’ç·šã§çµã¶
        similar_pairs = [("cat", "dog"), ("cat", "animal"), ("car", "vehicle")]
        for w1, w2 in similar_pairs:
            idx1 = words.index(w1)
            idx2 = words.index(w2)
            plt.plot([vectors_2d[idx1, 0], vectors_2d[idx2, 0]],
                    [vectors_2d[idx1, 1], vectors_2d[idx2, 1]],
                    'k--', alpha=0.3)
        
        plt.xlabel('ç¬¬1ä¸»æˆåˆ†')
        plt.ylabel('ç¬¬2ä¸»æˆåˆ†')
        plt.title('å˜èªåŸ‹ã‚è¾¼ã¿ã®2æ¬¡å…ƒå¯è¦–åŒ–')
        plt.grid(True, alpha=0.3)
        plt.show()
```

### åŸ‹ã‚è¾¼ã¿å±¤ã®å®Ÿè£…ã¨å­¦ç¿’

```python
class EmbeddingLayer:
    """åŸ‹ã‚è¾¼ã¿å±¤ã®å®Ÿè£…ã¨å­¦ç¿’éç¨‹ã®ç†è§£"""
    
    def __init__(self, vocab_size: int, embedding_dim: int):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        
        # åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã®åˆæœŸåŒ–
        self.embedding_matrix = torch.nn.Embedding(vocab_size, embedding_dim)
        
        # åˆæœŸåŒ–æ–¹æ³•ã®æ¯”è¼ƒ
        self._compare_initialization_methods()
    
    def _compare_initialization_methods(self):
        """ç•°ãªã‚‹åˆæœŸåŒ–æ–¹æ³•ã®æ¯”è¼ƒ"""
        methods = {
            'uniform': lambda: torch.nn.init.uniform_(
                torch.empty(self.vocab_size, self.embedding_dim), -0.1, 0.1
            ),
            'normal': lambda: torch.nn.init.normal_(
                torch.empty(self.vocab_size, self.embedding_dim), 0, 0.02
            ),
            'xavier': lambda: torch.nn.init.xavier_uniform_(
                torch.empty(self.vocab_size, self.embedding_dim)
            ),
        }
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for idx, (name, init_fn) in enumerate(methods.items()):
            matrix = init_fn()
            
            ax = axes[idx]
            im = ax.imshow(matrix[:50, :50], cmap='coolwarm', aspect='auto')
            ax.set_title(f'{name.capitalize()} Initialization')
            ax.set_xlabel('Embedding Dimension')
            ax.set_ylabel('Word Index')
            plt.colorbar(im, ax=ax)
        
        plt.tight_layout()
        plt.show()
    
    def forward_pass_explained(self, word_indices: torch.Tensor) -> torch.Tensor:
        """åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­ã‚’è©³ã—ãèª¬æ˜"""
        print("=== åŸ‹ã‚è¾¼ã¿å±¤ã®é †ä¼æ’­ ===")
        
        # å…¥åŠ›ã®å½¢çŠ¶
        print(f"å…¥åŠ›ï¼ˆå˜èªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰: {word_indices}")
        print(f"å…¥åŠ›ã®å½¢çŠ¶: {word_indices.shape}")
        
        # åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã®å½¢çŠ¶
        print(f"\nåŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã®å½¢çŠ¶: {self.embedding_matrix.weight.shape}")
        print(f"  â†’ {self.vocab_size} words Ã— {self.embedding_dim} dimensions")
        
        # ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—æ“ä½œ
        embeddings = self.embedding_matrix(word_indices)
        print(f"\nå‡ºåŠ›ã®å½¢çŠ¶: {embeddings.shape}")
        
        # å¯è¦–åŒ–
        self._visualize_lookup_operation(word_indices, embeddings)
        
        return embeddings
    
    def _visualize_lookup_operation(self, indices: torch.Tensor, embeddings: torch.Tensor):
        """ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—æ“ä½œã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # åŸ‹ã‚è¾¼ã¿è¡Œåˆ—å…¨ä½“
        ax = axes[0]
        im = ax.imshow(self.embedding_matrix.weight.data[:20, :20], 
                       cmap='viridis', aspect='auto')
        ax.set_title('åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ï¼ˆä¸€éƒ¨ï¼‰')
        ax.set_xlabel('Dimension')
        ax.set_ylabel('Word Index')
        
        # é¸æŠã•ã‚ŒãŸè¡Œ
        ax = axes[1]
        selected_indices = indices.flatten().tolist()[:5]  # æœ€åˆã®5ã¤
        for i, idx in enumerate(selected_indices):
            ax.axhline(y=idx, color='red', linewidth=2, alpha=0.7)
        ax.set_ylim(-0.5, 19.5)
        ax.set_title('é¸æŠã•ã‚ŒãŸå˜èª')
        ax.set_ylabel('Word Index')
        
        # çµæœã®åŸ‹ã‚è¾¼ã¿
        ax = axes[2]
        result = embeddings.reshape(-1, self.embedding_dim)[:5]
        im = ax.imshow(result.detach(), cmap='viridis', aspect='auto')
        ax.set_title('å–å¾—ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿')
        ax.set_xlabel('Dimension')
        ax.set_ylabel('Selected Words')
        
        plt.tight_layout()
        plt.show()
    
    def gradient_flow_in_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿å±¤ã§ã®å‹¾é…ã®æµã‚Œ"""
        print("\n=== åŸ‹ã‚è¾¼ã¿å±¤ã®å‹¾é…æ›´æ–° ===")
        
        # ç°¡å˜ãªä¾‹
        vocab_size = 10
        embedding_dim = 4
        embedding = torch.nn.Embedding(vocab_size, embedding_dim)
        
        # åˆæœŸã®åŸ‹ã‚è¾¼ã¿
        print("åˆæœŸåŸ‹ã‚è¾¼ã¿ï¼ˆword 3ï¼‰:")
        print(embedding.weight[3])
        
        # é †ä¼æ’­
        word_indices = torch.tensor([3, 5, 3])  # word 3ãŒ2å›å‡ºç¾
        embedded = embedding(word_indices)
        
        # ä»®æƒ³çš„ãªæå¤±
        loss = embedded.sum()
        loss.backward()
        
        # å‹¾é…ã‚’ç¢ºèª
        print("\nå‹¾é…ï¼ˆword 3ï¼‰:")
        print(embedding.weight.grad[3])
        print("â†’ word 3ã¯2å›ä½¿ã‚ã‚ŒãŸã®ã§ã€å‹¾é…ã‚‚2å€")
        
        # æ›´æ–°
        with torch.no_grad():
            embedding.weight -= 0.1 * embedding.weight.grad
        
        print("\næ›´æ–°å¾Œã®åŸ‹ã‚è¾¼ã¿ï¼ˆword 3ï¼‰:")
        print(embedding.weight[3])
```

### ä½ç½®ã‚’è€ƒæ…®ã—ãŸåŸ‹ã‚è¾¼ã¿

```python
class PositionalAwareEmbeddings:
    """ä½ç½®æƒ…å ±ã‚’è€ƒæ…®ã—ãŸåŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, vocab_size: int, max_length: int, d_model: int):
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.d_model = d_model
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿
        self.token_embedding = torch.nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
        self.position_embedding = torch.nn.Embedding(max_length, d_model)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆBERTã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        self.segment_embedding = torch.nn.Embedding(2, d_model)
    
    def combined_embeddings(self, token_ids: torch.Tensor, 
                          position_ids: Optional[torch.Tensor] = None,
                          segment_ids: Optional[torch.Tensor] = None) -> torch.Tensor:
        """è¤‡åˆåŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—"""
        batch_size, seq_length = token_ids.shape
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿
        token_embeds = self.token_embedding(token_ids)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        if position_ids is None:
            position_ids = torch.arange(seq_length).expand(batch_size, -1)
        position_embeds = self.position_embedding(position_ids)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿
        if segment_ids is None:
            segment_ids = torch.zeros_like(token_ids)
        segment_embeds = self.segment_embedding(segment_ids)
        
        # åˆè¨ˆ
        embeddings = token_embeds + position_embeds + segment_embeds
        
        # å¯è¦–åŒ–
        self._visualize_embedding_components(
            token_embeds[0], position_embeds[0], segment_embeds[0], embeddings[0]
        )
        
        return embeddings
    
    def _visualize_embedding_components(self, token_emb, pos_emb, seg_emb, total_emb):
        """åŸ‹ã‚è¾¼ã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        components = [
            (token_emb, 'Token Embeddings'),
            (pos_emb, 'Position Embeddings'),
            (seg_emb, 'Segment Embeddings'),
            (total_emb, 'Combined Embeddings')
        ]
        
        for idx, (emb, title) in enumerate(components):
            ax = axes[idx // 2, idx % 2]
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            im = ax.imshow(emb[:10, :50].detach(), cmap='coolwarm', aspect='auto')
            ax.set_title(title)
            ax.set_xlabel('Embedding Dimension')
            ax.set_ylabel('Position')
            plt.colorbar(im, ax=ax)
        
        plt.tight_layout()
        plt.show()
```

## 5.3 å®Ÿè·µçš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…

### ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ§‹ç¯‰

```python
class CustomTokenizer:
    """å®Ÿç”¨çš„ãªã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self, vocab_size: int = 10000):
        self.vocab_size = vocab_size
        self.word_to_idx = {"[PAD]": 0, "[UNK]": 1, "[CLS]": 2, "[SEP]": 3, "[MASK]": 4}
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.vocab_built = False
    
    def build_vocab(self, texts: List[str], min_freq: int = 2) -> None:
        """èªå½™ã‚’æ§‹ç¯‰"""
        print("=== èªå½™æ§‹ç¯‰ ===")
        
        # å˜èªé »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        word_freq = Counter()
        for text in texts:
            words = self._basic_tokenize(text)
            word_freq.update(words)
        
        print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°: {len(word_freq)}")
        print(f"æœ€é »å‡ºå˜èª: {word_freq.most_common(10)}")
        
        # é »åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        vocab_words = [word for word, freq in word_freq.items() if freq >= min_freq]
        vocab_words = vocab_words[:self.vocab_size - len(self.word_to_idx)]
        
        # èªå½™ã«è¿½åŠ 
        for word in vocab_words:
            if word not in self.word_to_idx:
                idx = len(self.word_to_idx)
                self.word_to_idx[word] = idx
                self.idx_to_word[idx] = word
        
        self.vocab_built = True
        print(f"æœ€çµ‚èªå½™ã‚µã‚¤ã‚º: {len(self.word_to_idx)}")
    
    def _basic_tokenize(self, text: str) -> List[str]:
        """åŸºæœ¬çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        # å°æ–‡å­—åŒ–
        text = text.lower()
        
        # å¥èª­ç‚¹ã‚’åˆ†é›¢
        text = re.sub(r'([.!?,;:])', r' \1 ', text)
        
        # ç©ºç™½ã§åˆ†å‰²
        tokens = text.split()
        
        return tokens
    
    def encode(self, text: str, max_length: Optional[int] = None, 
               padding: bool = True, truncation: bool = True) -> Dict[str, torch.Tensor]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›"""
        if not self.vocab_built:
            raise ValueError("èªå½™ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚build_vocab()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self._basic_tokenize(text)
        
        # CLSã¨SEPãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
        tokens = ["[CLS]"] + tokens + ["[SEP]"]
        
        # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        token_ids = []
        for token in tokens:
            token_ids.append(self.word_to_idx.get(token, self.word_to_idx["[UNK]"]))
        
        # åˆ‡ã‚Šè©°ã‚ã¾ãŸã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if max_length is not None:
            if truncation and len(token_ids) > max_length:
                token_ids = token_ids[:max_length-1] + [self.word_to_idx["[SEP]"]]
            
            if padding and len(token_ids) < max_length:
                padding_length = max_length - len(token_ids)
                token_ids = token_ids + [self.word_to_idx["[PAD]"]] * padding_length
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã®ä½œæˆ
        attention_mask = [1 if token_id != self.word_to_idx["[PAD]"] else 0 
                         for token_id in token_ids]
        
        return {
            "input_ids": torch.tensor(token_ids),
            "attention_mask": torch.tensor(attention_mask),
            "tokens": tokens[:len(token_ids)]
        }
    
    def decode(self, token_ids: torch.Tensor, skip_special_tokens: bool = True) -> str:
        """ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        tokens = []
        
        for token_id in token_ids:
            token = self.idx_to_word.get(token_id.item(), "[UNK]")
            
            if skip_special_tokens and token in ["[PAD]", "[CLS]", "[SEP]", "[MASK]"]:
                continue
            
            tokens.append(token)
        
        return " ".join(tokens)
    
    def batch_encode(self, texts: List[str], max_length: int = 128) -> Dict[str, torch.Tensor]:
        """ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        batch_encoding = {
            "input_ids": [],
            "attention_mask": []
        }
        
        for text in texts:
            encoding = self.encode(text, max_length=max_length)
            batch_encoding["input_ids"].append(encoding["input_ids"])
            batch_encoding["attention_mask"].append(encoding["attention_mask"])
        
        # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        batch_encoding["input_ids"] = torch.stack(batch_encoding["input_ids"])
        batch_encoding["attention_mask"] = torch.stack(batch_encoding["attention_mask"])
        
        return batch_encoding
    
    def visualize_tokenization(self, text: str, max_length: int = 20):
        """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–"""
        encoding = self.encode(text, max_length=max_length)
        
        fig, axes = plt.subplots(3, 1, figsize=(12, 8))
        
        # ãƒˆãƒ¼ã‚¯ãƒ³
        ax = axes[0]
        tokens = encoding["tokens"]
        y_pos = np.arange(len(tokens))
        colors = ['red' if t.startswith('[') else 'blue' for t in tokens]
        ax.barh(y_pos, [1]*len(tokens), color=colors, alpha=0.5)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(tokens)
        ax.set_title('Tokens')
        ax.set_xlim(0, 1)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ID
        ax = axes[1]
        token_ids = encoding["input_ids"].tolist()
        ax.barh(y_pos, [1]*len(token_ids), color='green', alpha=0.5)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(token_ids)
        ax.set_title('Token IDs')
        ax.set_xlim(0, 1)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
        ax = axes[2]
        attention_mask = encoding["attention_mask"].tolist()
        colors = ['blue' if m == 1 else 'gray' for m in attention_mask]
        ax.barh(y_pos, attention_mask, color=colors, alpha=0.5)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(attention_mask)
        ax.set_title('Attention Mask')
        ax.set_xlim(0, 1.1)
        
        plt.tight_layout()
        plt.show()
```

### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ€§èƒ½è©•ä¾¡

```python
class TokenizerEvaluation:
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ€§èƒ½è©•ä¾¡"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def evaluate_coverage(self, test_texts: List[str]) -> Dict[str, float]:
        """èªå½™ã‚«ãƒãƒ¼ç‡ã®è©•ä¾¡"""
        total_tokens = 0
        unknown_tokens = 0
        
        for text in test_texts:
            tokens = self.tokenizer._basic_tokenize(text)
            for token in tokens:
                total_tokens += 1
                if token not in self.tokenizer.word_to_idx:
                    unknown_tokens += 1
        
        coverage = 1 - (unknown_tokens / total_tokens)
        
        return {
            "total_tokens": total_tokens,
            "unknown_tokens": unknown_tokens,
            "coverage": coverage,
            "oov_rate": unknown_tokens / total_tokens
        }
    
    def evaluate_compression(self, texts: List[str]) -> Dict[str, float]:
        """åœ§ç¸®ç‡ã®è©•ä¾¡"""
        original_chars = sum(len(text) for text in texts)
        
        total_tokens = 0
        for text in texts:
            encoding = self.tokenizer.encode(text, padding=False)
            total_tokens += len(encoding["input_ids"])
        
        compression_ratio = original_chars / total_tokens
        
        return {
            "original_chars": original_chars,
            "total_tokens": total_tokens,
            "compression_ratio": compression_ratio,
            "avg_chars_per_token": compression_ratio
        }
    
    def visualize_evaluation(self, test_texts: List[str]):
        """è©•ä¾¡çµæœã®å¯è¦–åŒ–"""
        coverage_stats = self.evaluate_coverage(test_texts)
        compression_stats = self.evaluate_compression(test_texts)
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # ã‚«ãƒãƒ¼ç‡
        ax = axes[0, 0]
        ax.pie([coverage_stats["coverage"], coverage_stats["oov_rate"]], 
               labels=['Known', 'Unknown'], 
               autopct='%1.1f%%',
               colors=['#2ecc71', '#e74c3c'])
        ax.set_title('Vocabulary Coverage')
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å¸ƒ
        ax = axes[0, 1]
        token_lengths = []
        for text in test_texts[:100]:  # ã‚µãƒ³ãƒ—ãƒ«
            encoding = self.tokenizer.encode(text, padding=False)
            token_lengths.append(len(encoding["input_ids"]))
        
        ax.hist(token_lengths, bins=20, alpha=0.7, color='#3498db')
        ax.set_xlabel('Number of Tokens')
        ax.set_ylabel('Frequency')
        ax.set_title('Token Length Distribution')
        
        # åœ§ç¸®çµ±è¨ˆ
        ax = axes[1, 0]
        metrics = ['Original\nCharacters', 'Total\nTokens', 'Compression\nRatio']
        values = [
            compression_stats["original_chars"] / 1000,  # ã‚­ãƒ­å˜ä½
            compression_stats["total_tokens"] / 1000,
            compression_stats["compression_ratio"]
        ]
        bars = ax.bar(metrics, values, color=['#9b59b6', '#f39c12', '#1abc9c'])
        ax.set_ylabel('Value (K) / Ratio')
        ax.set_title('Compression Statistics')
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        ax = axes[1, 1]
        summary_text = f"""
Evaluation Summary:
==================
Coverage: {coverage_stats['coverage']:.1%}
OOV Rate: {coverage_stats['oov_rate']:.1%}
Unknown Tokens: {coverage_stats['unknown_tokens']:,}

Compression Ratio: {compression_stats['compression_ratio']:.2f}
Avg Chars/Token: {compression_stats['avg_chars_per_token']:.2f}
        """
        ax.text(0.1, 0.5, summary_text, transform=ax.transAxes, 
                fontsize=12, verticalalignment='center',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        ax.axis('off')
        
        plt.tight_layout()
        plt.show()
```

## ã¾ã¨ã‚ï¼šè¨€èªã‚’æ•°å€¤ã«å¤‰æ›ã™ã‚‹æŠ€è¡“

ã“ã®ç« ã§å­¦ã‚“ã ã“ã¨ã‚’æ•´ç†ã—ã¾ã—ã‚‡ã†ï¼š

1. **ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®é‡è¦æ€§**
   - ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã®å­—å¥è§£æã¨åŒæ§˜ã®å½¹å‰²
   - ã—ã‹ã—è‡ªç„¶è¨€èªç‰¹æœ‰ã®èª²é¡Œï¼ˆæ›–æ˜§æ€§ã€æ–°èªã€å¤šè¨€èªï¼‰ã¸ã®å¯¾å¿œãŒå¿…è¦
   - ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆBPEã€WordPieceï¼‰ã«ã‚ˆã‚‹æŸ”è»Ÿãªå¯¾å¿œ

2. **åŸ‹ã‚è¾¼ã¿ã®æœ¬è³ª**
   - ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®é™ç•Œã‚’è¶…ãˆã‚‹
   - æ„å‘³çš„ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹å¯†ãªãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾
   - å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ã®åŸ‹ã‚è¾¼ã¿è¡Œåˆ—

3. **å®Ÿè£…ä¸Šã®è€ƒæ…®ç‚¹**
   - ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ[CLS]ã€[SEP]ã€[PAD]ï¼‰ã®æ‰±ã„
   - ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã®çµ±ä¸€çš„ãªé•·ã•
   - ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã«ã‚ˆã‚‹ç„¡åŠ¹é ˜åŸŸã®ç®¡ç†

æ¬¡ç« ã§ã¯ã€ã“ã®æ•°å€¤è¡¨ç¾ã•ã‚ŒãŸè¨€èªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€Transformerã®æ ¸å¿ƒã§ã‚ã‚‹ã€Œæ³¨æ„æ©Ÿæ§‹ã€ãŒã©ã®ã‚ˆã†ã«åƒãã‹ã‚’è¦‹ã¦ã„ãã¾ã™ã€‚

## æ¼”ç¿’å•é¡Œ

1. **BPEå®Ÿè£…ã®æ‹¡å¼µ**: æ—¥æœ¬èªã®ã‚ˆã†ãªç©ºç™½ã§åŒºåˆ‡ã‚‰ã‚Œãªã„è¨€èªã«å¯¾å¿œã—ãŸBPEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

2. **åŸ‹ã‚è¾¼ã¿ã®å¯è¦–åŒ–**: Word2Vecã‚¹ã‚¿ã‚¤ãƒ«ã®é¡æ¨ã‚¿ã‚¹ã‚¯ï¼ˆking - man + woman = queenï¼‰ã‚’ã€å­¦ç¿’æ¸ˆã¿åŸ‹ã‚è¾¼ã¿ã§æ¤œè¨¼ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

3. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æœ€é©åŒ–**: ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ã«å¯¾ã—ã¦ã€æœ€é©ãªèªå½™ã‚µã‚¤ã‚ºã‚’è‡ªå‹•çš„ã«æ±ºå®šã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚

4. **ãƒãƒ«ãƒè¨€èªå¯¾å¿œ**: è¤‡æ•°è¨€èªã‚’åŒæ™‚ã«æ‰±ãˆã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè£…ã—ã€è¨€èªé–“ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å…±æœ‰ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

---

æ¬¡ç« ã€Œæ³¨æ„æ©Ÿæ§‹ã®ç›´æ„Ÿçš„ç†è§£ã€ã¸ç¶šãã€‚