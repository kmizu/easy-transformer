{"config":{"lang":["ja"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Transformer\u3092\u4e00\u304b\u3089\u7406\u89e3\u3059\u308b","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u5b9f\u88c5\u8005\u306e\u305f\u3081\u306eTransformer\u89e3\u8aac\u3068\u5b9f\u88c5</p>"},{"location":"#_1","title":"\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u3064\u3044\u3066","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u51e6\u7406\u7cfb\u3092\u4f5c\u3063\u305f\u7d4c\u9a13\u306f\u3042\u308b\u304c\u3001\u6a5f\u68b0\u5b66\u7fd2\u3084Transformer\u306b\u3064\u3044\u3066\u306f\u521d\u5fc3\u8005\u3068\u3044\u3046\u65b9\u3092\u5bfe\u8c61\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u30b3\u30f3\u30d1\u30a4\u30e9\u3084\u30a4\u30f3\u30bf\u30d7\u30ea\u30bf\u306e\u6982\u5ff5\u3092\u4f7f\u3044\u306a\u304c\u3089\u3001Transformer\u306e\u4ed5\u7d44\u307f\u3092\u57fa\u790e\u304b\u3089\u4e01\u5be7\u306b\u8aac\u660e\u3057\u3001\u5b9f\u969b\u306b\u52d5\u304f\u30b3\u30fc\u30c9\u3092\u4e00\u7dd2\u306b\u5b9f\u88c5\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"#_2","title":"\u5bfe\u8c61\u8aad\u8005","text":"<ul> <li>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u5b9f\u88c5\u7d4c\u9a13\u304c\u3042\u308b\u65b9</li> <li>\u30b3\u30f3\u30d1\u30a4\u30e9\u3001\u30a4\u30f3\u30bf\u30d7\u30ea\u30bf\u3001\u30d1\u30fc\u30b5\u30fc\u306a\u3069\u3092\u4f5c\u3063\u305f\u3053\u3068\u304c\u3042\u308b\u65b9</li> <li>\u6a5f\u68b0\u5b66\u7fd2\u3084\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u7d4c\u9a13\u306f\u5c11\u306a\u3044\u65b9</li> <li>Transformer\u3092\u7406\u89e3\u3057\u3001\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3044\u65b9</li> </ul>"},{"location":"#_3","title":"\u5b66\u7fd2\u5185\u5bb9","text":"<ol> <li>\u57fa\u790e\u6982\u5ff5\u306e\u7406\u89e3</li> <li>\u306a\u305cTransformer\u304c\u91cd\u8981\u306a\u306e\u304b</li> <li>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3068\u306e\u985e\u4f3c\u70b9</li> <li> <p>\u5fc5\u8981\u306a\u6570\u5b66\u7684\u57fa\u790e</p> </li> <li> <p>\u6bb5\u968e\u7684\u306a\u5b9f\u88c5</p> </li> <li>\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u4e00\u3064\u305a\u3064\u5b9f\u88c5</li> <li>\u52d5\u4f5c\u3092\u78ba\u8a8d\u3057\u306a\u304c\u3089\u7406\u89e3\u3092\u6df1\u3081\u308b</li> <li> <p>\u6700\u7d42\u7684\u306b\u5b8c\u5168\u306aTransformer\u3092\u69cb\u7bc9</p> </li> <li> <p>\u5b9f\u8df5\u7684\u306a\u5fdc\u7528</p> </li> <li>GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7406\u89e3</li> <li>\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0</li> <li>\u5b9f\u7528\u7684\u306a\u6700\u9069\u5316\u6280\u8853</li> </ol>"},{"location":"#_4","title":"\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u69cb\u6210","text":""},{"location":"#1","title":"\u7b2c1\u90e8\uff1a\u5c0e\u5165\u3068\u57fa\u790e\u6982\u5ff5","text":"<ul> <li>Transformer\u306e\u91cd\u8981\u6027\u3068\u5fdc\u7528\u4f8b</li> <li>\u30b3\u30f3\u30d1\u30a4\u30e9\u3068\u306e\u985e\u4f3c\u70b9\u3067\u7406\u89e3\u3059\u308b</li> <li>\u5fc5\u8981\u306a\u6570\u5b66\u3068PyTorch\u306e\u57fa\u790e</li> </ul>"},{"location":"#2transformer","title":"\u7b2c2\u90e8\uff1aTransformer\u3078\u306e\u9053\u306e\u308a","text":"<ul> <li>\u30c8\u30fc\u30af\u30f3\u5316\u3068\u5358\u8a9e\u306e\u8868\u73fe</li> <li>\u6ce8\u610f\u6a5f\u69cb\u306e\u76f4\u611f\u7684\u7406\u89e3</li> <li>\u4f4d\u7f6e\u60c5\u5831\u306e\u6271\u3044\u65b9</li> </ul>"},{"location":"#3transformer","title":"\u7b2c3\u90e8\uff1aTransformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u8a73\u89e3","text":"<ul> <li>Multi-Head Attention\u306e\u4ed5\u7d44\u307f</li> <li>Feed Forward Network\u306e\u5f79\u5272</li> <li>\u6b8b\u5dee\u63a5\u7d9a\u3068\u6b63\u898f\u5316\u306e\u91cd\u8981\u6027</li> </ul>"},{"location":"#4","title":"\u7b2c4\u90e8\uff1a\u5b9f\u88c5\u7de8","text":"<ul> <li>\u6700\u5c0f\u9650\u306eTransformer\u5b9f\u88c5</li> <li>\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8a73\u7d30\u5b9f\u88c5</li> <li>\u30c7\u30d0\u30c3\u30b0\u3068\u52d5\u4f5c\u78ba\u8a8d</li> </ul>"},{"location":"#5llm","title":"\u7b2c5\u90e8\uff1aLLM\u3078\u306e\u62e1\u5f35","text":"<ul> <li>GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7406\u89e3</li> <li>\u5927\u898f\u6a21\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u8a13\u7df4</li> <li>\u5b9f\u7528\u7684\u306a\u5fdc\u7528\u4f8b</li> </ul>"},{"location":"#_5","title":"\u6f14\u7fd2\u554f\u984c","text":"<ul> <li>\u5404\u7ae0\u306e\u7406\u89e3\u3092\u6df1\u3081\u308b\u6f14\u7fd2</li> <li>\u5b9f\u88c5\u8ab2\u984c\u3068\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c</li> <li>\u89e3\u7b54\u4f8b\u4ed8\u304d</li> </ul>"},{"location":"#_6","title":"\u767a\u5c55\u7684\u306a\u30c8\u30d4\u30c3\u30af","text":"<ul> <li>\u6700\u65b0\u306e\u6700\u9069\u5316\u6280\u8853</li> <li>\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30ebTransformer</li> <li>\u7814\u7a76\u306e\u6700\u524d\u7dda</li> </ul>"},{"location":"#_7","title":"\u5b66\u7fd2\u306e\u9032\u3081\u65b9","text":""},{"location":"#1_1","title":"1. \u9806\u756a\u306b\u8aad\u307f\u9032\u3081\u308b","text":"<p>\u5404\u7ae0\u306f\u524d\u306e\u7ae0\u306e\u5185\u5bb9\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3001\u9806\u756a\u306b\u8aad\u3080\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002</p>"},{"location":"#2","title":"2. \u30b3\u30fc\u30c9\u3092\u5b9f\u969b\u306b\u52d5\u304b\u3059","text":"<pre><code># \u74b0\u5883\u69cb\u7bc9\npython -m venv transformer-env\nsource transformer-env/bin/activate  # Windows: transformer-env\\Scripts\\activate\npip install -r requirements.txt\n\n# Jupyter\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u5b9f\u884c\njupyter notebook\n</code></pre>"},{"location":"#3","title":"3. \u6f14\u7fd2\u554f\u984c\u306b\u53d6\u308a\u7d44\u3080","text":"<p>\u5404\u7ae0\u306e\u6f14\u7fd2\u554f\u984c\u3067\u7406\u89e3\u3092\u78ba\u8a8d\u3057\u3001\u5b9f\u88c5\u529b\u3092\u8eab\u306b\u3064\u3051\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"#4_1","title":"4. \u81ea\u5206\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u8a66\u3059","text":"<p>\u5b66\u3093\u3060\u5185\u5bb9\u3092\u81ea\u5206\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u5fdc\u7528\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"#_8","title":"\u5fc5\u8981\u306a\u74b0\u5883","text":"<ul> <li>Python 3.8\u4ee5\u4e0a</li> <li>PyTorch 1.9\u4ee5\u4e0a</li> <li>CUDA\u5bfe\u5fdcGPU\uff08\u63a8\u5968\u3001\u306a\u304f\u3066\u3082\u52d5\u4f5c\u53ef\u80fd\uff09</li> <li>8GB\u4ee5\u4e0a\u306e\u30e1\u30e2\u30ea</li> </ul>"},{"location":"#_9","title":"\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u7279\u5fb4","text":""},{"location":"#_10","title":"\ud83c\udfaf \u30b3\u30f3\u30d1\u30a4\u30e9\u3068\u306e\u985e\u63a8","text":"<pre><code>\u5b57\u53e5\u89e3\u6790 (Lexing)        \u2192 \u30c8\u30fc\u30af\u30f3\u5316 (Tokenization)\n\u69cb\u6587\u89e3\u6790 (Parsing)       \u2192 \u69cb\u9020\u7406\u89e3 (Structure Understanding)\n\u610f\u5473\u89e3\u6790 (Semantic)      \u2192 \u6587\u8108\u7406\u89e3 (Context Understanding)\n\u30b3\u30fc\u30c9\u751f\u6210 (CodeGen)     \u2192 \u51fa\u529b\u751f\u6210 (Generation)\n</code></pre>"},{"location":"#_11","title":"\ud83d\udcca \u8c4a\u5bcc\u306a\u53ef\u8996\u5316","text":"<p>\u5404\u6982\u5ff5\u3092\u56f3\u3084\u30b0\u30e9\u30d5\u3067\u8996\u899a\u7684\u306b\u7406\u89e3\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"#_12","title":"\ud83d\udcbb \u5b9f\u88c5\u91cd\u8996","text":"<p>\u7406\u8ad6\u3060\u3051\u3067\u306a\u304f\u3001\u5b9f\u969b\u306b\u52d5\u304f\u30b3\u30fc\u30c9\u3092\u66f8\u304d\u306a\u304c\u3089\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"#_13","title":"\ud83d\udd27 \u5b9f\u8df5\u7684\u306a\u5185\u5bb9","text":"<p>\u6700\u65b0\u306e\u7814\u7a76\u6210\u679c\u3084\u5b9f\u7528\u7684\u306a\u30c6\u30af\u30cb\u30c3\u30af\u3082\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"#_14","title":"\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306b\u95a2\u3059\u308b\u3054\u610f\u898b\u30fb\u3054\u8cea\u554f\u306f\u3001GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306eIssues\u307e\u3067\u304a\u5bc4\u305b\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"#_15","title":"\u30e9\u30a4\u30bb\u30f3\u30b9","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306fMIT\u30e9\u30a4\u30bb\u30f3\u30b9\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>\u305d\u308c\u3067\u306f\u3001Transformer\u306e\u4e16\u754c\u3078\u306e\u65c5\u3092\u59cb\u3081\u307e\u3057\u3087\u3046\uff01</p> <p>\u7b2c1\u7ae0\uff1a\u306a\u305cTransformer\u304c\u91cd\u8981\u306a\u306e\u304b \u2192</p>"},{"location":"advanced/multimodal/","title":"\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30ebTransformer","text":""},{"location":"advanced/multimodal/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u8907\u6570\u306e\u30e2\u30c0\u30ea\u30c6\u30a3\u3092\u7d71\u5408\u3059\u308b","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u304c\u69d8\u3005\u306a\u5165\u529b\u5f62\u5f0f\uff08\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3001\u30ea\u30f3\u30ab\u30b9\u30af\u30ea\u30d7\u30c8\uff09\u3092\u51e6\u7406\u3057\u3066\u7d71\u4e00\u7684\u306a\u51fa\u529b\u3092\u751f\u6210\u3059\u308b\u3088\u3046\u306b\u3001\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30ebTransformer\u306f\u753b\u50cf\u3001\u30c6\u30ad\u30b9\u30c8\u3001\u97f3\u58f0\u306a\u3069\u306e\u7570\u306a\u308b\u30e2\u30c0\u30ea\u30c6\u30a3\u3092\u7d71\u5408\u3057\u3066\u51e6\u7406\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001Vision Transformer (ViT)\u304b\u3089\u59cb\u307e\u308a\u3001CLIP\u3001DALL-E\u3001Flamingo \u306a\u3069\u306e\u6700\u65b0\u306e\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u30e2\u30c7\u30eb\u307e\u3067\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002</p>"},{"location":"advanced/multimodal/#1-vision-transformer-vit","title":"1. Vision Transformer (ViT)","text":""},{"location":"advanced/multimodal/#11","title":"1.1 \u753b\u50cf\u306e\u30c8\u30fc\u30af\u30f3\u5316","text":"<p>\u753b\u50cf\u3092 Transformer \u3067\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u3001\u753b\u50cf\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5909\u63db\u3059\u308b\u3053\u3068\u3067\u3059\u3002</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"\u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3066\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # \u30d1\u30c3\u30c1\u3092\u7dda\u5f62\u5909\u63db\u3067\u57cb\u3081\u8fbc\u307f\u306b\u5909\u63db\n        self.patch_embed = nn.Sequential(\n            # (B, C, H, W) -&gt; (B, num_patches, embed_dim)\n            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n            Rearrange('b c h w -&gt; b (h w) c'),\n        )\n\n        # CLS\u30c8\u30fc\u30af\u30f3\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n\n        # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\n        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n\n        # \u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n\n        # CLS\u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0\n        cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b=batch_size)\n        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1 + num_patches, embed_dim)\n\n        # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3092\u8ffd\u52a0\n        x = x + self.pos_embed\n\n        return x\n\n    def visualize_patches(self, img):\n        \"\"\"\u30d1\u30c3\u30c1\u5206\u5272\u306e\u53ef\u8996\u5316\"\"\"\n        import matplotlib.pyplot as plt\n\n        # \u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\n        patches = rearrange(\n            img, \n            'c (h p1) (w p2) -&gt; (h w) p1 p2 c',\n            p1=self.patch_size, \n            p2=self.patch_size\n        )\n\n        # \u30b0\u30ea\u30c3\u30c9\u3067\u8868\u793a\n        n = int(np.sqrt(patches.shape[0]))\n        fig, axes = plt.subplots(n, n, figsize=(10, 10))\n\n        for i, ax in enumerate(axes.flat):\n            if i &lt; patches.shape[0]:\n                patch = patches[i].permute(1, 2, 0).numpy()\n                ax.imshow(patch)\n                ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer (ViT) \u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_channels=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        dropout=0.1,\n        attention_dropout=0.1,\n    ):\n        super().__init__()\n\n        # \u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\n        self.patch_embed = PatchEmbedding(\n            img_size, patch_size, in_channels, embed_dim\n        )\n\n        # Transformer \u30a8\u30f3\u30b3\u30fc\u30c0\n        self.transformer = nn.ModuleList([\n            TransformerBlock(\n                embed_dim, num_heads, \n                int(embed_dim * mlp_ratio),\n                dropout, attention_dropout\n            )\n            for _ in range(depth)\n        ])\n\n        # \u5206\u985e\u30d8\u30c3\u30c9\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        # \u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\n        x = self.patch_embed(x)\n\n        # Transformer \u30d6\u30ed\u30c3\u30af\n        for block in self.transformer:\n            x = block(x)\n\n        # \u5206\u985e\u7528\u306bCLS\u30c8\u30fc\u30af\u30f3\u3092\u4f7f\u7528\n        x = self.norm(x)\n        cls_token = x[:, 0]\n\n        # \u5206\u985e\n        return self.head(cls_token)\n</code></pre>"},{"location":"advanced/multimodal/#12-vision-transformer","title":"1.2 Vision Transformer \u306e\u8a13\u7df4","text":"<pre><code>class ViTTrainer:\n    \"\"\"Vision Transformer \u306e\u8a13\u7df4\"\"\"\n\n    def __init__(self, model, device='cuda'):\n        self.model = model.to(device)\n        self.device = device\n\n    def train(self, train_loader, val_loader, num_epochs=100, lr=1e-3):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.05)\n\n        # Cosine annealing with warmup\n        num_training_steps = num_epochs * len(train_loader)\n        num_warmup_steps = int(0.1 * num_training_steps)\n\n        scheduler = self.get_cosine_schedule_with_warmup(\n            optimizer, num_warmup_steps, num_training_steps\n        )\n\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(num_epochs):\n            # Training\n            self.model.train()\n            train_loss = 0.0\n            train_correct = 0\n            train_total = 0\n\n            for batch_idx, (images, labels) in enumerate(train_loader):\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                # Mixup augmentation\n                if np.random.random() &gt; 0.5:\n                    images, labels_a, labels_b, lam = self.mixup_data(images, labels)\n\n                    outputs = self.model(images)\n                    loss = lam * criterion(outputs, labels_a) + \\\n                           (1 - lam) * criterion(outputs, labels_b)\n                else:\n                    outputs = self.model(images)\n                    loss = criterion(outputs, labels)\n\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n\n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                train_total += labels.size(0)\n                train_correct += predicted.eq(labels).sum().item()\n\n            # Validation\n            val_acc = self.validate(val_loader)\n\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n            print(f\"Train Acc: {100.*train_correct/train_total:.2f}%\")\n            print(f\"Val Acc: {val_acc:.2f}%\")\n\n    def validate(self, val_loader):\n        self.model.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = self.model(images)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        return 100. * correct / total\n\n    @staticmethod\n    def mixup_data(x, y, alpha=0.2):\n        \"\"\"Mixup augmentation\"\"\"\n        lam = np.random.beta(alpha, alpha)\n        batch_size = x.size()[0]\n        index = torch.randperm(batch_size).to(x.device)\n\n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n\n        return mixed_x, y_a, y_b, lam\n\n    @staticmethod\n    def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n        def lr_lambda(current_step):\n            if current_step &lt; num_warmup_steps:\n                return float(current_step) / float(max(1, num_warmup_steps))\n            progress = float(current_step - num_warmup_steps) / \\\n                      float(max(1, num_training_steps - num_warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n</code></pre>"},{"location":"advanced/multimodal/#2-clip","title":"2. CLIP: \u30c6\u30ad\u30b9\u30c8\u3068\u753b\u50cf\u306e\u7d71\u5408","text":""},{"location":"advanced/multimodal/#21-clip","title":"2.1 CLIP \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3","text":"<pre><code>class CLIPModel(nn.Module):\n    \"\"\"CLIP (Contrastive Language-Image Pre-training) \u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(\n        self,\n        embed_dim=512,\n        # Vision\n        vision_width=768,\n        vision_layers=12,\n        vision_heads=12,\n        vision_patch_size=16,\n        image_size=224,\n        # Text\n        vocab_size=49408,\n        text_width=512,\n        text_layers=12,\n        text_heads=8,\n        text_max_length=77,\n    ):\n        super().__init__()\n\n        # Vision encoder\n        self.visual = VisionTransformer(\n            img_size=image_size,\n            patch_size=vision_patch_size,\n            embed_dim=vision_width,\n            depth=vision_layers,\n            num_heads=vision_heads,\n            num_classes=embed_dim,  # Project to shared embedding space\n        )\n\n        # Text encoder\n        self.text = TextTransformer(\n            vocab_size=vocab_size,\n            embed_dim=text_width,\n            num_layers=text_layers,\n            num_heads=text_heads,\n            max_length=text_max_length,\n        )\n\n        # Projection heads\n        self.visual_projection = nn.Linear(vision_width, embed_dim, bias=False)\n        self.text_projection = nn.Linear(text_width, embed_dim, bias=False)\n\n        # Temperature parameter\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def encode_image(self, image):\n        \"\"\"\u753b\u50cf\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\"\"\"\n        x = self.visual.patch_embed(image)\n\n        for block in self.visual.transformer:\n            x = block(x)\n\n        x = self.visual.norm(x)\n        x = x[:, 0]  # CLS\u30c8\u30fc\u30af\u30f3\n\n        # \u5171\u6709\u57cb\u3081\u8fbc\u307f\u7a7a\u9593\u306b\u6295\u5f71\n        x = self.visual_projection(x)\n        x = F.normalize(x, dim=-1)\n\n        return x\n\n    def encode_text(self, text):\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\"\"\"\n        x = self.text(text)\n\n        # EOS\u30c8\u30fc\u30af\u30f3\u306e\u4f4d\u7f6e\u3092\u53d6\u5f97\n        eos_indices = text.argmax(dim=-1)\n        x = x[torch.arange(x.shape[0]), eos_indices]\n\n        # \u5171\u6709\u57cb\u3081\u8fbc\u307f\u7a7a\u9593\u306b\u6295\u5f71\n        x = self.text_projection(x)\n        x = F.normalize(x, dim=-1)\n\n        return x\n\n    def forward(self, image, text):\n        \"\"\"\u753b\u50cf\u3068\u30c6\u30ad\u30b9\u30c8\u306e\u30da\u30a2\u306b\u5bfe\u3059\u308b\u640d\u5931\u3092\u8a08\u7b97\"\"\"\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # \u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\n        logit_scale = self.logit_scale.exp()\n        logits_per_image = logit_scale * image_features @ text_features.T\n        logits_per_text = logits_per_image.T\n\n        # \u5bfe\u7167\u5b66\u7fd2\u306e\u640d\u5931\n        batch_size = image.shape[0]\n        labels = torch.arange(batch_size, device=image.device)\n\n        loss_i = F.cross_entropy(logits_per_image, labels)\n        loss_t = F.cross_entropy(logits_per_text, labels)\n        loss = (loss_i + loss_t) / 2\n\n        return loss, logits_per_image\n\nclass TextTransformer(nn.Module):\n    \"\"\"CLIP\u7528\u306e\u30c6\u30ad\u30b9\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\"\"\"\n\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim=512,\n        num_layers=12,\n        num_heads=8,\n        max_length=77,\n    ):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.positional_embedding = nn.Parameter(\n            torch.empty(max_length, embed_dim)\n        )\n\n        self.transformer = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim * 4)\n            for _ in range(num_layers)\n        ])\n\n        self.ln_final = nn.LayerNorm(embed_dim)\n\n        # Initialize\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n    def forward(self, text):\n        x = self.token_embedding(text)\n        x = x + self.positional_embedding[:text.shape[1]]\n\n        # Causal mask for autoregressive\n        mask = torch.triu(torch.ones(text.shape[1], text.shape[1]), diagonal=1)\n        mask = mask.to(device=x.device, dtype=x.dtype)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        for block in self.transformer:\n            x = block(x, mask)\n\n        x = self.ln_final(x)\n\n        return x\n</code></pre>"},{"location":"advanced/multimodal/#22-clip","title":"2.2 CLIP \u306e\u8a13\u7df4\u3068\u5fdc\u7528","text":"<pre><code>class CLIPTrainer:\n    \"\"\"CLIP \u30e2\u30c7\u30eb\u306e\u8a13\u7df4\"\"\"\n\n    def __init__(self, model, device='cuda'):\n        self.model = model.to(device)\n        self.device = device\n\n    def train(self, dataloader, num_epochs=30, lr=5e-4):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.1)\n\n        for epoch in range(num_epochs):\n            self.model.train()\n            total_loss = 0\n\n            for batch_idx, (images, texts) in enumerate(dataloader):\n                images = images.to(self.device)\n                texts = texts.to(self.device)\n\n                loss, logits = self.model(images, texts)\n\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                optimizer.step()\n\n                total_loss += loss.item()\n\n                if batch_idx % 100 == 0:\n                    # \u7cbe\u5ea6\u3092\u8a08\u7b97\n                    accuracy = (logits.argmax(dim=1) == torch.arange(len(images), device=self.device)).float().mean()\n                    print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}\")\n\n            print(f\"Epoch {epoch} completed. Average Loss: {total_loss/len(dataloader):.4f}\")\n\nclass CLIPApplications:\n    \"\"\"CLIP \u306e\u5fdc\u7528\u4f8b\"\"\"\n\n    def __init__(self, model, device='cuda'):\n        self.model = model.to(device)\n        self.device = device\n        self.model.eval()\n\n    def zero_shot_classification(self, images, class_names):\n        \"\"\"\u30bc\u30ed\u30b7\u30e7\u30c3\u30c8\u753b\u50cf\u5206\u985e\"\"\"\n        # \u30af\u30e9\u30b9\u540d\u3092\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306b\u57cb\u3081\u8fbc\u3080\n        text_prompts = [f\"a photo of a {name}\" for name in class_names]\n\n        # \u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u5316\uff08\u4eee\u5b9a\uff09\n        text_tokens = self.tokenize(text_prompts)\n\n        with torch.no_grad():\n            # \u753b\u50cf\u3068\u30c6\u30ad\u30b9\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\n            image_features = self.model.encode_image(images)\n            text_features = self.model.encode_text(text_tokens)\n\n            # \u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\n            similarity = image_features @ text_features.T\n\n            # \u4e88\u6e2c\n            predictions = similarity.argmax(dim=1)\n\n        return predictions, similarity\n\n    def image_retrieval(self, query_text, image_database):\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u30af\u30a8\u30ea\u306b\u3088\u308b\u753b\u50cf\u691c\u7d22\"\"\"\n        # \u30c6\u30ad\u30b9\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\n        text_tokens = self.tokenize([query_text])\n\n        with torch.no_grad():\n            text_features = self.model.encode_text(text_tokens)\n\n            # \u3059\u3079\u3066\u306e\u753b\u50cf\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\n            all_image_features = []\n            for images in image_database:\n                image_features = self.model.encode_image(images)\n                all_image_features.append(image_features)\n\n            all_image_features = torch.cat(all_image_features, dim=0)\n\n            # \u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\n            similarities = text_features @ all_image_features.T\n\n            # Top-k \u3092\u53d6\u5f97\n            top_k = 10\n            top_indices = similarities.argsort(descending=True)[0, :top_k]\n\n        return top_indices, similarities[0, top_indices]\n\n    def tokenize(self, texts):\n        \"\"\"\u7c21\u6613\u7684\u306a\u30c8\u30fc\u30af\u30f3\u5316\uff08\u5b9f\u969b\u306fCLIP\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u4f7f\u7528\uff09\"\"\"\n        # \u30c0\u30df\u30fc\u5b9f\u88c5\n        return torch.randint(0, 49408, (len(texts), 77), device=self.device)\n</code></pre>"},{"location":"advanced/multimodal/#3-dall-e","title":"3. \u753b\u50cf\u751f\u6210: DALL-E \u30b9\u30bf\u30a4\u30eb","text":""},{"location":"advanced/multimodal/#31-vq-vae","title":"3.1 VQ-VAE: \u753b\u50cf\u306e\u96e2\u6563\u8868\u73fe","text":"<pre><code>class VectorQuantizer(nn.Module):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u91cf\u5b50\u5316\u5c64\"\"\"\n\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n        self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n\n    def forward(self, inputs):\n        # inputs: (B, C, H, W)\n        input_shape = inputs.shape\n        flat_input = inputs.view(-1, self.embedding_dim)\n\n        # \u6700\u8fd1\u508d\u3092\u898b\u3064\u3051\u308b\n        distances = (\n            torch.sum(flat_input**2, dim=1, keepdim=True)\n            + torch.sum(self.embedding.weight**2, dim=1)\n            - 2 * torch.matmul(flat_input, self.embedding.weight.t())\n        )\n\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n\n        # \u91cf\u5b50\u5316\n        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)\n\n        # \u52fe\u914d\u306e\u30b3\u30d4\u30fc\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n\n        quantized = inputs + (quantized - inputs).detach()\n\n        return quantized, loss, encoding_indices.view(input_shape[0], -1)\n\nclass VQVAE(nn.Module):\n    \"\"\"Vector Quantized VAE\"\"\"\n\n    def __init__(self, in_channels=3, hidden_dims=[128, 256], num_embeddings=512, embedding_dim=64):\n        super().__init__()\n\n        # Encoder\n        modules = []\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, h_dim, kernel_size=4, stride=2, padding=1),\n                    nn.BatchNorm2d(h_dim),\n                    nn.LeakyReLU()\n                )\n            )\n            in_channels = h_dim\n\n        modules.append(nn.Conv2d(hidden_dims[-1], embedding_dim, kernel_size=1))\n        self.encoder = nn.Sequential(*modules)\n\n        # Vector Quantizer\n        self.vq = VectorQuantizer(num_embeddings, embedding_dim)\n\n        # Decoder\n        modules = []\n        hidden_dims.reverse()\n\n        modules.append(nn.Conv2d(embedding_dim, hidden_dims[0], kernel_size=1))\n\n        for i in range(len(hidden_dims) - 1):\n            modules.append(\n                nn.Sequential(\n                    nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i + 1],\n                                     kernel_size=4, stride=2, padding=1),\n                    nn.BatchNorm2d(hidden_dims[i + 1]),\n                    nn.LeakyReLU()\n                )\n            )\n\n        modules.append(\n            nn.ConvTranspose2d(hidden_dims[-1], 3, kernel_size=4, stride=2, padding=1)\n        )\n\n        self.decoder = nn.Sequential(*modules)\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        quantized, vq_loss, indices = self.vq(encoded)\n        decoded = self.decoder(quantized)\n\n        return decoded, vq_loss, indices\n</code></pre>"},{"location":"advanced/multimodal/#32","title":"3.2 \u30c6\u30ad\u30b9\u30c8\u6761\u4ef6\u4ed8\u304d\u753b\u50cf\u751f\u6210","text":"<pre><code>class TextToImageTransformer(nn.Module):\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u753b\u50cf\u30c8\u30fc\u30af\u30f3\u3092\u751f\u6210\u3059\u308bTransformer\"\"\"\n\n    def __init__(\n        self,\n        text_vocab_size,\n        image_vocab_size,\n        d_model=512,\n        nhead=8,\n        num_layers=12,\n        max_text_len=256,\n        max_image_len=1024,\n    ):\n        super().__init__()\n\n        self.d_model = d_model\n        self.image_vocab_size = image_vocab_size\n\n        # \u30c6\u30ad\u30b9\u30c8\u3068\u753b\u50cf\u306e\u57cb\u3081\u8fbc\u307f\n        self.text_embedding = nn.Embedding(text_vocab_size, d_model)\n        self.image_embedding = nn.Embedding(image_vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\n        self.text_pos_embedding = nn.Parameter(torch.randn(1, max_text_len, d_model))\n        self.image_pos_embedding = nn.Parameter(torch.randn(1, max_image_len, d_model))\n\n        # Transformer\n        self.transformer = nn.ModuleList([\n            TransformerBlock(d_model, nhead, d_model * 4)\n            for _ in range(num_layers)\n        ])\n\n        # \u51fa\u529b\u30d8\u30c3\u30c9\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, image_vocab_size)\n\n    def forward(self, text_tokens, image_tokens=None):\n        \"\"\"\n        text_tokens: (B, text_len)\n        image_tokens: (B, image_len) - \u8a13\u7df4\u6642\u306e\u307f\n        \"\"\"\n        device = text_tokens.device\n        batch_size = text_tokens.shape[0]\n\n        # \u30c6\u30ad\u30b9\u30c8\u306e\u57cb\u3081\u8fbc\u307f\n        text_emb = self.text_embedding(text_tokens)\n        text_emb = text_emb + self.text_pos_embedding[:, :text_tokens.shape[1]]\n\n        if image_tokens is not None:\n            # \u8a13\u7df4\u6642: Teacher forcing\n            image_emb = self.image_embedding(image_tokens)\n            image_emb = image_emb + self.image_pos_embedding[:, :image_tokens.shape[1]]\n\n            # \u30c6\u30ad\u30b9\u30c8\u3068\u753b\u50cf\u3092\u7d50\u5408\n            x = torch.cat([text_emb, image_emb], dim=1)\n\n            # Causal mask\n            total_len = x.shape[1]\n            mask = torch.ones(total_len, total_len, device=device)\n            # \u30c6\u30ad\u30b9\u30c8\u90e8\u5206\u306f\u5168\u3066\u898b\u3048\u308b\n            text_len = text_tokens.shape[1]\n            mask[:text_len, :text_len] = 0\n            # \u753b\u50cf\u90e8\u5206\u306f\u56e0\u679c\u7684\n            mask[text_len:, text_len:] = torch.triu(\n                torch.ones(total_len - text_len, total_len - text_len, device=device),\n                diagonal=1\n            )\n            mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        else:\n            # \u63a8\u8ad6\u6642\n            x = text_emb\n            mask = None\n\n        # Transformer layers\n        for block in self.transformer:\n            x = block(x, mask)\n\n        # \u51fa\u529b\n        x = self.ln_f(x)\n        logits = self.head(x)\n\n        if image_tokens is not None:\n            # \u753b\u50cf\u90e8\u5206\u306e\u30ed\u30b8\u30c3\u30c8\u306e\u307f\u8fd4\u3059\n            return logits[:, text_tokens.shape[1]:]\n        else:\n            return logits\n\n    @torch.no_grad()\n    def generate(self, text_tokens, max_length=256, temperature=1.0, top_k=100):\n        \"\"\"\u753b\u50cf\u30c8\u30fc\u30af\u30f3\u3092\u751f\u6210\"\"\"\n        self.eval()\n        device = text_tokens.device\n        batch_size = text_tokens.shape[0]\n\n        # \u958b\u59cb\u30c8\u30fc\u30af\u30f3\n        generated = []\n\n        # \u30c6\u30ad\u30b9\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        text_emb = self.text_embedding(text_tokens)\n        text_emb = text_emb + self.text_pos_embedding[:, :text_tokens.shape[1]]\n\n        x = text_emb\n\n        for i in range(max_length):\n            # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3092\u8ffd\u52a0\n            if i &gt; 0:\n                image_tokens_so_far = torch.stack(generated, dim=1)\n                image_emb = self.image_embedding(image_tokens_so_far)\n                image_emb = image_emb + self.image_pos_embedding[:, :i]\n                x = torch.cat([text_emb, image_emb], dim=1)\n\n            # Forward pass\n            for block in self.transformer:\n                x_out = block(x)\n\n            x_out = self.ln_f(x_out)\n            logits = self.head(x_out[:, -1])  # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u307f\n\n            # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n            if temperature &gt; 0:\n                logits = logits / temperature\n\n                if top_k &gt; 0:\n                    # Top-k sampling\n                    top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n                    logits = torch.full_like(logits, float('-inf'))\n                    logits.scatter_(1, top_k_indices, top_k_logits)\n\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n            else:\n                # Greedy\n                next_token = torch.argmax(logits, dim=-1)\n\n            generated.append(next_token)\n\n        return torch.stack(generated, dim=1)\n</code></pre>"},{"location":"advanced/multimodal/#4-flamingo","title":"4. Flamingo: \u5c11\u6570\u30b7\u30e7\u30c3\u30c8\u5b66\u7fd2","text":""},{"location":"advanced/multimodal/#41-perceiver-resampler","title":"4.1 Perceiver Resampler","text":"<pre><code>class PerceiverResampler(nn.Module):\n    \"\"\"\u53ef\u5909\u9577\u306e\u8996\u899a\u7279\u5fb4\u3092\u56fa\u5b9a\u9577\u306b\u5909\u63db\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        depth=6,\n        num_latents=64,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n    ):\n        super().__init__()\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                # Cross attention (latents -&gt; visual features)\n                CrossAttention(dim, heads=heads, dim_head=dim_head),\n                nn.LayerNorm(dim),\n                # Self attention (latents)\n                SelfAttention(dim, heads=heads, dim_head=dim_head),\n                nn.LayerNorm(dim),\n                # FFN\n                FeedForward(dim, mult=ff_mult),\n                nn.LayerNorm(dim)\n            ]))\n\n    def forward(self, x):\n        \"\"\"\n        x: visual features (B, N, D)\n        returns: resampled features (B, num_latents, D)\n        \"\"\"\n        b = x.shape[0]\n\n        # Repeat latents for batch\n        latents = repeat(self.latents, 'n d -&gt; b n d', b=b)\n\n        for cross_attn, cross_norm, self_attn, self_norm, ff, ff_norm in self.layers:\n            # Cross attention\n            latents = latents + cross_attn(latents, context=x)\n            latents = cross_norm(latents)\n\n            # Self attention\n            latents = latents + self_attn(latents)\n            latents = self_norm(latents)\n\n            # FFN\n            latents = latents + ff(latents)\n            latents = ff_norm(latents)\n\n        return latents\n\nclass CrossAttention(nn.Module):\n    \"\"\"\u30af\u30ed\u30b9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\"\"\"\n\n    def __init__(self, dim, heads=8, dim_head=64):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x, context):\n        h = self.heads\n\n        q = self.to_q(x)\n        k, v = self.to_kv(context).chunk(2, dim=-1)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h=h), (q, k, v))\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        attn = dots.softmax(dim=-1)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -&gt; b n (h d)')\n\n        return self.to_out(out)\n</code></pre>"},{"location":"advanced/multimodal/#42-flamingo","title":"4.2 Flamingo \u30e2\u30c7\u30eb","text":"<pre><code>class FlamingoModel(nn.Module):\n    \"\"\"Flamingo: \u5c11\u6570\u30b7\u30e7\u30c3\u30c8\u30d3\u30b8\u30e7\u30f3\u8a00\u8a9e\u30e2\u30c7\u30eb\"\"\"\n\n    def __init__(\n        self,\n        vision_encoder,\n        language_model,\n        dim_visual=768,\n        dim_text=768,\n        resampler_depth=6,\n        resampler_num_latents=64,\n    ):\n        super().__init__()\n\n        self.vision_encoder = vision_encoder\n        self.perceiver_resampler = PerceiverResampler(\n            dim=dim_visual,\n            depth=resampler_depth,\n            num_latents=resampler_num_latents,\n        )\n\n        # Frozen language model with gated cross-attention\n        self.language_model = language_model\n        self._freeze_lm()\n        self._add_gated_cross_attention()\n\n        # Visual projection\n        self.visual_projection = nn.Linear(dim_visual, dim_text)\n\n    def _freeze_lm(self):\n        \"\"\"\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u51cd\u7d50\"\"\"\n        for param in self.language_model.parameters():\n            param.requires_grad = False\n\n    def _add_gated_cross_attention(self):\n        \"\"\"\u30b2\u30fc\u30c8\u4ed8\u304d\u30af\u30ed\u30b9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0\"\"\"\n        for i, layer in enumerate(self.language_model.transformer.layers):\n            layer.gated_cross_attention = GatedCrossAttention(\n                layer.hidden_size,\n                layer.num_attention_heads\n            )\n\n    def forward(self, images, text_tokens, image_positions):\n        \"\"\"\n        images: List of images (different sizes ok)\n        text_tokens: (B, L)\n        image_positions: positions in text where images should be attended to\n        \"\"\"\n        # Process all images\n        visual_features = []\n        for img in images:\n            # Vision encoder\n            vis_feat = self.vision_encoder(img.unsqueeze(0))\n            # Perceiver resampler\n            vis_feat = self.perceiver_resampler(vis_feat)\n            # Project to text dimension\n            vis_feat = self.visual_projection(vis_feat)\n            visual_features.append(vis_feat)\n\n        # Concatenate all visual features\n        visual_features = torch.cat(visual_features, dim=0)\n\n        # Run language model with cross-attention to visual features\n        output = self.language_model(\n            text_tokens,\n            visual_features=visual_features,\n            image_positions=image_positions\n        )\n\n        return output\n\nclass GatedCrossAttention(nn.Module):\n    \"\"\"\u30b2\u30fc\u30c8\u4ed8\u304d\u30af\u30ed\u30b9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\"\"\"\n\n    def __init__(self, hidden_size, num_heads):\n        super().__init__()\n        self.cross_attention = nn.MultiheadAttention(\n            hidden_size,\n            num_heads,\n            batch_first=True\n        )\n        self.gate = nn.Parameter(torch.zeros(1))\n        self.norm = nn.LayerNorm(hidden_size)\n\n    def forward(self, x, visual_features, attention_mask=None):\n        # Cross attention\n        residual = x\n        x = self.norm(x)\n        x, _ = self.cross_attention(\n            x, visual_features, visual_features,\n            key_padding_mask=attention_mask\n        )\n\n        # Gated residual connection\n        x = residual + self.gate.tanh() * x\n\n        return x\n</code></pre>"},{"location":"advanced/multimodal/#5","title":"5. \u5b9f\u8df5\u7684\u306a\u5fdc\u7528","text":""},{"location":"advanced/multimodal/#51","title":"5.1 \u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u691c\u7d22\u30b7\u30b9\u30c6\u30e0","text":"<pre><code>class MultimodalSearchEngine:\n    \"\"\"\u753b\u50cf\u3068\u30c6\u30ad\u30b9\u30c8\u306e\u7d71\u5408\u691c\u7d22\"\"\"\n\n    def __init__(self, model, index_size=100000):\n        self.model = model\n        self.index_size = index_size\n\n        # Feature database\n        self.image_features = torch.zeros(index_size, 512)\n        self.text_features = torch.zeros(index_size, 512)\n        self.metadata = {}\n        self.current_idx = 0\n\n    def index_multimodal_data(self, images, texts, metadata):\n        \"\"\"\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u30c7\u30fc\u30bf\u3092\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\"\"\"\n        with torch.no_grad():\n            # Extract features\n            img_feats = self.model.encode_image(images)\n            txt_feats = self.model.encode_text(texts)\n\n            # Store in database\n            batch_size = len(images)\n            self.image_features[self.current_idx:self.current_idx + batch_size] = img_feats\n            self.text_features[self.current_idx:self.current_idx + batch_size] = txt_feats\n\n            # Store metadata\n            for i, meta in enumerate(metadata):\n                self.metadata[self.current_idx + i] = meta\n\n            self.current_idx += batch_size\n\n    def search(self, query, modality='text', top_k=10):\n        \"\"\"\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u691c\u7d22\"\"\"\n        with torch.no_grad():\n            if modality == 'text':\n                query_feat = self.model.encode_text(query)\n                # Search in both image and text features\n                img_scores = query_feat @ self.image_features[:self.current_idx].T\n                txt_scores = query_feat @ self.text_features[:self.current_idx].T\n                scores = (img_scores + txt_scores) / 2\n            else:  # image query\n                query_feat = self.model.encode_image(query)\n                img_scores = query_feat @ self.image_features[:self.current_idx].T\n                txt_scores = query_feat @ self.text_features[:self.current_idx].T\n                scores = (img_scores + txt_scores) / 2\n\n            # Get top-k results\n            top_scores, top_indices = scores.topk(top_k, dim=1)\n\n            results = []\n            for i in range(top_indices.shape[0]):\n                batch_results = []\n                for j in range(top_k):\n                    idx = top_indices[i, j].item()\n                    batch_results.append({\n                        'score': top_scores[i, j].item(),\n                        'metadata': self.metadata[idx]\n                    })\n                results.append(batch_results)\n\n        return results\n</code></pre>"},{"location":"advanced/multimodal/#_2","title":"\u307e\u3068\u3081","text":"<p>\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30ebTransformer\u306f\u3001\u7570\u306a\u308b\u30e2\u30c0\u30ea\u30c6\u30a3\u9593\u306e\u95a2\u4fc2\u3092\u5b66\u7fd2\u3057\u3001\u7d71\u4e00\u7684\u306b\u51e6\u7406\u3059\u308b\u5f37\u529b\u306a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u3059\u3002\u4e3b\u8981\u306a\u6982\u5ff5\uff1a</p> <ol> <li>\u30e2\u30c0\u30ea\u30c6\u30a3\u56fa\u6709\u306e\u30a8\u30f3\u30b3\u30fc\u30c0: \u5404\u30e2\u30c0\u30ea\u30c6\u30a3\u306b\u9069\u3057\u305f\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</li> <li>\u5171\u6709\u57cb\u3081\u8fbc\u307f\u7a7a\u9593: \u7570\u306a\u308b\u30e2\u30c0\u30ea\u30c6\u30a3\u3092\u540c\u3058\u7a7a\u9593\u3067\u8868\u73fe</li> <li>\u30af\u30ed\u30b9\u30e2\u30fc\u30c0\u30eb\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3: \u30e2\u30c0\u30ea\u30c6\u30a3\u9593\u306e\u76f8\u4e92\u4f5c\u7528</li> <li>\u5bfe\u7167\u5b66\u7fd2: \u30e2\u30c0\u30ea\u30c6\u30a3\u9593\u306e\u5bfe\u5fdc\u95a2\u4fc2\u3092\u5b66\u7fd2</li> </ol> <p>\u3053\u308c\u3089\u306e\u6280\u8853\u306b\u3088\u308a\u3001\u753b\u50cf\u7406\u89e3\u3001\u753b\u50cf\u751f\u6210\u3001\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u691c\u7d22\u306a\u3069\u3001\u69d8\u3005\u306a\u5fdc\u7528\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002</p>"},{"location":"advanced/optimization/","title":"\u6700\u9069\u5316\u6280\u8853\u3068\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9","text":""},{"location":"advanced/optimization/#transformer","title":"\u306f\u3058\u3081\u306b\uff1a\u52b9\u7387\u7684\u306aTransformer\u306e\u5b9f\u88c5","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u3092\u8003\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002<code>-O0</code>\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u305f\u30b3\u30fc\u30c9\u3068<code>-O3</code>\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u305f\u30b3\u30fc\u30c9\u3067\u306f\u3001\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u304c\u6570\u500d\u304b\u3089\u6570\u5341\u500d\u9055\u3044\u307e\u3059\u3002\u540c\u69d8\u306b\u3001Transformer\u3082\u9069\u5207\u306a\u6700\u9069\u5316\u306b\u3088\u308a\u5287\u7684\u306b\u9ad8\u901f\u5316\u3067\u304d\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u5b9f\u8df5\u7684\u306a\u6700\u9069\u5316\u6280\u8853\u3068\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"advanced/optimization/#1","title":"1. \u30e1\u30e2\u30ea\u6700\u9069\u5316","text":""},{"location":"advanced/optimization/#11-gradient-checkpointing","title":"1.1 Gradient Checkpointing","text":"<p>\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u524a\u6e1b\u3059\u308b\u4ee3\u308f\u308a\u306b\u8a08\u7b97\u6642\u9593\u304c\u5897\u52a0\u3059\u308b\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u6280\u8853\u3067\u3059\u3002</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass OptimizedTransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None, use_checkpoint=True):\n        # Gradient checkpointing\u3092\u4f7f\u7528\n        if use_checkpoint and self.training:\n            # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u90e8\u5206\n            def attention_block(x, mask):\n                return self.dropout(self.attention(self.norm1(x), mask))\n\n            attn_output = checkpoint(attention_block, x, mask)\n            x = x + attn_output\n\n            # Feed-forward\u90e8\u5206\n            def ff_block(x):\n                return self.dropout(self.feed_forward(self.norm2(x)))\n\n            ff_output = checkpoint(ff_block, x)\n            x = x + ff_output\n        else:\n            # \u901a\u5e38\u306e\u8a08\u7b97\n            x = x + self.dropout(self.attention(self.norm1(x), mask))\n            x = x + self.dropout(self.feed_forward(self.norm2(x)))\n\n        return x\n\n# \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\ndef compare_memory_usage():\n    model = OptimizedTransformerBlock(512, 8, 2048)\n    x = torch.randn(32, 100, 512, requires_grad=True)\n\n    # \u901a\u5e38\u306e\u8a08\u7b97\n    torch.cuda.reset_peak_memory_stats()\n    output1 = model(x, use_checkpoint=False)\n    loss1 = output1.sum()\n    loss1.backward()\n    memory_without_checkpoint = torch.cuda.max_memory_allocated() / 1024**2\n\n    # Gradient checkpointing\n    model.zero_grad()\n    x.grad = None\n    torch.cuda.reset_peak_memory_stats()\n    output2 = model(x, use_checkpoint=True)\n    loss2 = output2.sum()\n    loss2.backward()\n    memory_with_checkpoint = torch.cuda.max_memory_allocated() / 1024**2\n\n    print(f\"Memory without checkpoint: {memory_without_checkpoint:.2f} MB\")\n    print(f\"Memory with checkpoint: {memory_with_checkpoint:.2f} MB\")\n    print(f\"Memory saved: {(1 - memory_with_checkpoint/memory_without_checkpoint)*100:.1f}%\")\n</code></pre>"},{"location":"advanced/optimization/#12","title":"1.2 \u52b9\u7387\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5b9f\u88c5","text":"<p>Flash Attention\u3084Memory-Efficient Attention\u306e\u5b9f\u88c5\uff1a</p> <pre><code>class FlashAttention(nn.Module):\n    \"\"\"Flash Attention\u306e\u7c21\u6613\u5b9f\u88c5\uff08\u6982\u5ff5\u7684\uff09\"\"\"\n\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.scale = 1.0 / math.sqrt(self.d_k)\n\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        B, T, C = x.shape\n\n        # QKV\u3092\u4e00\u5ea6\u306b\u8a08\u7b97\uff08\u30e1\u30e2\u30ea\u30a2\u30af\u30bb\u30b9\u3092\u524a\u6e1b\uff09\n        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.d_k)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, T, D]\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        # Flash Attention\u306e\u6838\u5fc3\uff1a\u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u8a08\u7b97\n        # \u5b9f\u969b\u306e\u5b9f\u88c5\u306fCUDA\u30ab\u30fc\u30cd\u30eb\u3067\u884c\u308f\u308c\u308b\n        if torch.cuda.is_available() and hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n            # PyTorch 2.0+\u306eFlash Attention\n            attn_output = torch.nn.functional.scaled_dot_product_attention(\n                q, k, v, \n                attn_mask=mask,\n                dropout_p=self.dropout.p if self.training else 0.0,\n                is_causal=True if mask is None else False\n            )\n        else:\n            # \u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\u5b9f\u88c5\n            attn_output = self._manual_attention(q, k, v, mask)\n\n        # \u51fa\u529b\u3092\u6574\u5f62\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n        return self.out_proj(attn_output)\n\n    def _manual_attention(self, q, k, v, mask=None):\n        \"\"\"\u624b\u52d5\u5b9f\u88c5\uff08\u6559\u80b2\u76ee\u7684\uff09\"\"\"\n        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        return torch.matmul(attn_weights, v)\n</code></pre>"},{"location":"advanced/optimization/#2","title":"2. \u8a08\u7b97\u6700\u9069\u5316","text":""},{"location":"advanced/optimization/#21-mixed-precision-training","title":"2.1 Mixed Precision Training","text":"<p>\u534a\u7cbe\u5ea6\u6d6e\u52d5\u5c0f\u6570\u70b9\u3092\u4f7f\u7528\u3057\u3066\u8a08\u7b97\u3092\u9ad8\u901f\u5316\uff1a</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\n\nclass MixedPrecisionTrainer:\n    def __init__(self, model, optimizer):\n        self.model = model\n        self.optimizer = optimizer\n        self.scaler = GradScaler()\n\n    def train_step(self, batch):\n        self.optimizer.zero_grad()\n\n        # \u81ea\u52d5\u6df7\u5408\u7cbe\u5ea6\n        with autocast():\n            outputs = self.model(batch['input_ids'])\n            loss = self.compute_loss(outputs, batch['labels'])\n\n        # \u30b9\u30b1\u30fc\u30eb\u3055\u308c\u305f\u9006\u4f1d\u64ad\n        self.scaler.scale(loss).backward()\n\n        # \u52fe\u914d\u306e\u30a2\u30f3\u30b9\u30b1\u30fc\u30eb\u3068\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        self.scaler.unscale_(self.optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n        # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30b9\u30c6\u30c3\u30d7\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        return loss.item()\n\n    def compute_loss(self, outputs, labels):\n        return torch.nn.functional.cross_entropy(\n            outputs.view(-1, outputs.size(-1)),\n            labels.view(-1)\n        )\n\n# \u901f\u5ea6\u6bd4\u8f03\ndef benchmark_mixed_precision():\n    import time\n\n    model = TransformerModel(vocab_size=50000, d_model=512, n_heads=8, n_layers=6)\n    model = model.cuda()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n    batch = {\n        'input_ids': torch.randint(0, 50000, (32, 128)).cuda(),\n        'labels': torch.randint(0, 50000, (32, 128)).cuda()\n    }\n\n    # \u901a\u5e38\u306e\u7cbe\u5ea6\n    start_time = time.time()\n    for _ in range(100):\n        optimizer.zero_grad()\n        outputs = model(batch['input_ids'])\n        loss = torch.nn.functional.cross_entropy(\n            outputs.view(-1, outputs.size(-1)),\n            batch['labels'].view(-1)\n        )\n        loss.backward()\n        optimizer.step()\n    fp32_time = time.time() - start_time\n\n    # Mixed precision\n    trainer = MixedPrecisionTrainer(model, optimizer)\n    start_time = time.time()\n    for _ in range(100):\n        trainer.train_step(batch)\n    amp_time = time.time() - start_time\n\n    print(f\"FP32 time: {fp32_time:.2f}s\")\n    print(f\"AMP time: {amp_time:.2f}s\")\n    print(f\"Speedup: {fp32_time/amp_time:.2f}x\")\n</code></pre>"},{"location":"advanced/optimization/#22","title":"2.2 \u52b9\u7387\u7684\u306a\u30d0\u30c3\u30c1\u51e6\u7406","text":"<p>\u52d5\u7684\u30d1\u30c7\u30a3\u30f3\u30b0\u3068\u30d0\u30b1\u30c3\u30c6\u30a3\u30f3\u30b0\uff1a</p> <pre><code>class EfficientDataLoader:\n    \"\"\"\u52b9\u7387\u7684\u306a\u30d0\u30c3\u30c1\u51e6\u7406\u306e\u305f\u3081\u306e\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\"\"\"\n\n    def __init__(self, dataset, batch_size, bucket_size=1000):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.bucket_size = bucket_size\n\n    def __iter__(self):\n        # \u9577\u3055\u3067\u30bd\u30fc\u30c8\n        indices = list(range(len(self.dataset)))\n        indices.sort(key=lambda i: len(self.dataset[i]['input_ids']))\n\n        # \u30d0\u30b1\u30c3\u30c8\u306b\u5206\u5272\n        buckets = []\n        for i in range(0, len(indices), self.bucket_size):\n            bucket = indices[i:i + self.bucket_size]\n            # \u30d0\u30b1\u30c3\u30c8\u5185\u3067\u30b7\u30e3\u30c3\u30d5\u30eb\uff08\u591a\u69d8\u6027\u3092\u4fdd\u3064\uff09\n            random.shuffle(bucket)\n            buckets.append(bucket)\n\n        # \u30d0\u30b1\u30c3\u30c8\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\n        random.shuffle(buckets)\n\n        # \u30d0\u30c3\u30c1\u3092\u751f\u6210\n        for bucket in buckets:\n            for i in range(0, len(bucket), self.batch_size):\n                batch_indices = bucket[i:i + self.batch_size]\n                yield self._collate_batch(batch_indices)\n\n    def _collate_batch(self, indices):\n        \"\"\"\u52d5\u7684\u30d1\u30c7\u30a3\u30f3\u30b0\u3067\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\"\"\"\n        batch = [self.dataset[i] for i in indices]\n\n        # \u6700\u5927\u9577\u3092\u898b\u3064\u3051\u308b\n        max_len = max(len(item['input_ids']) for item in batch)\n\n        # \u30d1\u30c7\u30a3\u30f3\u30b0\n        input_ids = []\n        attention_mask = []\n\n        for item in batch:\n            ids = item['input_ids']\n            pad_len = max_len - len(ids)\n\n            input_ids.append(ids + [0] * pad_len)\n            attention_mask.append([1] * len(ids) + [0] * pad_len)\n\n        return {\n            'input_ids': torch.tensor(input_ids),\n            'attention_mask': torch.tensor(attention_mask)\n        }\n</code></pre>"},{"location":"advanced/optimization/#3","title":"3. \u5206\u6563\u5b66\u7fd2","text":""},{"location":"advanced/optimization/#31-data-parallel","title":"3.1 Data Parallel","text":"<p>\u8907\u6570GPU\u3067\u306e\u4e26\u5217\u5b66\u7fd2\uff1a</p> <pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\n\nclass DistributedTrainer:\n    def __init__(self, model, rank, world_size):\n        self.rank = rank\n        self.world_size = world_size\n\n        # \u5206\u6563\u74b0\u5883\u306e\u521d\u671f\u5316\n        dist.init_process_group(\n            backend='nccl',\n            init_method='env://',\n            world_size=world_size,\n            rank=rank\n        )\n\n        # \u30e2\u30c7\u30eb\u3092GPU\u306b\u914d\u7f6e\n        torch.cuda.set_device(rank)\n        model = model.cuda(rank)\n\n        # DDP\u3067\u30e9\u30c3\u30d7\n        self.model = DDP(model, device_ids=[rank])\n\n    def train(self, dataset, epochs=10):\n        # \u5206\u6563\u30b5\u30f3\u30d7\u30e9\u30fc\n        sampler = DistributedSampler(\n            dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=True\n        )\n\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=32,\n            sampler=sampler,\n            num_workers=4,\n            pin_memory=True\n        )\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n\n        for epoch in range(epochs):\n            # \u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u30b5\u30f3\u30d7\u30e9\u30fc\u3092\u66f4\u65b0\n            sampler.set_epoch(epoch)\n\n            for batch in dataloader:\n                loss = self.train_step(batch, optimizer)\n\n                # \u3059\u3079\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u3067\u540c\u671f\n                if self.rank == 0:\n                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n    def train_step(self, batch, optimizer):\n        optimizer.zero_grad()\n\n        outputs = self.model(batch['input_ids'].cuda(self.rank))\n        loss = compute_loss(outputs, batch['labels'].cuda(self.rank))\n\n        loss.backward()\n\n        # \u52fe\u914d\u3092\u5168\u30d7\u30ed\u30bb\u30b9\u3067\u5e73\u5747\n        for param in self.model.parameters():\n            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n            param.grad.data /= self.world_size\n\n        optimizer.step()\n\n        return loss.item()\n\n# \u4f7f\u7528\u4f8b\uff08\u30de\u30eb\u30c1\u30d7\u30ed\u30bb\u30b9\u3067\u5b9f\u884c\uff09\ndef run_distributed_training(rank, world_size):\n    model = TransformerModel(vocab_size=50000, d_model=512)\n    trainer = DistributedTrainer(model, rank, world_size)\n    trainer.train(dataset)\n</code></pre>"},{"location":"advanced/optimization/#32-model-parallel","title":"3.2 Model Parallel","text":"<p>\u30e2\u30c7\u30eb\u3092\u8907\u6570GPU\u306b\u5206\u5272\uff1a</p> <pre><code>class ModelParallelTransformer(nn.Module):\n    \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u4e26\u5217\u306e\u4f8b\"\"\"\n\n    def __init__(self, vocab_size, d_model, n_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model).cuda(0)\n\n        # \u5c64\u30922\u3064\u306eGPU\u306b\u5206\u5272\n        self.layers_gpu0 = nn.ModuleList([\n            TransformerBlock(d_model) for _ in range(n_layers // 2)\n        ]).cuda(0)\n\n        self.layers_gpu1 = nn.ModuleList([\n            TransformerBlock(d_model) for _ in range(n_layers // 2)\n        ]).cuda(1)\n\n        self.output_proj = nn.Linear(d_model, vocab_size).cuda(1)\n\n    def forward(self, x):\n        # GPU 0\u3067\u306e\u8a08\u7b97\n        x = self.embedding(x)\n        for layer in self.layers_gpu0:\n            x = layer(x)\n\n        # GPU 1\u306b\u8ee2\u9001\n        x = x.cuda(1)\n\n        # GPU 1\u3067\u306e\u8a08\u7b97\n        for layer in self.layers_gpu1:\n            x = layer(x)\n\n        return self.output_proj(x)\n</code></pre>"},{"location":"advanced/optimization/#4","title":"4. \u63a8\u8ad6\u6700\u9069\u5316","text":""},{"location":"advanced/optimization/#41-kv","title":"4.1 KV\u30ad\u30e3\u30c3\u30b7\u30e5","text":"<p>\u751f\u6210\u6642\u306e\u8a08\u7b97\u3092\u524a\u6e1b\uff1a</p> <pre><code>class CachedAttention(nn.Module):\n    \"\"\"KV\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u4f7f\u7528\u3057\u305f\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\"\"\"\n\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        # \u30ad\u30e3\u30c3\u30b7\u30e5\n        self.cache_k = None\n        self.cache_v = None\n\n    def forward(self, x, use_cache=False):\n        B, T, C = x.shape\n\n        # \u65b0\u3057\u3044\u30af\u30a8\u30ea\n        q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n\n        if use_cache and self.cache_k is not None:\n            # \u30ad\u30e3\u30c3\u30b7\u30e5\u3055\u308c\u305fKV\u3092\u4f7f\u7528\n            k_new = self.W_k(x[:, -1:, :]).view(B, 1, self.n_heads, self.d_k).transpose(1, 2)\n            v_new = self.W_v(x[:, -1:, :]).view(B, 1, self.n_heads, self.d_k).transpose(1, 2)\n\n            k = torch.cat([self.cache_k, k_new], dim=2)\n            v = torch.cat([self.cache_v, v_new], dim=2)\n\n            # \u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u66f4\u65b0\n            self.cache_k = k\n            self.cache_v = v\n\n            # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u30af\u30a8\u30ea\u306e\u307f\u4f7f\u7528\n            q = q[:, :, -1:, :]\n        else:\n            # \u901a\u5e38\u306e\u8a08\u7b97\n            k = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n            v = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n\n            if use_cache:\n                self.cache_k = k\n                self.cache_v = v\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn_weights = torch.softmax(scores, dim=-1)\n        context = torch.matmul(attn_weights, v)\n\n        # \u51fa\u529b\n        context = context.transpose(1, 2).contiguous().view(B, -1, C)\n        return self.W_o(context)\n\n    def clear_cache(self):\n        \"\"\"\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u30af\u30ea\u30a2\"\"\"\n        self.cache_k = None\n        self.cache_v = None\n</code></pre>"},{"location":"advanced/optimization/#42","title":"4.2 \u91cf\u5b50\u5316","text":"<p>\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3068\u8a08\u7b97\u91cf\u3092\u524a\u6e1b\uff1a</p> <pre><code>def quantize_model(model, bits=8):\n    \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u91cf\u5b50\u5316\u306e\u4f8b\"\"\"\n\n    class QuantizedLinear(nn.Module):\n        def __init__(self, weight, bias, bits=8):\n            super().__init__()\n            self.bits = bits\n\n            # \u91cd\u307f\u3092\u91cf\u5b50\u5316\n            self.scale = weight.abs().max() / (2**(bits-1) - 1)\n            self.zero_point = 0\n\n            self.weight_int = torch.round(weight / self.scale).to(torch.int8)\n            self.bias = bias\n\n        def forward(self, x):\n            # \u9006\u91cf\u5b50\u5316\u3057\u3066\u8a08\u7b97\n            weight = self.weight_int.float() * self.scale\n            return torch.nn.functional.linear(x, weight, self.bias)\n\n    # \u3059\u3079\u3066\u306eLinear\u5c64\u3092\u91cf\u5b50\u5316\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # \u89aa\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u53d6\u5f97\n            parent_name = '.'.join(name.split('.')[:-1])\n            child_name = name.split('.')[-1]\n            parent = model\n\n            if parent_name:\n                for part in parent_name.split('.'):\n                    parent = getattr(parent, part)\n\n            # \u91cf\u5b50\u5316\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u7f6e\u63db\n            quantized = QuantizedLinear(\n                module.weight.data,\n                module.bias.data if module.bias is not None else None,\n                bits=bits\n            )\n            setattr(parent, child_name, quantized)\n\n    return model\n\n# \u91cf\u5b50\u5316\u306e\u52b9\u679c\u3092\u6e2c\u5b9a\ndef measure_quantization_impact():\n    model = TransformerModel(vocab_size=50000, d_model=512)\n\n    # \u30aa\u30ea\u30b8\u30ca\u30eb\u30e2\u30c7\u30eb\u306e\u30b5\u30a4\u30ba\n    original_size = sum(p.numel() * p.element_size() for p in model.parameters())\n\n    # \u91cf\u5b50\u5316\n    quantized_model = quantize_model(model, bits=8)\n\n    # \u91cf\u5b50\u5316\u5f8c\u306e\u30b5\u30a4\u30ba\uff08\u6982\u7b97\uff09\n    quantized_size = sum(\n        p.numel() if p.dtype == torch.int8 else p.numel() * p.element_size()\n        for p in quantized_model.parameters()\n    )\n\n    print(f\"Original size: {original_size / 1024**2:.2f} MB\")\n    print(f\"Quantized size: {quantized_size / 1024**2:.2f} MB\")\n    print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")\n</code></pre>"},{"location":"advanced/optimization/#5","title":"5. \u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\u3068\u30c7\u30d0\u30c3\u30b0","text":""},{"location":"advanced/optimization/#51","title":"5.1 \u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0","text":"<pre><code>import torch.profiler\n\ndef profile_model(model, input_data):\n    \"\"\"\u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\"\"\"\n\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n    ) as prof:\n        with torch.profiler.record_function(\"model_inference\"):\n            for _ in range(10):\n                output = model(input_data)\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n\n    # \u7d50\u679c\u306e\u5206\u6790\n    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n    # TensorBoard\u306b\u51fa\u529b\n    prof.export_chrome_trace(\"trace.json\")\n\n    # \u30dc\u30c8\u30eb\u30cd\u30c3\u30af\u306e\u7279\u5b9a\n    for event in prof.key_averages():\n        if event.cuda_time_total &gt; 1000000:  # 1ms\u4ee5\u4e0a\n            print(f\"Bottleneck: {event.key} - {event.cuda_time_total/1000:.2f}ms\")\n</code></pre>"},{"location":"advanced/optimization/#52","title":"5.2 \u30e1\u30e2\u30ea\u30ea\u30fc\u30af\u306e\u691c\u51fa","text":"<pre><code>class MemoryTracker:\n    \"\"\"\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u8ffd\u8de1\"\"\"\n\n    def __init__(self):\n        self.snapshots = []\n\n    def snapshot(self, label=\"\"):\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated() / 1024**2\n            reserved = torch.cuda.memory_reserved() / 1024**2\n        else:\n            allocated = reserved = 0\n\n        self.snapshots.append({\n            'label': label,\n            'allocated': allocated,\n            'reserved': reserved\n        })\n\n    def report(self):\n        print(\"=== Memory Usage Report ===\")\n        for i, snap in enumerate(self.snapshots):\n            print(f\"{i}: {snap['label']}\")\n            print(f\"   Allocated: {snap['allocated']:.2f} MB\")\n            print(f\"   Reserved: {snap['reserved']:.2f} MB\")\n\n            if i &gt; 0:\n                delta = snap['allocated'] - self.snapshots[i-1]['allocated']\n                if delta &gt; 0:\n                    print(f\"   Delta: +{delta:.2f} MB \u26a0\ufe0f\")\n\n# \u4f7f\u7528\u4f8b\ntracker = MemoryTracker()\ntracker.snapshot(\"Initial\")\n\nmodel = TransformerModel(vocab_size=50000, d_model=512)\ntracker.snapshot(\"After model creation\")\n\ndata = torch.randn(32, 100, 512)\ntracker.snapshot(\"After data creation\")\n\noutput = model(data)\ntracker.snapshot(\"After forward pass\")\n\nloss = output.sum()\nloss.backward()\ntracker.snapshot(\"After backward pass\")\n\ntracker.report()\n</code></pre>"},{"location":"advanced/optimization/#_2","title":"\u307e\u3068\u3081","text":"<p>Transformer\u306e\u6700\u9069\u5316\u306f\u3001\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u3068\u540c\u69d8\u306b\u3001\u591a\u5c64\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u5fc5\u8981\u3067\u3059\uff1a</p> <ol> <li>\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u30ec\u30d9\u30eb: Flash Attention\u3001\u52b9\u7387\u7684\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</li> <li>\u5b9f\u88c5\u30ec\u30d9\u30eb: Mixed Precision\u3001Gradient Checkpointing</li> <li>\u30b7\u30b9\u30c6\u30e0\u30ec\u30d9\u30eb: \u5206\u6563\u5b66\u7fd2\u3001\u30d0\u30c3\u30c1\u6700\u9069\u5316</li> <li>\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u30ec\u30d9\u30eb: \u91cf\u5b50\u5316\u3001\u30ab\u30fc\u30cd\u30eb\u878d\u5408</li> </ol> <p>\u3053\u308c\u3089\u306e\u6280\u8853\u3092\u9069\u5207\u306b\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001Transformer\u306e\u6027\u80fd\u3092\u5927\u5e45\u306b\u5411\u4e0a\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"appendix/glossary/","title":"\u7528\u8a9e\u96c6","text":""},{"location":"appendix/glossary/#a","title":"A","text":""},{"location":"appendix/glossary/#attention","title":"Attention\uff08\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3001\u6ce8\u610f\u6a5f\u69cb\uff09","text":"<p>\u5165\u529b\u7cfb\u5217\u306e\u5404\u8981\u7d20\u306b\u5bfe\u3057\u3066\u3001\u4ed6\u306e\u8981\u7d20\u3068\u306e\u95a2\u9023\u6027\u3092\u8a08\u7b97\u3057\u3001\u91cd\u307f\u4ed8\u3051\u3092\u884c\u3046\u6a5f\u69cb\u3002Transformer\u306e\u4e2d\u6838\u6280\u8853\u3002</p> <pre><code># \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u57fa\u672c\u8a08\u7b97\nattention_weights = softmax(Q @ K.T / sqrt(d_k))\noutput = attention_weights @ V\n</code></pre>"},{"location":"appendix/glossary/#autoregressive","title":"Autoregressive\uff08\u81ea\u5df1\u56de\u5e30\uff09","text":"<p>\u904e\u53bb\u306e\u51fa\u529b\u3092\u5165\u529b\u3068\u3057\u3066\u4f7f\u7528\u3057\u3001\u6b21\u306e\u51fa\u529b\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3002GPT\u306a\u3069\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u3067\u4f7f\u7528\u3002</p>"},{"location":"appendix/glossary/#b","title":"B","text":""},{"location":"appendix/glossary/#bert-bidirectional-encoder-representations-from-transformers","title":"BERT (Bidirectional Encoder Representations from Transformers)","text":"<p>Google\u304c\u958b\u767a\u3057\u305f\u53cc\u65b9\u5411Transformer\u3002\u6587\u8108\u306e\u4e21\u65b9\u5411\u304b\u3089\u60c5\u5831\u3092\u53d6\u5f97\u3002</p>"},{"location":"appendix/glossary/#batch-normalization","title":"Batch Normalization\uff08\u30d0\u30c3\u30c1\u6b63\u898f\u5316\uff09","text":"<p>\u30d0\u30c3\u30c1\u6b21\u5143\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u624b\u6cd5\u3002Transformer\u3067\u306f\u4e3b\u306bLayer Normalization\u304c\u4f7f\u7528\u3055\u308c\u308b\u3002</p>"},{"location":"appendix/glossary/#beam-search","title":"Beam Search\uff08\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\uff09","text":"<p>\u8907\u6570\u306e\u5019\u88dc\u3092\u4e26\u5217\u306b\u63a2\u7d22\u3057\u3001\u6700\u3082\u826f\u3044\u7cfb\u5217\u3092\u898b\u3064\u3051\u308b\u63a2\u7d22\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3002</p> <pre><code># \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u6982\u5ff5\nbeams = [(initial_sequence, 0.0)]  # (\u7cfb\u5217, \u30b9\u30b3\u30a2)\nfor step in range(max_length):\n    new_beams = []\n    for seq, score in beams:\n        # \u5404\u30d3\u30fc\u30e0\u304b\u3089\u6b21\u306e\u5019\u88dc\u3092\u751f\u6210\n        candidates = generate_next_tokens(seq)\n        new_beams.extend([(seq + [token], score + log_prob) \n                         for token, log_prob in candidates])\n    # \u4e0a\u4f4dk\u500b\u3092\u9078\u629e\n    beams = sorted(new_beams, key=lambda x: x[1])[:beam_size]\n</code></pre>"},{"location":"appendix/glossary/#bpe-byte-pair-encoding","title":"BPE (Byte Pair Encoding)","text":"<p>\u983b\u51fa\u3059\u308b\u6587\u5b57\u30da\u30a2\u3092\u7e70\u308a\u8fd4\u3057\u30de\u30fc\u30b8\u3059\u308b\u3053\u3068\u3067\u8a9e\u5f59\u3092\u69cb\u7bc9\u3059\u308b\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u3002</p>"},{"location":"appendix/glossary/#c","title":"C","text":""},{"location":"appendix/glossary/#causal-mask","title":"Causal Mask\uff08\u56e0\u679c\u30de\u30b9\u30af\uff09","text":"<p>\u672a\u6765\u306e\u60c5\u5831\u3092\u898b\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u30de\u30b9\u30af\u3002\u30c7\u30b3\u30fc\u30c0\u3084GPT\u3067\u4f7f\u7528\u3002</p> <pre><code># \u56e0\u679c\u30de\u30b9\u30af\u306e\u4f5c\u6210\ncausal_mask = torch.tril(torch.ones(seq_len, seq_len))\n</code></pre>"},{"location":"appendix/glossary/#cross-attention","title":"Cross-Attention\uff08\u30af\u30ed\u30b9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff09","text":"<p>\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u51fa\u529b\u3092\u30ad\u30fc\u30fb\u30d0\u30ea\u30e5\u30fc\u3068\u3057\u3001\u30c7\u30b3\u30fc\u30c0\u306e\u72b6\u614b\u3092\u30af\u30a8\u30ea\u3068\u3059\u308b\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3002</p>"},{"location":"appendix/glossary/#cross-entropy-loss","title":"Cross-Entropy Loss\uff08\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\uff09","text":"<p>\u5206\u985e\u30bf\u30b9\u30af\u3067\u4f7f\u7528\u3055\u308c\u308b\u640d\u5931\u95a2\u6570\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u3067\u6a19\u6e96\u7684\u306b\u4f7f\u7528\u3002</p>"},{"location":"appendix/glossary/#d","title":"D","text":""},{"location":"appendix/glossary/#decoder","title":"Decoder\uff08\u30c7\u30b3\u30fc\u30c0\uff09","text":"<p>\u30a8\u30f3\u30b3\u30fc\u30c0\u306e\u51fa\u529b\u3092\u53d7\u3051\u53d6\u308a\u3001\u76ee\u7684\u306e\u51fa\u529b\u3092\u751f\u6210\u3059\u308b\u90e8\u5206\u3002\u81ea\u5df1\u56de\u5e30\u7684\u306b\u52d5\u4f5c\u3002</p>"},{"location":"appendix/glossary/#dropout","title":"Dropout\uff08\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\uff09","text":"<p>\u8a13\u7df4\u6642\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u7121\u52b9\u5316\u3059\u308b\u6b63\u5247\u5316\u624b\u6cd5\u3002</p> <pre><code>dropout = nn.Dropout(p=0.1)  # 10%\u306e\u78ba\u7387\u3067\u30c9\u30ed\u30c3\u30d7\n</code></pre>"},{"location":"appendix/glossary/#e","title":"E","text":""},{"location":"appendix/glossary/#embedding","title":"Embedding\uff08\u57cb\u3081\u8fbc\u307f\uff09","text":"<p>\u96e2\u6563\u7684\u306a\u30c8\u30fc\u30af\u30f3\u3092\u9023\u7d9a\u7684\u306a\u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u306b\u5199\u50cf\u3059\u308b\u51e6\u7406\u3002</p> <pre><code>embedding = nn.Embedding(vocab_size, d_model)\ntoken_vectors = embedding(token_ids)\n</code></pre>"},{"location":"appendix/glossary/#encoder","title":"Encoder\uff08\u30a8\u30f3\u30b3\u30fc\u30c0\uff09","text":"<p>\u5165\u529b\u3092\u51e6\u7406\u3057\u3001\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u8868\u73fe\u3092\u751f\u6210\u3059\u308b\u90e8\u5206\u3002</p>"},{"location":"appendix/glossary/#f","title":"F","text":""},{"location":"appendix/glossary/#feed-forward-network-ffn","title":"Feed-Forward Network (FFN)","text":"<p>Transformer\u5185\u306e\u4f4d\u7f6e\u3054\u3068\u306b\u9069\u7528\u3055\u308c\u308b2\u5c64\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3002</p> <pre><code>class FFN(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.linear2(F.relu(self.linear1(x)))\n</code></pre>"},{"location":"appendix/glossary/#fine-tuning","title":"Fine-tuning\uff08\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\uff09","text":"<p>\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u306b\u9069\u5fdc\u3055\u305b\u308b\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u3002</p>"},{"location":"appendix/glossary/#flash-attention","title":"Flash Attention","text":"<p>\u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\u624b\u6cd5\u3002IO\u8907\u96d1\u5ea6\u3092\u524a\u6e1b\u3002</p>"},{"location":"appendix/glossary/#g","title":"G","text":""},{"location":"appendix/glossary/#gpt-generative-pre-trained-transformer","title":"GPT (Generative Pre-trained Transformer)","text":"<p>OpenAI\u304c\u958b\u767a\u3057\u305f\u81ea\u5df1\u56de\u5e30\u578b\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u3002\u30c7\u30b3\u30fc\u30c0\u306e\u307f\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3002</p>"},{"location":"appendix/glossary/#gradient-accumulation","title":"Gradient Accumulation\uff08\u52fe\u914d\u7d2f\u7a4d\uff09","text":"<p>\u8907\u6570\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u306e\u52fe\u914d\u3092\u7d2f\u7a4d\u3057\u3066\u304b\u3089\u66f4\u65b0\u3059\u308b\u624b\u6cd5\u3002\u30e1\u30e2\u30ea\u5236\u7d04\u4e0b\u3067\u5927\u304d\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u5b9f\u73fe\u3002</p>"},{"location":"appendix/glossary/#gradient-clipping","title":"Gradient Clipping\uff08\u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\uff09","text":"<p>\u52fe\u914d\u306e\u5927\u304d\u3055\u3092\u5236\u9650\u3057\u3066\u5b66\u7fd2\u3092\u5b89\u5b9a\u5316\u3055\u305b\u308b\u624b\u6cd5\u3002</p> <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre>"},{"location":"appendix/glossary/#h","title":"H","text":""},{"location":"appendix/glossary/#head","title":"Head\uff08\u30d8\u30c3\u30c9\uff09","text":"<p>Multi-Head Attention\u306b\u304a\u3051\u308b\u500b\u5225\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\u5358\u4f4d\u3002</p>"},{"location":"appendix/glossary/#hugging-face","title":"Hugging Face","text":"<p>Transformer\u30e2\u30c7\u30eb\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3092\u63d0\u4f9b\u3059\u308b\u4f01\u696d\u30fb\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3002</p>"},{"location":"appendix/glossary/#i","title":"I","text":""},{"location":"appendix/glossary/#inference","title":"Inference\uff08\u63a8\u8ad6\uff09","text":"<p>\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\u30d7\u30ed\u30bb\u30b9\u3002</p>"},{"location":"appendix/glossary/#k","title":"K","text":""},{"location":"appendix/glossary/#key","title":"Key\uff08\u30ad\u30fc\uff09","text":"<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u6a5f\u69cb\u306b\u304a\u3044\u3066\u3001\u5404\u4f4d\u7f6e\u306e\u60c5\u5831\u3092\u8868\u73fe\u3059\u308b\u30d9\u30af\u30c8\u30eb\u3002</p> <pre><code>K = W_k @ X  # \u30ad\u30fc\u306e\u8a08\u7b97\n</code></pre>"},{"location":"appendix/glossary/#l","title":"L","text":""},{"location":"appendix/glossary/#layer-normalization","title":"Layer Normalization\uff08\u5c64\u6b63\u898f\u5316\uff09","text":"<p>\u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u5fb4\u6b21\u5143\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u624b\u6cd5\u3002Transformer\u3067\u6a19\u6e96\u7684\u306b\u4f7f\u7528\u3002</p> <pre><code>layer_norm = nn.LayerNorm(d_model)\nnormalized = layer_norm(x)\n</code></pre>"},{"location":"appendix/glossary/#learning-rate-schedule","title":"Learning Rate Schedule\uff08\u5b66\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\uff09","text":"<p>\u8a13\u7df4\u4e2d\u306b\u5b66\u7fd2\u7387\u3092\u8abf\u6574\u3059\u308b\u6226\u7565\u3002Transformer\u3067\u306fwarmup + \u6e1b\u8870\u304c\u4e00\u822c\u7684\u3002</p>"},{"location":"appendix/glossary/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>\u52b9\u7387\u7684\u306a\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u624b\u6cd5\u3002\u4f4e\u30e9\u30f3\u30af\u884c\u5217\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u3092\u8fd1\u4f3c\u3002</p> <pre><code># LoRA\u306e\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\ndelta_W = A @ B  # A: d\u00d7r, B: r\u00d7d, r &lt;&lt; d\nW_new = W_original + alpha * delta_W\n</code></pre>"},{"location":"appendix/glossary/#m","title":"M","text":""},{"location":"appendix/glossary/#masked-language-model-mlm","title":"Masked Language Model (MLM)","text":"<p>BERT\u3067\u4f7f\u7528\u3055\u308c\u308b\u4e8b\u524d\u5b66\u7fd2\u30bf\u30b9\u30af\u3002\u30e9\u30f3\u30c0\u30e0\u306b\u30de\u30b9\u30af\u3057\u305f\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\u3002</p>"},{"location":"appendix/glossary/#multi-head-attention","title":"Multi-Head Attention\uff08\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff09","text":"<p>\u8907\u6570\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u4e26\u5217\u306b\u8a08\u7b97\u3057\u3001\u7570\u306a\u308b\u95a2\u4fc2\u6027\u3092\u6349\u3048\u308b\u6a5f\u69cb\u3002</p> <pre><code>class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_k = d_model // n_heads\n        self.n_heads = n_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n</code></pre>"},{"location":"appendix/glossary/#n","title":"N","text":""},{"location":"appendix/glossary/#next-token-prediction","title":"Next Token Prediction\uff08\u6b21\u30c8\u30fc\u30af\u30f3\u4e88\u6e2c\uff09","text":"<p>\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u57fa\u672c\u30bf\u30b9\u30af\u3002\u73fe\u5728\u307e\u3067\u306e\u30c8\u30fc\u30af\u30f3\u304b\u3089\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\u3002</p>"},{"location":"appendix/glossary/#nucleus-samplingtop-p-sampling","title":"Nucleus Sampling\uff08Top-p Sampling\uff09","text":"<p>\u7d2f\u7a4d\u78ba\u7387\u304cp\u3092\u8d85\u3048\u308b\u307e\u3067\u306e\u4e0a\u4f4d\u30c8\u30fc\u30af\u30f3\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u624b\u6cd5\u3002</p>"},{"location":"appendix/glossary/#o","title":"O","text":""},{"location":"appendix/glossary/#optimizer","title":"Optimizer\uff08\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\uff09","text":"<p>\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3002AdamW\u304c\u6a19\u6e96\u7684\u3002</p> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n</code></pre>"},{"location":"appendix/glossary/#p","title":"P","text":""},{"location":"appendix/glossary/#padding","title":"Padding\uff08\u30d1\u30c7\u30a3\u30f3\u30b0\uff09","text":"<p>\u7cfb\u5217\u3092\u540c\u3058\u9577\u3055\u306b\u63c3\u3048\u308b\u305f\u3081\u306e\u51e6\u7406\u3002</p>"},{"location":"appendix/glossary/#perplexity","title":"Perplexity\uff08\u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\uff09","text":"<p>\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u6307\u6a19\u3002\u4f4e\u3044\u307b\u3069\u826f\u3044\u3002</p> <pre><code>perplexity = torch.exp(cross_entropy_loss)\n</code></pre>"},{"location":"appendix/glossary/#position-encoding","title":"Position Encoding\uff08\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff09","text":"<p>\u7cfb\u5217\u5185\u306e\u4f4d\u7f6e\u60c5\u5831\u3092\u30e2\u30c7\u30eb\u306b\u4f1d\u3048\u308b\u4ed5\u7d44\u307f\u3002</p>"},{"location":"appendix/glossary/#pre-training","title":"Pre-training\uff08\u4e8b\u524d\u5b66\u7fd2\uff09","text":"<p>\u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u6c4e\u7528\u7684\u306a\u8868\u73fe\u3092\u5b66\u7fd2\u3059\u308b\u30d7\u30ed\u30bb\u30b9\u3002</p>"},{"location":"appendix/glossary/#q","title":"Q","text":""},{"location":"appendix/glossary/#query","title":"Query\uff08\u30af\u30a8\u30ea\uff09","text":"<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u6a5f\u69cb\u306b\u304a\u3044\u3066\u3001\u60c5\u5831\u3092\u691c\u7d22\u3059\u308b\u5074\u306e\u30d9\u30af\u30c8\u30eb\u3002</p> <pre><code>Q = W_q @ X  # \u30af\u30a8\u30ea\u306e\u8a08\u7b97\n</code></pre>"},{"location":"appendix/glossary/#quantization","title":"Quantization\uff08\u91cf\u5b50\u5316\uff09","text":"<p>\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3084\u6d3b\u6027\u5316\u3092\u4f4e\u7cbe\u5ea6\u3067\u8868\u73fe\u3059\u308b\u5727\u7e2e\u6280\u8853\u3002</p>"},{"location":"appendix/glossary/#r","title":"R","text":""},{"location":"appendix/glossary/#residual-connection","title":"Residual Connection\uff08\u6b8b\u5dee\u63a5\u7d9a\uff09","text":"<p>\u5165\u529b\u3092\u51fa\u529b\u306b\u76f4\u63a5\u52a0\u7b97\u3059\u308b\u63a5\u7d9a\u3002\u52fe\u914d\u6d88\u5931\u3092\u9632\u3050\u3002</p> <pre><code>output = layer(x) + x  # \u6b8b\u5dee\u63a5\u7d9a\n</code></pre>"},{"location":"appendix/glossary/#rope-rotary-position-embedding","title":"RoPE (Rotary Position Embedding)","text":"<p>\u56de\u8ee2\u884c\u5217\u3092\u4f7f\u7528\u3057\u305f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u3002</p>"},{"location":"appendix/glossary/#s","title":"S","text":""},{"location":"appendix/glossary/#self-attention","title":"Self-Attention\uff08\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\uff09","text":"<p>\u7cfb\u5217\u5185\u306e\u8981\u7d20\u9593\u306e\u95a2\u4fc2\u3092\u8a08\u7b97\u3059\u308b\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3002</p>"},{"location":"appendix/glossary/#sentencepiece","title":"SentencePiece","text":"<p>\u8a00\u8a9e\u306b\u4f9d\u5b58\u3057\u306a\u3044\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3002\u30b5\u30d6\u30ef\u30fc\u30c9\u5358\u4f4d\u3067\u5206\u5272\u3002</p>"},{"location":"appendix/glossary/#softmax","title":"Softmax","text":"<p>\u78ba\u7387\u5206\u5e03\u3092\u751f\u6210\u3059\u308b\u6d3b\u6027\u5316\u95a2\u6570\u3002\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u306e\u8a08\u7b97\u3067\u4f7f\u7528\u3002</p> <pre><code>softmax = torch.nn.functional.softmax(x, dim=-1)\n</code></pre>"},{"location":"appendix/glossary/#t","title":"T","text":""},{"location":"appendix/glossary/#temperature","title":"Temperature\uff08\u6e29\u5ea6\uff09","text":"<p>\u751f\u6210\u6642\u306e\u78ba\u7387\u5206\u5e03\u306e\u92ed\u3055\u3092\u5236\u5fa1\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3002</p> <pre><code>logits = logits / temperature  # \u6e29\u5ea6\u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\nprobs = softmax(logits)\n</code></pre>"},{"location":"appendix/glossary/#token","title":"Token\uff08\u30c8\u30fc\u30af\u30f3\uff09","text":"<p>\u30c6\u30ad\u30b9\u30c8\u3092\u51e6\u7406\u53ef\u80fd\u306a\u5358\u4f4d\u306b\u5206\u5272\u3057\u305f\u3082\u306e\u3002</p>"},{"location":"appendix/glossary/#tokenizer","title":"Tokenizer\uff08\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\uff09","text":"<p>\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5206\u5272\u3059\u308b\u30c4\u30fc\u30eb\u3002</p>"},{"location":"appendix/glossary/#top-k-sampling","title":"Top-k Sampling","text":"<p>\u4e0a\u4f4dk\u500b\u306e\u30c8\u30fc\u30af\u30f3\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u751f\u6210\u624b\u6cd5\u3002</p>"},{"location":"appendix/glossary/#training","title":"Training\uff08\u8a13\u7df4/\u5b66\u7fd2\uff09","text":"<p>\u30c7\u30fc\u30bf\u304b\u3089\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6700\u9069\u5316\u3059\u308b\u30d7\u30ed\u30bb\u30b9\u3002</p>"},{"location":"appendix/glossary/#transformer","title":"Transformer","text":"<p>\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\u3092\u57fa\u76e4\u3068\u3059\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3002</p>"},{"location":"appendix/glossary/#v","title":"V","text":""},{"location":"appendix/glossary/#value","title":"Value\uff08\u30d0\u30ea\u30e5\u30fc\uff09","text":"<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u6a5f\u69cb\u306b\u304a\u3044\u3066\u3001\u5b9f\u969b\u306b\u96c6\u7d04\u3055\u308c\u308b\u60c5\u5831\u3092\u542b\u3080\u30d9\u30af\u30c8\u30eb\u3002</p> <pre><code>V = W_v @ X  # \u30d0\u30ea\u30e5\u30fc\u306e\u8a08\u7b97\n</code></pre>"},{"location":"appendix/glossary/#vocabulary","title":"Vocabulary\uff08\u8a9e\u5f59\uff09","text":"<p>\u30e2\u30c7\u30eb\u304c\u6271\u3048\u308b\u30c8\u30fc\u30af\u30f3\u306e\u96c6\u5408\u3002</p>"},{"location":"appendix/glossary/#w","title":"W","text":""},{"location":"appendix/glossary/#warmup","title":"Warmup","text":"<p>\u5b66\u7fd2\u521d\u671f\u306b\u5b66\u7fd2\u7387\u3092\u5f90\u3005\u306b\u4e0a\u3052\u308b\u624b\u6cd5\u3002</p> <pre><code>def warmup_schedule(step, warmup_steps):\n    if step &lt; warmup_steps:\n        return step / warmup_steps\n    return 1.0\n</code></pre>"},{"location":"appendix/glossary/#weight-decay","title":"Weight Decay\uff08\u91cd\u307f\u6e1b\u8870\uff09","text":"<p>\u6b63\u5247\u5316\u306e\u4e00\u7a2e\u3002\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5927\u304d\u3055\u306b\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u8ab2\u3059\u3002</p>"},{"location":"appendix/glossary/#wordpiece","title":"WordPiece","text":"<p>BERT\u3067\u4f7f\u7528\u3055\u308c\u308b\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u3002</p>"},{"location":"appendix/glossary/#_2","title":"\u6570\u5f0f\u8a18\u53f7","text":""},{"location":"appendix/glossary/#d_model","title":"d_model","text":"<p>\u30e2\u30c7\u30eb\u306e\u96a0\u308c\u5c64\u306e\u6b21\u5143\u6570\uff08\u4f8b\uff1a512, 768, 1024\uff09</p>"},{"location":"appendix/glossary/#d_k-d_v","title":"d_k, d_v","text":"<p>\u30ad\u30fc\u3068\u30d0\u30ea\u30e5\u30fc\u306e\u6b21\u5143\u6570\u3002\u901a\u5e38 d_model / n_heads</p>"},{"location":"appendix/glossary/#d_ff","title":"d_ff","text":"<p>Feed-Forward Network\u306e\u4e2d\u9593\u5c64\u306e\u6b21\u5143\u6570\u3002\u901a\u5e38 4 * d_model</p>"},{"location":"appendix/glossary/#n_heads","title":"n_heads","text":"<p>Multi-Head Attention\u306e\u30d8\u30c3\u30c9\u6570\uff08\u4f8b\uff1a8, 12, 16\uff09</p>"},{"location":"appendix/glossary/#n_layers","title":"n_layers","text":"<p>Transformer\u30d6\u30ed\u30c3\u30af\u306e\u5c64\u6570\uff08\u4f8b\uff1a6, 12, 24\uff09</p>"},{"location":"appendix/glossary/#_3","title":"\u30b3\u30f3\u30d1\u30a4\u30e9\u3068\u306e\u5bfe\u5fdc\u95a2\u4fc2","text":"Transformer\u7528\u8a9e \u30b3\u30f3\u30d1\u30a4\u30e9\u7528\u8a9e \u8aac\u660e Tokenization \u5b57\u53e5\u89e3\u6790 \u30c6\u30ad\u30b9\u30c8\u3092\u57fa\u672c\u5358\u4f4d\u306b\u5206\u5272 Embedding \u30b7\u30f3\u30dc\u30eb\u30c6\u30fc\u30d6\u30eb \u30c8\u30fc\u30af\u30f3\u3092\u5185\u90e8\u8868\u73fe\u306b\u5909\u63db Self-Attention \u6587\u8108\u89e3\u6790 \u8981\u7d20\u9593\u306e\u4f9d\u5b58\u95a2\u4fc2\u3092\u89e3\u6790 Layer \u6700\u9069\u5316\u30d1\u30b9 \u6bb5\u968e\u7684\u306a\u5909\u63db\u51e6\u7406 Position Encoding \u884c\u756a\u53f7/\u4f4d\u7f6e\u60c5\u5831 \u8981\u7d20\u306e\u4f4d\u7f6e\u3092\u8a18\u9332 Feed-Forward \u5909\u63db\u898f\u5247 \u500b\u5225\u8981\u7d20\u306e\u5909\u63db Decoder \u30b3\u30fc\u30c9\u751f\u6210 \u6700\u7d42\u7684\u306a\u51fa\u529b\u3092\u751f\u6210"},{"location":"appendix/resources/","title":"\u53c2\u8003\u8cc7\u6599\u3068\u30ea\u30bd\u30fc\u30b9","text":""},{"location":"appendix/resources/#_2","title":"\u8ad6\u6587","text":""},{"location":"appendix/resources/#_3","title":"\u57fa\u790e\u8ad6\u6587","text":"<ol> <li>Attention Is All You Need (Vaswani et al., 2017)</li> <li>Transformer\u306e\u539f\u8ad6\u6587</li> <li> <p>arXiv:1706.03762</p> </li> <li> <p>BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)</p> </li> <li>\u53cc\u65b9\u5411Transformer\u306e\u4e8b\u524d\u5b66\u7fd2</li> <li> <p>arXiv:1810.04805</p> </li> <li> <p>Language Models are Few-Shot Learners (Brown et al., 2020)</p> </li> <li>GPT-3\u306e\u8ad6\u6587</li> <li>arXiv:2005.14165</li> </ol>"},{"location":"appendix/resources/#_4","title":"\u52b9\u7387\u5316\u6280\u8853","text":"<ol> <li>FlashAttention (Dao et al., 2022)</li> <li>\u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3</li> <li> <p>arXiv:2205.14135</p> </li> <li> <p>LoRA: Low-Rank Adaptation (Hu et al., 2021)</p> </li> <li>\u52b9\u7387\u7684\u306a\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0</li> <li> <p>arXiv:2106.09685</p> </li> <li> <p>Mixtral of Experts (Jiang et al., 2024)</p> </li> <li>Sparse MoE \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</li> <li>arXiv:2401.04088</li> </ol>"},{"location":"appendix/resources/#_5","title":"\u5b9f\u88c5\u30ea\u30bd\u30fc\u30b9","text":""},{"location":"appendix/resources/#pytorch","title":"PyTorch\u30d9\u30fc\u30b9","text":"<pre><code># \u516c\u5f0fPyTorch Transformer\u5b9f\u88c5\nimport torch.nn as nn\n\n# \u57fa\u672c\u7684\u306aTransformer\ntransformer = nn.Transformer(\n    d_model=512,\n    nhead=8,\n    num_encoder_layers=6,\n    num_decoder_layers=6\n)\n\n# \u3088\u308a\u8a73\u7d30\u306a\u5236\u5fa1\u304c\u5fc5\u8981\u306a\u5834\u5408\nencoder_layer = nn.TransformerEncoderLayer(\n    d_model=512,\n    nhead=8,\n    dim_feedforward=2048\n)\n</code></pre>"},{"location":"appendix/resources/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<pre><code>from transformers import AutoModel, AutoTokenizer\n\n# \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u4f7f\u7528\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# \u30ab\u30b9\u30bf\u30e0\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\nfrom transformers import PreTrainedModel, PretrainedConfig\n\nclass MyTransformerConfig(PretrainedConfig):\n    model_type = \"my_transformer\"\n\n    def __init__(self, vocab_size=30000, d_model=512, **kwargs):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"appendix/resources/#jaxflax","title":"JAX/Flax\u5b9f\u88c5","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass TransformerBlock(nn.Module):\n    d_model: int\n    n_heads: int\n\n    @nn.compact\n    def __call__(self, x, mask=None):\n        # Multi-head attention\n        attn_output = nn.MultiHeadDotProductAttention(\n            num_heads=self.n_heads,\n            qkv_features=self.d_model\n        )(x, x, mask=mask)\n\n        x = nn.LayerNorm()(x + attn_output)\n\n        # Feed-forward\n        ff_output = nn.Sequential([\n            nn.Dense(4 * self.d_model),\n            nn.gelu,\n            nn.Dense(self.d_model)\n        ])(x)\n\n        return nn.LayerNorm()(x + ff_output)\n</code></pre>"},{"location":"appendix/resources/#_6","title":"\u5b66\u7fd2\u30ea\u30bd\u30fc\u30b9","text":""},{"location":"appendix/resources/#_7","title":"\u30aa\u30f3\u30e9\u30a4\u30f3\u30b3\u30fc\u30b9","text":"<ol> <li>Stanford CS224N: Natural Language Processing with Deep Learning</li> <li>Transformer\u306e\u8a73\u7d30\u306a\u89e3\u8aac</li> <li> <p>Course Website</p> </li> <li> <p>Fast.ai Practical Deep Learning</p> </li> <li>\u5b9f\u8df5\u7684\u306aNLP\u5b9f\u88c5</li> <li> <p>Course Website</p> </li> <li> <p>Hugging Face Course</p> </li> <li>Transformers\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u4f7f\u3044\u65b9</li> <li>Course Website</li> </ol>"},{"location":"appendix/resources/#_8","title":"\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u53ef\u8996\u5316","text":"<ol> <li>The Illustrated Transformer (Jay Alammar)</li> <li>\u56f3\u89e3\u306b\u3088\u308bTransformer\u306e\u8aac\u660e</li> <li> <p>Blog Post</p> </li> <li> <p>Transformer Explainer</p> </li> <li>\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u53ef\u8996\u5316\u30c4\u30fc\u30eb</li> <li> <p>Demo</p> </li> <li> <p>BertViz</p> </li> <li>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea    <pre><code>from bertviz import model_view, head_view\n\n# \u30e2\u30c7\u30eb\u3068\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u6e96\u5099\nmodel_view(attention, tokens)\n</code></pre></li> </ol>"},{"location":"appendix/resources/#_9","title":"\u5b9f\u8df5\u7684\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8","text":""},{"location":"appendix/resources/#1-gpt","title":"1. \u30df\u30cbGPT\u306e\u5b9f\u88c5","text":"<pre><code># \u5b8c\u5168\u306aGPT\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\u4f8b\nclass MiniGPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        # Token + Position embeddings\n        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, config.max_len, config.d_model))\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(config) for _ in range(config.n_layers)\n        ])\n\n        # Output projection\n        self.ln_f = nn.LayerNorm(config.d_model)\n        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n    def forward(self, idx, targets=None):\n        b, t = idx.size()\n\n        # Embeddings\n        tok_emb = self.tok_emb(idx)\n        pos_emb = self.pos_emb[:, :t, :]\n        x = self.dropout(tok_emb + pos_emb)\n\n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x)\n\n        # Output\n        x = self.ln_f(x)\n        logits = self.head(x)\n\n        # Loss calculation\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1)\n            )\n\n        return logits, loss\n</code></pre>"},{"location":"appendix/resources/#2","title":"2. \u30ab\u30b9\u30bf\u30e0\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc","text":"<pre><code># SentencePiece\u3092\u4f7f\u3063\u305f\u65e5\u672c\u8a9e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\nimport sentencepiece as spm\n\nclass JapaneseTokenizer:\n    def __init__(self, model_path):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(model_path)\n\n    def encode(self, text):\n        return self.sp.encode_as_ids(text)\n\n    def decode(self, ids):\n        return self.sp.decode_pieces(ids)\n\n    def train(self, texts, vocab_size=8000):\n        # \u8a13\u7df4\u7528\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\n        with open('train.txt', 'w') as f:\n            for text in texts:\n                f.write(text + '\\n')\n\n        # SentencePiece\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\n        spm.SentencePieceTrainer.train(\n            input='train.txt',\n            model_prefix='tokenizer',\n            vocab_size=vocab_size,\n            character_coverage=0.9995,\n            model_type='bpe'\n        )\n</code></pre>"},{"location":"appendix/resources/#3","title":"3. \u52b9\u7387\u7684\u306a\u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc","text":"<pre><code># FastAPI\u3092\u4f7f\u3063\u305f\u63a8\u8ad6API\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport torch\nfrom typing import List, Optional\n\napp = FastAPI()\n\nclass GenerationRequest(BaseModel):\n    prompt: str\n    max_length: int = 100\n    temperature: float = 0.8\n    top_p: float = 0.9\n\nclass GenerationResponse(BaseModel):\n    generated_text: str\n    tokens_generated: int\n    generation_time: float\n\n# \u30e2\u30c7\u30eb\u3092\u30b0\u30ed\u30fc\u30d0\u30eb\u306b\u8aad\u307f\u8fbc\u307f\nmodel = load_model()\ntokenizer = load_tokenizer()\n\n@app.post(\"/generate\", response_model=GenerationResponse)\nasync def generate_text(request: GenerationRequest):\n    try:\n        start_time = time.time()\n\n        # \u30c8\u30fc\u30af\u30f3\u5316\n        input_ids = tokenizer.encode(request.prompt)\n\n        # \u751f\u6210\n        output_ids = model.generate(\n            input_ids,\n            max_length=request.max_length,\n            temperature=request.temperature,\n            top_p=request.top_p\n        )\n\n        # \u30c7\u30b3\u30fc\u30c9\n        generated_text = tokenizer.decode(output_ids)\n\n        return GenerationResponse(\n            generated_text=generated_text,\n            tokens_generated=len(output_ids) - len(input_ids),\n            generation_time=time.time() - start_time\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"appendix/resources/#_10","title":"\u30c7\u30d0\u30c3\u30b0\u3068\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0","text":""},{"location":"appendix/resources/#_11","title":"\u30e1\u30e2\u30ea\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0","text":"<pre><code>import torch.profiler as profiler\n\n# \u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\u306e\u8a2d\u5b9a\nwith profiler.profile(\n    activities=[\n        profiler.ProfilerActivity.CPU,\n        profiler.ProfilerActivity.CUDA,\n    ],\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n) as prof:\n    # \u30e2\u30c7\u30eb\u306e\u5b9f\u884c\n    output = model(input_ids)\n    loss = criterion(output, targets)\n    loss.backward()\n\n# \u7d50\u679c\u306e\u8868\u793a\nprint(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n\n# TensorBoard\u306b\u51fa\u529b\nprof.export_chrome_trace(\"trace.json\")\n</code></pre>"},{"location":"appendix/resources/#_12","title":"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u306e\u5206\u6790","text":"<pre><code>def analyze_attention_patterns(model, input_ids):\n    \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n    model.eval()\n\n    with torch.no_grad():\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u3092\u53d6\u5f97\n        outputs = model(input_ids, output_attentions=True)\n        attentions = outputs.attentions  # \u5404\u5c64\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\n\n    # \u7d71\u8a08\u60c5\u5831\u306e\u8a08\u7b97\n    for layer_idx, layer_attention in enumerate(attentions):\n        # [batch, heads, seq_len, seq_len]\n        avg_attention = layer_attention.mean(dim=1)  # \u30d8\u30c3\u30c9\u9593\u3067\u5e73\u5747\n\n        # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8a08\u7b97\uff08\u6ce8\u610f\u306e\u5206\u6563\u5ea6\uff09\n        entropy = -(avg_attention * torch.log(avg_attention + 1e-9)).sum(dim=-1)\n\n        print(f\"Layer {layer_idx}:\")\n        print(f\"  Average entropy: {entropy.mean():.4f}\")\n\n        # \u6700\u3082\u6ce8\u76ee\u3055\u308c\u3066\u3044\u308b\u30c8\u30fc\u30af\u30f3\n        max_attention_idx = avg_attention.sum(dim=1).argmax(dim=-1)\n        print(f\"  Most attended positions: {max_attention_idx}\")\n</code></pre>"},{"location":"appendix/resources/#_13","title":"\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u3068\u30b5\u30dd\u30fc\u30c8","text":""},{"location":"appendix/resources/#_14","title":"\u30d5\u30a9\u30fc\u30e9\u30e0\u3068\u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3","text":"<ul> <li>Hugging Face Forums: https://discuss.huggingface.co/</li> <li>PyTorch Forums: https://discuss.pytorch.org/</li> <li>Reddit r/MachineLearning: https://reddit.com/r/MachineLearning</li> </ul>"},{"location":"appendix/resources/#_15","title":"\u65e5\u672c\u8a9e\u30ea\u30bd\u30fc\u30b9","text":"<ul> <li>\u65e5\u672c\u8a9eBERT: https://github.com/cl-tohoku/bert-japanese</li> <li>Japanese GPT-2: https://github.com/tanreinama/gpt2-japanese</li> <li>Fugaku-LLM: https://github.com/fujitsu/fugaku-llm</li> </ul>"},{"location":"appendix/resources/#_16","title":"\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8","text":"<ul> <li>GLUE Benchmark: \u82f1\u8a9e\u306e\u8a00\u8a9e\u7406\u89e3\u30bf\u30b9\u30af</li> <li>JGLUE: \u65e5\u672c\u8a9e\u7248GLUE</li> <li>OpenWebText: GPT\u8a13\u7df4\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8</li> <li>CC-100: \u591a\u8a00\u8a9e\u30b3\u30fc\u30d1\u30b9</li> </ul>"},{"location":"appendix/resources/#_17","title":"\u4eca\u5f8c\u306e\u5b66\u7fd2\u30b9\u30c6\u30c3\u30d7","text":"<ol> <li>\u6700\u65b0\u306e\u7814\u7a76\u52d5\u5411\u3092\u30d5\u30a9\u30ed\u30fc</li> <li>arXiv\u306e\u65b0\u7740\u8ad6\u6587\u3092\u30c1\u30a7\u30c3\u30af</li> <li> <p>\u4e3b\u8981\u306a\u7814\u7a76\u6a5f\u95a2\u306e\u30d6\u30ed\u30b0\uff08OpenAI, Google Research, Meta AI\uff09</p> </li> <li> <p>\u5b9f\u88c5\u30d7\u30ed\u30b8\u30a7\u30af\u30c8</p> </li> <li>\u72ec\u81ea\u306e\u30bf\u30b9\u30af\u3067Transformer\u3092\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0</li> <li>\u65b0\u3057\u3044\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u5b9f\u9a13</li> <li> <p>\u52b9\u7387\u5316\u6280\u8853\u306e\u5b9f\u88c5</p> </li> <li> <p>\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3</p> </li> <li>\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3078\u306e\u8ca2\u732e</li> <li>\u81ea\u5206\u306e\u5b9f\u88c5\u3092\u516c\u958b</li> <li>\u30d6\u30ed\u30b0\u3084\u6280\u8853\u8a18\u4e8b\u306e\u57f7\u7b46</li> </ol> <p>\u7d99\u7d9a\u7684\u306a\u5b66\u7fd2\u3068\u5b9f\u8df5\u3092\u901a\u3058\u3066\u3001Transformer\u3068\u305d\u306e\u5fdc\u7528\u306b\u3064\u3044\u3066\u306e\u7406\u89e3\u3092\u6df1\u3081\u3066\u3044\u3063\u3066\u304f\u3060\u3055\u3044\uff01</p>"},{"location":"exercises/part1-exercises/","title":"\u7b2c1\u90e8 \u6f14\u7fd2\u554f\u984c","text":""},{"location":"exercises/part1-exercises/#11-transformer","title":"\u6f14\u7fd2 1.1: Transformer\u306e\u91cd\u8981\u6027","text":""},{"location":"exercises/part1-exercises/#1_1","title":"\u554f\u984c 1","text":"<p>\u4ee5\u4e0b\u306e\u30bf\u30b9\u30af\u306e\u3046\u3061\u3001Transformer\u304c\u7279\u306b\u512a\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3069\u308c\u3067\u3059\u304b\uff1f\u305d\u306e\u7406\u7531\u3082\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <ol> <li>\u753b\u50cf\u306e\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u3067\u306e\u5206\u985e</li> <li>\u9577\u6587\u306e\u8981\u7d04</li> <li>\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u306e\u97f3\u58f0\u8a8d\u8b58</li> <li>\u6570\u5024\u8a08\u7b97\u306e\u6700\u9069\u5316</li> </ol> \u89e3\u7b54 <p>\u7b54\u3048: 2. \u9577\u6587\u306e\u8981\u7d04</p> <p>\u7406\u7531\uff1a - Transformer\u306e\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\u306b\u3088\u308a\u3001\u6587\u66f8\u5168\u4f53\u306e\u6587\u8108\u3092\u52b9\u7387\u7684\u306b\u628a\u63e1\u3067\u304d\u308b - \u9577\u8ddd\u96e2\u4f9d\u5b58\u95a2\u4fc2\u3092\u76f4\u63a5\u30e2\u30c7\u30eb\u5316\u3067\u304d\u308b - \u4e26\u5217\u51e6\u7406\u306b\u3088\u308a\u3001\u9577\u6587\u3067\u3082\u9ad8\u901f\u306b\u51e6\u7406\u53ef\u80fd</p> <p>\u4ed6\u306e\u9078\u629e\u80a2\u306b\u3064\u3044\u3066\uff1a - 1: CNN\u306e\u65b9\u304c\u5c40\u6240\u7684\u306a\u30d1\u30bf\u30fc\u30f3\u62bd\u51fa\u306b\u9069\u3057\u3066\u3044\u308b - 3: \u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u6027\u3092\u8003\u3048\u308b\u3068RNN\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u3082\u691c\u8a0e\u3055\u308c\u308b - 4: \u6570\u5024\u8a08\u7b97\u306f\u5f93\u6765\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u65b9\u304c\u52b9\u7387\u7684</p>"},{"location":"exercises/part1-exercises/#2","title":"\u554f\u984c 2","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u30d1\u30b9\u3068Transformer\u306e\u5c64\u306e\u985e\u4f3c\u70b9\u30923\u3064\u6319\u3052\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <ol> <li>\u6bb5\u968e\u7684\u306a\u62bd\u8c61\u5316</li> <li>\u30b3\u30f3\u30d1\u30a4\u30e9: \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9 \u2192 AST \u2192 IR \u2192 \u6a5f\u68b0\u8a9e</li> <li> <p>Transformer: \u30c8\u30fc\u30af\u30f3 \u2192 \u57cb\u3081\u8fbc\u307f \u2192 \u6587\u8108\u8868\u73fe \u2192 \u51fa\u529b</p> </li> <li> <p>\u60c5\u5831\u306e\u4fdd\u6301\u3068\u5909\u63db</p> </li> <li>\u30b3\u30f3\u30d1\u30a4\u30e9: \u5404\u30d1\u30b9\u3067\u5fc5\u8981\u306a\u60c5\u5831\u3092\u4fdd\u6301\u3057\u3064\u3064\u5909\u63db</li> <li> <p>Transformer: \u6b8b\u5dee\u63a5\u7d9a\u306b\u3088\u308a\u5143\u306e\u60c5\u5831\u3092\u4fdd\u6301\u3057\u3064\u3064\u5909\u63db</p> </li> <li> <p>\u4e26\u5217\u51e6\u7406\u306e\u53ef\u80fd\u6027</p> </li> <li>\u30b3\u30f3\u30d1\u30a4\u30e9: \u72ec\u7acb\u3057\u305f\u6700\u9069\u5316\u306f\u4e26\u5217\u5b9f\u884c\u53ef\u80fd</li> <li>Transformer: \u81ea\u5df1\u6ce8\u610f\u306f\u5168\u4f4d\u7f6e\u3067\u4e26\u5217\u8a08\u7b97\u53ef\u80fd</li> </ol>"},{"location":"exercises/part1-exercises/#12","title":"\u6f14\u7fd2 1.2: \u6570\u5b66\u7684\u57fa\u790e","text":""},{"location":"exercises/part1-exercises/#3","title":"\u554f\u984c 3","text":"<p>\u4ee5\u4e0b\u306e\u884c\u5217\u306e\u7a4d\u3092\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044\uff1a</p> <pre><code>A = [[1, 2],    B = [[5, 6],\n     [3, 4]]         [7, 8]]\n</code></pre> \u89e3\u7b54 <pre><code>import numpy as np\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nC = A @ B\n# C = [[1*5 + 2*7, 1*6 + 2*8],\n#      [3*5 + 4*7, 3*6 + 4*8]]\n#   = [[19, 22],\n#      [43, 50]]\n</code></pre>"},{"location":"exercises/part1-exercises/#4","title":"\u554f\u984c 4","text":"<p>\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u3001\u4ee5\u4e0b\u306e\u30d9\u30af\u30c8\u30eb\u306b\u9069\u7528\u3057\u3066\u304f\u3060\u3055\u3044\uff1a <code>x = [2.0, 1.0, 0.1]</code></p> \u89e3\u7b54 <pre><code>import numpy as np\n\ndef softmax(x):\n    # \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\u306e\u305f\u3081\u6700\u5927\u5024\u3092\u5f15\u304f\n    x_shifted = x - np.max(x)\n    exp_x = np.exp(x_shifted)\n    return exp_x / np.sum(exp_x)\n\nx = np.array([2.0, 1.0, 0.1])\nresult = softmax(x)\nprint(result)\n# [0.6590, 0.2424, 0.0986]\n\n# \u78ba\u8a8d\uff1a\u5408\u8a08\u304c1\u306b\u306a\u308b\nprint(np.sum(result))  # 1.0\n</code></pre>"},{"location":"exercises/part1-exercises/#13-pytorch","title":"\u6f14\u7fd2 1.3: PyTorch\u5b9f\u8df5","text":""},{"location":"exercises/part1-exercises/#5","title":"\u554f\u984c 5","text":"<p>PyTorch\u3067\u7c21\u5358\u306a2\u5c64\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 - \u5165\u529b\u6b21\u5143: 10 - \u96a0\u308c\u5c64: 20\u30e6\u30cb\u30c3\u30c8\uff08ReLU\u6d3b\u6027\u5316\uff09 - \u51fa\u529b\u6b21\u5143: 3</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim=10, hidden_dim=20, output_dim=3):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# \u30c6\u30b9\u30c8\nmodel = SimpleNN()\nx = torch.randn(32, 10)  # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba32\noutput = model(x)\nprint(output.shape)  # torch.Size([32, 3])\n</code></pre>"},{"location":"exercises/part1-exercises/#6","title":"\u554f\u984c 6","text":"<p>\u52fe\u914d\u964d\u4e0b\u6cd5\u306e1\u30b9\u30c6\u30c3\u30d7\u3092\u624b\u52d5\u3067\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\n\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u52fe\u914d\nw = torch.tensor([1.0, 2.0], requires_grad=True)\n\n# \u7c21\u5358\u306a\u640d\u5931\u95a2\u6570: L = w[0]^2 + w[1]^2\nloss = w[0]**2 + w[1]**2\n\n# \u52fe\u914d\u8a08\u7b97\nloss.backward()\n\n# \u624b\u52d5\u3067\u52fe\u914d\u964d\u4e0b\nlearning_rate = 0.1\nwith torch.no_grad():\n    # w = w - lr * gradient\n    w_new = w - learning_rate * w.grad\n\nprint(f\"\u5143\u306e\u91cd\u307f: {w.data}\")\nprint(f\"\u52fe\u914d: {w.grad}\")\nprint(f\"\u66f4\u65b0\u5f8c\u306e\u91cd\u307f: {w_new}\")\n\n# \u671f\u5f85\u3055\u308c\u308b\u7d50\u679c:\n# \u52fe\u914d: [2.0, 4.0]\n# \u66f4\u65b0: [1.0 - 0.1*2.0, 2.0 - 0.1*4.0] = [0.8, 1.6]\n</code></pre>"},{"location":"exercises/part1-exercises/#14","title":"\u6f14\u7fd2 1.4: \u7dcf\u5408\u554f\u984c","text":""},{"location":"exercises/part1-exercises/#7","title":"\u554f\u984c 7","text":"<p>\u7c21\u5358\u306a\u6587\u5b57\u30ec\u30d9\u30eb\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u30eb\u30fc\u30d7\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u5165\u529b: \"hello world\" \u76ee\u6a19: \u5404\u6587\u5b57\u304b\u3089\u6b21\u306e\u6587\u5b57\u3092\u4e88\u6e2c</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# \u30c7\u30fc\u30bf\u6e96\u5099\ntext = \"hello world\"\nchars = sorted(list(set(text)))\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for i, ch in enumerate(chars)}\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\ndata = [char_to_idx[ch] for ch in text]\nx = torch.tensor(data[:-1])\ny = torch.tensor(data[1:])\n\n# \u7c21\u5358\u306a\u30e2\u30c7\u30eb\nclass CharModel(nn.Module):\n    def __init__(self, vocab_size, hidden_size=16):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.fc(x)\n        return x\n\n# \u8a13\u7df4\nmodel = CharModel(len(chars))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(100):\n    # \u9806\u4f1d\u64ad\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n\n    # \u9006\u4f1d\u64ad\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if epoch % 20 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# \u751f\u6210\u30c6\u30b9\u30c8\nwith torch.no_grad():\n    # \"h\"\u304b\u3089\u958b\u59cb\n    idx = char_to_idx['h']\n    result = 'h'\n\n    for _ in range(10):\n        x_test = torch.tensor([idx])\n        logits = model(x_test)\n        probs = F.softmax(logits, dim=-1)\n        idx = torch.argmax(probs, dim=-1).item()\n        result += idx_to_char[idx]\n\n    print(f\"\u751f\u6210\u7d50\u679c: {result}\")\n</code></pre>"},{"location":"exercises/part1-exercises/#_1","title":"\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c","text":""},{"location":"exercises/part1-exercises/#8","title":"\u554f\u984c 8 \ud83c\udf1f","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5b57\u53e5\u89e3\u6790\u5668\u306e\u3088\u3046\u306b\u3001\u7c21\u5358\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4ee5\u4e0b\u306e\u898f\u5247\u306b\u5f93\u3063\u3066\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u5316\u3057\u307e\u3059\uff1a - \u7a7a\u767d\u3067\u5358\u8a9e\u3092\u5206\u5272 - \u53e5\u8aad\u70b9\u306f\u72ec\u7acb\u3057\u305f\u30c8\u30fc\u30af\u30f3\u3068\u3057\u3066\u6271\u3046 - \u6570\u5b57\u306f1\u3064\u306e\u30c8\u30fc\u30af\u30f3\u3068\u3057\u3066\u307e\u3068\u3081\u308b</p> <p>\u5165\u529b\u4f8b: \"Hello, world! 123 test.\" \u671f\u5f85\u3055\u308c\u308b\u51fa\u529b: [\"Hello\", \",\", \"world\", \"!\", \"123\", \"test\", \".\"]</p> \u89e3\u7b54 <pre><code>import re\n\nclass SimpleTokenizer:\n    def __init__(self):\n        # \u30c8\u30fc\u30af\u30f3\u5316\u306e\u30d1\u30bf\u30fc\u30f3\n        self.patterns = [\n            (r'\\d+', 'NUMBER'),           # \u6570\u5b57\n            (r'[a-zA-Z]+', 'WORD'),       # \u5358\u8a9e\n            (r'[.,!?;:]', 'PUNCTUATION'), # \u53e5\u8aad\u70b9\n            (r'\\s+', 'SPACE'),            # \u7a7a\u767d\uff08\u30b9\u30ad\u30c3\u30d7\u7528\uff09\n        ]\n        self.regex = '|'.join(f'({pattern})' for pattern, _ in self.patterns)\n\n    def tokenize(self, text):\n        tokens = []\n\n        for match in re.finditer(self.regex, text):\n            token = match.group()\n\n            # \u30c8\u30fc\u30af\u30f3\u30bf\u30a4\u30d7\u3092\u7279\u5b9a\n            for i, (pattern, token_type) in enumerate(self.patterns):\n                if match.group(i + 1):  # \u30b0\u30eb\u30fc\u30d7\u304c\u30de\u30c3\u30c1\u3057\u305f\n                    if token_type != 'SPACE':  # \u7a7a\u767d\u306f\u30b9\u30ad\u30c3\u30d7\n                        tokens.append(token)\n                    break\n\n        return tokens\n\n# \u30c6\u30b9\u30c8\ntokenizer = SimpleTokenizer()\ntext = \"Hello, world! 123 test.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# ['Hello', ',', 'world', '!', '123', 'test', '.']\n\n# \u3088\u308a\u8907\u96d1\u306a\u4f8b\ntext2 = \"The price is $99.99, isn't it?\"\ntokens2 = tokenizer.tokenize(text2)\nprint(tokens2)\n# ['The', 'price', 'is', '99', '.', '99', ',', 'isn', 't', 'it', '?']\n</code></pre>"},{"location":"exercises/part1-exercises/#_2","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<p>\u3053\u308c\u3089\u306e\u6f14\u7fd2\u3092\u5b8c\u4e86\u3057\u305f\u3089\u3001\u7b2c2\u90e8\u306b\u9032\u3093\u3067Transformer\u306e\u6838\u5fc3\u7684\u306a\u4ed5\u7d44\u307f\u3092\u5b66\u3073\u307e\u3057\u3087\u3046\uff01</p> <p>\ud83d\udca1 \u30d2\u30f3\u30c8: \u89e3\u7b54\u3092\u898b\u308b\u524d\u306b\u3001\u307e\u305a\u81ea\u5206\u3067\u5b9f\u88c5\u3057\u3066\u307f\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u30a8\u30e9\u30fc\u304c\u51fa\u3066\u3082\u3001\u305d\u308c\u304c\u5b66\u7fd2\u306e\u4e00\u90e8\u3067\u3059\uff01</p>"},{"location":"exercises/part2-exercises/","title":"\u7b2c2\u90e8 \u6f14\u7fd2\u554f\u984c","text":""},{"location":"exercises/part2-exercises/#21","title":"\u6f14\u7fd2 2.1: \u30c8\u30fc\u30af\u30f3\u5316\u3068\u57cb\u3081\u8fbc\u307f","text":""},{"location":"exercises/part2-exercises/#1","title":"\u554f\u984c 1","text":"<p>\u7c21\u5358\u306aBPE\uff08Byte Pair Encoding\uff09\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4ee5\u4e0b\u306e\u30b3\u30fc\u30d1\u30b9\u306b\u5bfe\u3057\u3066\u30013\u56de\u306e\u30de\u30fc\u30b8\u64cd\u4f5c\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n</code></pre> \u89e3\u7b54 <pre><code>from collections import defaultdict, Counter\n\ndef get_vocab(corpus):\n    \"\"\"\u5358\u8a9e\u3092\u6587\u5b57\u5358\u4f4d\u306b\u5206\u89e3\u3057\u3001\u983b\u5ea6\u3092\u30ab\u30a6\u30f3\u30c8\"\"\"\n    vocab = defaultdict(int)\n    for word in corpus:\n        word_tokens = ' '.join(list(word)) + ' &lt;/w&gt;'\n        vocab[word_tokens] += 1\n    return vocab\n\ndef get_stats(vocab):\n    \"\"\"\u96a3\u63a5\u3059\u308b\u30c8\u30fc\u30af\u30f3\u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\"\"\"\n    pairs = defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[symbols[i], symbols[i + 1]] += freq\n    return pairs\n\ndef merge_vocab(pair, vocab):\n    \"\"\"\u6700\u983b\u51fa\u30da\u30a2\u3092\u30de\u30fc\u30b8\"\"\"\n    out = {}\n    bigram = ' '.join(pair)\n    replacement = ''.join(pair)\n\n    for word in vocab:\n        new_word = word.replace(bigram, replacement)\n        out[new_word] = vocab[word]\n    return out\n\n# BPE\u5b9f\u884c\ncorpus = [\"low\", \"lower\", \"newest\", \"widest\"]\nvocab = get_vocab(corpus)\nprint(\"\u521d\u671f\u8a9e\u5f59:\", vocab)\n\nfor i in range(3):\n    pairs = get_stats(vocab)\n    if not pairs:\n        break\n\n    best = max(pairs, key=pairs.get)\n    vocab = merge_vocab(best, vocab)\n    print(f\"\\n\u30de\u30fc\u30b8 {i+1}: {best} -&gt; {''.join(best)}\")\n    print(\"\u66f4\u65b0\u5f8c\u306e\u8a9e\u5f59:\", vocab)\n\n# \u51fa\u529b\u4f8b:\n# \u30de\u30fc\u30b8 1: ('e', 's') -&gt; 'es'\n# \u30de\u30fc\u30b8 2: ('es', 't') -&gt; 'est'\n# \u30de\u30fc\u30b8 3: ('l', 'o') -&gt; 'lo'\n</code></pre>"},{"location":"exercises/part2-exercises/#2_1","title":"\u554f\u984c 2","text":"<p>\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> \u89e3\u7b54 <pre><code>import numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\"\"\"\n    dot_product = np.dot(vec1, vec2)\n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n\n    # \u30bc\u30ed\u9664\u7b97\u3092\u907f\u3051\u308b\n    if norm1 == 0 or norm2 == 0:\n        return 0.0\n\n    return dot_product / (norm1 * norm2)\n\n# \u30c6\u30b9\u30c8\n# \u4eee\u60f3\u7684\u306a\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\nembeddings = {\n    \"cat\": np.array([0.2, 0.8, 0.1]),\n    \"dog\": np.array([0.3, 0.7, 0.2]),\n    \"car\": np.array([0.9, 0.1, 0.3]),\n    \"truck\": np.array([0.8, 0.2, 0.4])\n}\n\n# \u985e\u4f3c\u5ea6\u8a08\u7b97\nprint(f\"cat vs dog: {cosine_similarity(embeddings['cat'], embeddings['dog']):.3f}\")\nprint(f\"cat vs car: {cosine_similarity(embeddings['cat'], embeddings['car']):.3f}\")\nprint(f\"car vs truck: {cosine_similarity(embeddings['car'], embeddings['truck']):.3f}\")\n\n# \u6700\u3082\u985e\u4f3c\u3057\u305f\u5358\u8a9e\u3092\u898b\u3064\u3051\u308b\ndef find_most_similar(word, embeddings, top_k=2):\n    target_vec = embeddings[word]\n    similarities = []\n\n    for other_word, other_vec in embeddings.items():\n        if other_word != word:\n            sim = cosine_similarity(target_vec, other_vec)\n            similarities.append((other_word, sim))\n\n    similarities.sort(key=lambda x: x[1], reverse=True)\n    return similarities[:top_k]\n\nprint(f\"\\n'cat'\u306b\u6700\u3082\u985e\u4f3c: {find_most_similar('cat', embeddings)}\")\n</code></pre>"},{"location":"exercises/part2-exercises/#22","title":"\u6f14\u7fd2 2.2: \u6ce8\u610f\u6a5f\u69cb","text":""},{"location":"exercises/part2-exercises/#3","title":"\u554f\u984c 3","text":"<p>\u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u30de\u30b9\u30af\u306e\u30b5\u30dd\u30fc\u30c8\u3082\u542b\u3081\u307e\u3059\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn.functional as F\nimport math\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    \u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u5b9f\u88c5\n\n    Args:\n        Q: \u30af\u30a8\u30ea [batch_size, seq_len, d_k]\n        K: \u30ad\u30fc [batch_size, seq_len, d_k]\n        V: \u30d0\u30ea\u30e5\u30fc [batch_size, seq_len, d_v]\n        mask: \u30de\u30b9\u30af [batch_size, seq_len, seq_len] or None\n\n    Returns:\n        output: \u6ce8\u610f\u306e\u51fa\u529b [batch_size, seq_len, d_v]\n        attention_weights: \u6ce8\u610f\u306e\u91cd\u307f [batch_size, seq_len, seq_len]\n    \"\"\"\n    d_k = Q.size(-1)\n\n    # \u30b9\u30b3\u30a2\u306e\u8a08\u7b97: QK^T / \u221ad_k\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n    # \u30de\u30b9\u30af\u306e\u9069\u7528\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3067\u6ce8\u610f\u306e\u91cd\u307f\u3092\u8a08\u7b97\n    attention_weights = F.softmax(scores, dim=-1)\n\n    # \u91cd\u307f\u4ed8\u304d\u548c\u3092\u8a08\u7b97\n    output = torch.matmul(attention_weights, V)\n\n    return output, attention_weights\n\n# \u30c6\u30b9\u30c8\nbatch_size, seq_len, d_k = 2, 4, 8\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\n# \u30de\u30b9\u30af\u306a\u3057\noutput, weights = scaled_dot_product_attention(Q, K, V)\nprint(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\nprint(f\"\u6ce8\u610f\u91cd\u307f\u5f62\u72b6: {weights.shape}\")\n\n# \u56e0\u679c\u7684\u30de\u30b9\u30af\uff08\u4e0b\u4e09\u89d2\u884c\u5217\uff09\nmask = torch.tril(torch.ones(seq_len, seq_len))\nmask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n\noutput_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask)\nprint(f\"\\n\u30de\u30b9\u30af\u9069\u7528\u5f8c\u306e\u6ce8\u610f\u91cd\u307f\uff08\u6700\u521d\u306e\u30b5\u30f3\u30d7\u30eb\uff09:\")\nprint(weights_masked[0])\n</code></pre>"},{"location":"exercises/part2-exercises/#4","title":"\u554f\u984c 4","text":"<p>\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\u3092\u5b9f\u88c5\u3057\u3001\u4f4d\u7f6e\u306b\u3088\u308b\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u9055\u3044\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass SelfAttention(nn.Module):\n    def __init__(self, d_model, d_k=None):\n        super().__init__()\n        if d_k is None:\n            d_k = d_model\n\n        self.d_k = d_k\n        self.W_q = nn.Linear(d_model, d_k, bias=False)\n        self.W_k = nn.Linear(d_model, d_k, bias=False)\n        self.W_v = nn.Linear(d_model, d_k, bias=False)\n        self.W_o = nn.Linear(d_k, d_model, bias=False)\n\n    def forward(self, x, mask=None):\n        Q = self.W_q(x)\n        K = self.W_k(x)\n        V = self.W_v(x)\n\n        # \u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\n        d_k = self.d_k\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn_weights, V)\n\n        output = self.W_o(context)\n\n        return output, attn_weights\n\n# \u30c6\u30b9\u30c8\u3068\u53ef\u8996\u5316\ntorch.manual_seed(42)\nd_model = 64\nseq_len = 8\n\n# \u30c0\u30df\u30fc\u306e\u5165\u529b\uff08\u4f4d\u7f6e\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\uff09\nx = torch.randn(1, seq_len, d_model)\n\n# \u4f4d\u7f6e\u60c5\u5831\u3092\u8ffd\u52a0\uff08\u7c21\u6613\u7248\uff09\nfor i in range(seq_len):\n    x[0, i, :] += torch.sin(torch.arange(d_model) * i / 10)\n\n# \u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u9069\u7528\nself_attn = SelfAttention(d_model)\noutput, attn_weights = self_attn(x)\n\n# \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nsns.heatmap(attn_weights[0].detach().numpy(), \n            annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=[f'Pos{i}' for i in range(seq_len)],\n            yticklabels=[f'Pos{i}' for i in range(seq_len)])\nplt.title('Self-Attention Pattern')\nplt.xlabel('Keys')\nplt.ylabel('Queries')\nplt.tight_layout()\nplt.show()\n\n# \u5404\u4f4d\u7f6e\u304c\u3069\u3053\u306b\u6ce8\u76ee\u3057\u3066\u3044\u308b\u304b\u5206\u6790\nfor i in range(min(4, seq_len)):\n    top_3 = torch.topk(attn_weights[0, i], 3)\n    print(f\"\u4f4d\u7f6e{i}\u304c\u6700\u3082\u6ce8\u76ee\u3057\u3066\u3044\u308b\u4f4d\u7f6e: {top_3.indices.tolist()}\")\n</code></pre>"},{"location":"exercises/part2-exercises/#23","title":"\u6f14\u7fd2 2.3: \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":""},{"location":"exercises/part2-exercises/#5","title":"\u554f\u984c 5","text":"<p>\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u3001\u7570\u306a\u308b\u6b21\u5143\u3067\u306e\u5468\u671f\u6027\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef positional_encoding(max_len, d_model):\n    \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u751f\u6210\"\"\"\n    pe = np.zeros((max_len, d_model))\n    position = np.arange(0, max_len).reshape(-1, 1)\n\n    # \u5468\u6ce2\u6570\u9805\u306e\u8a08\u7b97\n    div_term = np.exp(np.arange(0, d_model, 2) * \n                      -(np.log(10000.0) / d_model))\n\n    # sin \u3068 cos \u3092\u9069\u7528\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n\n    return pe\n\n# \u751f\u6210\u3068\u53ef\u8996\u5316\nmax_len = 100\nd_model = 64\npe = positional_encoding(max_len, d_model)\n\n# \u7570\u306a\u308b\u6b21\u5143\u3067\u306e\u5468\u671f\u6027\u3092\u53ef\u8996\u5316\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# \u6700\u521d\u306e4\u6b21\u5143\nfor i, ax in enumerate(axes.flat):\n    ax.plot(pe[:, i], label=f'dim={i}')\n    ax.set_xlabel('Position')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Positional Encoding - Dimension {i}')\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u5168\u4f53\u50cf\u3092\u8868\u793a\nplt.figure(figsize=(12, 4))\nplt.imshow(pe.T, aspect='auto', cmap='RdBu', \n           extent=[0, max_len, d_model, 0])\nplt.colorbar()\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.title('Positional Encoding Heatmap')\nplt.show()\n\n# \u76f8\u5bfe\u4f4d\u7f6e\u306e\u6027\u8cea\u3092\u78ba\u8a8d\ndef check_relative_position_property(pe, pos1, pos2, k):\n    \"\"\"\u76f8\u5bfe\u4f4d\u7f6ek\u96e2\u308c\u305f\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5185\u7a4d\"\"\"\n    vec1 = pe[pos1]\n    vec2 = pe[pos2]\n    vec1_k = pe[pos1 + k] if pos1 + k &lt; len(pe) else None\n    vec2_k = pe[pos2 + k] if pos2 + k &lt; len(pe) else None\n\n    if vec1_k is not None and vec2_k is not None:\n        dot1 = np.dot(vec1, vec2)\n        dot2 = np.dot(vec1_k, vec2_k)\n        print(f\"\u4f4d\u7f6e{pos1}\u3068{pos2}\u306e\u5185\u7a4d: {dot1:.3f}\")\n        print(f\"\u4f4d\u7f6e{pos1+k}\u3068{pos2+k}\u306e\u5185\u7a4d: {dot2:.3f}\")\n        print(f\"\u5dee: {abs(dot1 - dot2):.3f}\")\n\nprint(\"\\n\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6027\u8cea:\")\ncheck_relative_position_property(pe, 10, 15, 5)\n</code></pre>"},{"location":"exercises/part2-exercises/#6","title":"\u554f\u984c 6","text":"<p>\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3068\u6b63\u5f26\u6ce2\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u6bd4\u8f03\u3059\u308b\u5b9f\u9a13\u3092\u8a2d\u8a08\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LearnablePositionalEmbedding(nn.Module):\n    \"\"\"\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\"\"\"\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        positions = torch.arange(seq_len, device=x.device)\n        return x + self.pos_embedding(positions)\n\nclass SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(np.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# \u7c21\u5358\u306a\u7cfb\u5217\u30bf\u30b9\u30af\u3067\u6bd4\u8f03\nclass PositionAwareModel(nn.Module):\n    def __init__(self, vocab_size, d_model, pos_encoding_type='learnable'):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n        if pos_encoding_type == 'learnable':\n            self.pos_encoding = LearnablePositionalEmbedding(100, d_model)\n        else:\n            self.pos_encoding = SinusoidalPositionalEncoding(100, d_model)\n\n        self.lstm = nn.LSTM(d_model, d_model, batch_first=True)\n        self.output = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        x, _ = self.lstm(x)\n        return self.output(x)\n\n# \u4f4d\u7f6e\u4f9d\u5b58\u30bf\u30b9\u30af\u306e\u4f5c\u6210\uff08\u4f4d\u7f6e\u3092\u53cd\u8ee2\u3059\u308b\uff09\ndef create_position_task(seq_len=10, vocab_size=20, n_samples=100):\n    data = []\n    for _ in range(n_samples):\n        # \u30e9\u30f3\u30c0\u30e0\u306a\u7cfb\u5217\u3092\u751f\u6210\n        seq = torch.randint(3, vocab_size, (seq_len,))\n        # \u53cd\u8ee2\u3057\u305f\u7cfb\u5217\u304c\u76ee\u6a19\n        target = torch.flip(seq, [0])\n        data.append((seq, target))\n    return data\n\n# \u8a13\u7df4\u3068\u8a55\u4fa1\ndef train_and_evaluate(model, data, epochs=50):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for seq, target in data:\n            seq = seq.unsqueeze(0)\n            target = target.unsqueeze(0)\n\n            output = model(seq)\n            loss = F.cross_entropy(output.reshape(-1, output.size(-1)), \n                                 target.reshape(-1))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n            # \u7cbe\u5ea6\u8a08\u7b97\n            _, predicted = output.max(-1)\n            correct += (predicted == target).sum().item()\n            total += target.numel()\n\n        accuracy = correct / total\n        losses.append(total_loss / len(data))\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Loss={losses[-1]:.3f}, Acc={accuracy:.3f}\")\n\n    return losses\n\n# \u5b9f\u9a13\u5b9f\u884c\nprint(\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u6bd4\u8f03\u5b9f\u9a13\")\nprint(\"\u30bf\u30b9\u30af: \u5165\u529b\u7cfb\u5217\u3092\u53cd\u8ee2\u3059\u308b\\n\")\n\nvocab_size = 20\nd_model = 32\ndata = create_position_task()\n\n# \u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\nprint(\"1. \u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f:\")\nmodel_learnable = PositionAwareModel(vocab_size, d_model, 'learnable')\nlosses_learnable = train_and_evaluate(model_learnable, data)\n\nprint(\"\\n2. \u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0:\")\nmodel_sinusoidal = PositionAwareModel(vocab_size, d_model, 'sinusoidal')\nlosses_sinusoidal = train_and_evaluate(model_sinusoidal, data)\n\n# \u7d50\u679c\u306e\u53ef\u8996\u5316\nplt.figure(figsize=(8, 6))\nplt.plot(losses_learnable, label='Learnable')\nplt.plot(losses_sinusoidal, label='Sinusoidal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Position Encoding Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"exercises/part2-exercises/#24","title":"\u6f14\u7fd2 2.4: \u6df1\u5c64\u5b66\u7fd2\u306e\u57fa\u790e","text":""},{"location":"exercises/part2-exercises/#7","title":"\u554f\u984c 7","text":"<p>\u6b8b\u5dee\u63a5\u7d9a\u3092\u542b\u3080\u7c21\u5358\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u88c5\u3057\u3001\u52fe\u914d\u306e\u6d41\u308c\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        out = out + residual  # \u6b8b\u5dee\u63a5\u7d9a\n        return out\n\nclass DeepNetwork(nn.Module):\n    def __init__(self, input_dim, n_blocks, use_residual=True):\n        super().__init__()\n        self.use_residual = use_residual\n        self.blocks = nn.ModuleList()\n\n        for _ in range(n_blocks):\n            if use_residual:\n                self.blocks.append(ResidualBlock(input_dim))\n            else:\n                self.blocks.append(nn.Sequential(\n                    nn.Linear(input_dim, input_dim),\n                    nn.ReLU(),\n                    nn.Linear(input_dim, input_dim)\n                ))\n\n    def forward(self, x):\n        activations = [x]\n        for block in self.blocks:\n            x = block(x)\n            activations.append(x)\n        return x, activations\n\n# \u52fe\u914d\u306e\u6d41\u308c\u3092\u8a18\u9332\ndef record_gradients(model, input_data, target):\n    gradients = []\n\n    def hook_fn(module, grad_input, grad_output):\n        gradients.append(grad_output[0].norm().item())\n\n    # \u30d5\u30c3\u30af\u3092\u767b\u9332\n    handles = []\n    for block in model.blocks:\n        handle = block.register_backward_hook(hook_fn)\n        handles.append(handle)\n\n    # \u9806\u4f1d\u64ad\u3068\u9006\u4f1d\u64ad\n    output, _ = model(input_data)\n    loss = nn.MSELoss()(output, target)\n    loss.backward()\n\n    # \u30d5\u30c3\u30af\u3092\u524a\u9664\n    for handle in handles:\n        handle.remove()\n\n    return gradients[::-1]  # \u9006\u9806\u306b\u3057\u3066\u5165\u529b\u5074\u304b\u3089\u4e26\u3079\u308b\n\n# \u5b9f\u9a13\ninput_dim = 64\nn_blocks = 10\nbatch_size = 32\n\n# \u30c7\u30fc\u30bf\nx = torch.randn(batch_size, input_dim)\ntarget = torch.randn(batch_size, input_dim)\n\n# \u6b8b\u5dee\u63a5\u7d9a\u3042\u308a\u3068\u306a\u3057\u3067\u6bd4\u8f03\nmodel_with_residual = DeepNetwork(input_dim, n_blocks, use_residual=True)\nmodel_without_residual = DeepNetwork(input_dim, n_blocks, use_residual=False)\n\ngrad_with = record_gradients(model_with_residual, x.clone(), target)\ngrad_without = record_gradients(model_without_residual, x.clone(), target)\n\n# \u53ef\u8996\u5316\nplt.figure(figsize=(10, 6))\nblocks = list(range(1, n_blocks + 1))\n\nplt.semilogy(blocks, grad_with, 'bo-', label='With Residual', linewidth=2)\nplt.semilogy(blocks, grad_without, 'ro-', label='Without Residual', linewidth=2)\n\nplt.xlabel('Block Number (from input)')\nplt.ylabel('Gradient Norm (log scale)')\nplt.title('Gradient Flow in Deep Networks')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\u6700\u521d\u306e\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0:\")\nprint(f\"  \u6b8b\u5dee\u63a5\u7d9a\u3042\u308a: {grad_with[0]:.6f}\")\nprint(f\"  \u6b8b\u5dee\u63a5\u7d9a\u306a\u3057: {grad_without[0]:.6f}\")\nprint(f\"  \u6bd4\u7387: {grad_with[0] / grad_without[0]:.2f}x\")\n</code></pre>"},{"location":"exercises/part2-exercises/#_1","title":"\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c","text":""},{"location":"exercises/part2-exercises/#8","title":"\u554f\u984c 8 \ud83c\udf1f","text":"<p>\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u7c21\u6613\u5b9f\u88c5\u3057\u3001\u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n\n        # \u7dda\u5f62\u5909\u63db\u3068\u5f62\u72b6\u5909\u66f4\n        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k)\n        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k)\n        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k)\n\n        # \u8ee2\u7f6e\u3057\u3066\u30d8\u30c3\u30c9\u3092\u5225\u6b21\u5143\u306b\n        Q = Q.transpose(1, 2)  # [batch, heads, seq_len, d_k]\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n\n        # \u5404\u30d8\u30c3\u30c9\u3067\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn_weights, V)\n\n        # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n        context = context.transpose(1, 2).contiguous()\n        context = context.view(batch_size, seq_len, self.d_model)\n\n        output = self.W_o(context)\n\n        return output, attn_weights\n\n# \u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u3055\u305b\u308b\u5b9f\u9a13\ndef create_pattern_data():\n    \"\"\"\u7570\u306a\u308b\u4f9d\u5b58\u95a2\u4fc2\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\"\"\"\n    seq_len = 8\n    d_model = 64\n    batch_size = 100\n\n    data = []\n\n    for _ in range(batch_size):\n        # \u30d1\u30bf\u30fc\u30f31: \u96a3\u63a5\u4f9d\u5b58\n        x1 = torch.randn(seq_len, d_model)\n        for i in range(1, seq_len):\n            x1[i] += 0.5 * x1[i-1]\n\n        # \u30d1\u30bf\u30fc\u30f32: \u9577\u8ddd\u96e2\u4f9d\u5b58\n        x2 = torch.randn(seq_len, d_model)\n        x2[seq_len//2:] += x2[:seq_len//2]\n\n        # \u6df7\u5408\n        x = x1 + x2\n        x = x / x.norm(dim=-1, keepdim=True)\n\n        data.append(x)\n\n    return torch.stack(data)\n\n# \u30e2\u30c7\u30eb\u306e\u8a13\u7df4\ndef train_multihead_attention():\n    d_model = 64\n    n_heads = 4\n    mha = MultiHeadAttention(d_model, n_heads)\n\n    # \u30c7\u30fc\u30bf\u4f5c\u6210\n    data = create_pattern_data()\n\n    # \u81ea\u5df1\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff08\u5165\u529b\u3092\u518d\u69cb\u6210\uff09\n    optimizer = torch.optim.Adam(mha.parameters(), lr=0.001)\n\n    for epoch in range(100):\n        # \u30ce\u30a4\u30ba\u3092\u52a0\u3048\u305f\u5165\u529b\n        noisy_input = data + 0.1 * torch.randn_like(data)\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u9069\u7528\n        output, attn_weights = mha(noisy_input, noisy_input, noisy_input)\n\n        # \u518d\u69cb\u6210\u8aa4\u5dee\n        loss = F.mse_loss(output, data)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if epoch % 20 == 0:\n            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n\n    return mha, data, attn_weights\n\n# \u5b9f\u884c\u3068\u53ef\u8996\u5316\nmha, data, attn_weights = train_multihead_attention()\n\n# \u5404\u30d8\u30c3\u30c9\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nsample_idx = 0  # \u6700\u521d\u306e\u30b5\u30f3\u30d7\u30eb\n\nfor head in range(4):\n    ax = axes[head]\n    attn = attn_weights[sample_idx, head].detach().numpy()\n\n    sns.heatmap(attn, ax=ax, cmap='Blues', cbar=True)\n    ax.set_title(f'Head {head + 1} Attention Pattern')\n    ax.set_xlabel('Key Position')\n    ax.set_ylabel('Query Position')\n\n    # \u4e3b\u8981\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\n    avg_attn = attn.mean(axis=0)\n    main_focus = avg_attn.argmax()\n    print(f\"Head {head + 1} - \u5e73\u5747\u7684\u306a\u7126\u70b9\u4f4d\u7f6e: {main_focus}\")\n\nplt.tight_layout()\nplt.show()\n\n# \u30d8\u30c3\u30c9\u9593\u306e\u591a\u69d8\u6027\u3092\u5b9a\u91cf\u5316\ndef attention_diversity(attn_weights):\n    \"\"\"\u30d8\u30c3\u30c9\u9593\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u591a\u69d8\u6027\u3092\u8a08\u7b97\"\"\"\n    n_heads = attn_weights.shape[1]\n\n    # \u5404\u30d8\u30c3\u30c9\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u3092\u5e73\u5766\u5316\n    flattened = attn_weights.reshape(attn_weights.shape[0], n_heads, -1)\n\n    # \u30d8\u30c3\u30c9\u9593\u306e\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\n    similarities = []\n    for i in range(n_heads):\n        for j in range(i + 1, n_heads):\n            sim = F.cosine_similarity(\n                flattened[:, i], \n                flattened[:, j], \n                dim=-1\n            ).mean().item()\n            similarities.append(sim)\n\n    # \u591a\u69d8\u6027 = 1 - \u5e73\u5747\u985e\u4f3c\u5ea6\n    diversity = 1 - np.mean(similarities)\n    return diversity\n\ndiversity = attention_diversity(attn_weights)\nprint(f\"\\n\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d8\u30c3\u30c9\u306e\u591a\u69d8\u6027: {diversity:.3f}\")\nprint(\"(1\u306b\u8fd1\u3044\u307b\u3069\u591a\u69d8\u30010\u306b\u8fd1\u3044\u307b\u3069\u985e\u4f3c)\")\n</code></pre>"},{"location":"exercises/part2-exercises/#_2","title":"\u5b9f\u8df5\u30d7\u30ed\u30b8\u30a7\u30af\u30c8 \ud83d\ude80","text":""},{"location":"exercises/part2-exercises/#_3","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8: \u30df\u30cb\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5","text":"<p>\u7b2c2\u90e8\u3067\u5b66\u3093\u3060\u3059\u3079\u3066\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u5c0f\u3055\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>\u8981\u4ef6\uff1a - \u30c8\u30fc\u30af\u30f3\u5316\uff08\u7c21\u5358\u306a\u7a7a\u767d\u533a\u5207\u308a\uff09 - \u57cb\u3081\u8fbc\u307f\u5c64 - \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 - \u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff081\u5c64\uff09 - \u51fa\u529b\u5c64</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MiniLanguageModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, max_len=100):\n        super().__init__()\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        self.position_embedding = nn.Embedding(max_len, d_model)\n\n        # \u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\n        self.attention = nn.MultiheadAttention(d_model, num_heads=4, \n                                              batch_first=True)\n\n        # \u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),\n            nn.ReLU(),\n            nn.Linear(d_model * 4, d_model)\n        )\n\n        # \u5c64\u6b63\u898f\u5316\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n        # \u51fa\u529b\u5c64\n        self.output = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x, mask=None):\n        seq_len = x.size(1)\n\n        # \u57cb\u3081\u8fbc\u307f\n        token_emb = self.token_embedding(x)\n        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        pos_emb = self.position_embedding(pos_ids)\n\n        x = token_emb + pos_emb\n\n        # \u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff08\u6b8b\u5dee\u63a5\u7d9a\u4ed8\u304d\uff09\n        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n        x = self.norm1(x + attn_output)\n\n        # \u30d5\u30a3\u30fc\u30c9\u30d5\u30a9\u30ef\u30fc\u30c9\uff08\u6b8b\u5dee\u63a5\u7d9a\u4ed8\u304d\uff09\n        ff_output = self.ffn(x)\n        x = self.norm2(x + ff_output)\n\n        # \u51fa\u529b\n        return self.output(x)\n\n    def generate(self, start_tokens, max_length=50):\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u751f\u6210\"\"\"\n        self.eval()\n        generated = start_tokens.clone()\n\n        with torch.no_grad():\n            for _ in range(max_length):\n                # \u73fe\u5728\u306e\u7cfb\u5217\u3067\u4e88\u6e2c\n                outputs = self(generated)\n\n                # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u4e88\u6e2c\u3092\u53d6\u5f97\n                next_token_logits = outputs[:, -1, :]\n\n                # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, 1)\n\n                # \u8ffd\u52a0\n                generated = torch.cat([generated, next_token], dim=1)\n\n                # \u7d42\u4e86\u30c8\u30fc\u30af\u30f3\u306e\u30c1\u30a7\u30c3\u30af\uff08\u5b9f\u88c5\u306b\u3088\u308b\uff09\n\n        return generated\n\n# \u7c21\u5358\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\nclass SimpleTokenizer:\n    def __init__(self, texts):\n        self.word_to_id = {'&lt;pad&gt;': 0, '&lt;unk&gt;': 1, '&lt;sos&gt;': 2, '&lt;eos&gt;': 3}\n        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n\n        # \u8a9e\u5f59\u69cb\u7bc9\n        for text in texts:\n            for word in text.lower().split():\n                if word not in self.word_to_id:\n                    idx = len(self.word_to_id)\n                    self.word_to_id[word] = idx\n                    self.id_to_word[idx] = word\n\n    def encode(self, text):\n        tokens = []\n        for word in text.lower().split():\n            tokens.append(self.word_to_id.get(word, 1))  # 1 = &lt;unk&gt;\n        return tokens\n\n    def decode(self, tokens):\n        words = []\n        for token in tokens:\n            if token in self.id_to_word:\n                words.append(self.id_to_word[token])\n        return ' '.join(words)\n\n# \u4f7f\u7528\u4f8b\ntexts = [\n    \"the cat sat on the mat\",\n    \"the dog played in the park\",\n    \"cats and dogs are pets\",\n    \"the sun shines bright\",\n    \"birds fly in the sky\"\n]\n\n# \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u4f5c\u6210\ntokenizer = SimpleTokenizer(texts)\nprint(f\"\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(tokenizer.word_to_id)}\")\n\n# \u30e2\u30c7\u30eb\u4f5c\u6210\nmodel = MiniLanguageModel(len(tokenizer.word_to_id))\n\n# \u30c7\u30fc\u30bf\u6e96\u5099\nencoded_texts = [torch.tensor(tokenizer.encode(text)) for text in texts]\n\n# \u7c21\u5358\u306a\u8a13\u7df4\u30eb\u30fc\u30d7\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(50):\n    total_loss = 0\n\n    for encoded in encoded_texts:\n        # \u30d0\u30c3\u30c1\u6b21\u5143\u3092\u8ffd\u52a0\n        x = encoded[:-1].unsqueeze(0)\n        y = encoded[1:].unsqueeze(0)\n\n        # \u4e88\u6e2c\n        outputs = model(x)\n        loss = F.cross_entropy(outputs.reshape(-1, outputs.size(-1)), \n                             y.reshape(-1))\n\n        # \u6700\u9069\u5316\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: Loss = {total_loss / len(texts):.4f}\")\n\n# \u751f\u6210\u30c6\u30b9\u30c8\nstart = torch.tensor([[tokenizer.word_to_id['the']]])\ngenerated = model.generate(start, max_length=10)\nprint(f\"\\n\u751f\u6210: {tokenizer.decode(generated[0].tolist())}\")\n</code></pre> <p>\u3053\u308c\u3067\u7b2c2\u90e8\u306e\u6f14\u7fd2\u306f\u5b8c\u4e86\u3067\u3059\uff01\u6b21\u306f\u7b2c3\u90e8\u3067Transformer\u306e\u8a73\u7d30\u306a\u69cb\u6210\u8981\u7d20\u3092\u5b66\u3073\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"exercises/part3-exercises/","title":"\u7b2c3\u90e8 \u6f14\u7fd2\u554f\u984c","text":""},{"location":"exercises/part3-exercises/#31-multi-head-attention","title":"\u6f14\u7fd2 3.1: Multi-Head Attention","text":""},{"location":"exercises/part3-exercises/#1","title":"\u554f\u984c 1","text":"<p>8\u30d8\u30c3\u30c9\u306eMulti-Head Attention\u3092\u5b9f\u88c5\u3057\u3001\u5404\u30d8\u30c3\u30c9\u304c\u5b66\u7fd2\u3059\u308b\u7279\u5fb4\u306e\u9055\u3044\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nclass MultiHeadAttentionDetailed(nn.Module):\n    def __init__(self, d_model=512, n_heads=8, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        # \u5404\u30d8\u30c3\u30c9\u7528\u306e\u91cd\u307f\u884c\u5217\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.scale = 1.0 / np.sqrt(self.d_k)\n\n        # \u30d8\u30c3\u30c9\u3054\u3068\u306e\u7d71\u8a08\u3092\u8a18\u9332\n        self.head_statistics = {}\n\n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n\n        # Q, K, V \u306e\u8a08\u7b97\n        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k)\n        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k)\n        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k)\n\n        # \u8ee2\u7f6e: [batch, n_heads, seq_len, d_k]\n        Q = Q.transpose(1, 2)\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30b3\u30a2\n        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # \u5404\u30d8\u30c3\u30c9\u306e\u7d71\u8a08\u3092\u8a18\u9332\n        self._record_head_statistics(attn_weights)\n\n        # \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\n        context = torch.matmul(attn_weights, V)\n\n        # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n        context = context.transpose(1, 2).contiguous()\n        context = context.view(batch_size, seq_len, self.d_model)\n\n        # \u51fa\u529b\u6295\u5f71\n        output = self.W_o(context)\n\n        return output, attn_weights\n\n    def _record_head_statistics(self, attn_weights):\n        \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u8a18\u9332\"\"\"\n        with torch.no_grad():\n            batch_size, n_heads, seq_len, _ = attn_weights.shape\n\n            for head in range(n_heads):\n                head_attn = attn_weights[:, head, :, :]\n\n                # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\uff08\u96c6\u4e2d\u5ea6\u306e\u6307\u6a19\uff09\n                entropy = -(head_attn * torch.log(head_attn + 1e-9)).sum(dim=-1).mean()\n\n                # \u5e73\u5747\u7684\u306a\u6ce8\u610f\u8ddd\u96e2\n                positions = torch.arange(seq_len, device=attn_weights.device)\n                pos_diff = positions.unsqueeze(0) - positions.unsqueeze(1)\n                avg_distance = (head_attn * pos_diff.abs().float()).sum(dim=-1).mean()\n\n                # \u5bfe\u89d2\u6210\u5206\u306e\u5f37\u3055\uff08\u81ea\u5df1\u6ce8\u610f\u306e\u5ea6\u5408\u3044\uff09\n                diag_strength = torch.diagonal(head_attn, dim1=-2, dim2=-1).mean()\n\n                if head not in self.head_statistics:\n                    self.head_statistics[head] = {\n                        'entropy': [],\n                        'avg_distance': [],\n                        'diag_strength': []\n                    }\n\n                self.head_statistics[head]['entropy'].append(entropy.item())\n                self.head_statistics[head]['avg_distance'].append(avg_distance.item())\n                self.head_statistics[head]['diag_strength'].append(diag_strength.item())\n\n# \u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u5206\u6790\u3059\u308b\u5b9f\u9a13\ndef analyze_head_specialization():\n    # \u30e2\u30c7\u30eb\u4f5c\u6210\n    d_model = 512\n    n_heads = 8\n    seq_len = 20\n    batch_size = 32\n\n    mha = MultiHeadAttentionDetailed(d_model, n_heads)\n\n    # \u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3067\u8a13\u7df4\n    print(\"\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3067\u8a13\u7df4\u4e2d...\")\n\n    optimizer = torch.optim.Adam(mha.parameters(), lr=0.001)\n\n    for step in range(100):\n        # \u30d1\u30bf\u30fc\u30f31: \u5c40\u6240\u7684\u306a\u4f9d\u5b58\u95a2\u4fc2\n        local_data = torch.randn(batch_size, seq_len, d_model)\n        for i in range(1, seq_len):\n            local_data[:, i] += 0.5 * local_data[:, i-1]\n\n        # \u30d1\u30bf\u30fc\u30f32: \u9577\u8ddd\u96e2\u4f9d\u5b58\n        long_range_data = torch.randn(batch_size, seq_len, d_model)\n        long_range_data[:, seq_len//2:] += long_range_data[:, :seq_len//2]\n\n        # \u30d1\u30bf\u30fc\u30f33: \u5468\u671f\u7684\u30d1\u30bf\u30fc\u30f3\n        periodic_data = torch.randn(batch_size, seq_len, d_model)\n        period = 5\n        for i in range(period, seq_len):\n            periodic_data[:, i] += 0.3 * periodic_data[:, i-period]\n\n        # \u6df7\u5408\u30c7\u30fc\u30bf\n        data = (local_data + long_range_data + periodic_data) / 3\n\n        # Multi-Head Attention\u9069\u7528\n        output, attn_weights = mha(data, data, data)\n\n        # \u81ea\u5df1\u6559\u5e2b\u3042\u308a\u640d\u5931\uff08\u5165\u529b\u306e\u518d\u69cb\u6210\uff09\n        loss = F.mse_loss(output, data)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if step % 20 == 0:\n            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n\n    # \u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u53ef\u8996\u5316\n    visualize_head_characteristics(mha, attn_weights)\n\ndef visualize_head_characteristics(mha, sample_attn_weights):\n    \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u53ef\u8996\u5316\"\"\"\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.flatten()\n\n    # \u30b5\u30f3\u30d7\u30eb\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\n    sample_attn = sample_attn_weights[0].detach().cpu().numpy()\n\n    for head in range(8):\n        ax = axes[head]\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        sns.heatmap(sample_attn[head], ax=ax, cmap='Blues', \n                   cbar_kws={'label': 'Weight'})\n\n        # \u7d71\u8a08\u60c5\u5831\u3092\u8ffd\u52a0\n        if head in mha.head_statistics:\n            stats = mha.head_statistics[head]\n            avg_entropy = np.mean(stats['entropy'])\n            avg_distance = np.mean(stats['avg_distance'])\n            avg_diag = np.mean(stats['diag_strength'])\n\n            ax.set_title(f'Head {head+1}\\n'\n                       f'Ent:{avg_entropy:.2f}, '\n                       f'Dist:{avg_distance:.1f}, '\n                       f'Diag:{avg_diag:.2f}',\n                       fontsize=10)\n        else:\n            ax.set_title(f'Head {head+1}')\n\n        ax.set_xlabel('Key Position')\n        ax.set_ylabel('Query Position')\n\n    plt.suptitle('Multi-Head Attention Pattern Analysis', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    # \u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u30ec\u30fc\u30c0\u30fc\u30c1\u30e3\u30fc\u30c8\u3067\u8868\u793a\n    plot_head_characteristics_radar(mha)\n\ndef plot_head_characteristics_radar(mha):\n    \"\"\"\u30d8\u30c3\u30c9\u306e\u7279\u6027\u3092\u30ec\u30fc\u30c0\u30fc\u30c1\u30e3\u30fc\u30c8\u3067\u8868\u793a\"\"\"\n    if not mha.head_statistics:\n        return\n\n    # \u5404\u30d8\u30c3\u30c9\u306e\u5e73\u5747\u7d71\u8a08\u3092\u8a08\u7b97\n    head_profiles = []\n    for head in range(8):\n        if head in mha.head_statistics:\n            stats = mha.head_statistics[head]\n            profile = [\n                np.mean(stats['entropy']),\n                np.mean(stats['avg_distance']),\n                np.mean(stats['diag_strength'])\n            ]\n            head_profiles.append(profile)\n\n    # \u6b63\u898f\u5316\n    head_profiles = np.array(head_profiles)\n    head_profiles = (head_profiles - head_profiles.min(axis=0)) / \\\n                   (head_profiles.max(axis=0) - head_profiles.min(axis=0) + 1e-8)\n\n    # \u30ec\u30fc\u30c0\u30fc\u30c1\u30e3\u30fc\u30c8\n    categories = ['Entropy', 'Avg Distance', 'Diagonal']\n    fig = plt.figure(figsize=(10, 8))\n\n    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n    angles += angles[:1]\n\n    for i, profile in enumerate(head_profiles):\n        values = profile.tolist()\n        values += values[:1]\n\n        ax = plt.subplot(2, 4, i+1, projection='polar')\n        ax.plot(angles, values, 'o-', linewidth=2, label=f'Head {i+1}')\n        ax.fill(angles, values, alpha=0.25)\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels(categories)\n        ax.set_ylim(0, 1)\n        ax.set_title(f'Head {i+1}', y=1.08)\n        ax.grid(True)\n\n    plt.suptitle('Head Characteristic Profiles', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# \u5b9f\u884c\nanalyze_head_specialization()\n</code></pre>"},{"location":"exercises/part3-exercises/#2","title":"\u554f\u984c 2","text":"<p>Grouped Query Attention (GQA) \u3092\u5b9f\u88c5\u3057\u3001\u901a\u5e38\u306eMulti-Head Attention\u3068\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class GroupedQueryAttention(nn.Module):\n    \"\"\"Grouped Query Attention (GQA) \u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model=512, n_heads=8, n_kv_heads=2):\n        super().__init__()\n        assert d_model % n_heads == 0\n        assert n_heads % n_kv_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_kv_heads = n_kv_heads\n        self.n_groups = n_heads // n_kv_heads\n        self.d_k = d_model // n_heads\n\n        # Query\u7528\u306e\u6295\u5f71\uff08\u5168\u30d8\u30c3\u30c9\u5206\uff09\n        self.W_q = nn.Linear(d_model, d_model)\n\n        # Key/Value\u7528\u306e\u6295\u5f71\uff08\u30b0\u30eb\u30fc\u30d7\u6570\u5206\u306e\u307f\uff09\n        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k)\n        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k)\n\n        # \u51fa\u529b\u6295\u5f71\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n\n        # Query: \u5168\u30d8\u30c3\u30c9\u5206\n        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k)\n        Q = Q.transpose(1, 2)  # [batch, n_heads, seq_len, d_k]\n\n        # Key/Value: \u30b0\u30eb\u30fc\u30d7\u6570\u5206\u306e\u307f\n        K = self.W_k(key).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n        K = K.transpose(1, 2)  # [batch, n_kv_heads, seq_len, d_k]\n\n        V = self.W_v(value).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n        V = V.transpose(1, 2)  # [batch, n_kv_heads, seq_len, d_k]\n\n        # Key/Value\u3092\u5404\u30b0\u30eb\u30fc\u30d7\u3067\u5171\u6709\n        K = K.repeat_interleave(self.n_groups, dim=1)\n        V = V.repeat_interleave(self.n_groups, dim=1)\n\n        # \u901a\u5e38\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn_weights, V)\n\n        # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n        context = context.transpose(1, 2).contiguous()\n        context = context.view(batch_size, seq_len, self.d_model)\n\n        output = self.W_o(context)\n\n        return output, attn_weights\n\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3068\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\ndef compare_attention_variants():\n    d_model = 512\n    seq_len = 100\n    batch_size = 8\n\n    # \u901a\u5e38\u306eMHA\n    mha = MultiHeadAttentionDetailed(d_model, n_heads=8)\n\n    # GQA\uff082\u3064\u306eKV\u30d8\u30c3\u30c9\uff09\n    gqa = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=2)\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u6bd4\u8f03\n    mha_params = sum(p.numel() for p in mha.parameters())\n    gqa_params = sum(p.numel() for p in gqa.parameters())\n\n    print(\"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u6bd4\u8f03:\")\n    print(f\"Multi-Head Attention: {mha_params:,}\")\n    print(f\"Grouped Query Attention: {gqa_params:,}\")\n    print(f\"\u524a\u6e1b\u7387: {(1 - gqa_params/mha_params)*100:.1f}%\\n\")\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\uff08KV\u30ad\u30e3\u30c3\u30b7\u30e5\uff09\n    kv_cache_mha = 2 * batch_size * 8 * seq_len * (d_model // 8) * 4  # float32\n    kv_cache_gqa = 2 * batch_size * 2 * seq_len * (d_model // 8) * 4  # float32\n\n    print(\"KV\u30ad\u30e3\u30c3\u30b7\u30e5\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf:\")\n    print(f\"Multi-Head Attention: {kv_cache_mha / 1024**2:.2f} MB\")\n    print(f\"Grouped Query Attention: {kv_cache_gqa / 1024**2:.2f} MB\")\n    print(f\"\u524a\u6e1b\u7387: {(1 - kv_cache_gqa/kv_cache_mha)*100:.1f}%\\n\")\n\n    # \u901f\u5ea6\u6bd4\u8f03\n    import time\n\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # MHA\n    start = time.time()\n    for _ in range(100):\n        _ = mha(x, x, x)\n    mha_time = time.time() - start\n\n    # GQA\n    start = time.time()\n    for _ in range(100):\n        _ = gqa(x, x, x)\n    gqa_time = time.time() - start\n\n    print(\"\u63a8\u8ad6\u901f\u5ea6 (100\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3):\")\n    print(f\"Multi-Head Attention: {mha_time:.3f}\u79d2\")\n    print(f\"Grouped Query Attention: {gqa_time:.3f}\u79d2\")\n    print(f\"\u9ad8\u901f\u5316: {mha_time/gqa_time:.2f}x\")\n\ncompare_attention_variants()\n</code></pre>"},{"location":"exercises/part3-exercises/#32-feed-forward-network","title":"\u6f14\u7fd2 3.2: Feed Forward Network","text":""},{"location":"exercises/part3-exercises/#3_1","title":"\u554f\u984c 3","text":"<p>\u7570\u306a\u308b\u6d3b\u6027\u5316\u95a2\u6570\uff08ReLU, GELU, SwiGLU\uff09\u3092\u4f7f\u7528\u3057\u305fFFN\u3092\u5b9f\u88c5\u3057\u3001\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class FFNComparison:\n    \"\"\"\u7570\u306a\u308b\u6d3b\u6027\u5316\u95a2\u6570\u3092\u6301\u3064FFN\u306e\u6bd4\u8f03\"\"\"\n\n    def __init__(self, d_model=512, d_ff=2048):\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n    def create_ffn_variants(self):\n        \"\"\"\u7570\u306a\u308bFFN\u30d0\u30ea\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\"\"\"\n\n        class FFN_ReLU(nn.Module):\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                self.fc1 = nn.Linear(d_model, d_ff)\n                self.fc2 = nn.Linear(d_ff, d_model)\n                self.dropout = nn.Dropout(0.1)\n\n            def forward(self, x):\n                x = F.relu(self.fc1(x))\n                x = self.dropout(x)\n                x = self.fc2(x)\n                return x\n\n        class FFN_GELU(nn.Module):\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                self.fc1 = nn.Linear(d_model, d_ff)\n                self.fc2 = nn.Linear(d_ff, d_model)\n                self.dropout = nn.Dropout(0.1)\n\n            def forward(self, x):\n                x = F.gelu(self.fc1(x))\n                x = self.dropout(x)\n                x = self.fc2(x)\n                return x\n\n        class FFN_SwiGLU(nn.Module):\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                # SwiGLU\u306f2\u500d\u306e\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\u304c\u5fc5\u8981\n                self.fc1 = nn.Linear(d_model, d_ff * 2)\n                self.fc2 = nn.Linear(d_ff, d_model)\n                self.dropout = nn.Dropout(0.1)\n\n            def forward(self, x):\n                x = self.fc1(x)\n                # \u534a\u5206\u306b\u5206\u5272\n                x1, x2 = x.chunk(2, dim=-1)\n                # Swish(x1) * x2\n                x = F.silu(x1) * x2\n                x = self.dropout(x)\n                x = self.fc2(x)\n                return x\n\n        return {\n            'ReLU': FFN_ReLU(self.d_model, self.d_ff),\n            'GELU': FFN_GELU(self.d_model, self.d_ff),\n            'SwiGLU': FFN_SwiGLU(self.d_model, self.d_ff)\n        }\n\n    def compare_activations(self):\n        \"\"\"\u6d3b\u6027\u5316\u95a2\u6570\u306e\u6bd4\u8f03\"\"\"\n        # \u5165\u529b\u7bc4\u56f2\n        x = torch.linspace(-3, 3, 1000)\n\n        # \u6d3b\u6027\u5316\u95a2\u6570\n        relu = F.relu(x)\n        gelu = F.gelu(x)\n        swish = F.silu(x)\n\n        # \u30d7\u30ed\u30c3\u30c8\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 3, 1)\n        plt.plot(x, relu, label='ReLU', linewidth=2)\n        plt.plot(x, gelu, label='GELU', linewidth=2)\n        plt.plot(x, swish, label='Swish/SiLU', linewidth=2)\n        plt.xlabel('Input')\n        plt.ylabel('Output')\n        plt.title('Activation Functions')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n\n        # \u5c0e\u95a2\u6570\n        x.requires_grad = True\n\n        relu_grad = torch.autograd.grad(F.relu(x).sum(), x, retain_graph=True)[0]\n        gelu_grad = torch.autograd.grad(F.gelu(x).sum(), x, retain_graph=True)[0]\n        swish_grad = torch.autograd.grad(F.silu(x).sum(), x, retain_graph=True)[0]\n\n        plt.subplot(1, 3, 2)\n        plt.plot(x.detach(), relu_grad.detach(), label='ReLU', linewidth=2)\n        plt.plot(x.detach(), gelu_grad.detach(), label='GELU', linewidth=2)\n        plt.plot(x.detach(), swish_grad.detach(), label='Swish/SiLU', linewidth=2)\n        plt.xlabel('Input')\n        plt.ylabel('Gradient')\n        plt.title('Derivatives')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n\n        # \u30b9\u30d1\u30fc\u30b9\u6027\u306e\u6bd4\u8f03\n        plt.subplot(1, 3, 3)\n        sparsity_threshold = 0.01\n        relu_sparsity = (relu &lt; sparsity_threshold).float().mean()\n        gelu_sparsity = (gelu.abs() &lt; sparsity_threshold).float().mean()\n        swish_sparsity = (swish.abs() &lt; sparsity_threshold).float().mean()\n\n        plt.bar(['ReLU', 'GELU', 'Swish'], \n               [relu_sparsity, gelu_sparsity, swish_sparsity])\n        plt.ylabel('Sparsity Rate')\n        plt.title('Output Sparsity')\n\n        plt.tight_layout()\n        plt.show()\n\n    def train_and_compare(self):\n        \"\"\"\u7570\u306a\u308bFFN\u306e\u8a13\u7df4\u3068\u6bd4\u8f03\"\"\"\n        ffn_variants = self.create_ffn_variants()\n\n        # \u7c21\u5358\u306a\u30bf\u30b9\u30af\uff1a\u975e\u7dda\u5f62\u5909\u63db\u306e\u5b66\u7fd2\n        batch_size = 64\n        seq_len = 50\n\n        # \u30c7\u30fc\u30bf\u751f\u6210\n        X = torch.randn(1000, seq_len, self.d_model)\n        # \u8907\u96d1\u306a\u975e\u7dda\u5f62\u5909\u63db\n        Y = torch.sin(X) + torch.cos(2 * X) * 0.5\n\n        results = {}\n\n        for name, model in ffn_variants.items():\n            print(f\"\\n\u8a13\u7df4\u4e2d: {name} FFN\")\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n            losses = []\n\n            for epoch in range(100):\n                # \u30df\u30cb\u30d0\u30c3\u30c1\n                idx = torch.randperm(len(X))[:batch_size]\n                batch_x = X[idx]\n                batch_y = Y[idx]\n\n                # \u4e88\u6e2c\n                pred = model(batch_x)\n                loss = F.mse_loss(pred, batch_y)\n\n                # \u6700\u9069\u5316\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                losses.append(loss.item())\n\n                if epoch % 20 == 0:\n                    print(f\"  Epoch {epoch}: Loss = {loss.item():.4f}\")\n\n            results[name] = losses\n\n        # \u7d50\u679c\u306e\u53ef\u8996\u5316\n        plt.figure(figsize=(10, 6))\n        for name, losses in results.items():\n            plt.plot(losses, label=name, linewidth=2)\n\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('FFN Training Comparison')\n        plt.legend()\n        plt.yscale('log')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n        # \u6700\u7d42\u6027\u80fd\u306e\u6bd4\u8f03\n        print(\"\\n\u6700\u7d42\u640d\u5931:\")\n        for name, losses in results.items():\n            print(f\"{name}: {losses[-1]:.4f}\")\n\n# \u5b9f\u884c\nffn_comp = FFNComparison()\nffn_comp.compare_activations()\nffn_comp.train_and_compare()\n</code></pre>"},{"location":"exercises/part3-exercises/#4","title":"\u554f\u984c 4","text":"<p>Mixture of Experts (MoE) \u30ec\u30a4\u30e4\u30fc\u3092\u5b9f\u88c5\u3057\u3001\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u9078\u629e\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class MixtureOfExperts(nn.Module):\n    \"\"\"Mixture of Experts (MoE) \u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model=512, d_ff=2048, n_experts=8, top_k=2):\n        super().__init__()\n        self.d_model = d_model\n        self.n_experts = n_experts\n        self.top_k = top_k\n\n        # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\uff08\u5404\u3005\u304cFFN\uff09\n        self.experts = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(d_model, d_ff),\n                nn.ReLU(),\n                nn.Linear(d_ff, d_model)\n            ) for _ in range(n_experts)\n        ])\n\n        # \u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n        self.gate = nn.Linear(d_model, n_experts)\n\n        # \u30ed\u30fc\u30c9\u30d0\u30e9\u30f3\u30b7\u30f3\u30b0\u7528\u306e\u640d\u5931\u4fc2\u6570\n        self.load_balance_loss = 0.0\n\n        # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u7d71\u8a08\n        self.expert_usage = torch.zeros(n_experts)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u30b2\u30fc\u30c8\u5024\u306e\u8a08\u7b97\n        gate_logits = self.gate(x)  # [batch, seq_len, n_experts]\n\n        # Top-k\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u9078\u629e\n        topk_gate_values, topk_indices = torch.topk(\n            gate_logits, self.top_k, dim=-1\n        )\n\n        # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3067\u6b63\u898f\u5316\n        topk_gate_values = F.softmax(topk_gate_values, dim=-1)\n\n        # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u7d71\u8a08\u306e\u66f4\u65b0\n        self._update_expert_usage(topk_indices)\n\n        # \u51fa\u529b\u306e\u521d\u671f\u5316\n        output = torch.zeros_like(x)\n\n        # \u5404\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u51e6\u7406\n        for i in range(self.top_k):\n            # \u5404\u4f4d\u7f6e\u3067i\u756a\u76ee\u306b\u9078\u3070\u308c\u305f\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\n            expert_idx = topk_indices[..., i]  # [batch, seq_len]\n            gate_value = topk_gate_values[..., i:i+1]  # [batch, seq_len, 1]\n\n            # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3054\u3068\u306b\u51e6\u7406\n            for e in range(self.n_experts):\n                # \u3053\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u304c\u9078\u3070\u308c\u305f\u4f4d\u7f6e\n                mask = (expert_idx == e)\n                if mask.any():\n                    # \u30de\u30b9\u30af\u3055\u308c\u305f\u5165\u529b\u3092\u62bd\u51fa\n                    expert_input = x[mask]\n\n                    # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3092\u9069\u7528\n                    expert_output = self.experts[e](expert_input)\n\n                    # \u91cd\u307f\u4ed8\u3051\u3057\u3066\u51fa\u529b\u306b\u52a0\u7b97\n                    output[mask] += expert_output * gate_value[mask]\n\n        # \u30ed\u30fc\u30c9\u30d0\u30e9\u30f3\u30b7\u30f3\u30b0\u640d\u5931\u306e\u8a08\u7b97\n        self._compute_load_balance_loss(gate_logits)\n\n        return output\n\n    def _update_expert_usage(self, selected_experts):\n        \"\"\"\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u7d71\u8a08\u3092\u66f4\u65b0\"\"\"\n        with torch.no_grad():\n            for e in range(self.n_experts):\n                usage = (selected_experts == e).float().sum()\n                self.expert_usage[e] = 0.9 * self.expert_usage[e] + 0.1 * usage\n\n    def _compute_load_balance_loss(self, gate_logits):\n        \"\"\"\u30ed\u30fc\u30c9\u30d0\u30e9\u30f3\u30b7\u30f3\u30b0\u640d\u5931\u3092\u8a08\u7b97\"\"\"\n        # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3054\u3068\u306e\u5e73\u5747\u30b2\u30fc\u30c8\u5024\n        gate_probs = F.softmax(gate_logits, dim=-1)\n        expert_probs = gate_probs.mean(dim=[0, 1])\n\n        # \u5747\u7b49\u5206\u5e03\u304b\u3089\u306e\u4e56\u96e2\n        uniform_prob = 1.0 / self.n_experts\n        self.load_balance_loss = ((expert_probs - uniform_prob) ** 2).sum()\n\n    def visualize_expert_usage(self):\n        \"\"\"\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\"\"\"\n        plt.figure(figsize=(10, 6))\n\n        # \u4f7f\u7528\u983b\u5ea6\n        plt.subplot(1, 2, 1)\n        plt.bar(range(self.n_experts), self.expert_usage.numpy())\n        plt.xlabel('Expert ID')\n        plt.ylabel('Usage Count')\n        plt.title('Expert Usage Distribution')\n\n        # \u4f7f\u7528\u7387\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\uff08\u6642\u7cfb\u5217\uff09\n        plt.subplot(1, 2, 2)\n        # \u30c0\u30df\u30fc\u306e\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\uff08\u5b9f\u969b\u306f\u8a13\u7df4\u4e2d\u306b\u8a18\u9332\uff09\n        usage_history = torch.rand(50, self.n_experts)\n        plt.imshow(usage_history.T, aspect='auto', cmap='hot')\n        plt.xlabel('Time Step')\n        plt.ylabel('Expert ID')\n        plt.title('Expert Usage Over Time')\n        plt.colorbar(label='Usage Rate')\n\n        plt.tight_layout()\n        plt.show()\n\n# MoE\u306e\u8a13\u7df4\u3068\u5206\u6790\ndef train_and_analyze_moe():\n    d_model = 256\n    moe = MixtureOfExperts(d_model=d_model, n_experts=8, top_k=2)\n\n    # \u7570\u306a\u308b\u7279\u6027\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3092\u751f\u6210\n    n_samples = 1000\n    seq_len = 20\n\n    # \u30bf\u30a4\u30d71: \u4f4e\u5468\u6ce2\u30d1\u30bf\u30fc\u30f3\n    data_type1 = torch.sin(torch.linspace(0, 4*np.pi, seq_len)).unsqueeze(0).unsqueeze(-1)\n    data_type1 = data_type1.expand(n_samples//3, seq_len, d_model)\n    data_type1 += torch.randn_like(data_type1) * 0.1\n\n    # \u30bf\u30a4\u30d72: \u9ad8\u5468\u6ce2\u30d1\u30bf\u30fc\u30f3\n    data_type2 = torch.sin(torch.linspace(0, 20*np.pi, seq_len)).unsqueeze(0).unsqueeze(-1)\n    data_type2 = data_type2.expand(n_samples//3, seq_len, d_model)\n    data_type2 += torch.randn_like(data_type2) * 0.1\n\n    # \u30bf\u30a4\u30d73: \u30e9\u30f3\u30c0\u30e0\u30ce\u30a4\u30ba\n    data_type3 = torch.randn(n_samples//3, seq_len, d_model)\n\n    # \u5168\u30c7\u30fc\u30bf\u3092\u7d50\u5408\n    all_data = torch.cat([data_type1, data_type2, data_type3], dim=0)\n    labels = torch.cat([\n        torch.zeros(n_samples//3),\n        torch.ones(n_samples//3),\n        torch.ones(n_samples//3) * 2\n    ])\n\n    # \u8a13\u7df4\n    optimizer = torch.optim.Adam(moe.parameters(), lr=0.001)\n\n    print(\"MoE\u8a13\u7df4\u4e2d...\")\n    for epoch in range(100):\n        # \u30b7\u30e3\u30c3\u30d5\u30eb\n        perm = torch.randperm(n_samples)\n        all_data = all_data[perm]\n        labels = labels[perm]\n\n        # \u30d0\u30c3\u30c1\u51e6\u7406\n        batch_size = 32\n        total_loss = 0\n\n        for i in range(0, n_samples, batch_size):\n            batch_data = all_data[i:i+batch_size]\n            batch_labels = labels[i:i+batch_size]\n\n            # MoE\u9069\u7528\n            output = moe(batch_data)\n\n            # \u30bf\u30b9\u30af\u640d\u5931\uff08\u30c0\u30df\u30fc\uff09\n            task_loss = F.mse_loss(output, batch_data)\n\n            # \u5168\u4f53\u306e\u640d\u5931\n            loss = task_loss + 0.01 * moe.load_balance_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        if epoch % 20 == 0:\n            print(f\"Epoch {epoch}: Loss = {total_loss/n_samples*batch_size:.4f}\")\n\n    # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\n    print(\"\\n\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\u4e2d...\")\n\n    # \u5404\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u3067\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u9078\u629e\u3092\u8a18\u9332\n    expert_selection_by_type = {0: [], 1: [], 2: []}\n\n    with torch.no_grad():\n        for data_type in range(3):\n            # \u5404\u30bf\u30a4\u30d7\u306e\u30c7\u30fc\u30bf\u3092\u9078\u629e\n            type_mask = (labels == data_type)\n            type_data = all_data[type_mask][:10]  # \u6700\u521d\u306e10\u30b5\u30f3\u30d7\u30eb\n\n            # \u30b2\u30fc\u30c8\u5024\u3092\u53d6\u5f97\n            gate_logits = moe.gate(type_data)\n            _, selected_experts = torch.topk(gate_logits, moe.top_k, dim=-1)\n\n            # \u7d71\u8a08\u3092\u8a18\u9332\n            for e in range(moe.n_experts):\n                usage = (selected_experts == e).float().mean().item()\n                expert_selection_by_type[data_type].append(usage)\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    plt.figure(figsize=(12, 5))\n\n    # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u4f7f\u7528\u5206\u5e03\n    plt.subplot(1, 2, 1)\n    moe.visualize_expert_usage()\n\n    # \u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u5225\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u9078\u629e\n    plt.subplot(1, 2, 2)\n    x = np.arange(moe.n_experts)\n    width = 0.25\n\n    for i, (data_type, usage) in enumerate(expert_selection_by_type.items()):\n        plt.bar(x + i*width, usage, width, \n               label=f'Type {data_type}')\n\n    plt.xlabel('Expert ID')\n    plt.ylabel('Selection Rate')\n    plt.title('Expert Selection by Data Type')\n    plt.legend()\n    plt.xticks(x + width)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u5206\u6790\u7d50\u679c:\")\n    print(\"\u7570\u306a\u308b\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u306b\u5bfe\u3057\u3066\u3001\u7570\u306a\u308b\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u304c\u9078\u629e\u3055\u308c\u308b\u50be\u5411\u304c\u898b\u3089\u308c\u307e\u3059\u3002\")\n    print(\"\u3053\u308c\u306f\u3001MoE\u304c\u5165\u529b\u306e\u7279\u6027\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3092\u9078\u629e\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\")\n\n# \u5b9f\u884c\ntrain_and_analyze_moe()\n</code></pre>"},{"location":"exercises/part3-exercises/#33","title":"\u6f14\u7fd2 3.3: \u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316","text":""},{"location":"exercises/part3-exercises/#5","title":"\u554f\u984c 5","text":"<p>Pre-LayerNorm\u3068Post-LayerNorm\u306e\u4e21\u65b9\u3092\u5b9f\u88c5\u3057\u3001\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306e\u5b66\u7fd2\u5b89\u5b9a\u6027\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class NormalizationComparison:\n    \"\"\"\u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n\n    def __init__(self, d_model=256, n_layers=20):\n        self.d_model = d_model\n        self.n_layers = n_layers\n\n    def create_models(self):\n        \"\"\"\u7570\u306a\u308b\u6b63\u898f\u5316\u69cb\u6210\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\"\"\"\n\n        class PreNormBlock(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.norm1 = nn.LayerNorm(d_model)\n                self.attn = nn.MultiheadAttention(d_model, 4, batch_first=True)\n                self.norm2 = nn.LayerNorm(d_model)\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, 4 * d_model),\n                    nn.ReLU(),\n                    nn.Linear(4 * d_model, d_model)\n                )\n\n            def forward(self, x):\n                # Pre-Norm: \u6b63\u898f\u5316\u3057\u3066\u304b\u3089\u51e6\u7406\n                normalized = self.norm1(x)\n                attn_out, _ = self.attn(normalized, normalized, normalized)\n                x = x + attn_out\n\n                normalized = self.norm2(x)\n                ffn_out = self.ffn(normalized)\n                x = x + ffn_out\n\n                return x\n\n        class PostNormBlock(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.attn = nn.MultiheadAttention(d_model, 4, batch_first=True)\n                self.norm1 = nn.LayerNorm(d_model)\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, 4 * d_model),\n                    nn.ReLU(),\n                    nn.Linear(4 * d_model, d_model)\n                )\n                self.norm2 = nn.LayerNorm(d_model)\n\n            def forward(self, x):\n                # Post-Norm: \u51e6\u7406\u3057\u3066\u304b\u3089\u6b63\u898f\u5316\n                attn_out, _ = self.attn(x, x, x)\n                x = self.norm1(x + attn_out)\n\n                ffn_out = self.ffn(x)\n                x = self.norm2(x + ffn_out)\n\n                return x\n\n        # \u6df1\u3044\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\n        pre_norm_model = nn.Sequential(\n            *[PreNormBlock(self.d_model) for _ in range(self.n_layers)]\n        )\n\n        post_norm_model = nn.Sequential(\n            *[PostNormBlock(self.d_model) for _ in range(self.n_layers)]\n        )\n\n        return pre_norm_model, post_norm_model\n\n    def analyze_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u30d5\u30ed\u30fc\u306e\u5206\u6790\"\"\"\n        pre_norm_model, post_norm_model = self.create_models()\n\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n        batch_size = 16\n        seq_len = 50\n        x = torch.randn(batch_size, seq_len, self.d_model)\n        target = torch.randn(batch_size, seq_len, self.d_model)\n\n        models = {\n            'Pre-Norm': pre_norm_model,\n            'Post-Norm': post_norm_model\n        }\n\n        results = {}\n\n        for name, model in models.items():\n            print(f\"\\n{name} \u306e\u52fe\u914d\u30d5\u30ed\u30fc\u5206\u6790\u4e2d...\")\n\n            # \u5404\u5c64\u306e\u52fe\u914d\u3092\u8a18\u9332\n            gradients = []\n\n            def hook_fn(module, grad_input, grad_output):\n                gradients.append(grad_output[0].norm().item())\n\n            # \u30d5\u30c3\u30af\u3092\u767b\u9332\n            hooks = []\n            for layer in model:\n                hook = layer.register_backward_hook(hook_fn)\n                hooks.append(hook)\n\n            # \u9806\u4f1d\u64ad\u3068\u9006\u4f1d\u64ad\n            output = model(x)\n            loss = F.mse_loss(output, target)\n            loss.backward()\n\n            # \u30d5\u30c3\u30af\u3092\u524a\u9664\n            for hook in hooks:\n                hook.remove()\n\n            results[name] = gradients[::-1]  # \u5165\u529b\u5074\u304b\u3089\u9806\u306b\n\n        # \u52fe\u914d\u30d5\u30ed\u30fc\u306e\u53ef\u8996\u5316\n        plt.figure(figsize=(12, 6))\n\n        for name, grads in results.items():\n            plt.plot(range(1, len(grads) + 1), grads, \n                    marker='o', label=name, linewidth=2)\n\n        plt.xlabel('Layer (from input)')\n        plt.ylabel('Gradient Norm')\n        plt.title(f'Gradient Flow in {self.n_layers}-Layer Network')\n        plt.legend()\n        plt.yscale('log')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n        # \u7d71\u8a08\n        for name, grads in results.items():\n            print(f\"\\n{name}:\")\n            print(f\"  \u6700\u521d\u306e\u5c64\u306e\u52fe\u914d: {grads[0]:.6f}\")\n            print(f\"  \u6700\u5f8c\u306e\u5c64\u306e\u52fe\u914d: {grads[-1]:.6f}\")\n            print(f\"  \u52fe\u914d\u306e\u6e1b\u8870\u7387: {grads[0] / grads[-1]:.2f}\")\n\n    def compare_training_stability(self):\n        \"\"\"\u8a13\u7df4\u306e\u5b89\u5b9a\u6027\u3092\u6bd4\u8f03\"\"\"\n        pre_norm_model, post_norm_model = self.create_models()\n\n        # \u8a13\u7df4\u8a2d\u5b9a\n        batch_size = 32\n        seq_len = 20\n        n_steps = 200\n\n        models = {\n            'Pre-Norm': pre_norm_model,\n            'Post-Norm': post_norm_model\n        }\n\n        training_curves = {}\n\n        for name, model in models.items():\n            print(f\"\\n{name} \u306e\u8a13\u7df4\u4e2d...\")\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n            losses = []\n            gradient_norms = []\n\n            for step in range(n_steps):\n                # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n                x = torch.randn(batch_size, seq_len, self.d_model)\n                # \u30bf\u30b9\u30af\uff1a\u5165\u529b\u306e\u5909\u63db\u3092\u5b66\u7fd2\n                target = torch.sin(x) + torch.cos(x * 2)\n\n                # \u8a13\u7df4\u30b9\u30c6\u30c3\u30d7\n                output = model(x)\n                loss = F.mse_loss(output, target)\n\n                optimizer.zero_grad()\n                loss.backward()\n\n                # \u52fe\u914d\u30ce\u30eb\u30e0\u3092\u8a18\u9332\n                total_norm = 0\n                for p in model.parameters():\n                    if p.grad is not None:\n                        total_norm += p.grad.norm().item() ** 2\n                total_norm = total_norm ** 0.5\n                gradient_norms.append(total_norm)\n\n                # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n                optimizer.step()\n\n                losses.append(loss.item())\n\n                if step % 50 == 0:\n                    print(f\"  Step {step}: Loss = {loss.item():.4f}\")\n\n            training_curves[name] = {\n                'losses': losses,\n                'gradient_norms': gradient_norms\n            }\n\n        # \u7d50\u679c\u306e\u53ef\u8996\u5316\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n        # \u640d\u5931\u66f2\u7dda\n        for name, data in training_curves.items():\n            ax1.plot(data['losses'], label=name, linewidth=2)\n        ax1.set_xlabel('Step')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('Training Loss')\n        ax1.legend()\n        ax1.set_yscale('log')\n        ax1.grid(True, alpha=0.3)\n\n        # \u52fe\u914d\u30ce\u30eb\u30e0\n        for name, data in training_curves.items():\n            ax2.plot(data['gradient_norms'], label=name, linewidth=2, alpha=0.7)\n        ax2.set_xlabel('Step')\n        ax2.set_ylabel('Gradient Norm')\n        ax2.set_title('Gradient Norm During Training')\n        ax2.legend()\n        ax2.set_yscale('log')\n        ax2.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n        # \u6700\u7d42\u7684\u306a\u7d71\u8a08\n        print(\"\\n\u8a13\u7df4\u306e\u7d71\u8a08:\")\n        for name, data in training_curves.items():\n            final_loss = np.mean(data['losses'][-10:])\n            grad_std = np.std(data['gradient_norms'])\n            print(f\"{name}:\")\n            print(f\"  \u6700\u7d42\u640d\u5931: {final_loss:.4f}\")\n            print(f\"  \u52fe\u914d\u306e\u6a19\u6e96\u504f\u5dee: {grad_std:.4f}\")\n\n# \u5b9f\u884c\nnorm_comp = NormalizationComparison()\nnorm_comp.analyze_gradient_flow()\nnorm_comp.compare_training_stability()\n</code></pre>"},{"location":"exercises/part3-exercises/#34","title":"\u6f14\u7fd2 3.4: \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc","text":""},{"location":"exercises/part3-exercises/#6","title":"\u554f\u984c 6","text":"<p>\u5b8c\u5168\u306a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u30e2\u30c7\u30eb\u3092\u5b9f\u88c5\u3057\u3001\u7c21\u5358\u306a\u7ffb\u8a33\u30bf\u30b9\u30af\u3067\u52d5\u4f5c\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class SimpleTransformer(nn.Module):\n    \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fcTransformer\"\"\"\n\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, \n                 n_heads=8, n_layers=3, d_ff=1024, max_len=100):\n        super().__init__()\n\n        self.d_model = d_model\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        self.pos_encoding = self._create_positional_encoding(max_len, d_model)\n\n        # Transformer\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=n_heads,\n            num_encoder_layers=n_layers,\n            num_decoder_layers=n_layers,\n            dim_feedforward=d_ff,\n            batch_first=True\n        )\n\n        # \u51fa\u529b\u5c64\n        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n\n        # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n        self.scale = math.sqrt(d_model)\n\n    def _create_positional_encoding(self, max_len, d_model):\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        return nn.Parameter(pe.unsqueeze(0), requires_grad=False)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n                src_padding_mask=None, tgt_padding_mask=None):\n        # \u57cb\u3081\u8fbc\u307f + \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        src_emb = self.src_embedding(src) * self.scale\n        src_emb = src_emb + self.pos_encoding[:, :src.size(1)]\n\n        tgt_emb = self.tgt_embedding(tgt) * self.scale\n        tgt_emb = tgt_emb + self.pos_encoding[:, :tgt.size(1)]\n\n        # Transformer\n        output = self.transformer(\n            src_emb, tgt_emb,\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask\n        )\n\n        # \u51fa\u529b\u6295\u5f71\n        output = self.output_projection(output)\n\n        return output\n\n    def generate_square_subsequent_mask(self, sz):\n        \"\"\"\u30c7\u30b3\u30fc\u30c0\u30fc\u7528\u306e\u56e0\u679c\u7684\u30de\u30b9\u30af\u3092\u751f\u6210\"\"\"\n        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n        return mask\n\n# \u7c21\u5358\u306a\u7ffb\u8a33\u30bf\u30b9\u30af\u306e\u5b9f\u88c5\nclass SimpleTranslationTask:\n    \"\"\"\u6570\u5b57\u306e\u82f1\u8a9e\u2192\u65e5\u672c\u8a9e\u7ffb\u8a33\u30bf\u30b9\u30af\"\"\"\n\n    def __init__(self):\n        # \u7c21\u5358\u306a\u8a9e\u5f59\n        self.src_vocab = {\n            '&lt;pad&gt;': 0, '&lt;sos&gt;': 1, '&lt;eos&gt;': 2,\n            'one': 3, 'two': 4, 'three': 5, 'four': 6, 'five': 7,\n            'six': 8, 'seven': 9, 'eight': 10, 'nine': 11, 'ten': 12\n        }\n\n        self.tgt_vocab = {\n            '&lt;pad&gt;': 0, '&lt;sos&gt;': 1, '&lt;eos&gt;': 2,\n            '\u4e00': 3, '\u4e8c': 4, '\u4e09': 5, '\u56db': 6, '\u4e94': 7,\n            '\u516d': 8, '\u4e03': 9, '\u516b': 10, '\u4e5d': 11, '\u5341': 12\n        }\n\n        # \u9006\u5f15\u304d\u8f9e\u66f8\n        self.src_id2word = {v: k for k, v in self.src_vocab.items()}\n        self.tgt_id2word = {v: k for k, v in self.tgt_vocab.items()}\n\n        # \u7ffb\u8a33\u30da\u30a2\n        self.pairs = [\n            (['one'], ['\u4e00']),\n            (['two'], ['\u4e8c']),\n            (['three'], ['\u4e09']),\n            (['four'], ['\u56db']),\n            (['five'], ['\u4e94']),\n            (['six'], ['\u516d']),\n            (['seven'], ['\u4e03']),\n            (['eight'], ['\u516b']),\n            (['nine'], ['\u4e5d']),\n            (['ten'], ['\u5341']),\n            (['one', 'two'], ['\u4e00', '\u4e8c']),\n            (['three', 'four'], ['\u4e09', '\u56db']),\n            (['five', 'six'], ['\u4e94', '\u516d']),\n            (['seven', 'eight'], ['\u4e03', '\u516b']),\n            (['nine', 'ten'], ['\u4e5d', '\u5341'])\n        ]\n\n    def encode_src(self, words):\n        return [self.src_vocab.get(w, 0) for w in words]\n\n    def encode_tgt(self, words):\n        return [self.tgt_vocab.get(w, 0) for w in words]\n\n    def decode_src(self, ids):\n        return [self.src_id2word.get(i, '&lt;unk&gt;') for i in ids]\n\n    def decode_tgt(self, ids):\n        return [self.tgt_id2word.get(i, '&lt;unk&gt;') for i in ids]\n\n    def create_batch(self, pairs, pad_id=0):\n        \"\"\"\u30d0\u30c3\u30c1\u3092\u4f5c\u6210\uff08\u30d1\u30c7\u30a3\u30f3\u30b0\u4ed8\u304d\uff09\"\"\"\n        src_batch = []\n        tgt_batch = []\n\n        for src_words, tgt_words in pairs:\n            # \u30a8\u30f3\u30b3\u30fc\u30c9\n            src_ids = [self.src_vocab['&lt;sos&gt;']] + self.encode_src(src_words) + [self.src_vocab['&lt;eos&gt;']]\n            tgt_ids = [self.tgt_vocab['&lt;sos&gt;']] + self.encode_tgt(tgt_words) + [self.tgt_vocab['&lt;eos&gt;']]\n\n            src_batch.append(src_ids)\n            tgt_batch.append(tgt_ids)\n\n        # \u30d1\u30c7\u30a3\u30f3\u30b0\n        max_src_len = max(len(s) for s in src_batch)\n        max_tgt_len = max(len(t) for t in tgt_batch)\n\n        src_padded = []\n        tgt_padded = []\n        src_masks = []\n        tgt_masks = []\n\n        for src, tgt in zip(src_batch, tgt_batch):\n            # \u30d1\u30c7\u30a3\u30f3\u30b0\n            src_pad_len = max_src_len - len(src)\n            tgt_pad_len = max_tgt_len - len(tgt)\n\n            src_padded.append(src + [pad_id] * src_pad_len)\n            tgt_padded.append(tgt + [pad_id] * tgt_pad_len)\n\n            # \u30de\u30b9\u30af\uff08True = \u30d1\u30c7\u30a3\u30f3\u30b0\uff09\n            src_masks.append([False] * len(src) + [True] * src_pad_len)\n            tgt_masks.append([False] * len(tgt) + [True] * tgt_pad_len)\n\n        return (torch.tensor(src_padded), torch.tensor(tgt_padded),\n                torch.tensor(src_masks), torch.tensor(tgt_masks))\n\n# \u8a13\u7df4\u3068\u8a55\u4fa1\ndef train_translation_model():\n    # \u30bf\u30b9\u30af\u3068\u30e2\u30c7\u30eb\u306e\u6e96\u5099\n    task = SimpleTranslationTask()\n    model = SimpleTransformer(\n        src_vocab_size=len(task.src_vocab),\n        tgt_vocab_size=len(task.tgt_vocab),\n        d_model=128,\n        n_heads=4,\n        n_layers=2,\n        d_ff=512\n    )\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # \u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u7121\u8996\n\n    # \u8a13\u7df4\n    print(\"\u7ffb\u8a33\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u4e2d...\")\n    model.train()\n\n    for epoch in range(200):\n        total_loss = 0\n\n        # \u30c7\u30fc\u30bf\u3092\u30b7\u30e3\u30c3\u30d5\u30eb\n        import random\n        pairs = task.pairs.copy()\n        random.shuffle(pairs)\n\n        # \u30d0\u30c3\u30c1\u51e6\u7406\n        batch_size = 5\n        for i in range(0, len(pairs), batch_size):\n            batch_pairs = pairs[i:i+batch_size]\n            src, tgt, src_mask, tgt_mask = task.create_batch(batch_pairs)\n\n            # Teacher forcing: \u30c7\u30b3\u30fc\u30c0\u30fc\u5165\u529b\u306f\u76ee\u6a19\u306e1\u3064\u524d\u307e\u3067\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            tgt_mask_input = tgt_mask[:, :-1]\n\n            # \u56e0\u679c\u7684\u30de\u30b9\u30af\n            tgt_seq_len = tgt_input.size(1)\n            tgt_attn_mask = model.generate_square_subsequent_mask(tgt_seq_len)\n\n            # \u4e88\u6e2c\n            output = model(src, tgt_input, \n                         tgt_mask=tgt_attn_mask,\n                         src_padding_mask=src_mask,\n                         tgt_padding_mask=tgt_mask_input)\n\n            # \u640d\u5931\u8a08\u7b97\n            loss = criterion(output.reshape(-1, output.size(-1)), \n                           tgt_output.reshape(-1))\n\n            # \u6700\u9069\u5316\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        if epoch % 50 == 0:\n            avg_loss = total_loss / len(pairs) * batch_size\n            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n\n    # \u8a55\u4fa1\n    print(\"\\n\u7ffb\u8a33\u30c6\u30b9\u30c8:\")\n    model.eval()\n\n    test_pairs = [\n        (['five'], ['\u4e94']),\n        (['one', 'two'], ['\u4e00', '\u4e8c']),\n        (['seven', 'eight'], ['\u4e03', '\u516b'])\n    ]\n\n    with torch.no_grad():\n        for src_words, expected_tgt in test_pairs:\n            # \u30bd\u30fc\u30b9\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\n            src_ids = [task.src_vocab['&lt;sos&gt;']] + task.encode_src(src_words) + [task.src_vocab['&lt;eos&gt;']]\n            src_tensor = torch.tensor([src_ids])\n\n            # \u7ffb\u8a33\uff08\u8caa\u6b32\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff09\n            max_len = 10\n            tgt_ids = [task.tgt_vocab['&lt;sos&gt;']]\n\n            for _ in range(max_len):\n                tgt_tensor = torch.tensor([tgt_ids])\n\n                # \u30c7\u30b3\u30fc\u30c0\u30fc\u30de\u30b9\u30af\n                tgt_attn_mask = model.generate_square_subsequent_mask(len(tgt_ids))\n\n                # \u4e88\u6e2c\n                output = model(src_tensor, tgt_tensor, tgt_mask=tgt_attn_mask)\n\n                # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u4e88\u6e2c\n                next_token = output[0, -1].argmax().item()\n                tgt_ids.append(next_token)\n\n                # \u7d42\u4e86\u6761\u4ef6\n                if next_token == task.tgt_vocab['&lt;eos&gt;']:\n                    break\n\n            # \u7d50\u679c\u3092\u8868\u793a\n            predicted = task.decode_tgt(tgt_ids[1:-1])  # &lt;sos&gt;\u3068&lt;eos&gt;\u3092\u9664\u304f\n            print(f\"\u5165\u529b: {src_words}\")\n            print(f\"\u671f\u5f85: {expected_tgt}\")\n            print(f\"\u4e88\u6e2c: {predicted}\")\n            print(f\"\u6b63\u89e3: {'\u2713' if predicted == expected_tgt else '\u2717'}\\n\")\n\n# \u5b9f\u884c\ntrain_translation_model()\n</code></pre>"},{"location":"exercises/part3-exercises/#_1","title":"\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c","text":""},{"location":"exercises/part3-exercises/#7","title":"\u554f\u984c 7 \ud83c\udf1f","text":"<p>Flash Attention\u306e\u7c21\u6613\u7248\u3092\u5b9f\u88c5\u3057\u3001\u30e1\u30e2\u30ea\u52b9\u7387\u3092\u6539\u5584\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>class FlashAttentionSimple(nn.Module):\n    \"\"\"Flash Attention\u306e\u7c21\u6613\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model, n_heads, block_size=64):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.block_size = block_size\n        self.scale = 1.0 / math.sqrt(self.d_k)\n\n        # \u6295\u5f71\u884c\u5217\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n\n        # Q, K, V\u306e\u8a08\u7b97\n        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # \u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u306e\u51e6\u7406\n        if seq_len &lt;= self.block_size:\n            # \u77ed\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306f\u901a\u5e38\u306e\u51e6\u7406\n            output = self._standard_attention(Q, K, V, mask)\n        else:\n            # \u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306f\u30d6\u30ed\u30c3\u30af\u51e6\u7406\n            output = self._flash_attention(Q, K, V, mask)\n\n        # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        output = self.W_o(output)\n\n        return output\n\n    def _standard_attention(self, Q, K, V, mask):\n        \"\"\"\u6a19\u6e96\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\"\"\"\n        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attn_weights, V)\n\n        return output\n\n    def _flash_attention(self, Q, K, V, mask):\n        \"\"\"Flash Attention (\u7c21\u6613\u7248)\"\"\"\n        batch_size, n_heads, seq_len, d_k = Q.shape\n        block_size = self.block_size\n\n        # \u51fa\u529b\u306e\u521d\u671f\u5316\n        O = torch.zeros_like(Q)\n\n        # \u30d6\u30ed\u30c3\u30af\u6570\n        n_blocks = (seq_len + block_size - 1) // block_size\n\n        # \u5404\u30af\u30a8\u30ea\u30d6\u30ed\u30c3\u30af\u306b\u5bfe\u3057\u3066\u51e6\u7406\n        for i in range(n_blocks):\n            q_start = i * block_size\n            q_end = min((i + 1) * block_size, seq_len)\n\n            # \u30af\u30a8\u30ea\u30d6\u30ed\u30c3\u30af\n            Q_block = Q[:, :, q_start:q_end]\n\n            # \u3053\u306e\u30d6\u30ed\u30c3\u30af\u306e\u6700\u5927\u5024\u3068\u7d2f\u7a4d\u548c\u3092\u521d\u671f\u5316\n            block_max = torch.full((batch_size, n_heads, q_end - q_start, 1), \n                                 -1e9, device=Q.device)\n            block_sum = torch.zeros_like(block_max)\n            block_output = torch.zeros(batch_size, n_heads, q_end - q_start, d_k, \n                                     device=Q.device)\n\n            # \u5404\u30ad\u30fc/\u30d0\u30ea\u30e5\u30fc\u30d6\u30ed\u30c3\u30af\u306b\u5bfe\u3057\u3066\u51e6\u7406\n            for j in range(n_blocks):\n                k_start = j * block_size\n                k_end = min((j + 1) * block_size, seq_len)\n\n                # \u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u30c1\u30a7\u30c3\u30af\n                if mask is not None and k_start &gt; q_end:\n                    continue\n\n                # \u30ad\u30fc/\u30d0\u30ea\u30e5\u30fc\u30d6\u30ed\u30c3\u30af\n                K_block = K[:, :, k_start:k_end]\n                V_block = V[:, :, k_start:k_end]\n\n                # \u30b9\u30b3\u30a2\u8a08\u7b97\n                scores = torch.matmul(Q_block, K_block.transpose(-2, -1)) * self.scale\n\n                # \u30de\u30b9\u30af\u9069\u7528\n                if mask is not None:\n                    block_mask = self._get_block_mask(q_start, q_end, k_start, k_end, seq_len)\n                    if block_mask is not None:\n                        scores = scores.masked_fill(block_mask.unsqueeze(0).unsqueeze(0) == 0, -1e9)\n\n                # \u5b89\u5b9a\u3057\u305f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u306e\u305f\u3081\u306e\u51e6\u7406\n                scores_max = scores.max(dim=-1, keepdim=True)[0]\n                scores_stable = scores - scores_max\n                scores_exp = torch.exp(scores_stable)\n\n                # \u30aa\u30f3\u30e9\u30a4\u30f3\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u306e\u66f4\u65b0\n                new_max = torch.maximum(block_max, scores_max)\n\n                # \u7d2f\u7a4d\u548c\u306e\u66f4\u65b0\n                block_sum = block_sum * torch.exp(block_max - new_max) + \\\n                           scores_exp.sum(dim=-1, keepdim=True) * torch.exp(scores_max - new_max)\n\n                # \u51fa\u529b\u306e\u66f4\u65b0\n                block_output = block_output * torch.exp(block_max - new_max) + \\\n                             torch.matmul(scores_exp * torch.exp(scores_max - new_max), V_block)\n\n                block_max = new_max\n\n            # \u6b63\u898f\u5316\n            O[:, :, q_start:q_end] = block_output / block_sum\n\n        return O\n\n    def _get_block_mask(self, q_start, q_end, k_start, k_end, seq_len):\n        \"\"\"\u30d6\u30ed\u30c3\u30af\u7528\u306e\u30de\u30b9\u30af\u3092\u751f\u6210\"\"\"\n        if k_start &gt;= q_end:\n            # \u672a\u6765\u306e\u30d6\u30ed\u30c3\u30af\u306f\u5b8c\u5168\u306b\u30de\u30b9\u30af\n            return torch.zeros(q_end - q_start, k_end - k_start)\n\n        # \u90e8\u5206\u7684\u306a\u30de\u30b9\u30af\u304c\u5fc5\u8981\u306a\u5834\u5408\n        mask = torch.ones(q_end - q_start, k_end - k_start)\n        for i in range(q_end - q_start):\n            for j in range(k_end - k_start):\n                if q_start + i &lt; k_start + j:\n                    mask[i, j] = 0\n\n        return mask\n\n# \u30e1\u30e2\u30ea\u52b9\u7387\u306e\u6bd4\u8f03\ndef compare_memory_efficiency():\n    d_model = 512\n    n_heads = 8\n\n    # \u7570\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u9577\u3067\u30c6\u30b9\u30c8\n    seq_lengths = [128, 256, 512, 1024]\n\n    print(\"\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03:\")\n    print(\"\u30b7\u30fc\u30b1\u30f3\u30b9\u9577 | \u6a19\u6e96Attention | Flash Attention | \u524a\u6e1b\u7387\")\n    print(\"-\" * 60)\n\n    for seq_len in seq_lengths:\n        batch_size = 4\n\n        # \u6a19\u6e96\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\uff08\u6982\u7b97\uff09\n        # O(batch * heads * seq_len * seq_len)\n        standard_memory = batch_size * n_heads * seq_len * seq_len * 4  # float32\n\n        # Flash Attention\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\uff08\u6982\u7b97\uff09\n        # O(batch * heads * seq_len * block_size)\n        block_size = 64\n        flash_memory = batch_size * n_heads * seq_len * block_size * 4  # float32\n\n        reduction = (1 - flash_memory / standard_memory) * 100\n\n        print(f\"{seq_len:^12} | {standard_memory/1024**2:^14.2f}MB | \"\n              f\"{flash_memory/1024**2:^15.2f}MB | {reduction:^7.1f}%\")\n\n    # \u5b9f\u969b\u306e\u52d5\u4f5c\u78ba\u8a8d\n    print(\"\\n\u5b9f\u969b\u306e\u52d5\u4f5c\u78ba\u8a8d:\")\n\n    standard_attn = MultiHeadAttentionDetailed(d_model, n_heads)\n    flash_attn = FlashAttentionSimple(d_model, n_heads, block_size=64)\n\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n    x = torch.randn(2, 256, d_model)\n\n    # \u51fa\u529b\u306e\u6bd4\u8f03\n    with torch.no_grad():\n        standard_out, _ = standard_attn(x, x, x)\n        flash_out = flash_attn(x)\n\n        # \u5dee\u5206\n        diff = (standard_out - flash_out).abs().mean()\n        print(f\"\\n\u51fa\u529b\u306e\u5dee\u5206: {diff:.6f}\")\n        print(\"\uff08\u5c0f\u3055\u3044\u5024\u307b\u3069\u5b9f\u88c5\u304c\u6b63\u78ba\uff09\")\n\n    # \u901f\u5ea6\u6bd4\u8f03\n    import time\n\n    x_large = torch.randn(1, 1024, d_model)\n\n    # \u6a19\u6e96\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\n    start = time.time()\n    for _ in range(10):\n        _ = standard_attn(x_large, x_large, x_large)\n    standard_time = time.time() - start\n\n    # Flash Attention\n    start = time.time()\n    for _ in range(10):\n        _ = flash_attn(x_large)\n    flash_time = time.time() - start\n\n    print(f\"\\n\u901f\u5ea6\u6bd4\u8f03 (10\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3):\")\n    print(f\"\u6a19\u6e96Attention: {standard_time:.3f}\u79d2\")\n    print(f\"Flash Attention: {flash_time:.3f}\u79d2\")\n    print(f\"\u9ad8\u901f\u5316: {standard_time/flash_time:.2f}x\")\n\n# \u5b9f\u884c\ncompare_memory_efficiency()\n</code></pre>"},{"location":"exercises/part3-exercises/#_2","title":"\u307e\u3068\u3081","text":"<p>\u7b2c3\u90e8\u3067\u306f\u3001Transformer\u306e\u4e3b\u8981\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u8a73\u3057\u304f\u5b66\u3073\u307e\u3057\u305f\uff1a</p> <ol> <li>Multi-Head Attention: \u8907\u6570\u306e\u8996\u70b9\u304b\u3089\u306e\u6ce8\u610f\u6a5f\u69cb</li> <li>Feed Forward Network: \u4f4d\u7f6e\u3054\u3068\u306e\u975e\u7dda\u5f62\u5909\u63db</li> <li>\u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316: \u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b89\u5b9a\u5316</li> <li>\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc: \u5165\u529b\u304b\u3089\u51fa\u529b\u3078\u306e\u5909\u63db</li> </ol> <p>\u3053\u308c\u3089\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3001\u5f37\u529b\u306aTransformer\u30e2\u30c7\u30eb\u304c\u69cb\u7bc9\u3055\u308c\u307e\u3059\u3002\u6b21\u306e\u7b2c4\u90e8\u3067\u306f\u3001\u3053\u308c\u3089\u3092\u7d71\u5408\u3057\u305f\u5b8c\u5168\u306a\u5b9f\u88c5\u306b\u6311\u6226\u3057\u307e\u3057\u3087\u3046\uff01</p>"},{"location":"exercises/part4-exercises/","title":"\u7b2c4\u90e8 \u6f14\u7fd2\u554f\u984c","text":""},{"location":"exercises/part4-exercises/#41-transformer","title":"\u6f14\u7fd2 4.1: \u6700\u5c0f\u9650\u306eTransformer\u5b9f\u88c5","text":""},{"location":"exercises/part4-exercises/#1","title":"\u554f\u984c 1","text":"<p>\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u3001\u7570\u306a\u308b\u6b21\u5143\u3067\u306e\u5468\u671f\u6027\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model=512, max_len=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n\n        # \u5468\u6ce2\u6570\u306e\u8a08\u7b97\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0).transpose(0, 1))\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\n\n# \u53ef\u8996\u5316\npos_enc = PositionalEncoding(d_model=128, max_len=100)\npe_data = pos_enc.pe.squeeze(1).numpy()\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# \u7570\u306a\u308b\u6b21\u5143\u3067\u306e\u5468\u671f\u6027\ndimensions = [0, 1, 10, 50]\nfor i, dim in enumerate(dimensions):\n    ax = axes[i//2, i%2]\n    ax.plot(pe_data[:100, dim])\n    ax.set_title(f'Dimension {dim} ({\"sin\" if dim%2==0 else \"cos\"})')\n    ax.set_xlabel('Position')\n    ax.set_ylabel('Value')\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u5168\u4f53\u50cf\nplt.figure(figsize=(15, 8))\nplt.imshow(pe_data[:100, :64].T, cmap='viridis', aspect='auto')\nplt.colorbar()\nplt.title('Positional Encoding Heatmap')\nplt.xlabel('Position')\nplt.ylabel('Dimension')\nplt.show()\n</code></pre>"},{"location":"exercises/part4-exercises/#2","title":"\u554f\u984c 2","text":"<p>\u30b7\u30f3\u30d7\u30eb\u306aTransformer\u30a8\u30f3\u30b3\u30fc\u30c0\u3092\u5b9f\u88c5\u3057\u3001\u6bb5\u968e\u7684\u306b\u8907\u96d1\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MinimalTransformerEncoder(nn.Module):\n    \"\"\"\u6700\u5c0f\u9650\u306eTransformer\u30a8\u30f3\u30b3\u30fc\u30c0\"\"\"\n\n    def __init__(self, vocab_size=1000, d_model=256, n_heads=8, \n                 n_layers=4, d_ff=1024, max_len=512, dropout=0.1):\n        super().__init__()\n\n        self.d_model = d_model\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask=None):\n        # \u57cb\u3081\u8fbc\u307f + \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        x = self.embedding(x) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\u3092\u9806\u6b21\u9069\u7528\n        for block in self.transformer_blocks:\n            x = block(x, mask)\n\n        return self.layer_norm(x)\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\u5358\u4e00\u306eTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, dropout):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Self-Attention + \u6b8b\u5dee\u63a5\u7d9a\n        attn_out = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n\n        # Feed Forward + \u6b8b\u5dee\u63a5\u7d9a\n        ff_out = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_out))\n\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Q, K, V \u306e\u8a08\u7b97\n        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Scaled Dot-Product Attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n\n        # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model)\n\n        return self.W_o(attn_output)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        d_k = Q.size(-1)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        return torch.matmul(attn_weights, V)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\n# \u30c6\u30b9\u30c8\nmodel = MinimalTransformerEncoder(vocab_size=1000, d_model=256)\nx = torch.randint(0, 1000, (2, 50))  # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba2, \u7cfb\u5217\u957750\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # [2, 50, 256]\n</code></pre>"},{"location":"exercises/part4-exercises/#42","title":"\u6f14\u7fd2 4.2: \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u5b9f\u88c5\u306e\u8a73\u7d30","text":""},{"location":"exercises/part4-exercises/#3","title":"\u554f\u984c 3","text":"<p>\u7570\u306a\u308b\u6ce8\u610f\u6a5f\u69cb\uff08additive attention, scaled dot-product attention\uff09\u3092\u5b9f\u88c5\u3057\u3001\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport matplotlib.pyplot as plt\n\nclass AdditiveAttention(nn.Module):\n    \"\"\"Additive Attention (Bahdanau Attention)\"\"\"\n\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.v = nn.Linear(hidden_size, 1, bias=False)\n\n    def forward(self, query, key, value, mask=None):\n        # query: [batch, seq_len_q, hidden]\n        # key, value: [batch, seq_len_k, hidden]\n\n        batch_size, seq_len_q, _ = query.shape\n        seq_len_k = key.shape[1]\n\n        # \u30af\u30a8\u30ea\u3068\u30ad\u30fc\u3092\u7d50\u5408\u3059\u308b\u305f\u3081\u6b21\u5143\u3092\u62e1\u5f35\n        q_transformed = self.W_q(query).unsqueeze(2)  # [batch, seq_len_q, 1, hidden]\n        k_transformed = self.W_k(key).unsqueeze(1)    # [batch, 1, seq_len_k, hidden]\n\n        # \u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3067\u7d50\u5408\n        combined = torch.tanh(q_transformed + k_transformed)  # [batch, seq_len_q, seq_len_k, hidden]\n\n        # \u30b9\u30b3\u30a2\u8a08\u7b97\n        scores = self.v(combined).squeeze(-1)  # [batch, seq_len_q, seq_len_k]\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n\n        # \u91cd\u307f\u4ed8\u3051\u548c\n        context = torch.bmm(attn_weights, value)  # [batch, seq_len_q, hidden]\n\n        return context, attn_weights\n\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot-Product Attention\"\"\"\n\n    def __init__(self, hidden_size, dropout=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        d_k = query.size(-1)\n\n        # \u5185\u7a4d\u8a08\u7b97\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context = torch.matmul(attn_weights, value)\n\n        return context, attn_weights\n\ndef benchmark_attention_mechanisms():\n    \"\"\"\u6ce8\u610f\u6a5f\u69cb\u306e\u6027\u80fd\u6bd4\u8f03\"\"\"\n\n    hidden_size = 256\n    batch_size = 32\n    seq_lengths = [64, 128, 256, 512]\n\n    additive_attn = AdditiveAttention(hidden_size)\n    scaled_attn = ScaledDotProductAttention(hidden_size)\n\n    results = {'additive': [], 'scaled': []}\n\n    for seq_len in seq_lengths:\n        # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u751f\u6210\n        query = torch.randn(batch_size, seq_len, hidden_size)\n        key = torch.randn(batch_size, seq_len, hidden_size)\n        value = torch.randn(batch_size, seq_len, hidden_size)\n\n        # Additive Attention\n        start_time = time.time()\n        for _ in range(10):\n            _, _ = additive_attn(query, key, value)\n        additive_time = (time.time() - start_time) / 10\n        results['additive'].append(additive_time)\n\n        # Scaled Dot-Product Attention\n        start_time = time.time()\n        for _ in range(10):\n            _, _ = scaled_attn(query, key, value)\n        scaled_time = (time.time() - start_time) / 10\n        results['scaled'].append(scaled_time)\n\n        print(f\"Seq Length {seq_len}:\")\n        print(f\"  Additive: {additive_time:.4f}s\")\n        print(f\"  Scaled: {scaled_time:.4f}s\")\n        print(f\"  Speedup: {additive_time/scaled_time:.2f}x\")\n        print()\n\n    # \u53ef\u8996\u5316\n    plt.figure(figsize=(10, 6))\n    plt.plot(seq_lengths, results['additive'], 'ro-', label='Additive Attention')\n    plt.plot(seq_lengths, results['scaled'], 'bo-', label='Scaled Dot-Product')\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Time (seconds)')\n    plt.title('Attention Mechanism Performance Comparison')\n    plt.legend()\n    plt.grid(True)\n    plt.yscale('log')\n    plt.show()\n\n    return results\n\n# \u30d9\u30f3\u30c1\u30de\u30fc\u30af\u5b9f\u884c\nresults = benchmark_attention_mechanisms()\n</code></pre>"},{"location":"exercises/part4-exercises/#4_1","title":"\u554f\u984c 4","text":"<p>\u5c64\u6b63\u898f\u5316\u3068\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u306e\u9055\u3044\u3092\u5b9f\u88c5\u3092\u901a\u3057\u3066\u7406\u89e3\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass LayerNorm(nn.Module):\n    \"\"\"Layer Normalization \u306e\u624b\u52d5\u5b9f\u88c5\"\"\"\n\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        # \u6700\u5f8c\u306e\u6b21\u5143\u3067\u6b63\u898f\u5316\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True, unbiased=False)\n\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass BatchNorm1D(nn.Module):\n    \"\"\"Batch Normalization \u306e\u624b\u52d5\u5b9f\u88c5\"\"\"\n\n    def __init__(self, features, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n        self.momentum = momentum\n\n        # \u5b9f\u884c\u6642\u7d71\u8a08\n        self.register_buffer('running_mean', torch.zeros(features))\n        self.register_buffer('running_var', torch.ones(features))\n\n    def forward(self, x):\n        if self.training:\n            # \u30d0\u30c3\u30c1\u6b21\u5143\u3067\u6b63\u898f\u5316\n            mean = x.mean(dim=0)\n            var = x.var(dim=0, unbiased=False)\n\n            # \u5b9f\u884c\u6642\u7d71\u8a08\u3092\u66f4\u65b0\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        return self.gamma * (x - mean) / torch.sqrt(var + self.eps) + self.beta\n\ndef compare_normalizations():\n    \"\"\"\u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u751f\u6210\n    batch_size, seq_len, features = 32, 50, 128\n    x = torch.randn(batch_size, seq_len, features) * 2 + 1\n\n    # \u6b63\u898f\u5316\u624b\u6cd5\n    layer_norm = LayerNorm(features)\n    batch_norm = BatchNorm1D(features)\n\n    # Layer Normalization\u9069\u7528\n    x_ln = layer_norm(x)\n\n    # Batch Normalization\u9069\u7528\uff082D\u5165\u529b\u306b\u5909\u63db\uff09\n    x_2d = x.view(-1, features)\n    x_bn_2d = batch_norm(x_2d)\n    x_bn = x_bn_2d.view(batch_size, seq_len, features)\n\n    # \u7d71\u8a08\u60c5\u5831\u3092\u8a08\u7b97\n    print(\"=== \u6b63\u898f\u5316\u524d ===\")\n    print(f\"Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n    print(f\"Shape: {x.shape}\")\n\n    print(\"\\\\n=== Layer Normalization\u5f8c ===\")\n    print(f\"Mean: {x_ln.mean():.4f}, Std: {x_ln.std():.4f}\")\n    # \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u5404\u6642\u523b\u3067\u306e\u7d71\u8a08\n    print(f\"Per-sample mean: {x_ln.mean(dim=-1).mean():.4f}\")\n    print(f\"Per-sample std: {x_ln.std(dim=-1).mean():.4f}\")\n\n    print(\"\\\\n=== Batch Normalization\u5f8c ===\")\n    print(f\"Mean: {x_bn.mean():.4f}, Std: {x_bn.std():.4f}\")\n    # \u5404\u7279\u5fb4\u6b21\u5143\u3067\u306e\u7d71\u8a08\n    print(f\"Per-feature mean: {x_bn.mean(dim=(0,1)).mean():.4f}\")\n    print(f\"Per-feature std: {x_bn.mean(dim=(0,1)).std():.4f}\")\n\n    # \u53ef\u8996\u5316\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n    # \u30aa\u30ea\u30b8\u30ca\u30eb\n    axes[0, 0].hist(x.flatten().numpy(), bins=50, alpha=0.7, color='red')\n    axes[0, 0].set_title('Original Distribution')\n    axes[0, 0].set_ylabel('Frequency')\n\n    # Layer Norm\n    axes[0, 1].hist(x_ln.flatten().numpy(), bins=50, alpha=0.7, color='blue')\n    axes[0, 1].set_title('After Layer Normalization')\n\n    # Batch Norm\n    axes[0, 2].hist(x_bn.flatten().numpy(), bins=50, alpha=0.7, color='green')\n    axes[0, 2].set_title('After Batch Normalization')\n\n    # \u7279\u5fb4\u91cf\u3054\u3068\u306e\u5206\u5e03\uff08\u6700\u521d\u306e\u6570\u500b\uff09\n    for i in range(3):\n        axes[1, i].plot(x[0, :, i].numpy(), 'r-', alpha=0.7, label='Original')\n        axes[1, i].plot(x_ln[0, :, i].numpy(), 'b-', alpha=0.7, label='LayerNorm')\n        axes[1, i].plot(x_bn[0, :, i].numpy(), 'g-', alpha=0.7, label='BatchNorm')\n        axes[1, i].set_title(f'Feature {i} - First Sample')\n        axes[1, i].legend()\n        axes[1, i].set_xlabel('Sequence Position')\n\n    plt.tight_layout()\n    plt.show()\n\n    return x, x_ln, x_bn\n\n# \u6bd4\u8f03\u5b9f\u884c\noriginal, layer_normed, batch_normed = compare_normalizations()\n</code></pre>"},{"location":"exercises/part4-exercises/#43","title":"\u6f14\u7fd2 4.3: \u30c7\u30d0\u30c3\u30b0\u3068\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3","text":""},{"location":"exercises/part4-exercises/#5","title":"\u554f\u984c 5","text":"<p>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u3092\u53ef\u8996\u5316\u3059\u308b\u30c4\u30fc\u30eb\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom typing import List, Optional\n\nclass AttentionVisualizer:\n    \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u306e\u53ef\u8996\u5316\u30c4\u30fc\u30eb\"\"\"\n\n    def __init__(self):\n        self.attention_weights = []\n        self.layer_names = []\n\n    def add_attention_weights(self, weights: torch.Tensor, layer_name: str):\n        \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u3092\u8ffd\u52a0\"\"\"\n        self.attention_weights.append(weights.detach().cpu())\n        self.layer_names.append(layer_name)\n\n    def visualize_head_patterns(self, layer_idx: int = 0, sample_idx: int = 0, \n                              tokens: Optional[List[str]] = None):\n        \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\"\"\"\n\n        if layer_idx &gt;= len(self.attention_weights):\n            print(f\"Layer {layer_idx} not found\")\n            return\n\n        weights = self.attention_weights[layer_idx]  # [batch, heads, seq, seq]\n        sample_weights = weights[sample_idx]  # [heads, seq, seq]\n\n        n_heads = sample_weights.shape[0]\n        seq_len = sample_weights.shape[1]\n\n        # \u30c8\u30fc\u30af\u30f3\u30e9\u30d9\u30eb\n        if tokens is None:\n            tokens = [f\"T{i}\" for i in range(seq_len)]\n\n        # \u30d8\u30c3\u30c9\u6570\u306b\u5fdc\u3058\u3066\u30b5\u30d6\u30d7\u30ed\u30c3\u30c8\u914d\u7f6e\u3092\u6c7a\u5b9a\n        cols = min(4, n_heads)\n        rows = (n_heads + cols - 1) // cols\n\n        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n        if rows == 1 and cols == 1:\n            axes = [axes]\n        elif rows == 1 or cols == 1:\n            axes = axes.flatten()\n        else:\n            axes = axes.flatten()\n\n        for head in range(n_heads):\n            ax = axes[head] if n_heads &gt; 1 else axes[0]\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n            sns.heatmap(sample_weights[head].numpy(), \n                       xticklabels=tokens, yticklabels=tokens,\n                       cmap='Blues', ax=ax, cbar=True,\n                       square=True, annot=True if seq_len &lt;= 10 else False,\n                       fmt='.2f')\n\n            ax.set_title(f'Head {head + 1}')\n            ax.set_xlabel('Key Position')\n            ax.set_ylabel('Query Position')\n\n        # \u672a\u4f7f\u7528\u306e\u30b5\u30d6\u30d7\u30ed\u30c3\u30c8\u3092\u975e\u8868\u793a\n        for i in range(n_heads, len(axes)):\n            axes[i].set_visible(False)\n\n        plt.suptitle(f'{self.layer_names[layer_idx]} - Sample {sample_idx}')\n        plt.tight_layout()\n        plt.show()\n\n    def analyze_attention_patterns(self, layer_idx: int = 0):\n        \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n\n        weights = self.attention_weights[layer_idx]  # [batch, heads, seq, seq]\n        batch_size, n_heads, seq_len, _ = weights.shape\n\n        # \u5404\u30d8\u30c3\u30c9\u306e\u7279\u6027\u5206\u6790\n        head_stats = []\n\n        for head in range(n_heads):\n            head_weights = weights[:, head, :, :]  # [batch, seq, seq]\n\n            # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\uff08\u6ce8\u610f\u306e\u5206\u6563\u5ea6\uff09\n            entropy = -(head_weights * torch.log(head_weights + 1e-9)).sum(dim=-1).mean()\n\n            # \u5bfe\u89d2\u7dda\u306e\u5f37\u3055\uff08self-attention \u306e\u5ea6\u5408\u3044\uff09\n            diag_strength = torch.diagonal(head_weights, dim1=-2, dim2=-1).mean()\n\n            # \u5c40\u6240\u6027\uff08\u8fd1\u3044\u4f4d\u7f6e\u3078\u306e\u6ce8\u610f\u306e\u5f37\u3055\uff09\n            positions = torch.arange(seq_len).float()\n            pos_diff = (positions.unsqueeze(0) - positions.unsqueeze(1)).abs()\n            locality = (head_weights * (-pos_diff).exp()).sum(dim=-1).mean()\n\n            head_stats.append({\n                'head': head,\n                'entropy': entropy.item(),\n                'diag_strength': diag_strength.item(),\n                'locality': locality.item()\n            })\n\n        # \u7d50\u679c\u8868\u793a\n        print(f\"=== {self.layer_names[layer_idx]} Analysis ===\")\n        print(f\"{'Head':&lt;6} {'Entropy':&lt;10} {'Self-Attn':&lt;10} {'Locality':&lt;10}\")\n        print(\"-\" * 40)\n\n        for stats in head_stats:\n            print(f\"{stats['head']:&lt;6} {stats['entropy']:&lt;10.4f} \"\n                  f\"{stats['diag_strength']:&lt;10.4f} {stats['locality']:&lt;10.4f}\")\n\n        # \u53ef\u8996\u5316\n        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n        metrics = ['entropy', 'diag_strength', 'locality']\n        titles = ['Attention Entropy', 'Self-Attention Strength', 'Locality']\n\n        for i, (metric, title) in enumerate(zip(metrics, titles)):\n            values = [stats[metric] for stats in head_stats]\n            axes[i].bar(range(n_heads), values, color=f'C{i}')\n            axes[i].set_title(title)\n            axes[i].set_xlabel('Head')\n            axes[i].set_xticks(range(n_heads))\n            axes[i].set_xticklabels([f'H{i}' for i in range(n_heads)])\n            axes[i].grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n        return head_stats\n\n    def compare_layers(self):\n        \"\"\"\u5c64\u9593\u3067\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d1\u30bf\u30fc\u30f3\u6bd4\u8f03\"\"\"\n\n        if len(self.attention_weights) &lt; 2:\n            print(\"\u6bd4\u8f03\u306b\u306f\u5c11\u306a\u304f\u3068\u30822\u5c64\u306e\u30c7\u30fc\u30bf\u304c\u5fc5\u8981\u3067\u3059\")\n            return\n\n        fig, axes = plt.subplots(len(self.attention_weights), 1, \n                               figsize=(10, 3*len(self.attention_weights)))\n\n        if len(self.attention_weights) == 1:\n            axes = [axes]\n\n        for layer_idx, (weights, name) in enumerate(zip(self.attention_weights, self.layer_names)):\n            # \u6700\u521d\u306e\u30b5\u30f3\u30d7\u30eb\u306e\u6700\u521d\u306e\u30d8\u30c3\u30c9\u3092\u4f7f\u7528\n            sample_weights = weights[0, 0].numpy()  # [seq, seq]\n\n            sns.heatmap(sample_weights, ax=axes[layer_idx], \n                       cmap='Blues', cbar=True, square=True)\n            axes[layer_idx].set_title(f'{name} (Head 0)')\n            axes[layer_idx].set_xlabel('Key Position')\n            axes[layer_idx].set_ylabel('Query Position')\n\n        plt.tight_layout()\n        plt.show()\n\n# \u4f7f\u7528\u4f8b\ndef test_attention_visualizer():\n    \"\"\"\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30b6\u30fc\u306e\u30c6\u30b9\u30c8\"\"\"\n\n    # \u30c0\u30df\u30fc\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u751f\u6210\n    torch.manual_seed(42)\n    batch_size, n_heads, seq_len = 2, 8, 12\n\n    # \u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\n    patterns = []\n\n    # \u30d1\u30bf\u30fc\u30f31: \u5c40\u6240\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\n    local_attn = torch.zeros(batch_size, n_heads, seq_len, seq_len)\n    for i in range(seq_len):\n        for j in range(max(0, i-2), min(seq_len, i+3)):\n            local_attn[:, :4, i, j] = torch.exp(-abs(i-j))\n    local_attn[:, :4] = torch.softmax(local_attn[:, :4], dim=-1)\n\n    # \u30d1\u30bf\u30fc\u30f32: \u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\n    global_attn = torch.randn(batch_size, n_heads, seq_len, seq_len)\n    global_attn[:, 4:] = torch.softmax(global_attn[:, 4:], dim=-1)\n\n    combined_attn = local_attn + global_attn\n    combined_attn = torch.softmax(combined_attn, dim=-1)\n\n    # \u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30b6\u30fc\u306e\u30c6\u30b9\u30c8\n    visualizer = AttentionVisualizer()\n    visualizer.add_attention_weights(combined_attn, \"Layer 1\")\n\n    # \u30c8\u30fc\u30af\u30f3\u30ea\u30b9\u30c8\n    tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"looked\", \"at\", \"the\", \"dog\", \".\"]\n\n    # \u53ef\u8996\u5316\n    visualizer.visualize_head_patterns(0, 0, tokens)\n\n    # \u5206\u6790\n    stats = visualizer.analyze_attention_patterns(0)\n\n    return visualizer\n\n# \u30c6\u30b9\u30c8\u5b9f\u884c\nvisualizer = test_attention_visualizer()\n</code></pre>"},{"location":"exercises/part4-exercises/#44","title":"\u6f14\u7fd2 4.4: \u52d5\u4f5c\u78ba\u8a8d\u3068\u30c6\u30b9\u30c8","text":""},{"location":"exercises/part4-exercises/#6","title":"\u554f\u984c 6","text":"<p>Transformer\u30e2\u30c7\u30eb\u306e\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306b\u5bfe\u3059\u308b\u5358\u4f53\u30c6\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport unittest\nimport math\n\nclass TestTransformerComponents(unittest.TestCase):\n    \"\"\"Transformer\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5358\u4f53\u30c6\u30b9\u30c8\"\"\"\n\n    def setUp(self):\n        \"\"\"\u30c6\u30b9\u30c8\u524d\u306e\u6e96\u5099\"\"\"\n        self.batch_size = 2\n        self.seq_len = 10\n        self.d_model = 64\n        self.n_heads = 8\n        self.vocab_size = 100\n\n        torch.manual_seed(42)\n\n    def test_positional_encoding_shape(self):\n        \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5f62\u72b6\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import PositionalEncoding\n\n        pe = PositionalEncoding(self.d_model, max_len=100)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        output = pe(x)\n\n        self.assertEqual(output.shape, x.shape)\n        print(\"\u2713 Positional Encoding shape test passed\")\n\n    def test_positional_encoding_properties(self):\n        \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u6570\u5b66\u7684\u6027\u8cea\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import PositionalEncoding\n\n        pe = PositionalEncoding(self.d_model)\n\n        # sin/cos \u306e\u5468\u671f\u6027\u3092\u30c6\u30b9\u30c8\n        pe_matrix = pe.pe.squeeze(1)  # [max_len, d_model]\n\n        # \u5076\u6570\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306fsin\u3001\u5947\u6570\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306fcos\n        for i in range(0, min(self.d_model, 10), 2):\n            pos_vals = pe_matrix[:, i]\n            # sin \u306e\u5024\u57df\u306f [-1, 1]\n            self.assertTrue(torch.all(pos_vals &gt;= -1.1))\n            self.assertTrue(torch.all(pos_vals &lt;= 1.1))\n\n        print(\"\u2713 Positional Encoding properties test passed\")\n\n    def test_multihead_attention_shape(self):\n        \"\"\"Multi-Head Attention\u306e\u5f62\u72b6\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import MultiHeadAttention\n\n        mha = MultiHeadAttention(self.d_model, self.n_heads)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        output = mha(x, x, x)\n\n        self.assertEqual(output.shape, x.shape)\n        print(\"\u2713 Multi-Head Attention shape test passed\")\n\n    def test_attention_mask(self):\n        \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u306e\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import MultiHeadAttention\n\n        mha = MultiHeadAttention(self.d_model, self.n_heads)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        # \u56e0\u679c\u30de\u30b9\u30af\uff08\u30c7\u30b3\u30fc\u30c0\u7528\uff09\n        mask = torch.tril(torch.ones(self.seq_len, self.seq_len))\n        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n\n        output = mha(x, x, x, mask)\n\n        self.assertEqual(output.shape, x.shape)\n        print(\"\u2713 Attention mask test passed\")\n\n    def test_feed_forward_shape(self):\n        \"\"\"Feed Forward Network\u306e\u5f62\u72b6\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import FeedForward\n\n        ff = FeedForward(self.d_model, self.d_model * 4)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        output = ff(x)\n\n        self.assertEqual(output.shape, x.shape)\n        print(\"\u2713 Feed Forward shape test passed\")\n\n    def test_layer_norm_properties(self):\n        \"\"\"Layer Normalization\u306e\u6027\u8cea\u30c6\u30b9\u30c8\"\"\"\n        layer_norm = nn.LayerNorm(self.d_model)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model) * 5 + 10\n\n        output = layer_norm(x)\n\n        # \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u5404\u4f4d\u7f6e\u3067\u5e73\u5747\u22480\u3001\u5206\u6563\u22481\n        mean = output.mean(dim=-1)\n        var = output.var(dim=-1, unbiased=False)\n\n        self.assertTrue(torch.allclose(mean, torch.zeros_like(mean), atol=1e-5))\n        self.assertTrue(torch.allclose(var, torch.ones_like(var), atol=1e-5))\n        print(\"\u2713 Layer Normalization properties test passed\")\n\n    def test_transformer_block_residual(self):\n        \"\"\"Transformer\u30d6\u30ed\u30c3\u30af\u306e\u6b8b\u5dee\u63a5\u7d9a\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import TransformerBlock\n\n        block = TransformerBlock(self.d_model, self.n_heads, self.d_model * 4, 0.0)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        # dropout=0\u306a\u306e\u3067\u3001\u6b8b\u5dee\u63a5\u7d9a\u306e\u52b9\u679c\u3092\u78ba\u8a8d\u3067\u304d\u308b\n        output = block(x)\n\n        # \u51fa\u529b\u304c\u5165\u529b\u3068\u5927\u304d\u304f\u7570\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\uff08\u5b66\u7fd2\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\uff09\n        diff = torch.norm(output - x, dim=-1).mean()\n        self.assertTrue(diff &gt; 0.1)  # \u4f55\u3089\u304b\u306e\u5909\u63db\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\n        print(\"\u2713 Transformer block residual test passed\")\n\n    def test_full_model_forward(self):\n        \"\"\"\u5b8c\u5168\u306a\u30e2\u30c7\u30eb\u306e\u9806\u4f1d\u64ad\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import MinimalTransformerEncoder\n\n        model = MinimalTransformerEncoder(\n            vocab_size=self.vocab_size,\n            d_model=self.d_model,\n            n_heads=self.n_heads,\n            n_layers=2\n        )\n\n        x = torch.randint(0, self.vocab_size, (self.batch_size, self.seq_len))\n\n        output = model(x)\n\n        expected_shape = (self.batch_size, self.seq_len, self.d_model)\n        self.assertEqual(output.shape, expected_shape)\n        print(\"\u2713 Full model forward test passed\")\n\n    def test_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u306e\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import MinimalTransformerEncoder\n\n        model = MinimalTransformerEncoder(\n            vocab_size=self.vocab_size,\n            d_model=self.d_model,\n            n_heads=self.n_heads,\n            n_layers=2\n        )\n\n        x = torch.randint(0, self.vocab_size, (self.batch_size, self.seq_len))\n\n        # \u9806\u4f1d\u64ad\n        output = model(x)\n        loss = output.sum()\n\n        # \u9006\u4f1d\u64ad\n        loss.backward()\n\n        # \u5168\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u52fe\u914d\u304c\u6d41\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.assertIsNotNone(param.grad, f\"No gradient for {name}\")\n                self.assertFalse(torch.allclose(param.grad, torch.zeros_like(param.grad)), \n                               f\"Zero gradient for {name}\")\n\n        print(\"\u2713 Gradient flow test passed\")\n\n    def test_attention_weights_sum(self):\n        \"\"\"\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u306e\u548c\u304c1\u306b\u306a\u308b\u3053\u3068\u3092\u30c6\u30b9\u30c8\"\"\"\n        from part4.minimal_transformer import MultiHeadAttention\n\n        class AttentionWithWeights(MultiHeadAttention):\n            def forward(self, query, key, value, mask=None):\n                batch_size = query.size(0)\n\n                Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n                K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n                V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n                d_k = Q.size(-1)\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                attn_weights = torch.softmax(scores, dim=-1)\n                context = torch.matmul(attn_weights, V)\n                context = context.transpose(1, 2).contiguous().view(\n                    batch_size, -1, self.d_model)\n\n                return self.W_o(context), attn_weights\n\n        mha = AttentionWithWeights(self.d_model, self.n_heads)\n        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n        output, weights = mha(x, x, x)\n\n        # \u6700\u5f8c\u306e\u6b21\u5143\u3067\u306e\u548c\u304c1\u306b\u306a\u308b\u3053\u3068\u3092\u78ba\u8a8d\n        weight_sums = weights.sum(dim=-1)\n        expected_sums = torch.ones_like(weight_sums)\n\n        self.assertTrue(torch.allclose(weight_sums, expected_sums, atol=1e-5))\n        print(\"\u2713 Attention weights sum test passed\")\n\ndef run_all_tests():\n    \"\"\"\u5168\u30c6\u30b9\u30c8\u3092\u5b9f\u884c\"\"\"\n    print(\"=== Transformer Components Unit Tests ===\\\\n\")\n\n    # \u30c6\u30b9\u30c8\u30b9\u30a4\u30fc\u30c8\u3092\u4f5c\u6210\n    suite = unittest.TestLoader().loadTestsFromTestCase(TestTransformerComponents)\n\n    # \u30c6\u30b9\u30c8\u3092\u5b9f\u884c\n    runner = unittest.TextTestRunner(verbosity=0)\n    result = runner.run(suite)\n\n    print(f\"\\\\n=== Test Results ===\")\n    print(f\"Tests run: {result.testsRun}\")\n    print(f\"Failures: {len(result.failures)}\")\n    print(f\"Errors: {len(result.errors)}\")\n\n    if result.failures:\n        print(\"\\\\nFailures:\")\n        for test, traceback in result.failures:\n            print(f\"- {test}: {traceback}\")\n\n    if result.errors:\n        print(\"\\\\nErrors:\")\n        for test, traceback in result.errors:\n            print(f\"- {test}: {traceback}\")\n\n    return result.wasSuccessful()\n\n# \u30c6\u30b9\u30c8\u5b9f\u884c\nif __name__ == \"__main__\":\n    success = run_all_tests()\n    if success:\n        print(\"\\\\n\ud83c\udf89 All tests passed!\")\n    else:\n        print(\"\\\\n\u274c Some tests failed!\")\n</code></pre>"},{"location":"exercises/part4-exercises/#7","title":"\u554f\u984c 7","text":"<p>\u30e2\u30c7\u30eb\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3068\u8a08\u7b97\u91cf\u3092\u5206\u6790\u3059\u308b\u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport time\nimport psutil\nimport os\nfrom typing import Dict, List, Tuple\nimport matplotlib.pyplot as plt\n\nclass TransformerProfiler:\n    \"\"\"Transformer\u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\"\"\"\n\n    def __init__(self):\n        self.profile_data = {}\n\n    def profile_memory_usage(self, model: nn.Module, input_sizes: List[Tuple[int, int]]) -&gt; Dict:\n        \"\"\"\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\"\"\"\n\n        results = {\n            'input_sizes': [],\n            'model_memory': [],\n            'forward_memory': [],\n            'backward_memory': [],\n            'total_memory': []\n        }\n\n        for batch_size, seq_len in input_sizes:\n            # \u30e1\u30e2\u30ea\u3092\u30af\u30ea\u30a2\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n            # \u30e2\u30c7\u30eb\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n            model_params = sum(p.numel() * p.element_size() for p in model.parameters())\n            model_buffers = sum(b.numel() * b.element_size() for b in model.buffers())\n            model_memory = (model_params + model_buffers) / 1024**2  # MB\n\n            # \u5165\u529b\u30c7\u30fc\u30bf\u4f5c\u6210\n            device = next(model.parameters()).device\n            x = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n\n            # Forward pass \u30e1\u30e2\u30ea\n            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n\n            output = model(x)\n            loss = output.sum()\n\n            memory_after_forward = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n            forward_memory = (memory_after_forward - memory_before) / 1024**2\n\n            # Backward pass \u30e1\u30e2\u30ea\n            loss.backward()\n\n            memory_after_backward = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n            backward_memory = (memory_after_backward - memory_after_forward) / 1024**2\n\n            total_memory = memory_after_backward / 1024**2\n\n            results['input_sizes'].append(f\"{batch_size}x{seq_len}\")\n            results['model_memory'].append(model_memory)\n            results['forward_memory'].append(forward_memory)\n            results['backward_memory'].append(backward_memory)\n            results['total_memory'].append(total_memory)\n\n            print(f\"Size {batch_size}x{seq_len}: \"\n                  f\"Model: {model_memory:.1f}MB, \"\n                  f\"Forward: {forward_memory:.1f}MB, \"\n                  f\"Backward: {backward_memory:.1f}MB, \"\n                  f\"Total: {total_memory:.1f}MB\")\n\n            # \u30e1\u30e2\u30ea\u30ea\u30fc\u30af\u9632\u6b62\n            del x, output, loss\n            model.zero_grad()\n\n        return results\n\n    def profile_computation_time(self, model: nn.Module, input_sizes: List[Tuple[int, int]], \n                               num_runs: int = 10) -&gt; Dict:\n        \"\"\"\u8a08\u7b97\u6642\u9593\u306e\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\"\"\"\n\n        results = {\n            'input_sizes': [],\n            'forward_time': [],\n            'backward_time': [],\n            'total_time': []\n        }\n\n        model.eval()  # \u5b89\u5b9a\u3057\u305f\u6e2c\u5b9a\u306e\u305f\u3081\n\n        for batch_size, seq_len in input_sizes:\n            device = next(model.parameters()).device\n\n            forward_times = []\n            backward_times = []\n\n            for _ in range(num_runs):\n                x = torch.randint(0, 1000, (batch_size, seq_len), device=device)\n\n                # Forward\u6642\u9593\u6e2c\u5b9a\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n\n                start_time = time.time()\n                output = model(x)\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n\n                forward_time = time.time() - start_time\n                forward_times.append(forward_time)\n\n                # Backward\u6642\u9593\u6e2c\u5b9a\n                loss = output.sum()\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n\n                start_time = time.time()\n                loss.backward()\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n\n                backward_time = time.time() - start_time\n                backward_times.append(backward_time)\n\n                # \u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7\n                del x, output, loss\n                model.zero_grad()\n\n            avg_forward = sum(forward_times) / len(forward_times)\n            avg_backward = sum(backward_times) / len(backward_times)\n            avg_total = avg_forward + avg_backward\n\n            results['input_sizes'].append(f\"{batch_size}x{seq_len}\")\n            results['forward_time'].append(avg_forward * 1000)  # ms\n            results['backward_time'].append(avg_backward * 1000)  # ms\n            results['total_time'].append(avg_total * 1000)  # ms\n\n            print(f\"Size {batch_size}x{seq_len}: \"\n                  f\"Forward: {avg_forward*1000:.2f}ms, \"\n                  f\"Backward: {avg_backward*1000:.2f}ms, \"\n                  f\"Total: {avg_total*1000:.2f}ms\")\n\n        return results\n\n    def analyze_complexity(self, model: nn.Module, max_seq_len: int = 512) -&gt; Dict:\n        \"\"\"\u8a08\u7b97\u8907\u96d1\u5ea6\u306e\u5206\u6790\"\"\"\n\n        # \u30e2\u30c7\u30eb\u60c5\u5831\u306e\u53d6\u5f97\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        print(f\"=== Model Analysis ===\")\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n\n        # \u5c64\u5225\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\n        layer_params = {}\n        for name, param in model.named_parameters():\n            layer_name = name.split('.')[0]  # \u6700\u4e0a\u4f4d\u30ec\u30a4\u30e4\u30fc\u540d\n            if layer_name not in layer_params:\n                layer_params[layer_name] = 0\n            layer_params[layer_name] += param.numel()\n\n        print(f\"\\\\n=== Parameters by Layer ===\")\n        for layer_name, params in sorted(layer_params.items(), key=lambda x: x[1], reverse=True):\n            percentage = params / total_params * 100\n            print(f\"{layer_name}: {params:,} ({percentage:.1f}%)\")\n\n        # \u7406\u8ad6\u7684\u8907\u96d1\u5ea6\u5206\u6790\n        if hasattr(model, 'd_model') and hasattr(model, 'n_heads'):\n            d_model = model.d_model\n            n_heads = model.n_heads\n            n_layers = len(model.transformer_blocks) if hasattr(model, 'transformer_blocks') else 1\n\n            print(f\"\\\\n=== Theoretical Complexity Analysis ===\")\n            print(f\"d_model: {d_model}, n_heads: {n_heads}, n_layers: {n_layers}\")\n\n            # Self-attention complexity: O(n^2 * d)\n            attention_ops = lambda n: n * n * d_model * n_layers\n\n            # Feed-forward complexity: O(n * d^2)\n            ff_ops = lambda n: n * d_model * d_model * 4 * n_layers  # assuming d_ff = 4 * d_model\n\n            seq_lengths = [64, 128, 256, 512]\n\n            print(f\"\\\\n{'Seq Len':&lt;8} {'Attention':&lt;12} {'FF':&lt;12} {'Total':&lt;12} {'Memory (MB)':&lt;12}\")\n            print(\"-\" * 60)\n\n            for seq_len in seq_lengths:\n                attn_ops = attention_ops(seq_len)\n                ff_ops_val = ff_ops(seq_len)\n                total_ops = attn_ops + ff_ops_val\n\n                # \u30e1\u30e2\u30ea\u63a8\u5b9a (activations)\n                memory_mb = seq_len * d_model * 4 / 1024**2  # float32\n\n                print(f\"{seq_len:&lt;8} {attn_ops/1e6:.1f}M{'':&lt;6} {ff_ops_val/1e6:.1f}M{'':&lt;6} \"\n                      f\"{total_ops/1e6:.1f}M{'':&lt;6} {memory_mb:.2f}\")\n\n        return {\n            'total_params': total_params,\n            'trainable_params': trainable_params,\n            'layer_params': layer_params\n        }\n\n    def visualize_profiles(self, memory_results: Dict, time_results: Dict):\n        \"\"\"\u30d7\u30ed\u30d5\u30a1\u30a4\u30eb\u7d50\u679c\u306e\u53ef\u8996\u5316\"\"\"\n\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n        # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n        ax = axes[0, 0]\n        x_pos = range(len(memory_results['input_sizes']))\n\n        ax.bar([i-0.2 for i in x_pos], memory_results['model_memory'], \n               width=0.4, label='Model', alpha=0.7)\n        ax.bar([i+0.2 for i in x_pos], memory_results['forward_memory'], \n               width=0.4, label='Forward', alpha=0.7)\n\n        ax.set_title('Memory Usage by Component')\n        ax.set_xlabel('Input Size')\n        ax.set_ylabel('Memory (MB)')\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(memory_results['input_sizes'], rotation=45)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # \u7dcf\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n        ax = axes[0, 1]\n        ax.plot(memory_results['input_sizes'], memory_results['total_memory'], \n                'ro-', linewidth=2, markersize=6)\n        ax.set_title('Total Memory Usage')\n        ax.set_xlabel('Input Size')\n        ax.set_ylabel('Memory (MB)')\n        ax.tick_params(axis='x', rotation=45)\n        ax.grid(True, alpha=0.3)\n\n        # \u8a08\u7b97\u6642\u9593\n        ax = axes[1, 0]\n        x_pos = range(len(time_results['input_sizes']))\n\n        ax.bar([i-0.2 for i in x_pos], time_results['forward_time'], \n               width=0.4, label='Forward', alpha=0.7)\n        ax.bar([i+0.2 for i in x_pos], time_results['backward_time'], \n               width=0.4, label='Backward', alpha=0.7)\n\n        ax.set_title('Computation Time by Phase')\n        ax.set_xlabel('Input Size')\n        ax.set_ylabel('Time (ms)')\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(time_results['input_sizes'], rotation=45)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # \u7dcf\u8a08\u7b97\u6642\u9593\n        ax = axes[1, 1]\n        ax.plot(time_results['input_sizes'], time_results['total_time'], \n                'bo-', linewidth=2, markersize=6)\n        ax.set_title('Total Computation Time')\n        ax.set_xlabel('Input Size')\n        ax.set_ylabel('Time (ms)')\n        ax.tick_params(axis='x', rotation=45)\n        ax.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n# \u4f7f\u7528\u4f8b\ndef run_profiling():\n    \"\"\"\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\u306e\u5b9f\u884c\u4f8b\"\"\"\n    from part4.minimal_transformer import MinimalTransformerEncoder\n\n    # \u30e2\u30c7\u30eb\u4f5c\u6210\n    model = MinimalTransformerEncoder(\n        vocab_size=1000,\n        d_model=256,\n        n_heads=8,\n        n_layers=4\n    )\n\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    # \u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\u521d\u671f\u5316\n    profiler = TransformerProfiler()\n\n    # \u30c6\u30b9\u30c8\u7528\u306e\u5165\u529b\u30b5\u30a4\u30ba\n    input_sizes = [(4, 64), (4, 128), (4, 256), (2, 512)]\n\n    print(\"=== Memory Profiling ===\")\n    memory_results = profiler.profile_memory_usage(model, input_sizes)\n\n    print(\"\\\\n=== Time Profiling ===\")\n    time_results = profiler.profile_computation_time(model, input_sizes)\n\n    print(\"\\\\n=== Complexity Analysis ===\")\n    complexity_results = profiler.analyze_complexity(model)\n\n    # \u53ef\u8996\u5316\n    profiler.visualize_profiles(memory_results, time_results)\n\n    return profiler, memory_results, time_results, complexity_results\n\n# \u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\u5b9f\u884c\nif __name__ == \"__main__\":\n    profiler, mem_results, time_results, complexity = run_profiling()\n</code></pre>"},{"location":"exercises/part4-exercises/#_1","title":"\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c","text":""},{"location":"exercises/part4-exercises/#8","title":"\u554f\u984c 8 \ud83c\udf1f","text":"<p>\u52b9\u7387\u7684\u306aTransformer\u5b9f\u88c5\u306e\u6700\u9069\u5316\u6280\u8853\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\uff1a - Flash Attention - Gradient Checkpointing - Mixed Precision Training</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nimport math\nfrom typing import Optional\n\nclass OptimizedTransformer(nn.Module):\n    \"\"\"\u6700\u9069\u5316\u6280\u8853\u3092\u542b\u3080Transformer\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size=1000, d_model=512, n_heads=8, n_layers=6,\n                 d_ff=2048, max_len=2048, dropout=0.1, \n                 use_flash_attention=True, use_gradient_checkpointing=True):\n        super().__init__()\n\n        self.d_model = d_model\n        self.use_gradient_checkpointing = use_gradient_checkpointing\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = OptimizedPositionalEncoding(d_model, max_len)\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\n        self.transformer_blocks = nn.ModuleList([\n            OptimizedTransformerBlock(\n                d_model, n_heads, d_ff, dropout, use_flash_attention\n            ) for _ in range(n_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask=None):\n        with autocast():  # Mixed precision\n            x = self.embedding(x) * math.sqrt(self.d_model)\n            x = self.pos_encoding(x)\n            x = self.dropout(x)\n\n            # Gradient checkpointing\n            if self.use_gradient_checkpointing and self.training:\n                for block in self.transformer_blocks:\n                    x = torch.utils.checkpoint.checkpoint(block, x, mask)\n            else:\n                for block in self.transformer_blocks:\n                    x = block(x, mask)\n\n            return self.layer_norm(x)\n\nclass OptimizedPositionalEncoding(nn.Module):\n    \"\"\"\u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        self.d_model = d_model\n\n        # \u4e8b\u524d\u8a08\u7b97\u305b\u305a\u3001\u5fc5\u8981\u6642\u306b\u8a08\u7b97\n        self.register_buffer('inv_freq', \n                           torch.exp(torch.arange(0, d_model, 2).float() * \n                                   (-math.log(10000.0) / d_model)))\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        device = x.device\n\n        # \u52d5\u7684\u306b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u8a08\u7b97\n        position = torch.arange(seq_len, device=device).float().unsqueeze(1)\n\n        pe = torch.zeros(seq_len, self.d_model, device=device)\n        pe[:, 0::2] = torch.sin(position * self.inv_freq)\n        pe[:, 1::2] = torch.cos(position * self.inv_freq)\n\n        return x + pe.unsqueeze(0)\n\nclass OptimizedTransformerBlock(nn.Module):\n    \"\"\"\u6700\u9069\u5316\u3055\u308c\u305fTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n\n    def __init__(self, d_model, n_heads, d_ff, dropout, use_flash_attention=True):\n        super().__init__()\n\n        if use_flash_attention:\n            self.attention = FlashMultiHeadAttention(d_model, n_heads, dropout)\n        else:\n            self.attention = StandardMultiHeadAttention(d_model, n_heads, dropout)\n\n        self.feed_forward = OptimizedFeedForward(d_model, d_ff, dropout)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Pre-norm (Post-norm\u3088\u308a\u5b89\u5b9a)\n        normed_x = self.norm1(x)\n        attn_out = self.attention(normed_x, normed_x, normed_x, mask)\n        x = x + self.dropout(attn_out)\n\n        normed_x = self.norm2(x)\n        ff_out = self.feed_forward(normed_x)\n        x = x + self.dropout(ff_out)\n\n        return x\n\nclass FlashMultiHeadAttention(nn.Module):\n    \"\"\"Flash Attention\u3092\u6a21\u3057\u305f\u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.scale = 1.0 / math.sqrt(self.d_k)\n\n        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, _ = query.shape\n\n        # QKV\u3092\u4e00\u5ea6\u306b\u8a08\u7b97\uff08\u30e1\u30e2\u30ea\u5e2f\u57df\u5e45\u306e\u52b9\u7387\u5316\uff09\n        qkv = self.qkv(query).chunk(3, dim=-1)\n        q, k, v = [x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2) \n                  for x in qkv]\n\n        # \u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n        if torch.cuda.is_available() and hasattr(F, 'scaled_dot_product_attention'):\n            # PyTorch 2.0+ \u306eFlash Attention\n            attn_output = F.scaled_dot_product_attention(\n                q, k, v, attn_mask=mask, dropout_p=self.dropout.p if self.training else 0.0\n            )\n        else:\n            # \u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\u5b9f\u88c5\n            attn_output = self._flash_attention_fallback(q, k, v, mask)\n\n        # \u51fa\u529b\u6574\u5f62\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, self.d_model\n        )\n\n        return self.out_proj(attn_output)\n\n    def _flash_attention_fallback(self, q, k, v, mask=None):\n        \"\"\"Flash Attention\u306e\u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\u5b9f\u88c5\"\"\"\n        # \u30d6\u30ed\u30c3\u30af\u5316\u3057\u3066\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u524a\u6e1b\n        batch_size, n_heads, seq_len, d_k = q.shape\n        block_size = min(64, seq_len)  # \u30d6\u30ed\u30c3\u30af\u30b5\u30a4\u30ba\n\n        output = torch.zeros_like(q)\n\n        for i in range(0, seq_len, block_size):\n            end_i = min(i + block_size, seq_len)\n            q_block = q[:, :, i:end_i, :]\n\n            # \u73fe\u5728\u306e\u30af\u30a8\u30ea\u30d6\u30ed\u30c3\u30af\u306b\u5bfe\u3059\u308b\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n            scores = torch.matmul(q_block, k.transpose(-2, -1)) * self.scale\n\n            if mask is not None:\n                scores = scores.masked_fill(mask[:, :, i:end_i, :] == 0, -1e9)\n\n            attn_weights = F.softmax(scores, dim=-1)\n            attn_weights = self.dropout(attn_weights)\n\n            output[:, :, i:end_i, :] = torch.matmul(attn_weights, v)\n\n        return output\n\nclass StandardMultiHeadAttention(nn.Module):\n    \"\"\"\u6a19\u6e96\u7684\u306aMulti-Head Attention\uff08\u6bd4\u8f03\u7528\uff09\"\"\"\n\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n        d_k = Q.size(-1)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context = torch.matmul(attn_weights, V)\n        context = context.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model\n        )\n\n        return self.W_o(context)\n\nclass OptimizedFeedForward(nn.Module):\n    \"\"\"\u6700\u9069\u5316\u3055\u308c\u305fFeed Forward Network\"\"\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n\n        # SwiGLU activation (GPT\u306a\u3069\u3067\u4f7f\u7528)\n        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # SwiGLU: x -&gt; SiLU(W1(x)) * W3(x) -&gt; W2\n        return self.w2(self.dropout(F.silu(self.w1(x)) * self.w3(x)))\n\nclass MixedPrecisionTrainer:\n    \"\"\"Mixed Precision Training \u306e\u30d8\u30eb\u30d1\u30fc\u30af\u30e9\u30b9\"\"\"\n\n    def __init__(self, model, optimizer, enabled=True):\n        self.model = model\n        self.optimizer = optimizer\n        self.scaler = GradScaler(enabled=enabled)\n        self.enabled = enabled\n\n    def train_step(self, x, targets, criterion):\n        \"\"\"1\u56de\u306e\u8a13\u7df4\u30b9\u30c6\u30c3\u30d7\"\"\"\n        self.optimizer.zero_grad()\n\n        with autocast(enabled=self.enabled):\n            outputs = self.model(x)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n\n        # \u30b9\u30b1\u30fc\u30eb\u3055\u308c\u305f\u52fe\u914d\u3067\u306e\u9006\u4f1d\u64ad\n        self.scaler.scale(loss).backward()\n\n        # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        self.scaler.unscale_(self.optimizer)\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n\n        return loss.item()\n\n# \u4f7f\u7528\u4f8b\u3068\u30d9\u30f3\u30c1\u30de\u30fc\u30af\ndef benchmark_optimizations():\n    \"\"\"\u6700\u9069\u5316\u6280\u8853\u306e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\"\"\"\n\n    if not torch.cuda.is_available():\n        print(\"CUDA not available, skipping benchmark\")\n        return\n\n    device = torch.device('cuda')\n\n    # \u30e2\u30c7\u30eb\u8a2d\u5b9a\n    configs = [\n        (\"Standard\", False, False),\n        (\"Flash Attention\", True, False),\n        (\"Flash + Gradient Checkpointing\", True, True),\n    ]\n\n    results = {}\n\n    for name, use_flash, use_checkpoint in configs:\n        print(f\"\\\\n=== Testing {name} ===\")\n\n        model = OptimizedTransformer(\n            vocab_size=32000,\n            d_model=512,\n            n_heads=8,\n            n_layers=6,\n            use_flash_attention=use_flash,\n            use_gradient_checkpointing=use_checkpoint\n        ).to(device)\n\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n        batch_size, seq_len = 4, 1024\n        x = torch.randint(0, 32000, (batch_size, seq_len), device=device)\n\n        # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u6e2c\u5b9a\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n        # Forward pass\n        start_time = torch.cuda.Event(enable_timing=True)\n        end_time = torch.cuda.Event(enable_timing=True)\n\n        start_time.record()\n        with autocast():\n            output = model(x)\n            loss = output.sum()\n\n        # Backward pass\n        loss.backward()\n        end_time.record()\n\n        torch.cuda.synchronize()\n\n        time_ms = start_time.elapsed_time(end_time)\n        peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n\n        results[name] = {\n            'time_ms': time_ms,\n            'memory_mb': peak_memory\n        }\n\n        print(f\"Time: {time_ms:.1f} ms\")\n        print(f\"Peak Memory: {peak_memory:.1f} MB\")\n\n        del model, x, output, loss\n        torch.cuda.empty_cache()\n\n    # \u7d50\u679c\u6bd4\u8f03\n    print(f\"\\\\n=== Benchmark Results ===\")\n    baseline = results[\"Standard\"]\n\n    for name, metrics in results.items():\n        time_ratio = baseline['time_ms'] / metrics['time_ms']\n        memory_ratio = baseline['memory_mb'] / metrics['memory_mb']\n\n        print(f\"{name}:\")\n        print(f\"  Speedup: {time_ratio:.2f}x\")\n        print(f\"  Memory Efficiency: {memory_ratio:.2f}x\")\n\n    return results\n\n# \u30d9\u30f3\u30c1\u30de\u30fc\u30af\u5b9f\u884c\nif __name__ == \"__main__\":\n    results = benchmark_optimizations()\n</code></pre>"},{"location":"exercises/part4-exercises/#_2","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<p>\u3053\u308c\u3089\u306e\u6f14\u7fd2\u3092\u5b8c\u4e86\u3057\u305f\u3089\u3001\u7b2c5\u90e8\u306b\u9032\u3093\u3067\u5b9f\u969b\u306eLLM\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3068\u305d\u306e\u5fdc\u7528\u3092\u5b66\u3073\u307e\u3057\u3087\u3046\uff01</p> <p>\ud83d\udca1 \u5b66\u7fd2\u306e\u30b3\u30c4:  - \u5404\u5b9f\u88c5\u3092\u6bb5\u968e\u7684\u306b\u7406\u89e3\u3057\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u7c21\u7565\u5316\u3057\u305f\u30d0\u30fc\u30b8\u30e7\u30f3\u304b\u3089\u59cb\u3081\u308b - \u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\u30c4\u30fc\u30eb\u3092\u4f7f\u3063\u3066\u6027\u80fd\u7279\u6027\u3092\u7406\u89e3\u3059\u308b - \u5b9f\u969b\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u5c0f\u898f\u6a21\u306a\u5b9f\u9a13\u3092\u884c\u3063\u3066\u307f\u308b - \u6700\u9069\u5316\u6280\u8853\u306f\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u6bb5\u968e\u7684\u306b\u5c0e\u5165\u3059\u308b</p>"},{"location":"exercises/part5-exercises/","title":"\u7b2c5\u90e8 \u6f14\u7fd2\u554f\u984c","text":""},{"location":"exercises/part5-exercises/#51-gpt","title":"\u6f14\u7fd2 5.1: GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3","text":""},{"location":"exercises/part5-exercises/#1","title":"\u554f\u984c 1","text":"<p>\u56e0\u679c\u7684\uff08causal\uff09\u30de\u30b9\u30ad\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u3001\u305d\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nclass CausalMasking:\n    \"\"\"\u56e0\u679c\u7684\u30de\u30b9\u30ad\u30f3\u30b0\u306e\u5b9f\u88c5\u3068\u53ef\u8996\u5316\"\"\"\n\n    @staticmethod\n    def create_causal_mask(seq_len: int, device=None):\n        \"\"\"\u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u4f5c\u6210\"\"\"\n        # \u4e0b\u4e09\u89d2\u884c\u5217\u3092\u4f5c\u6210\uff08\u5bfe\u89d2\u6210\u5206\u3092\u542b\u3080\uff09\n        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n        return mask\n\n    @staticmethod\n    def visualize_masks():\n        \"\"\"\u69d8\u3005\u306a\u30de\u30b9\u30ad\u30f3\u30b0\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n        seq_len = 10\n\n        # 1. \u56e0\u679c\u7684\u30de\u30b9\u30af\n        causal_mask = CausalMasking.create_causal_mask(seq_len).numpy()\n        sns.heatmap(causal_mask, ax=axes[0, 0], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[0, 0].set_title('Causal Mask (GPT-style)')\n\n        # 2. \u53cc\u65b9\u5411\u30de\u30b9\u30af\uff08BERT-style\uff09\n        bidirectional_mask = torch.ones(seq_len, seq_len).numpy()\n        sns.heatmap(bidirectional_mask, ax=axes[0, 1], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[0, 1].set_title('Bidirectional Mask (BERT-style)')\n\n        # 3. Prefix LM\u30de\u30b9\u30af\n        prefix_len = 4\n        prefix_mask = torch.ones(seq_len, seq_len)\n        prefix_mask[prefix_len:, prefix_len:] = torch.tril(\n            torch.ones(seq_len - prefix_len, seq_len - prefix_len)\n        )\n        sns.heatmap(prefix_mask.numpy(), ax=axes[0, 2], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[0, 2].set_title(f'Prefix LM Mask (prefix={prefix_len})')\n\n        # 4. \u30b9\u30e9\u30a4\u30c7\u30a3\u30f3\u30b0\u30a6\u30a3\u30f3\u30c9\u30a6\u30de\u30b9\u30af\n        window_size = 3\n        sliding_mask = torch.zeros(seq_len, seq_len)\n        for i in range(seq_len):\n            start = max(0, i - window_size + 1)\n            end = min(seq_len, i + 1)\n            sliding_mask[i, start:end] = 1\n        sns.heatmap(sliding_mask.numpy(), ax=axes[1, 0], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[1, 0].set_title(f'Sliding Window (size={window_size})')\n\n        # 5. \u30e9\u30f3\u30c0\u30e0\u30de\u30b9\u30af\uff08\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u98a8\uff09\n        random_mask = (torch.rand(seq_len, seq_len) &gt; 0.2).float()\n        random_mask = torch.tril(random_mask)  # \u56e0\u679c\u6027\u3092\u4fdd\u6301\n        sns.heatmap(random_mask.numpy(), ax=axes[1, 1], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[1, 1].set_title('Random Causal Mask (80% keep)')\n\n        # 6. \u30d6\u30ed\u30c3\u30af\u5bfe\u89d2\u30de\u30b9\u30af\n        block_size = 3\n        block_mask = torch.zeros(seq_len, seq_len)\n        for i in range(0, seq_len, block_size):\n            end = min(i + block_size, seq_len)\n            block_mask[i:end, i:end] = 1\n        sns.heatmap(block_mask.numpy(), ax=axes[1, 2], cmap='Blues', \n                   cbar=False, square=True, annot=True, fmt='.0f')\n        axes[1, 2].set_title(f'Block Diagonal (size={block_size})')\n\n        plt.tight_layout()\n        plt.show()\n\nclass CausalGPTAttention(nn.Module):\n    \"\"\"\u56e0\u679c\u7684\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model, n_heads, max_len=1024, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.scale = 1.0 / math.sqrt(self.d_k)\n\n        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n        self.out_proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        # \u56e0\u679c\u7684\u30de\u30b9\u30af\u3092\u30d0\u30c3\u30d5\u30a1\u3068\u3057\u3066\u767b\u9332\n        self.register_buffer(\n            'causal_mask',\n            torch.tril(torch.ones(max_len, max_len)).unsqueeze(0).unsqueeze(0)\n        )\n\n    def forward(self, x, output_attentions=False):\n        batch_size, seq_len, _ = x.shape\n\n        # Q, K, V \u3092\u8a08\u7b97\n        qkv = self.qkv(x).chunk(3, dim=-1)\n        q, k, v = [t.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2) \n                  for t in qkv]\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30b9\u30b3\u30a2\u8a08\u7b97\n        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n\n        # \u56e0\u679c\u7684\u30de\u30b9\u30af\u9069\u7528\n        causal_mask = self.causal_mask[:, :, :seq_len, :seq_len]\n        scores = scores.masked_fill(causal_mask == 0, -1e9)\n\n        # \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3068\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # \u5024\u3068\u306e\u7a4d\n        context = torch.matmul(attn_weights, v)\n        context = context.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, self.d_model\n        )\n\n        output = self.out_proj(context)\n\n        if output_attentions:\n            return output, attn_weights\n        return output\n\ndef test_causal_masking():\n    \"\"\"\u56e0\u679c\u7684\u30de\u30b9\u30ad\u30f3\u30b0\u306e\u30c6\u30b9\u30c8\"\"\"\n\n    # \u30de\u30b9\u30af\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\n    CausalMasking.visualize_masks()\n\n    # \u5b9f\u969b\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\u3067\u306e\u30de\u30b9\u30af\u52b9\u679c\n    d_model, n_heads = 64, 4\n    model = CausalGPTAttention(d_model, n_heads)\n\n    # \u30c6\u30b9\u30c8\u5165\u529b\n    batch_size, seq_len = 2, 8\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u3092\u53d6\u5f97\n    output, attn_weights = model(x, output_attentions=True)\n\n    # \u6700\u521d\u306e\u30d0\u30c3\u30c1\u3001\u6700\u521d\u306e\u30d8\u30c3\u30c9\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u91cd\u307f\u3092\u53ef\u8996\u5316\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(attn_weights[0, 0].detach().numpy(), \n               cmap='Blues', annot=True, fmt='.2f', square=True)\n    plt.title('Causal Attention Weights (Batch 0, Head 0)')\n    plt.xlabel('Key Position')\n    plt.ylabel('Query Position')\n    plt.show()\n\n    # \u30de\u30b9\u30af\u306e\u52b9\u679c\u3092\u78ba\u8a8d\uff08\u672a\u6765\u306e\u60c5\u5831\u304c0\u306b\u306a\u3063\u3066\u3044\u308b\u304b\uff09\n    print(\"Attention weights upper triangle (should be ~0):\")\n    upper_triangle = torch.triu(attn_weights[0, 0], diagonal=1)\n    print(upper_triangle)\n    print(f\"Max value in upper triangle: {upper_triangle.max().item():.6f}\")\n\n# \u30c6\u30b9\u30c8\u5b9f\u884c\ntest_causal_masking()\n</code></pre>"},{"location":"exercises/part5-exercises/#2","title":"\u554f\u984c 2","text":"<p>GPT\u306e\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff08learned vs sinusoidal\uff09\u3092\u6bd4\u8f03\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List\n\nclass PositionalEncodingComparison:\n    \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n\n    @staticmethod\n    def create_sinusoidal_encoding(seq_len: int, d_model: int) -&gt; torch.Tensor:\n        \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n        pe = torch.zeros(seq_len, d_model)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        return pe\n\n    @staticmethod\n    def create_learned_encoding(seq_len: int, d_model: int) -&gt; nn.Parameter:\n        \"\"\"\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n        pe = nn.Parameter(torch.randn(seq_len, d_model) * 0.01)\n        return pe\n\nclass GPTWithPositionalEncoding(nn.Module):\n    \"\"\"\u7570\u306a\u308b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u6301\u3064GPT\u30e2\u30c7\u30eb\"\"\"\n\n    def __init__(self, vocab_size, d_model, n_heads, n_layers, \n                 max_len=1024, encoding_type='sinusoidal'):\n        super().__init__()\n\n        self.d_model = d_model\n        self.encoding_type = encoding_type\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        if encoding_type == 'sinusoidal':\n            pe = PositionalEncodingComparison.create_sinusoidal_encoding(max_len, d_model)\n            self.register_buffer('position_encoding', pe)\n        elif encoding_type == 'learned':\n            self.position_encoding = PositionalEncodingComparison.create_learned_encoding(\n                max_len, d_model\n            )\n        elif encoding_type == 'rotary':\n            self.rotary_emb = RotaryPositionalEmbedding(d_model // n_heads)\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\n        self.blocks = nn.ModuleList([\n            GPTBlock(d_model, n_heads) for _ in range(n_layers)\n        ])\n\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n\n    def forward(self, input_ids):\n        batch_size, seq_len = input_ids.shape\n        device = input_ids.device\n\n        # \u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\n        x = self.token_embedding(input_ids)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u8ffd\u52a0\n        if self.encoding_type in ['sinusoidal', 'learned']:\n            positions = torch.arange(seq_len, device=device)\n            x = x + self.position_encoding[positions]\n        # rotary\u306e\u5834\u5408\u306f\u5404\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u5c64\u3067\u9069\u7528\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\n        for block in self.blocks:\n            if self.encoding_type == 'rotary':\n                x = block(x, rotary_emb=self.rotary_emb)\n            else:\n                x = block(x)\n\n        # \u6700\u7d42\u5c64\u6b63\u898f\u5316\u3068\u51fa\u529b\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        return logits\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n        super().__init__()\n        inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        # \u30ad\u30e3\u30c3\u30b7\u30e5\n        self._cos_cached = None\n        self._sin_cached = None\n        self._seq_len_cached = None\n\n    def forward(self, x, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[1]\n\n        if seq_len != self._seq_len_cached:\n            self._seq_len_cached = seq_len\n            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n            freqs = torch.einsum('i,j-&gt;ij', t, self.inv_freq)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            self._cos_cached = emb.cos()[None, :, None, :]\n            self._sin_cached = emb.sin()[None, :, None, :]\n\n        return self._cos_cached, self._sin_cached\n\n    @staticmethod\n    def rotate_half(x):\n        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        q_embed = (q * cos) + (self.rotate_half(q) * sin)\n        k_embed = (k * cos) + (self.rotate_half(k) * sin)\n        return q_embed, k_embed\n\nclass GPTBlock(nn.Module):\n    \"\"\"GPT\u306eTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(d_model)\n        self.attn = CausalGPTAttention(d_model, n_heads)\n        self.ln_2 = nn.LayerNorm(d_model)\n        self.mlp = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(0.1)\n        )\n\n    def forward(self, x, rotary_emb=None):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\ndef compare_positional_encodings():\n    \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u6bd4\u8f03\u5b9f\u9a13\"\"\"\n\n    # \u8a2d\u5b9a\n    vocab_size = 1000\n    d_model = 128\n    n_heads = 4\n    n_layers = 2\n    seq_len = 50\n\n    # \u5404\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30bf\u30a4\u30d7\u306e\u30e2\u30c7\u30eb\u4f5c\u6210\n    models = {\n        'sinusoidal': GPTWithPositionalEncoding(\n            vocab_size, d_model, n_heads, n_layers, encoding_type='sinusoidal'\n        ),\n        'learned': GPTWithPositionalEncoding(\n            vocab_size, d_model, n_heads, n_layers, encoding_type='learned'\n        )\n    }\n\n    # \u53ef\u8996\u5316\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # 1. Sinusoidal encoding\n    sinusoidal_pe = PositionalEncodingComparison.create_sinusoidal_encoding(seq_len, d_model)\n    im1 = axes[0, 0].imshow(sinusoidal_pe[:20, :64].T, cmap='viridis', aspect='auto')\n    axes[0, 0].set_title('Sinusoidal Position Encoding')\n    axes[0, 0].set_xlabel('Position')\n    axes[0, 0].set_ylabel('Dimension')\n    plt.colorbar(im1, ax=axes[0, 0])\n\n    # 2. Learned encoding (\u521d\u671f\u5024)\n    learned_pe = models['learned'].position_encoding.detach()[:20, :64].T\n    im2 = axes[0, 1].imshow(learned_pe, cmap='viridis', aspect='auto')\n    axes[0, 1].set_title('Learned Position Encoding (Initial)')\n    axes[0, 1].set_xlabel('Position')\n    axes[0, 1].set_ylabel('Dimension')\n    plt.colorbar(im2, ax=axes[0, 1])\n\n    # 3. \u4f4d\u7f6e\u306b\u3088\u308b\u985e\u4f3c\u5ea6\uff08Sinusoidal\uff09\n    cos_sim_sin = torch.nn.functional.cosine_similarity(\n        sinusoidal_pe.unsqueeze(1), sinusoidal_pe.unsqueeze(0), dim=2\n    )\n    im3 = axes[1, 0].imshow(cos_sim_sin[:20, :20], cmap='coolwarm', vmin=-1, vmax=1)\n    axes[1, 0].set_title('Position Similarity (Sinusoidal)')\n    axes[1, 0].set_xlabel('Position')\n    axes[1, 0].set_ylabel('Position')\n    plt.colorbar(im3, ax=axes[1, 0])\n\n    # 4. \u5404\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u7279\u6027\n    positions = torch.arange(seq_len)\n\n    # \u5468\u671f\u6027\u306e\u5206\u6790\n    for i, dim in enumerate([0, 10, 20, 30]):\n        if dim &lt; d_model:\n            axes[1, 1].plot(positions[:30], sinusoidal_pe[:30, dim], \n                          label=f'Dim {dim}', alpha=0.7)\n\n    axes[1, 1].set_title('Sinusoidal Encoding Periodicity')\n    axes[1, 1].set_xlabel('Position')\n    axes[1, 1].set_ylabel('Value')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u7c21\u5358\u306a\u5b66\u7fd2\u5b9f\u9a13\n    print(\"=== Training Comparison ===\")\n\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u3067\u7c21\u5358\u306a\u8a13\u7df4\n    batch_size = 32\n    criterion = nn.CrossEntropyLoss()\n\n    for name, model in models.items():\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        losses = []\n        for step in range(100):\n            # \u30e9\u30f3\u30c0\u30e0\u306a\u5165\u529b\u751f\u6210\n            input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n            targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n\n            # \u9806\u4f1d\u64ad\n            logits = model(input_ids)\n            loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n\n            # \u9006\u4f1d\u64ad\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n\n        print(f\"{name} - Final loss: {losses[-1]:.4f}, \"\n              f\"Improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")\n\n    return models\n\n# \u6bd4\u8f03\u5b9f\u884c\nmodels = compare_positional_encodings()\n</code></pre>"},{"location":"exercises/part5-exercises/#52","title":"\u6f14\u7fd2 5.2: \u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0","text":""},{"location":"exercises/part5-exercises/#3","title":"\u554f\u984c 3","text":"<p>\u7c21\u5358\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u4e8b\u524d\u5b66\u7fd2\u30eb\u30fc\u30d7\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Optional\nimport json\n\nclass TextDataset(Dataset):\n    \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\"\"\"\n\n    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.texts = texts\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tokens = self.tokenizer.encode(text)\n\n        # \u30d1\u30c7\u30a3\u30f3\u30b0\u307e\u305f\u306f\u30c8\u30e9\u30f3\u30b1\u30fc\u30c8\n        if len(tokens) &gt; self.max_length:\n            tokens = tokens[:self.max_length]\n        else:\n            tokens = tokens + [self.tokenizer.pad_token_id] * (self.max_length - len(tokens))\n\n        tokens = torch.tensor(tokens, dtype=torch.long)\n\n        # \u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u3067\u306f\u5165\u529b\u3068\u76ee\u6a19\u304c1\u3064\u305a\u308c\u308b\n        return {\n            'input_ids': tokens[:-1],\n            'labels': tokens[1:]\n        }\n\nclass SimpleTokenizer:\n    \"\"\"\u6587\u5b57\u30ec\u30d9\u30eb\u306e\u7c21\u5358\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\"\"\"\n\n    def __init__(self, vocab: Optional[Dict[str, int]] = None):\n        if vocab is None:\n            # \u57fa\u672c\u7684\u306a\u6587\u5b57\u30bb\u30c3\u30c8\n            chars = list(\"abcdefghijklmnopqrstuvwxyz \")\n            chars += list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n            chars += list(\"0123456789\")\n            chars += list(\".,!?-'\\\"\")\n\n            self.vocab = {'&lt;pad&gt;': 0, '&lt;unk&gt;': 1, '&lt;bos&gt;': 2, '&lt;eos&gt;': 3}\n            for i, char in enumerate(chars, start=4):\n                self.vocab[char] = i\n        else:\n            self.vocab = vocab\n\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.pad_token_id = self.vocab['&lt;pad&gt;']\n        self.unk_token_id = self.vocab['&lt;unk&gt;']\n        self.bos_token_id = self.vocab['&lt;bos&gt;']\n        self.eos_token_id = self.vocab['&lt;eos&gt;']\n\n    def encode(self, text: str) -&gt; List[int]:\n        tokens = [self.bos_token_id]\n        for char in text:\n            tokens.append(self.vocab.get(char, self.unk_token_id))\n        tokens.append(self.eos_token_id)\n        return tokens\n\n    def decode(self, tokens: List[int]) -&gt; str:\n        chars = []\n        for token in tokens:\n            if token in self.inv_vocab:\n                char = self.inv_vocab[token]\n                if char not in ['&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;unk&gt;']:\n                    chars.append(char)\n        return ''.join(chars)\n\nclass PretrainingTrainer:\n    \"\"\"\u4e8b\u524d\u5b66\u7fd2\u30c8\u30ec\u30fc\u30ca\u30fc\"\"\"\n\n    def __init__(self, model, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.tokenizer = tokenizer\n        self.device = device\n        self.history = {'train_loss': [], 'val_loss': [], 'perplexity': []}\n\n    def train(self, train_dataset, val_dataset=None, \n             batch_size=32, num_epochs=10, learning_rate=1e-3,\n             warmup_steps=1000, gradient_clip=1.0):\n        \"\"\"\u4e8b\u524d\u5b66\u7fd2\u306e\u5b9f\u884c\"\"\"\n\n        # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, \n                                shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, \n                              shuffle=False) if val_dataset else None\n\n        # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3068\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate,\n                                    weight_decay=0.01)\n\n        # \u7dda\u5f62\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7 + \u30b3\u30b5\u30a4\u30f3\u6e1b\u8870\n        total_steps = len(train_loader) * num_epochs\n        scheduler = self.get_linear_schedule_with_warmup(\n            optimizer, warmup_steps, total_steps\n        )\n\n        # \u8a13\u7df4\u30eb\u30fc\u30d7\n        self.model.train()\n        global_step = 0\n\n        for epoch in range(num_epochs):\n            epoch_loss = 0\n            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n\n            for batch in progress_bar:\n                input_ids = batch['input_ids'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                # \u9806\u4f1d\u64ad\n                logits = self.model(input_ids)\n                loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    labels.view(-1),\n                    ignore_index=self.tokenizer.pad_token_id\n                )\n\n                # \u9006\u4f1d\u64ad\n                loss.backward()\n\n                # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clip)\n\n                # \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n                # \u7d71\u8a08\u60c5\u5831\n                epoch_loss += loss.item()\n                global_step += 1\n\n                # \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u66f4\u65b0\n                progress_bar.set_postfix({\n                    'loss': f'{loss.item():.4f}',\n                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',\n                    'ppl': f'{torch.exp(loss).item():.2f}'\n                })\n\n            # \u30a8\u30dd\u30c3\u30af\u7d42\u4e86\u6642\u306e\u51e6\u7406\n            avg_train_loss = epoch_loss / len(train_loader)\n            self.history['train_loss'].append(avg_train_loss)\n            self.history['perplexity'].append(np.exp(avg_train_loss))\n\n            # \u691c\u8a3c\n            if val_loader:\n                val_loss = self.evaluate(val_loader)\n                self.history['val_loss'].append(val_loss)\n                print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, \"\n                      f\"Val Loss = {val_loss:.4f}, \"\n                      f\"Val Perplexity = {np.exp(val_loss):.2f}\")\n            else:\n                print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, \"\n                      f\"Perplexity = {np.exp(avg_train_loss):.2f}\")\n\n            # \u751f\u6210\u30b5\u30f3\u30d7\u30eb\n            if (epoch + 1) % 2 == 0:\n                self.generate_samples()\n\n    def evaluate(self, dataloader):\n        \"\"\"\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\"\"\"\n        self.model.eval()\n        total_loss = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                logits = self.model(input_ids)\n                loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    labels.view(-1),\n                    ignore_index=self.tokenizer.pad_token_id\n                )\n\n                total_loss += loss.item()\n\n        self.model.train()\n        return total_loss / len(dataloader)\n\n    def generate_samples(self, num_samples=3, max_length=50, temperature=0.8):\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u30b5\u30f3\u30d7\u30eb\"\"\"\n        self.model.eval()\n\n        print(\"\\n=== Generated Samples ===\")\n\n        for i in range(num_samples):\n            # BOS \u30c8\u30fc\u30af\u30f3\u304b\u3089\u958b\u59cb\n            input_ids = torch.tensor([[self.tokenizer.bos_token_id]], \n                                    device=self.device)\n\n            generated_tokens = [self.tokenizer.bos_token_id]\n\n            for _ in range(max_length):\n                with torch.no_grad():\n                    logits = self.model(input_ids)\n                    next_token_logits = logits[0, -1, :] / temperature\n\n                    # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                    probs = F.softmax(next_token_logits, dim=-1)\n                    next_token = torch.multinomial(probs, num_samples=1)\n\n                    generated_tokens.append(next_token.item())\n\n                    # EOS \u30c8\u30fc\u30af\u30f3\u3067\u7d42\u4e86\n                    if next_token.item() == self.tokenizer.eos_token_id:\n                        break\n\n                    # \u6b21\u306e\u5165\u529b\n                    input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n\n            # \u30c7\u30b3\u30fc\u30c9\n            generated_text = self.tokenizer.decode(generated_tokens)\n            print(f\"Sample {i+1}: {generated_text}\")\n\n        print()\n        self.model.train()\n\n    @staticmethod\n    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n        \"\"\"\u7dda\u5f62\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7 + \u7dda\u5f62\u6e1b\u8870\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\"\"\"\n        def lr_lambda(current_step):\n            if current_step &lt; num_warmup_steps:\n                return float(current_step) / float(max(1, num_warmup_steps))\n            return max(0.0, float(num_training_steps - current_step) / \n                      float(max(1, num_training_steps - num_warmup_steps)))\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    def plot_training_history(self):\n        \"\"\"\u8a13\u7df4\u5c65\u6b74\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u640d\u5931\n        axes[0].plot(self.history['train_loss'], label='Train Loss')\n        if self.history['val_loss']:\n            axes[0].plot(self.history['val_loss'], label='Val Loss')\n        axes[0].set_xlabel('Epoch')\n        axes[0].set_ylabel('Loss')\n        axes[0].set_title('Training and Validation Loss')\n        axes[0].legend()\n        axes[0].grid(True, alpha=0.3)\n\n        # \u30d1\u30fc\u30d7\u30ec\u30ad\u30b7\u30c6\u30a3\n        axes[1].plot(self.history['perplexity'], label='Train Perplexity')\n        axes[1].set_xlabel('Epoch')\n        axes[1].set_ylabel('Perplexity')\n        axes[1].set_title('Model Perplexity')\n        axes[1].set_yscale('log')\n        axes[1].grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n# \u4f7f\u7528\u4f8b\ndef run_pretraining_example():\n    \"\"\"\u4e8b\u524d\u5b66\u7fd2\u306e\u5b9f\u884c\u4f8b\"\"\"\n\n    # \u30b5\u30f3\u30d7\u30eb\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\n    train_texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Machine learning is transforming the world.\",\n        \"Natural language processing enables computers to understand human language.\",\n        \"Deep learning models can learn complex patterns from data.\",\n        \"Transformers have revolutionized NLP tasks.\",\n        \"Attention is all you need for sequence modeling.\",\n        \"Pre-training helps models learn general language understanding.\",\n        \"Fine-tuning adapts models to specific tasks.\",\n    ] * 100  # \u30c7\u30fc\u30bf\u3092\u5897\u3084\u3059\n\n    val_texts = [\n        \"AI systems are becoming more sophisticated.\",\n        \"Language models can generate coherent text.\",\n    ] * 10\n\n    # \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    tokenizer = SimpleTokenizer()\n    train_dataset = TextDataset(train_texts, tokenizer, max_length=64)\n    val_dataset = TextDataset(val_texts, tokenizer, max_length=64)\n\n    # \u30e2\u30c7\u30eb\u4f5c\u6210\n    vocab_size = len(tokenizer.vocab)\n    model = GPTWithPositionalEncoding(\n        vocab_size=vocab_size,\n        d_model=128,\n        n_heads=4,\n        n_layers=2,\n        encoding_type='learned'\n    )\n\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    # \u30c8\u30ec\u30fc\u30ca\u30fc\u4f5c\u6210\u3068\u8a13\u7df4\n    trainer = PretrainingTrainer(model, tokenizer)\n\n    trainer.train(\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        batch_size=16,\n        num_epochs=10,\n        learning_rate=5e-4,\n        warmup_steps=100\n    )\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    trainer.plot_training_history()\n\n    return trainer, model\n\n# \u5b9f\u884c\ntrainer, pretrained_model = run_pretraining_example()\n</code></pre>"},{"location":"exercises/part5-exercises/#4","title":"\u554f\u984c 4","text":"<p>\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306e\u7570\u306a\u308b\u6226\u7565\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u30d5\u30eb\u3001\u6700\u7d42\u5c64\u306e\u307f\u3001LoRA\uff09\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\nclass LoRALayer(nn.Module):\n    \"\"\"Low-Rank Adaptation (LoRA) \u30ec\u30a4\u30e4\u30fc\"\"\"\n\n    def __init__(self, in_features: int, out_features: int, \n                 rank: int = 16, alpha: float = 16.0):\n        super().__init__()\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank\n\n        # LoRA \u30d1\u30e9\u30e1\u30fc\u30bf\n        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n\n        # \u5143\u306e\u91cd\u307f\u306f\u51cd\u7d50\u3055\u308c\u308b\uff08\u5916\u90e8\u3067\u7ba1\u7406\uff09\n        self.merged = False\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # LoRA \u306e\u8ffd\u52a0\u9805\u3092\u8a08\u7b97\n        lora_output = x @ self.lora_A @ self.lora_B * self.scaling\n        return lora_output\n\nclass FineTuningStrategies:\n    \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565\u306e\u5b9f\u88c5\"\"\"\n\n    @staticmethod\n    def freeze_all_but_last(model: nn.Module) -&gt; nn.Module:\n        \"\"\"\u6700\u7d42\u5c64\u4ee5\u5916\u3092\u3059\u3079\u3066\u51cd\u7d50\"\"\"\n        # \u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u51cd\u7d50\n        for param in model.parameters():\n            param.requires_grad = False\n\n        # \u6700\u7d42\u5c64\uff08\u8a00\u8a9e\u30e2\u30c7\u30eb\u30d8\u30c3\u30c9\uff09\u306e\u307f\u89e3\u51cd\n        if hasattr(model, 'lm_head'):\n            for param in model.lm_head.parameters():\n                param.requires_grad = True\n\n        return model\n\n    @staticmethod\n    def apply_lora(model: nn.Module, rank: int = 16, alpha: float = 16.0) -&gt; nn.Module:\n        \"\"\"LoRA\u3092\u9069\u7528\"\"\"\n        # \u3059\u3079\u3066\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u51cd\u7d50\n        for param in model.parameters():\n            param.requires_grad = False\n\n        # Linear\u5c64\u306bLoRA\u3092\u8ffd\u52a0\n        lora_layers = {}\n\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Linear) and 'lm_head' not in name:\n                # LoRA\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u6210\n                lora_layer = LoRALayer(\n                    module.in_features, \n                    module.out_features,\n                    rank=rank,\n                    alpha=alpha\n                )\n                lora_layers[name] = lora_layer\n\n        # LoRA\u30ec\u30a4\u30e4\u30fc\u3092\u30e2\u30c7\u30eb\u306b\u8ffd\u52a0\n        for name, lora_layer in lora_layers.items():\n            parent_name = '.'.join(name.split('.')[:-1])\n            child_name = name.split('.')[-1]\n            parent = model\n\n            if parent_name:\n                for part in parent_name.split('.'):\n                    parent = getattr(parent, part)\n\n            # \u5143\u306eLinear\u5c64\u3092\u30e9\u30c3\u30d7\n            original_layer = getattr(parent, child_name)\n\n            class LoRAWrapper(nn.Module):\n                def __init__(self, original, lora):\n                    super().__init__()\n                    self.original = original\n                    self.lora = lora\n\n                def forward(self, x):\n                    return self.original(x) + self.lora(x)\n\n            wrapped_layer = LoRAWrapper(original_layer, lora_layer)\n            setattr(parent, child_name, wrapped_layer)\n\n        return model\n\n    @staticmethod\n    def count_trainable_parameters(model: nn.Module) -&gt; Tuple[int, int]:\n        \"\"\"\u8a13\u7df4\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u30ab\u30a6\u30f3\u30c8\"\"\"\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        return trainable_params, total_params\n\nclass ClassificationDataset(Dataset):\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u5206\u985e\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\"\"\"\n\n    def __init__(self, texts: List[str], labels: List[int], \n                 tokenizer, max_length: int = 128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # \u30c8\u30fc\u30af\u30f3\u5316\n        tokens = self.tokenizer.encode(text)\n\n        # \u30d1\u30c7\u30a3\u30f3\u30b0/\u30c8\u30e9\u30f3\u30b1\u30fc\u30c8\n        if len(tokens) &gt; self.max_length:\n            tokens = tokens[:self.max_length]\n        else:\n            tokens = tokens + [self.tokenizer.pad_token_id] * (self.max_length - len(tokens))\n\n        return {\n            'input_ids': torch.tensor(tokens, dtype=torch.long),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nclass FineTuningTrainer:\n    \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7528\u30c8\u30ec\u30fc\u30ca\u30fc\"\"\"\n\n    def __init__(self, pretrained_model, num_classes: int, strategy: str = 'full'):\n        self.strategy = strategy\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # \u5206\u985e\u30d8\u30c3\u30c9\u3092\u8ffd\u52a0\n        self.model = self._prepare_model_for_classification(\n            pretrained_model, num_classes, strategy\n        )\n        self.model = self.model.to(self.device)\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u8868\u793a\n        trainable, total = FineTuningStrategies.count_trainable_parameters(self.model)\n        print(f\"Strategy: {strategy}\")\n        print(f\"Trainable parameters: {trainable:,} / {total:,} \"\n              f\"({trainable/total*100:.2f}%)\")\n\n    def _prepare_model_for_classification(self, base_model, num_classes, strategy):\n        \"\"\"\u5206\u985e\u7528\u306b\u30e2\u30c7\u30eb\u3092\u6e96\u5099\"\"\"\n\n        class ClassificationModel(nn.Module):\n            def __init__(self, base_model, num_classes):\n                super().__init__()\n                self.base_model = base_model\n                self.dropout = nn.Dropout(0.1)\n\n                # \u5206\u985e\u30d8\u30c3\u30c9\n                hidden_size = base_model.d_model\n                self.classifier = nn.Linear(hidden_size, num_classes)\n\n            def forward(self, input_ids):\n                # \u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u53d6\u5f97\n                outputs = self.base_model(input_ids)  # [batch, seq_len, hidden]\n\n                # \u6700\u521d\u306e\u30c8\u30fc\u30af\u30f3\uff08[CLS]\u30c8\u30fc\u30af\u30f3\u306e\u4ee3\u308f\u308a\uff09\u3092\u4f7f\u7528\n                pooled_output = outputs[:, 0, :]\n                pooled_output = self.dropout(pooled_output)\n\n                # \u5206\u985e\n                logits = self.classifier(pooled_output)\n                return logits\n\n        model = ClassificationModel(deepcopy(base_model), num_classes)\n\n        # \u6226\u7565\u306b\u5fdc\u3058\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8abf\u6574\n        if strategy == 'last_layer':\n            model = FineTuningStrategies.freeze_all_but_last(model)\n            # \u5206\u985e\u30d8\u30c3\u30c9\u3082\u8a13\u7df4\u53ef\u80fd\u306b\u3059\u308b\n            for param in model.classifier.parameters():\n                param.requires_grad = True\n\n        elif strategy == 'lora':\n            model.base_model = FineTuningStrategies.apply_lora(\n                model.base_model, rank=8, alpha=16\n            )\n            # \u5206\u985e\u30d8\u30c3\u30c9\u306f\u8a13\u7df4\u53ef\u80fd\n            for param in model.classifier.parameters():\n                param.requires_grad = True\n\n        # strategy == 'full' \u306e\u5834\u5408\u306f\u3059\u3079\u3066\u8a13\u7df4\u53ef\u80fd\n\n        return model\n\n    def train(self, train_dataset, val_dataset, num_epochs=5, \n             batch_size=16, learning_rate=2e-5):\n        \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u884c\"\"\"\n\n        # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n        # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\uff08\u8a13\u7df4\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u307f\uff09\n        optimizer = torch.optim.AdamW(\n            [p for p in self.model.parameters() if p.requires_grad],\n            lr=learning_rate,\n            weight_decay=0.01\n        )\n\n        # \u640d\u5931\u95a2\u6570\n        criterion = nn.CrossEntropyLoss()\n\n        # \u8a13\u7df4\u5c65\u6b74\n        history = {'train_loss': [], 'train_acc': [], \n                  'val_loss': [], 'val_acc': []}\n\n        # \u8a13\u7df4\u30eb\u30fc\u30d7\n        for epoch in range(num_epochs):\n            # \u8a13\u7df4\n            self.model.train()\n            train_loss = 0\n            train_correct = 0\n            train_total = 0\n\n            for batch in train_loader:\n                input_ids = batch['input_ids'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                # \u9806\u4f1d\u64ad\n                logits = self.model(input_ids)\n                loss = criterion(logits, labels)\n\n                # \u9006\u4f1d\u64ad\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    [p for p in self.model.parameters() if p.requires_grad], \n                    max_norm=1.0\n                )\n                optimizer.step()\n\n                # \u7d71\u8a08\n                train_loss += loss.item()\n                _, predicted = torch.max(logits, 1)\n                train_correct += (predicted == labels).sum().item()\n                train_total += labels.size(0)\n\n            # \u691c\u8a3c\n            val_loss, val_acc = self.evaluate(val_loader, criterion)\n\n            # \u8a18\u9332\n            train_acc = train_correct / train_total\n            avg_train_loss = train_loss / len(train_loader)\n\n            history['train_loss'].append(avg_train_loss)\n            history['train_acc'].append(train_acc)\n            history['val_loss'].append(val_loss)\n            history['val_acc'].append(val_acc)\n\n            print(f\"Epoch {epoch+1}/{num_epochs}:\")\n            print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        return history\n\n    def evaluate(self, dataloader, criterion):\n        \"\"\"\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\"\"\"\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(self.device)\n                labels = batch['labels'].to(self.device)\n\n                logits = self.model(input_ids)\n                loss = criterion(logits, labels)\n\n                total_loss += loss.item()\n                _, predicted = torch.max(logits, 1)\n                correct += (predicted == labels).sum().item()\n                total += labels.size(0)\n\n        avg_loss = total_loss / len(dataloader)\n        accuracy = correct / total\n\n        return avg_loss, accuracy\n\ndef compare_finetuning_strategies():\n    \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565\u306e\u6bd4\u8f03\"\"\"\n\n    # \u30c0\u30df\u30fc\u306e\u5206\u985e\u30c7\u30fc\u30bf\n    positive_texts = [\n        \"This movie is fantastic! I loved every minute of it.\",\n        \"Amazing performance by the actors. Highly recommended!\",\n        \"Best film I've seen this year. Absolutely brilliant!\",\n    ] * 50\n\n    negative_texts = [\n        \"Terrible movie. Complete waste of time.\",\n        \"Boring plot and bad acting. Very disappointed.\",\n        \"One of the worst films I've ever seen.\",\n    ] * 50\n\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\n    texts = positive_texts + negative_texts\n    labels = [1] * len(positive_texts) + [0] * len(negative_texts)\n\n    # \u30b7\u30e3\u30c3\u30d5\u30eb\n    import random\n    data = list(zip(texts, labels))\n    random.shuffle(data)\n    texts, labels = zip(*data)\n\n    # \u8a13\u7df4/\u691c\u8a3c\u5206\u5272\n    split_idx = int(0.8 * len(texts))\n    train_texts, val_texts = texts[:split_idx], texts[split_idx:]\n    train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n\n    # \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\n    tokenizer = SimpleTokenizer()\n    train_dataset = ClassificationDataset(train_texts, train_labels, tokenizer)\n    val_dataset = ClassificationDataset(val_texts, val_labels, tokenizer)\n\n    # \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\uff08\u524d\u306e\u554f\u984c\u3067\u4f5c\u6210\u3057\u305f\u3082\u306e\uff09\n    pretrained_model = GPTWithPositionalEncoding(\n        vocab_size=len(tokenizer.vocab),\n        d_model=128,\n        n_heads=4,\n        n_layers=2\n    )\n\n    # \u5404\u6226\u7565\u3067\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\n    strategies = ['full', 'last_layer', 'lora']\n    results = {}\n\n    for strategy in strategies:\n        print(f\"\\n=== {strategy.upper()} Fine-tuning ===\")\n\n        trainer = FineTuningTrainer(pretrained_model, num_classes=2, strategy=strategy)\n        history = trainer.train(\n            train_dataset, val_dataset,\n            num_epochs=5,\n            batch_size=16,\n            learning_rate=2e-5 if strategy == 'full' else 5e-4\n        )\n\n        results[strategy] = history\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    for strategy, history in results.items():\n        ax1.plot(history['val_loss'], label=f'{strategy} - Val Loss')\n        ax2.plot(history['val_acc'], label=f'{strategy} - Val Acc')\n\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Validation Loss Comparison')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Validation Accuracy Comparison')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    return results\n\n# \u6bd4\u8f03\u5b9f\u884c\nresults = compare_finetuning_strategies()\n</code></pre>"},{"location":"exercises/part5-exercises/#53","title":"\u6f14\u7fd2 5.3: \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u8a73\u7d30","text":""},{"location":"exercises/part5-exercises/#5_1","title":"\u554f\u984c 5","text":"<p>BPE\uff08Byte Pair Encoding\uff09\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u5b8c\u5168\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import re\nfrom collections import defaultdict, Counter\nfrom typing import Dict, List, Tuple, Optional, Set\nimport json\nimport numpy as np\n\nclass BPETokenizer:\n    \"\"\"Byte Pair Encoding \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b8c\u5168\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size: int = 1000, min_frequency: int = 2):\n        self.vocab_size = vocab_size\n        self.min_frequency = min_frequency\n\n        # \u7279\u6b8a\u30c8\u30fc\u30af\u30f3\n        self.pad_token = '&lt;pad&gt;'\n        self.unk_token = '&lt;unk&gt;'\n        self.bos_token = '&lt;bos&gt;'\n        self.eos_token = '&lt;eos&gt;'\n\n        # \u8a9e\u5f59\n        self.word_tokenizer = re.compile(r'\\w+|[^\\w\\s]')\n        self.vocab = {}\n        self.merges = []\n        self.word_freq = defaultdict(int)\n\n    def train(self, texts: List[str]):\n        \"\"\"BPE\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\"\"\"\n        print(\"Training BPE tokenizer...\")\n\n        # 1. \u5358\u8a9e\u983b\u5ea6\u3092\u30ab\u30a6\u30f3\u30c8\n        for text in texts:\n            words = self.word_tokenizer.findall(text.lower())\n            for word in words:\n                self.word_freq[word] += 1\n\n        # 2. \u5358\u8a9e\u3092\u6587\u5b57\u306b\u5206\u5272\uff08\u7d42\u7aef\u8a18\u53f7\u4ed8\u304d\uff09\n        word_splits = {}\n        for word, freq in self.word_freq.items():\n            if freq &gt;= self.min_frequency:\n                word_splits[word] = list(word) + ['&lt;/w&gt;']\n\n        # 3. \u521d\u671f\u8a9e\u5f59\u3092\u4f5c\u6210\uff08\u500b\u5225\u6587\u5b57\uff09\n        self.vocab = {\n            self.pad_token: 0,\n            self.unk_token: 1,\n            self.bos_token: 2,\n            self.eos_token: 3\n        }\n\n        char_freq = defaultdict(int)\n        for word, freq in self.word_freq.items():\n            if freq &gt;= self.min_frequency:\n                for char in word:\n                    char_freq[char] += freq\n                char_freq['&lt;/w&gt;'] += freq\n\n        # \u6587\u5b57\u3092\u8a9e\u5f59\u306b\u8ffd\u52a0\n        for char, freq in sorted(char_freq.items(), key=lambda x: -x[1]):\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n        # 4. BPE\u30de\u30fc\u30b8\u3092\u5b66\u7fd2\n        while len(self.vocab) &lt; self.vocab_size:\n            # \u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\n            pair_freq = self._get_pair_frequencies(word_splits)\n\n            if not pair_freq:\n                break\n\n            # \u6700\u983b\u51fa\u30da\u30a2\u3092\u9078\u629e\n            best_pair = max(pair_freq, key=pair_freq.get)\n            freq = pair_freq[best_pair]\n\n            if freq &lt; self.min_frequency:\n                break\n\n            # \u30de\u30fc\u30b8\u3092\u5b9f\u884c\n            self.merges.append(best_pair)\n            new_token = ''.join(best_pair)\n            self.vocab[new_token] = len(self.vocab)\n\n            # \u5358\u8a9e\u5206\u5272\u3092\u66f4\u65b0\n            word_splits = self._merge_pair(word_splits, best_pair)\n\n            if len(self.vocab) % 100 == 0:\n                print(f\"Vocabulary size: {len(self.vocab)}\")\n\n        print(f\"Training complete. Final vocabulary size: {len(self.vocab)}\")\n\n        # \u9006\u5f15\u304d\u8f9e\u66f8\u3092\u4f5c\u6210\n        self.id_to_token = {v: k for k, v in self.vocab.items()}\n\n    def _get_pair_frequencies(self, word_splits: Dict[str, List[str]]) -&gt; Dict[Tuple[str, str], int]:\n        \"\"\"\u96a3\u63a5\u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\"\"\"\n        pair_freq = defaultdict(int)\n\n        for word, split in word_splits.items():\n            word_freq = self.word_freq[word]\n\n            for i in range(len(split) - 1):\n                pair = (split[i], split[i + 1])\n                pair_freq[pair] += word_freq\n\n        return pair_freq\n\n    def _merge_pair(self, word_splits: Dict[str, List[str]], \n                   pair: Tuple[str, str]) -&gt; Dict[str, List[str]]:\n        \"\"\"\u6307\u5b9a\u3055\u308c\u305f\u30da\u30a2\u3092\u30de\u30fc\u30b8\"\"\"\n        new_word_splits = {}\n        merged = ''.join(pair)\n\n        for word, split in word_splits.items():\n            new_split = []\n            i = 0\n\n            while i &lt; len(split):\n                if i &lt; len(split) - 1 and (split[i], split[i + 1]) == pair:\n                    new_split.append(merged)\n                    i += 2\n                else:\n                    new_split.append(split[i])\n                    i += 1\n\n            new_word_splits[word] = new_split\n\n        return new_word_splits\n\n    def _tokenize_word(self, word: str) -&gt; List[str]:\n        \"\"\"\u5358\u8a9e\u3092BPE\u30c8\u30fc\u30af\u30f3\u306b\u5206\u5272\"\"\"\n        word = word.lower()\n        splits = list(word) + ['&lt;/w&gt;']\n\n        # \u5b66\u7fd2\u3057\u305f\u30de\u30fc\u30b8\u3092\u9806\u756a\u306b\u9069\u7528\n        for pair in self.merges:\n            new_splits = []\n            i = 0\n\n            while i &lt; len(splits):\n                if i &lt; len(splits) - 1 and (splits[i], splits[i + 1]) == pair:\n                    new_splits.append(''.join(pair))\n                    i += 2\n                else:\n                    new_splits.append(splits[i])\n                    i += 1\n\n            splits = new_splits\n\n        return splits\n\n    def encode(self, text: str) -&gt; List[int]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\"\"\"\n        tokens = [self.bos_token]\n\n        words = self.word_tokenizer.findall(text.lower())\n        for word in words:\n            word_tokens = self._tokenize_word(word)\n            for token in word_tokens:\n                if token in self.vocab:\n                    tokens.append(token)\n                else:\n                    tokens.append(self.unk_token)\n\n        tokens.append(self.eos_token)\n\n        # \u30c8\u30fc\u30af\u30f3\u3092ID\u306b\u5909\u63db\n        return [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n\n    def decode(self, ids: List[int]) -&gt; str:\n        \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u30c6\u30ad\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = []\n\n        for id in ids:\n            if id in self.id_to_token:\n                token = self.id_to_token[id]\n                if token not in [self.pad_token, self.unk_token, \n                               self.bos_token, self.eos_token]:\n                    tokens.append(token)\n\n        # \u30c8\u30fc\u30af\u30f3\u3092\u7d50\u5408\n        text = ''.join(tokens)\n        text = text.replace('&lt;/w&gt;', ' ')\n        return text.strip()\n\n    def save(self, path: str):\n        \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u4fdd\u5b58\"\"\"\n        data = {\n            'vocab': self.vocab,\n            'merges': self.merges,\n            'vocab_size': self.vocab_size,\n            'min_frequency': self.min_frequency\n        }\n\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n\n    def load(self, path: str):\n        \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u8aad\u307f\u8fbc\u307f\"\"\"\n        with open(path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        self.vocab = data['vocab']\n        self.merges = [tuple(pair) for pair in data['merges']]\n        self.vocab_size = data['vocab_size']\n        self.min_frequency = data['min_frequency']\n        self.id_to_token = {v: k for k, v in self.vocab.items()}\n\n    def analyze_vocabulary(self):\n        \"\"\"\u8a9e\u5f59\u306e\u5206\u6790\"\"\"\n        print(f\"=== Vocabulary Analysis ===\")\n        print(f\"Total vocabulary size: {len(self.vocab)}\")\n        print(f\"Number of merges: {len(self.merges)}\")\n\n        # \u30c8\u30fc\u30af\u30f3\u9577\u306e\u5206\u5e03\n        token_lengths = [len(token.replace('&lt;/w&gt;', '')) for token in self.vocab.keys()\n                       if token not in [self.pad_token, self.unk_token, \n                                      self.bos_token, self.eos_token]]\n\n        print(f\"\\nToken length distribution:\")\n        length_counts = Counter(token_lengths)\n        for length in sorted(length_counts.keys()):\n            print(f\"  Length {length}: {length_counts[length]} tokens\")\n\n        # \u6700\u3082\u9577\u3044\u30c8\u30fc\u30af\u30f3\n        longest_tokens = sorted(\n            [(token, len(token.replace('&lt;/w&gt;', ''))) for token in self.vocab.keys()\n             if token not in [self.pad_token, self.unk_token, \n                            self.bos_token, self.eos_token]],\n            key=lambda x: -x[1]\n        )[:10]\n\n        print(f\"\\nLongest tokens:\")\n        for token, length in longest_tokens:\n            print(f\"  '{token}' (length: {length})\")\n\n        # \u6700\u521d\u306e\u30de\u30fc\u30b8\n        print(f\"\\nFirst 10 merges:\")\n        for i, (a, b) in enumerate(self.merges[:10]):\n            print(f\"  {i+1}. '{a}' + '{b}' -&gt; '{a}{b}'\")\n\n# \u30c6\u30b9\u30c8\u3068\u53ef\u8996\u5316\ndef test_bpe_tokenizer():\n    \"\"\"BPE\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u30c6\u30b9\u30c8\"\"\"\n\n    # \u8a13\u7df4\u30c7\u30fc\u30bf\n    texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Machine learning is transforming artificial intelligence.\",\n        \"Natural language processing enables computers to understand text.\",\n        \"Deep learning models can learn complex patterns.\",\n        \"Transformers have revolutionized NLP tasks.\",\n        \"Attention mechanism is the key innovation.\",\n        \"Pre-training on large corpora improves performance.\",\n        \"Fine-tuning adapts models to specific tasks.\",\n        \"Tokenization is an important preprocessing step.\",\n        \"Byte pair encoding is a subword tokenization method.\",\n    ] * 10  # \u30c7\u30fc\u30bf\u3092\u5897\u3084\u3059\n\n    # BPE\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u8a13\u7df4\n    tokenizer = BPETokenizer(vocab_size=500, min_frequency=2)\n    tokenizer.train(texts)\n\n    # \u8a9e\u5f59\u306e\u5206\u6790\n    tokenizer.analyze_vocabulary()\n\n    # \u30a8\u30f3\u30b3\u30fc\u30c9/\u30c7\u30b3\u30fc\u30c9\u306e\u30c6\u30b9\u30c8\n    print(\"\\n=== Encoding/Decoding Test ===\")\n    test_texts = [\n        \"Machine learning is amazing.\",\n        \"Transformers revolutionized NLP.\",\n        \"Unknown words like cryptocurrency.\",\n    ]\n\n    for text in test_texts:\n        print(f\"\\nOriginal: {text}\")\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c9\n        ids = tokenizer.encode(text)\n        tokens = [tokenizer.id_to_token.get(id, '?') for id in ids]\n        print(f\"Tokens: {tokens}\")\n        print(f\"IDs: {ids}\")\n\n        # \u30c7\u30b3\u30fc\u30c9\n        decoded = tokenizer.decode(ids)\n        print(f\"Decoded: {decoded}\")\n\n    # \u5727\u7e2e\u7387\u306e\u8a08\u7b97\n    print(\"\\n=== Compression Analysis ===\")\n    total_chars = 0\n    total_tokens = 0\n\n    for text in texts:\n        chars = len(text)\n        tokens = len(tokenizer.encode(text))\n        total_chars += chars\n        total_tokens += tokens\n\n    compression_ratio = total_chars / total_tokens\n    print(f\"Average compression ratio: {compression_ratio:.2f} chars/token\")\n\n    return tokenizer\n\n# \u5b9f\u884c\nbpe_tokenizer = test_bpe_tokenizer()\n</code></pre>"},{"location":"exercises/part5-exercises/#6","title":"\u554f\u984c 6","text":"<p>\u7570\u306a\u308b\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\uff08BPE\u3001WordPiece\u3001SentencePiece\uff09\u306e\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import time\nfrom typing import Dict, List, Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass WordPieceTokenizer:\n    \"\"\"WordPiece \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u7c21\u6613\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size: int = 1000, min_frequency: int = 2):\n        self.vocab_size = vocab_size\n        self.min_frequency = min_frequency\n        self.vocab = {}\n        self.unk_token = '[UNK]'\n        self.pad_token = '[PAD]'\n        self.cls_token = '[CLS]'\n        self.sep_token = '[SEP]'\n        self.prefix = '##'  # \u30b5\u30d6\u30ef\u30fc\u30c9\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\n\n    def train(self, texts: List[str]):\n        \"\"\"WordPiece\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\"\"\"\n        # \u7c21\u7565\u5316\u306e\u305f\u3081\u3001\u4e8b\u524d\u5b9a\u7fa9\u3055\u308c\u305f\u8a9e\u5f59\u3092\u4f7f\u7528\n        # \u5b9f\u969b\u306eWordPiece\u306f\u3088\u308a\u8907\u96d1\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\n\n        # \u7279\u6b8a\u30c8\u30fc\u30af\u30f3\n        self.vocab = {\n            self.pad_token: 0,\n            self.unk_token: 1,\n            self.cls_token: 2,\n            self.sep_token: 3,\n        }\n\n        # \u6587\u5b57\u3068\u4e00\u822c\u7684\u306a\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u8ffd\u52a0\n        chars = set()\n        words = []\n\n        for text in texts:\n            for word in text.lower().split():\n                words.append(word)\n                chars.update(word)\n\n        # \u500b\u5225\u6587\u5b57\u3092\u8ffd\u52a0\n        for char in sorted(chars):\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n        # \u983b\u51fa\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u8ffd\u52a0\uff08\u7c21\u7565\u5316\uff09\n        from collections import Counter\n        word_freq = Counter(words)\n\n        # \u5358\u8a9e\u306e\u90e8\u5206\u6587\u5b57\u5217\u3092\u5019\u88dc\u3068\u3057\u3066\u751f\u6210\n        subword_freq = Counter()\n        for word, freq in word_freq.items():\n            if freq &gt;= self.min_frequency:\n                # \u5358\u8a9e\u5168\u4f53\n                if len(word) &lt;= 5 and word not in self.vocab:\n                    self.vocab[word] = len(self.vocab)\n\n                # \u30b5\u30d6\u30ef\u30fc\u30c9\n                for i in range(1, len(word)):\n                    for j in range(i + 1, min(i + 6, len(word) + 1)):\n                        subword = self.prefix + word[i:j]\n                        subword_freq[subword] += freq\n\n        # \u983b\u51fa\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u8a9e\u5f59\u306b\u8ffd\u52a0\n        for subword, freq in subword_freq.most_common():\n            if len(self.vocab) &gt;= self.vocab_size:\n                break\n            if freq &gt;= self.min_frequency and subword not in self.vocab:\n                self.vocab[subword] = len(self.vocab)\n\n        self.id_to_token = {v: k for k, v in self.vocab.items()}\n\n    def tokenize_word(self, word: str) -&gt; List[str]:\n        \"\"\"\u5358\u8a9e\u3092WordPiece\u30c8\u30fc\u30af\u30f3\u306b\u5206\u5272\"\"\"\n        tokens = []\n        start = 0\n\n        while start &lt; len(word):\n            end = len(word)\n            cur_substr = None\n\n            while start &lt; end:\n                substr = word[start:end]\n                if start &gt; 0:\n                    substr = self.prefix + substr\n\n                if substr in self.vocab:\n                    cur_substr = substr\n                    break\n\n                end -= 1\n\n            if cur_substr is None:\n                tokens.append(self.unk_token)\n                start += 1\n            else:\n                tokens.append(cur_substr)\n                start = end\n\n        return tokens\n\n    def encode(self, text: str) -&gt; List[int]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\"\"\"\n        tokens = [self.cls_token]\n\n        for word in text.lower().split():\n            word_tokens = self.tokenize_word(word)\n            tokens.extend(word_tokens)\n\n        tokens.append(self.sep_token)\n\n        return [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n\n    def decode(self, ids: List[int]) -&gt; str:\n        \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u30c6\u30ad\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = []\n\n        for id in ids:\n            if id in self.id_to_token:\n                token = self.id_to_token[id]\n                if token not in [self.pad_token, self.unk_token, \n                               self.cls_token, self.sep_token]:\n                    if token.startswith(self.prefix):\n                        tokens.append(token[len(self.prefix):])\n                    else:\n                        if tokens:\n                            tokens.append(' ')\n                        tokens.append(token)\n\n        return ''.join(tokens).strip()\n\nclass SentencePieceTokenizer:\n    \"\"\"SentencePiece \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u7c21\u6613\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size: int = 1000, character_coverage: float = 0.9995):\n        self.vocab_size = vocab_size\n        self.character_coverage = character_coverage\n        self.vocab = {}\n        self.unk_token = '&lt;unk&gt;'\n        self.pad_token = '&lt;pad&gt;'\n        self.bos_token = '&lt;s&gt;'\n        self.eos_token = '&lt;/s&gt;'\n        self.space_symbol = '\u2581'  # \u30b9\u30da\u30fc\u30b9\u3092\u8868\u3059\u8a18\u53f7\n\n    def train(self, texts: List[str]):\n        \"\"\"SentencePiece\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\uff08\u7c21\u7565\u5316\u7248\uff09\"\"\"\n        # \u5b9f\u969b\u306eSentencePiece\u306f unigram language model \u3092\u4f7f\u7528\n\n        self.vocab = {\n            self.pad_token: 0,\n            self.unk_token: 1,\n            self.bos_token: 2,\n            self.eos_token: 3,\n        }\n\n        # \u6587\u5b57\u983b\u5ea6\u3092\u8a08\u7b97\n        char_freq = Counter()\n        for text in texts:\n            # \u30b9\u30da\u30fc\u30b9\u3092\u7279\u6b8a\u8a18\u53f7\u306b\u7f6e\u63db\n            normalized_text = self.space_symbol + text.replace(' ', self.space_symbol)\n            char_freq.update(normalized_text)\n\n        # \u6587\u5b57\u30ab\u30d0\u30ec\u30c3\u30b8\u306b\u57fa\u3065\u3044\u3066\u6587\u5b57\u3092\u9078\u629e\n        total_chars = sum(char_freq.values())\n        covered_chars = 0\n\n        for char, freq in char_freq.most_common():\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n                covered_chars += freq\n\n                if covered_chars / total_chars &gt;= self.character_coverage:\n                    break\n\n        # \u30b5\u30d6\u30ef\u30fc\u30c9\u5019\u88dc\u3092\u751f\u6210\uff08\u7c21\u7565\u5316\uff09\n        subword_scores = {}\n\n        for text in texts[:100]:  # \u8a08\u7b97\u91cf\u524a\u6e1b\u306e\u305f\u3081\u4e00\u90e8\u306e\u307f\u4f7f\u7528\n            normalized = self.space_symbol + text.replace(' ', self.space_symbol)\n\n            # \u5168\u3066\u306e\u30b5\u30d6\u30b9\u30c8\u30ea\u30f3\u30b0\u3092\u5019\u88dc\u3068\u3057\u3066\u751f\u6210\n            for i in range(len(normalized)):\n                for j in range(i + 1, min(i + 10, len(normalized) + 1)):\n                    subword = normalized[i:j]\n                    if len(subword) &gt; 1:\n                        # \u30b9\u30b3\u30a2\u8a08\u7b97\uff08\u7c21\u7565\u5316\uff1a\u983b\u5ea6\u306e\u307f\uff09\n                        subword_scores[subword] = subword_scores.get(subword, 0) + 1\n\n        # \u30b9\u30b3\u30a2\u306e\u9ad8\u3044\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u8a9e\u5f59\u306b\u8ffd\u52a0\n        for subword, score in sorted(subword_scores.items(), \n                                   key=lambda x: -x[1]):\n            if len(self.vocab) &gt;= self.vocab_size:\n                break\n\n            if subword not in self.vocab and score &gt; 1:\n                self.vocab[subword] = len(self.vocab)\n\n        self.id_to_token = {v: k for k, v in self.vocab.items()}\n\n    def encode(self, text: str) -&gt; List[int]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\uff08\u7c21\u7565\u5316\u7248\uff09\"\"\"\n        # \u5b9f\u969b\u306eSentencePiece\u306fViterbi\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\n\n        normalized = self.space_symbol + text.replace(' ', self.space_symbol)\n        tokens = [self.bos_token]\n\n        i = 0\n        while i &lt; len(normalized):\n            # \u6700\u9577\u4e00\u81f4\u3067\u8caa\u6b32\u306b\u5206\u5272\n            match_found = False\n\n            for length in range(min(10, len(normalized) - i), 0, -1):\n                subword = normalized[i:i + length]\n\n                if subword in self.vocab:\n                    tokens.append(subword)\n                    i += length\n                    match_found = True\n                    break\n\n            if not match_found:\n                tokens.append(self.unk_token)\n                i += 1\n\n        tokens.append(self.eos_token)\n\n        return [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n\n    def decode(self, ids: List[int]) -&gt; str:\n        \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u30c6\u30ad\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = []\n\n        for id in ids:\n            if id in self.id_to_token:\n                token = self.id_to_token[id]\n                if token not in [self.pad_token, self.unk_token, \n                               self.bos_token, self.eos_token]:\n                    tokens.append(token)\n\n        text = ''.join(tokens)\n        return text.replace(self.space_symbol, ' ').strip()\n\ndef compare_tokenizers():\n    \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6bd4\u8f03\"\"\"\n\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n    train_texts = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Machine learning transforms artificial intelligence.\",\n        \"Natural language processing is fascinating.\",\n        \"Deep neural networks learn complex patterns.\",\n        \"Tokenization is crucial for text processing.\",\n    ] * 20\n\n    test_texts = [\n        \"Machine learning is amazing!\",\n        \"Tokenizers split text into tokens.\",\n        \"Unknown words like cryptocurrency appear.\",\n        \"The quick brown fox runs fast.\",\n        \"Deep learning revolutionizes AI.\",\n    ]\n\n    # \u5404\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u8a13\u7df4\n    tokenizers = {\n        'BPE': BPETokenizer(vocab_size=500),\n        'WordPiece': WordPieceTokenizer(vocab_size=500),\n        'SentencePiece': SentencePieceTokenizer(vocab_size=500),\n    }\n\n    print(\"Training tokenizers...\")\n    for name, tokenizer in tokenizers.items():\n        print(f\"\\nTraining {name}...\")\n        start_time = time.time()\n        tokenizer.train(train_texts)\n        train_time = time.time() - start_time\n        print(f\"{name} training time: {train_time:.2f}s\")\n\n    # \u6bd4\u8f03\u30e1\u30c8\u30ea\u30af\u30b9\n    results = {name: {\n        'compression_ratio': [],\n        'unk_rate': [],\n        'encode_time': [],\n        'decode_time': [],\n        'vocab_size': len(tokenizer.vocab)\n    } for name, tokenizer in tokenizers.items()}\n\n    print(\"\\n=== Tokenization Comparison ===\")\n\n    for text in test_texts:\n        print(f\"\\nText: {text}\")\n\n        for name, tokenizer in tokenizers.items():\n            # \u30a8\u30f3\u30b3\u30fc\u30c9\n            start_time = time.time()\n            ids = tokenizer.encode(text)\n            encode_time = time.time() - start_time\n\n            # \u30c7\u30b3\u30fc\u30c9\n            start_time = time.time()\n            decoded = tokenizer.decode(ids)\n            decode_time = time.time() - start_time\n\n            # \u30e1\u30c8\u30ea\u30af\u30b9\u8a08\u7b97\n            compression_ratio = len(text) / len(ids)\n\n            if name == 'BPE':\n                unk_count = sum(1 for id in ids if id == tokenizer.vocab[tokenizer.unk_token])\n            elif name == 'WordPiece':\n                unk_count = sum(1 for id in ids if id == tokenizer.vocab[tokenizer.unk_token])\n            else:  # SentencePiece\n                unk_count = sum(1 for id in ids if id == tokenizer.vocab[tokenizer.unk_token])\n\n            unk_rate = unk_count / len(ids) if len(ids) &gt; 0 else 0\n\n            # \u7d50\u679c\u4fdd\u5b58\n            results[name]['compression_ratio'].append(compression_ratio)\n            results[name]['unk_rate'].append(unk_rate)\n            results[name]['encode_time'].append(encode_time)\n            results[name]['decode_time'].append(decode_time)\n\n            # \u30c8\u30fc\u30af\u30f3\u8868\u793a\n            if name == 'BPE':\n                tokens = [tokenizer.id_to_token.get(id, '?') for id in ids]\n            elif name == 'WordPiece':\n                tokens = [tokenizer.id_to_token.get(id, '?') for id in ids]\n            else:  # SentencePiece\n                tokens = [tokenizer.id_to_token.get(id, '?') for id in ids]\n\n            print(f\"\\n{name}:\")\n            print(f\"  Tokens: {tokens}\")\n            print(f\"  Compression: {compression_ratio:.2f}\")\n            print(f\"  UNK rate: {unk_rate:.2%}\")\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. \u5727\u7e2e\u7387\n    ax = axes[0, 0]\n    for name in tokenizers.keys():\n        avg_compression = np.mean(results[name]['compression_ratio'])\n        ax.bar(name, avg_compression, alpha=0.7)\n    ax.set_title('Average Compression Ratio')\n    ax.set_ylabel('Characters per Token')\n    ax.grid(True, alpha=0.3)\n\n    # 2. UNK\u30c8\u30fc\u30af\u30f3\u7387\n    ax = axes[0, 1]\n    for name in tokenizers.keys():\n        avg_unk = np.mean(results[name]['unk_rate']) * 100\n        ax.bar(name, avg_unk, alpha=0.7)\n    ax.set_title('Average UNK Token Rate')\n    ax.set_ylabel('UNK Rate (%)')\n    ax.grid(True, alpha=0.3)\n\n    # 3. \u30a8\u30f3\u30b3\u30fc\u30c9\u6642\u9593\n    ax = axes[1, 0]\n    for name in tokenizers.keys():\n        avg_encode = np.mean(results[name]['encode_time']) * 1000\n        ax.bar(name, avg_encode, alpha=0.7)\n    ax.set_title('Average Encoding Time')\n    ax.set_ylabel('Time (ms)')\n    ax.grid(True, alpha=0.3)\n\n    # 4. \u8a9e\u5f59\u30b5\u30a4\u30ba\n    ax = axes[1, 1]\n    for name in tokenizers.keys():\n        ax.bar(name, results[name]['vocab_size'], alpha=0.7)\n    ax.set_title('Vocabulary Size')\n    ax.set_ylabel('Number of Tokens')\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u30b5\u30de\u30ea\u30fc\u8868\u793a\n    print(\"\\n=== Summary ===\")\n    print(f\"{'Tokenizer':&lt;15} {'Compression':&lt;12} {'UNK Rate':&lt;10} {'Encode(ms)':&lt;12} {'Vocab Size':&lt;10}\")\n    print(\"-\" * 65)\n\n    for name in tokenizers.keys():\n        avg_comp = np.mean(results[name]['compression_ratio'])\n        avg_unk = np.mean(results[name]['unk_rate']) * 100\n        avg_encode = np.mean(results[name]['encode_time']) * 1000\n        vocab_size = results[name]['vocab_size']\n\n        print(f\"{name:&lt;15} {avg_comp:&lt;12.2f} {avg_unk:&lt;10.1f} {avg_encode:&lt;12.3f} {vocab_size:&lt;10}\")\n\n    return tokenizers, results\n\n# \u6bd4\u8f03\u5b9f\u884c\ntokenizers, comparison_results = compare_tokenizers()\n</code></pre>"},{"location":"exercises/part5-exercises/#54","title":"\u6f14\u7fd2 5.4: \u63a8\u8ad6\u6642\u306e\u5de5\u592b","text":""},{"location":"exercises/part5-exercises/#7","title":"\u554f\u984c 7","text":"<p>\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u3068\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u624b\u6cd5\uff08top-k, top-p\uff09\u3092\u5b9f\u88c5\u3057\u3001\u751f\u6210\u54c1\u8cea\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> \u89e3\u7b54 <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import List, Tuple, Optional, Dict\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\n\n@dataclass\nclass GenerationConfig:\n    \"\"\"\u751f\u6210\u8a2d\u5b9a\"\"\"\n    max_length: int = 50\n    temperature: float = 1.0\n    top_k: int = 50\n    top_p: float = 0.9\n    repetition_penalty: float = 1.0\n    num_beams: int = 4\n    do_sample: bool = True\n\nclass TextGenerator:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u30af\u30e9\u30b9\"\"\"\n\n    def __init__(self, model, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.tokenizer = tokenizer\n        self.device = device\n        self.model.eval()\n\n    def generate(self, prompt: str, config: GenerationConfig, \n                method: str = 'sampling') -&gt; Dict[str, any]:\n        \"\"\"\u6307\u5b9a\u3055\u308c\u305f\u624b\u6cd5\u3067\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\"\"\"\n\n        # \u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\n        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n\n        if method == 'greedy':\n            output_ids, scores = self._greedy_search(input_ids, config)\n        elif method == 'beam_search':\n            output_ids, scores = self._beam_search(input_ids, config)\n        elif method == 'sampling':\n            output_ids, scores = self._sampling(input_ids, config)\n        elif method == 'top_k':\n            output_ids, scores = self._top_k_sampling(input_ids, config)\n        elif method == 'top_p':\n            output_ids, scores = self._top_p_sampling(input_ids, config)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        # \u30c7\u30b3\u30fc\u30c9\n        generated_text = self.tokenizer.decode(output_ids[0].tolist())\n\n        return {\n            'text': generated_text,\n            'ids': output_ids,\n            'scores': scores,\n            'method': method\n        }\n\n    def _greedy_search(self, input_ids: torch.Tensor, \n                      config: GenerationConfig) -&gt; Tuple[torch.Tensor, List[float]]:\n        \"\"\"\u8caa\u6b32\u6cd5\u306b\u3088\u308b\u751f\u6210\"\"\"\n        generated = input_ids.clone()\n        scores = []\n\n        for _ in range(config.max_length - input_ids.shape[1]):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                next_token_logits = outputs[:, -1, :]\n\n                # Repetition penalty\n                if config.repetition_penalty != 1.0:\n                    self._apply_repetition_penalty(\n                        next_token_logits, generated, config.repetition_penalty\n                    )\n\n                # \u6700\u3082\u78ba\u7387\u306e\u9ad8\u3044\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\n                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n                score = F.softmax(next_token_logits, dim=-1).max().item()\n                scores.append(score)\n\n                generated = torch.cat([generated, next_token], dim=1)\n\n                # EOS\u30c8\u30fc\u30af\u30f3\u3067\u7d42\u4e86\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        return generated, scores\n\n    def _beam_search(self, input_ids: torch.Tensor, \n                    config: GenerationConfig) -&gt; Tuple[torch.Tensor, List[float]]:\n        \"\"\"\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306b\u3088\u308b\u751f\u6210\"\"\"\n        batch_size = input_ids.shape[0]\n        num_beams = config.num_beams\n\n        # \u30d3\u30fc\u30e0\u306e\u521d\u671f\u5316\n        beam_scores = torch.zeros((batch_size, num_beams), device=self.device)\n        beam_scores[:, 1:] = -1e9  # \u6700\u521d\u306f1\u3064\u306e\u30d3\u30fc\u30e0\u306e\u307f\u6709\u52b9\n\n        # \u5404\u30d3\u30fc\u30e0\u306e\u7cfb\u5217\n        beam_sequences = input_ids.unsqueeze(1).repeat(1, num_beams, 1)\n        beam_sequences = beam_sequences.view(batch_size * num_beams, -1)\n\n        # \u5b8c\u4e86\u3057\u305f\u30d3\u30fc\u30e0\n        done = [False] * (batch_size * num_beams)\n        scores_history = []\n\n        for step in range(config.max_length - input_ids.shape[1]):\n            with torch.no_grad():\n                outputs = self.model(beam_sequences)\n                next_token_logits = outputs[:, -1, :]\n\n                # \u30b9\u30b3\u30a2\u8a08\u7b97\n                vocab_size = next_token_logits.shape[-1]\n                next_token_scores = F.log_softmax(next_token_logits, dim=-1)\n\n                # \u73fe\u5728\u306e\u30d3\u30fc\u30e0\u30b9\u30b3\u30a2\u3092\u52a0\u7b97\n                next_token_scores = next_token_scores.view(batch_size, num_beams, -1)\n                next_token_scores = next_token_scores + beam_scores.unsqueeze(-1)\n\n                # \u5168\u5019\u88dc\u304b\u3089\u4e0a\u4f4dk\u500b\u3092\u9078\u629e\n                next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n                next_scores, next_tokens = torch.topk(\n                    next_token_scores, 2 * num_beams, dim=-1, largest=True, sorted=True\n                )\n\n                # \u6b21\u306e\u30d3\u30fc\u30e0\u3092\u69cb\u7bc9\n                next_batch_beam = []\n\n                for batch_idx in range(batch_size):\n                    next_sent_beam = []\n\n                    for rank, (token_score, token_id) in enumerate(\n                        zip(next_scores[batch_idx], next_tokens[batch_idx])\n                    ):\n                        beam_id = token_id // vocab_size\n                        token_id = token_id % vocab_size\n\n                        effective_beam_id = batch_idx * num_beams + beam_id\n\n                        # EOS\u30c8\u30fc\u30af\u30f3\u306e\u51e6\u7406\n                        if token_id.item() == self.tokenizer.eos_token_id:\n                            done[effective_beam_id] = True\n\n                        next_sent_beam.append({\n                            'score': token_score,\n                            'token_id': token_id,\n                            'beam_id': effective_beam_id\n                        })\n\n                        if len(next_sent_beam) &gt;= num_beams:\n                            break\n\n                    next_batch_beam.extend(next_sent_beam)\n\n                # \u30d3\u30fc\u30e0\u3092\u66f4\u65b0\n                beam_scores = beam_scores.new_zeros((batch_size, num_beams))\n                beam_sequences = []\n\n                for beam_idx, beam in enumerate(next_batch_beam):\n                    batch_idx = beam_idx // num_beams\n                    beam_scores[batch_idx, beam_idx % num_beams] = beam['score']\n\n                    prev_seq = beam_sequences[beam['beam_id']]\n                    new_seq = torch.cat([prev_seq, beam['token_id'].unsqueeze(0)])\n                    beam_sequences.append(new_seq)\n\n                beam_sequences = torch.stack(beam_sequences, dim=0)\n                scores_history.append(beam_scores[0, 0].item())\n\n                if all(done):\n                    break\n\n        # \u6700\u826f\u306e\u30d3\u30fc\u30e0\u3092\u8fd4\u3059\n        best_beam_idx = beam_scores[0].argmax()\n        best_sequence = beam_sequences[best_beam_idx].unsqueeze(0)\n\n        return best_sequence, scores_history\n\n    def _sampling(self, input_ids: torch.Tensor, \n                 config: GenerationConfig) -&gt; Tuple[torch.Tensor, List[float]]:\n        \"\"\"\u6e29\u5ea6\u4ed8\u304d\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\"\"\"\n        generated = input_ids.clone()\n        scores = []\n\n        for _ in range(config.max_length - input_ids.shape[1]):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                next_token_logits = outputs[:, -1, :] / config.temperature\n\n                # Repetition penalty\n                if config.repetition_penalty != 1.0:\n                    self._apply_repetition_penalty(\n                        next_token_logits, generated, config.repetition_penalty\n                    )\n\n                # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n                score = probs.gather(-1, next_token).item()\n                scores.append(score)\n\n                generated = torch.cat([generated, next_token], dim=1)\n\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        return generated, scores\n\n    def _top_k_sampling(self, input_ids: torch.Tensor, \n                       config: GenerationConfig) -&gt; Tuple[torch.Tensor, List[float]]:\n        \"\"\"Top-k\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\"\"\"\n        generated = input_ids.clone()\n        scores = []\n\n        for _ in range(config.max_length - input_ids.shape[1]):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                next_token_logits = outputs[:, -1, :] / config.temperature\n\n                # Top-k\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n                top_k_logits, top_k_indices = torch.topk(\n                    next_token_logits, config.top_k, dim=-1\n                )\n\n                # \u78ba\u7387\u3092\u518d\u8a08\u7b97\n                probs = F.softmax(top_k_logits, dim=-1)\n\n                # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                sample_idx = torch.multinomial(probs, num_samples=1)\n                next_token = top_k_indices.gather(-1, sample_idx)\n                score = probs.gather(-1, sample_idx).item()\n                scores.append(score)\n\n                generated = torch.cat([generated, next_token], dim=1)\n\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        return generated, scores\n\n    def _top_p_sampling(self, input_ids: torch.Tensor, \n                       config: GenerationConfig) -&gt; Tuple[torch.Tensor, List[float]]:\n        \"\"\"Top-p (Nucleus) \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\"\"\"\n        generated = input_ids.clone()\n        scores = []\n\n        for _ in range(config.max_length - input_ids.shape[1]):\n            with torch.no_grad():\n                outputs = self.model(generated)\n                next_token_logits = outputs[:, -1, :] / config.temperature\n\n                # \u30bd\u30fc\u30c8\u3057\u3066\u7d2f\u7a4d\u78ba\u7387\u3092\u8a08\u7b97\n                sorted_logits, sorted_indices = torch.sort(\n                    next_token_logits, descending=True\n                )\n                cumulative_probs = torch.cumsum(\n                    F.softmax(sorted_logits, dim=-1), dim=-1\n                )\n\n                # Top-p\u3092\u8d85\u3048\u308b\u4f4d\u7f6e\u3092\u898b\u3064\u3051\u308b\n                sorted_indices_to_remove = cumulative_probs &gt; config.top_p\n                # \u5c11\u306a\u304f\u3068\u30821\u3064\u306f\u6b8b\u3059\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                # \u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n                indices_to_remove = sorted_indices_to_remove.scatter(\n                    1, sorted_indices, sorted_indices_to_remove\n                )\n                next_token_logits[indices_to_remove] = -float('inf')\n\n                # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n                score = probs.gather(-1, next_token).item()\n                scores.append(score)\n\n                generated = torch.cat([generated, next_token], dim=1)\n\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        return generated, scores\n\n    def _apply_repetition_penalty(self, logits: torch.Tensor, \n                                generated: torch.Tensor, penalty: float):\n        \"\"\"\u7e70\u308a\u8fd4\u3057\u30da\u30ca\u30eb\u30c6\u30a3\u3092\u9069\u7528\"\"\"\n        for token_id in generated[0].unique():\n            if logits[:, token_id] &lt; 0:\n                logits[:, token_id] *= penalty\n            else:\n                logits[:, token_id] /= penalty\n\ndef compare_generation_methods():\n    \"\"\"\u751f\u6210\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n\n    # \u30c0\u30df\u30fc\u30e2\u30c7\u30eb\u3068\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\uff08\u524d\u306e\u6f14\u7fd2\u304b\u3089\uff09\n    vocab_size = 100\n    model = GPTWithPositionalEncoding(\n        vocab_size=vocab_size,\n        d_model=128,\n        n_heads=4,\n        n_layers=2\n    )\n\n    tokenizer = SimpleTokenizer()\n\n    # \u30c6\u30ad\u30b9\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u30fc\n    generator = TextGenerator(model, tokenizer)\n\n    # \u30d7\u30ed\u30f3\u30d7\u30c8\n    prompts = [\n        \"The weather today is\",\n        \"Machine learning can\",\n        \"In the future, we will\",\n    ]\n\n    # \u5404\u624b\u6cd5\u3067\u751f\u6210\n    methods = ['greedy', 'beam_search', 'sampling', 'top_k', 'top_p']\n    configs = {\n        'greedy': GenerationConfig(temperature=1.0),\n        'beam_search': GenerationConfig(num_beams=4),\n        'sampling': GenerationConfig(temperature=0.8, do_sample=True),\n        'top_k': GenerationConfig(temperature=0.8, top_k=40),\n        'top_p': GenerationConfig(temperature=0.8, top_p=0.95),\n    }\n\n    results = {method: [] for method in methods}\n\n    print(\"=== Generation Method Comparison ===\\n\")\n\n    for prompt in prompts:\n        print(f\"Prompt: '{prompt}'\")\n        print(\"-\" * 50)\n\n        for method in methods:\n            result = generator.generate(prompt, configs[method], method=method)\n            results[method].append(result)\n\n            print(f\"\\n{method.upper()}:\")\n            print(f\"Generated: {result['text']}\")\n            print(f\"Avg Score: {np.mean(result['scores']):.4f}\")\n\n    # \u591a\u69d8\u6027\u3068\u54c1\u8cea\u306e\u5206\u6790\n    analyze_generation_quality(results, prompts)\n\n    return results\n\ndef analyze_generation_quality(results: Dict[str, List[Dict]], prompts: List[str]):\n    \"\"\"\u751f\u6210\u54c1\u8cea\u306e\u5206\u6790\"\"\"\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. \u5e73\u5747\u30b9\u30b3\u30a2\uff08\u4fe1\u983c\u5ea6\uff09\n    ax = axes[0, 0]\n    avg_scores = {}\n    for method, method_results in results.items():\n        scores = [np.mean(r['scores']) for r in method_results]\n        avg_scores[method] = np.mean(scores)\n        ax.bar(method, avg_scores[method], alpha=0.7)\n\n    ax.set_title('Average Generation Confidence')\n    ax.set_ylabel('Average Score')\n    ax.set_xticklabels(list(results.keys()), rotation=45)\n    ax.grid(True, alpha=0.3)\n\n    # 2. \u751f\u6210\u9577\u306e\u5206\u5e03\n    ax = axes[0, 1]\n    for method, method_results in results.items():\n        lengths = [len(r['ids'][0]) for r in method_results]\n        ax.plot(lengths, label=method, marker='o')\n\n    ax.set_title('Generation Length by Method')\n    ax.set_xlabel('Example Index')\n    ax.set_ylabel('Sequence Length')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # 3. \u30c8\u30fc\u30af\u30f3\u306e\u591a\u69d8\u6027\uff08\u30e6\u30cb\u30fc\u30af\u30c8\u30fc\u30af\u30f3\u6570\uff09\n    ax = axes[1, 0]\n    diversity_scores = {}\n    for method, method_results in results.items():\n        unique_ratios = []\n        for r in method_results:\n            tokens = r['ids'][0].tolist()\n            unique_ratio = len(set(tokens)) / len(tokens)\n            unique_ratios.append(unique_ratio)\n        diversity_scores[method] = np.mean(unique_ratios)\n        ax.bar(method, diversity_scores[method], alpha=0.7)\n\n    ax.set_title('Token Diversity (Unique Token Ratio)')\n    ax.set_ylabel('Unique Token Ratio')\n    ax.set_xticklabels(list(results.keys()), rotation=45)\n    ax.grid(True, alpha=0.3)\n\n    # 4. \u30b9\u30b3\u30a2\u306e\u5b89\u5b9a\u6027\uff08\u6a19\u6e96\u504f\u5dee\uff09\n    ax = axes[1, 1]\n    stability_scores = {}\n    for method, method_results in results.items():\n        score_stds = [np.std(r['scores']) if len(r['scores']) &gt; 1 else 0 \n                     for r in method_results]\n        stability_scores[method] = np.mean(score_stds)\n        ax.bar(method, stability_scores[method], alpha=0.7)\n\n    ax.set_title('Score Stability (Lower is More Stable)')\n    ax.set_ylabel('Average Score Std Dev')\n    ax.set_xticklabels(list(results.keys()), rotation=45)\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u30b5\u30de\u30ea\u30fc\u7d71\u8a08\n    print(\"\\n=== Generation Quality Summary ===\")\n    print(f\"{'Method':&lt;15} {'Avg Score':&lt;12} {'Diversity':&lt;12} {'Stability':&lt;12}\")\n    print(\"-\" * 55)\n\n    for method in results.keys():\n        print(f\"{method:&lt;15} {avg_scores[method]:&lt;12.4f} \"\n              f\"{diversity_scores[method]:&lt;12.4f} \"\n              f\"{stability_scores[method]:&lt;12.4f}\")\n\n# \u5b9f\u884c\ngeneration_results = compare_generation_methods()\n</code></pre>"},{"location":"exercises/part5-exercises/#_1","title":"\u30c1\u30e3\u30ec\u30f3\u30b8\u554f\u984c","text":""},{"location":"exercises/part5-exercises/#8","title":"\u554f\u984c 8 \ud83c\udf1f","text":"<p>\u5b9f\u7528\u7684\u306a\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u30b7\u30b9\u30c6\u30e0\u3092\u69cb\u7bc9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u4e0b\u306e\u6a5f\u80fd\u3092\u542b\u3080\uff1a - \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\uff08\u4f1a\u8a71\u5c65\u6b74\uff09 - \u30d7\u30ed\u30f3\u30d7\u30c8\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0 - \u5b89\u5168\u6027\u30d5\u30a3\u30eb\u30bf\u30fc - \u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u51fa\u529b</p> \u89e3\u7b54 <p>```python import torch import torch.nn as nn from typing import List, Dict, Optional, Generator, Tuple import asyncio from dataclasses import dataclass, field from datetime import datetime import re from collections import deque</p> <p>@dataclass class Message:     \"\"\"\u30c1\u30e3\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\"\"\"     role: str  # 'user' or 'assistant'     content: str     timestamp: datetime = field(default_factory=datetime.now)     metadata: Dict = field(default_factory=dict)</p> <p>@dataclass class ChatConfig:     \"\"\"\u30c1\u30e3\u30c3\u30c8\u8a2d\u5b9a\"\"\"     max_context_length: int = 2048     max_response_length: int = 512     temperature: float = 0.7     top_p: float = 0.9     repetition_penalty: float = 1.1     system_prompt: str = \"You are a helpful assistant.\"     safety_threshold: float = 0.8     streaming: bool = True</p> <p>class SafetyFilter:     \"\"\"\u5b89\u5168\u6027\u30d5\u30a3\u30eb\u30bf\u30fc\"\"\"</p> <pre><code>def __init__(self, threshold: float = 0.8):\n    self.threshold = threshold\n    # \u7c21\u6613\u7684\u306a\u7981\u6b62\u8a9e\u30ea\u30b9\u30c8\uff08\u5b9f\u969b\u306f\u3088\u308a\u6d17\u7df4\u3055\u308c\u305f\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\uff09\n    self.banned_patterns = [\n        r'\\b(hate|violence|illegal)\\b',\n        r'\\b(harmful|dangerous)\\b',\n    ]\n\ndef is_safe(self, text: str) -&gt; Tuple[bool, Optional[str]]:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u306e\u5b89\u5168\u6027\u3092\u30c1\u30a7\u30c3\u30af\"\"\"\n    text_lower = text.lower()\n\n    # \u7981\u6b62\u30d1\u30bf\u30fc\u30f3\u306e\u30c1\u30a7\u30c3\u30af\n    for pattern in self.banned_patterns:\n        if re.search(pattern, text_lower):\n            return False, f\"Content contains prohibited pattern: {pattern}\"\n\n    # \u305d\u306e\u4ed6\u306e\u5b89\u5168\u6027\u30c1\u30a7\u30c3\u30af\uff08\u7c21\u7565\u5316\uff09\n    if len(text) &gt; 10000:\n        return False, \"Content too long\"\n\n    return True, None\n\ndef sanitize(self, text: str) -&gt; str:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30b5\u30cb\u30bf\u30a4\u30ba\"\"\"\n    # \u57fa\u672c\u7684\u306a\u30b5\u30cb\u30bf\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\n    text = text.strip()\n    # \u9023\u7d9a\u3059\u308b\u7a7a\u767d\u30921\u3064\u306b\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n</code></pre> <p>class ConversationManager:     \"\"\"\u4f1a\u8a71\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\"\"\"</p> <pre><code>def __init__(self, max_context_length: int = 2048):\n    self.max_context_length = max_context_length\n    self.messages: deque[Message] = deque()\n    self.token_counts: deque[int] = deque()\n\ndef add_message(self, message: Message, token_count: int):\n    \"\"\"\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\"\"\"\n    self.messages.append(message)\n    self.token_counts.append(token_count)\n\n    # \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u9577\u3092\u7ba1\u7406\n    while sum(self.token_counts) &gt; self.max_context_length and len(self.messages) &gt; 2:\n        self.messages.popleft()\n        self.token_counts.popleft()\n\ndef get_context(self) -&gt; List[Message]:\n    \"\"\"\u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\"\"\"\n    return list(self.messages)\n\ndef clear(self):\n    \"\"\"\u4f1a\u8a71\u5c65\u6b74\u3092\u30af\u30ea\u30a2\"\"\"\n    self.messages.clear()\n    self.token_counts.clear()\n\ndef format_for_model(self, tokenizer) -&gt; str:\n    \"\"\"\u30e2\u30c7\u30eb\u5165\u529b\u7528\u306b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\"\"\"\n    formatted_messages = []\n\n    for msg in self.messages:\n        if msg.role == 'user':\n            formatted_messages.append(f\"User: {msg.content}\")\n        else:\n            formatted_messages.append(f\"Assistant: {msg.content}\")\n\n    return \"\\n\".join(formatted_messages) + \"\\nAssistant:\"\n</code></pre> <p>class PromptEngineering:     \"\"\"\u30d7\u30ed\u30f3\u30d7\u30c8\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\"\"\"</p> <pre><code>@staticmethod\ndef create_system_prompt(config: ChatConfig) -&gt; str:\n    \"\"\"\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u4f5c\u6210\"\"\"\n    return f\"\"\"&lt;|system|&gt;\n</code></pre> <p>{config.system_prompt}</p> <p>Guidelines: 1. Be helpful, harmless, and honest 2. Provide clear and concise responses 3. Admit when you don't know something 4. Refuse inappropriate requests politely &lt;|endofsystem|&gt;\"\"\"</p> <pre><code>    @staticmethod\n    def format_chat_prompt(messages: List[Message], config: ChatConfig) -&gt; str:\n        \"\"\"\u30c1\u30e3\u30c3\u30c8\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\"\"\"\n        prompt_parts = [PromptEngineering.create_system_prompt(config)]\n\n        for msg in messages:\n            if msg.role == 'user':\n                prompt_parts.append(f\"&lt;|user|&gt;\\n{msg.content}\\n&lt;|endofuser|&gt;\")\n            else:\n                prompt_parts.append(f\"&lt;|assistant|&gt;\\n{msg.content}\\n&lt;|endofassistant|&gt;\")\n\n        # \u6700\u5f8c\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8ffd\u52a0\n        prompt_parts.append(\"&lt;|assistant|&gt;\")\n\n        return \"\\n\".join(prompt_parts)\n\nclass StreamingChatbot:\n    \"\"\"\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u5bfe\u5fdc\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\"\"\"\n\n    def __init__(self, model, tokenizer, config: ChatConfig):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n        self.device = next(model.parameters()).device\n\n        self.conversation = ConversationManager(config.max_context_length)\n        self.safety_filter = SafetyFilter(config.safety_threshold)\n        self.prompt_engineering = PromptEngineering()\n\n    def generate_streaming(self, prompt: str) -&gt; Generator[str, None, None]:\n        \"\"\"\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u751f\u6210\"\"\"\n        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n        generated_ids = input_ids.clone()\n\n        past_key_values = None\n        generated_text = \"\"\n\n        for _ in range(self.config.max_response_length):\n            with torch.no_grad():\n                if past_key_values is None:\n                    outputs = self.model(generated_ids)\n                else:\n                    # \u52b9\u7387\u5316\u306e\u305f\u3081\u524d\u56de\u306e\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u4f7f\u7528\n                    outputs = self.model(\n                        generated_ids[:, -1:],\n                        past_key_values=past_key_values\n                    )\n\n                next_token_logits = outputs[:, -1, :]\n\n                # \u6e29\u5ea6\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n                next_token_logits = next_token_logits / self.config.temperature\n\n                # Repetition penalty\n                if self.config.repetition_penalty != 1.0:\n                    for token_id in generated_ids[0].unique():\n                        if next_token_logits[:, token_id] &lt; 0:\n                            next_token_logits[:, token_id] *= self.config.repetition_penalty\n                        else:\n                            next_token_logits[:, token_id] /= self.config.repetition_penalty\n\n                # Top-p sampling\n                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs &gt; self.config.top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices_to_remove.scatter(\n                    1, sorted_indices, sorted_indices_to_remove\n                )\n                next_token_logits[indices_to_remove] = -float('inf')\n\n                # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n\n                # \u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0\n                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n\n                # \u30c7\u30b3\u30fc\u30c9\n                token_text = self.tokenizer.decode([next_token.item()])\n                generated_text += token_text\n\n                # \u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u51fa\u529b\n                yield token_text\n\n                # \u7d42\u4e86\u6761\u4ef6\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n                # \u5b89\u5168\u6027\u30c1\u30a7\u30c3\u30af\uff08\u5b9a\u671f\u7684\u306b\uff09\n                if len(generated_text) % 50 == 0:\n                    is_safe, _ = self.safety_filter.is_safe(generated_text)\n                    if not is_safe:\n                        yield \"\\n[Content filtered]\"\n                        break\n\n    async def chat_async(self, user_input: str) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\u975e\u540c\u671f\u30c1\u30e3\u30c3\u30c8\"\"\"\n        # \u5165\u529b\u306e\u5b89\u5168\u6027\u30c1\u30a7\u30c3\u30af\n        is_safe, reason = self.safety_filter.is_safe(user_input)\n        if not is_safe:\n            yield f\"I cannot process this request. Reason: {reason}\"\n            return\n\n        # \u30b5\u30cb\u30bf\u30a4\u30ba\n        user_input = self.safety_filter.sanitize(user_input)\n\n        # \u30e6\u30fc\u30b6\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n        user_msg = Message(role='user', content=user_input)\n        user_token_count = len(self.tokenizer.encode(user_input))\n        self.conversation.add_message(user_msg, user_token_count)\n\n        # \u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u69cb\u7bc9\n        prompt = self.prompt_engineering.format_chat_prompt(\n            self.conversation.get_context(), \n            self.config\n        )\n\n        # \u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u751f\u6210\n        response_text = \"\"\n        async for token in self._async_generate(prompt):\n            response_text += token\n            yield token\n\n        # \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n        assistant_msg = Message(role='assistant', content=response_text)\n        assistant_token_count = len(self.tokenizer.encode(response_text))\n        self.conversation.add_message(assistant_msg, assistant_token_count)\n\n    async def _async_generate(self, prompt: str) -&gt; AsyncGenerator[str, None]:\n        \"\"\"\u975e\u540c\u671f\u751f\u6210\uff08\u5b9f\u969b\u306e\u5b9f\u88c5\u3067\u306f\u5225\u30b9\u30ec\u30c3\u30c9\u3067\u5b9f\u884c\uff09\"\"\"\n        for token in self.generate_streaming(prompt):\n            await asyncio.sleep(0.01)  # \u30b7\u30df\u30e5\u30ec\u30fc\u30c8\u3055\u308c\u305f\u9045\u5ef6\n            yield token\n\n    def chat(self, user_input: str) -&gt; str:\n        \"\"\"\u540c\u671f\u7684\u306a\u30c1\u30e3\u30c3\u30c8\uff08\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u306a\u3057\uff09\"\"\"\n        # \u5165\u529b\u30c1\u30a7\u30c3\u30af\n        is_safe, reason = self.safety_filter.is_safe(user_input)\n        if not is_safe:\n            return f\"I cannot process this request. Reason: {reason}\"\n\n        user_input = self.safety_filter.sanitize(user_input)\n\n        # \u30e1\u30c3\u30bb\u30fc\u30b8\u8ffd\u52a0\n        user_msg = Message(role='user', content=user_input)\n        user_token_count = len(self.tokenizer.encode(user_input))\n        self.conversation.add_message(user_msg, user_token_count)\n\n        # \u30d7\u30ed\u30f3\u30d7\u30c8\u69cb\u7bc9\n        prompt = self.prompt_engineering.format_chat_prompt(\n            self.conversation.get_context(),\n            self.config\n        )\n\n        # \u751f\u6210\n        response_text = \"\"\n        for token in self.generate_streaming(prompt):\n            response_text += token\n\n        # \u6700\u7d42\u7684\u306a\u5b89\u5168\u6027\u30c1\u30a7\u30c3\u30af\n        is_safe, _ = self.safety_filter.is_safe(response_text)\n        if not is_safe:\n            response_text = \"I apologize, but I cannot provide that response.\"\n\n        # \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u8ffd\u52a0\n        assistant_msg = Message(role='assistant', content=response_text)\n        assistant_token_count = len(self.tokenizer.encode(response_text))\n        self.conversation.add_message(assistant_msg, assistant_token_count)\n\n        return response_text\n\nclass ChatbotUI:\n    \"\"\"\u7c21\u6613\u7684\u306a\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8UI\"\"\"\n\n    def __init__(self, chatbot: StreamingChatbot):\n        self.chatbot = chatbot\n\n    def run(self):\n        \"\"\"\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30c1\u30e3\u30c3\u30c8\u30eb\u30fc\u30d7\"\"\"\n        print(\"=== Chatbot Started ===\")\n        print(\"Type 'quit' to exit, 'clear' to clear conversation history\")\n        print(\"-\" * 50)\n\n        while True:\n            try:\n                user_input = input(\"\\nYou: \").strip()\n\n                if user_input.lower() == 'quit':\n                    print(\"Goodbye!\")\n                    break\n\n                if user_input.lower() == 'clear':\n                    self.chatbot.conversation.clear()\n                    print(\"Conversation history cleared.\")\n                    continue\n\n                if not user_input:\n                    continue\n\n                print(\"\\nAssistant: \", end='', flush=True)\n\n                if self.chatbot.config.streaming:\n                    # \u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u51fa\u529b\n                    for token in self.chatbot.generate_streaming(user_input):\n                        print(token, end='', flush=True)\n                    print()  # \u6539\u884c\n                else:\n                    # \u901a\u5e38\u306e\u51fa\u529b\n                    response = self.chatbot.chat(user_input)\n                    print(response)\n\n            except KeyboardInterrupt:\n                print(\"\\n\\nInterrupted. Type 'quit' to exit.\")\n            except Exception as e:\n                print(f\"\\nError: {e}\")\n\n# \u4f7f\u7528\u4f8b\ndef demo_chatbot():\n    \"\"\"\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u306e\u30c7\u30e2\"\"\"\n\n    # \u30e2\u30c7\u30eb\u3068\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\uff08\u524d\u306e\u6f14\u7fd2\u304b\u3089\uff09\n    model = GPTWithPositionalEncoding(\n        vocab_size=1000,\n        d_model=256,\n        n_heads=8,\n        n_layers=4\n    )\n\n    tokenizer = SimpleTokenizer()\n\n    # \u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u8a2d\u5b9a\n    config = ChatConfig(\n        max_context_length=1024,\n        temperature=0.8,\n        top_p=0.95,\n        system_prompt=\"You are a helpful AI assistant.\",\n        streaming=True\n    )\n\n    # \u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u4f5c\u6210\n    chatbot = StreamingChatbot(model, tokenizer, config)\n\n    # \u30c7\u30e2\u4f1a\u8a71\n    print(\"=== Chatbot Demo ===\\n\")\n\n    test_conversations = [\n        \"Hello! How are you?\",\n        \"Can you explain machine learning?\",\n        \"What's the weather like?\",\n        \"Tell me a joke.\",\n    ]\n\n    for user_input in test_conversations:\n        print(f\"User: {user_input}\")\n        print(\"Assistant: \", end='')\n\n        response = chatbot.chat(user_input)\n        print(response)\n        print(\"-\" * 50)\n\n    # \u4f1a\u8a71\u5c65\u6b74\u306e\u8868\u793a\n    print(\"\\n=== Conversation History ===\")\n    for msg in chatbot.conversation.get_context():\n        print(f\"{msg.role.upper()}: {msg.content[:50]}...\")\n\n    return chatbot\n\n# \u30c7\u30e2\u5b9f\u884c\nchatbot_instance = demo_chatbot()\n\n# \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u30e2\u30fc\u30c9\u3092\u958b\u59cb\u3059\u308b\u5834\u5408\n# ui = ChatbotUI(chatbot_instance)\n# ui.run()\n```\n</code></pre>"},{"location":"exercises/part5-exercises/#_2","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<p>\u3053\u308c\u3089\u306e\u6f14\u7fd2\u3092\u5b8c\u4e86\u3057\u305f\u3089\u3001\u4ee5\u4e0b\u306e\u767a\u5c55\u7684\u306a\u30c8\u30d4\u30c3\u30af\u306b\u6311\u6226\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\uff1a</p> <ol> <li>\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30ebTransformer: \u753b\u50cf\u3068\u30c6\u30ad\u30b9\u30c8\u3092\u540c\u6642\u306b\u6271\u3046</li> <li>\u52b9\u7387\u7684\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3: Reformer\u3001Linformer\u3001Performer\u306e\u5b9f\u88c5</li> <li>\u5f37\u5316\u5b66\u7fd2\u3068\u306e\u7d71\u5408: RLHF\uff08Reinforcement Learning from Human Feedback\uff09</li> <li>\u5206\u6563\u5b66\u7fd2: \u30e2\u30c7\u30eb\u4e26\u5217\u3001\u30c7\u30fc\u30bf\u4e26\u5217\u306e\u5b9f\u88c5</li> </ol> <p>\ud83d\udca1 \u5b66\u7fd2\u306e\u30a2\u30c9\u30d0\u30a4\u30b9: - \u5404\u5b9f\u88c5\u3092\u5c0f\u3055\u306a\u90e8\u5206\u304b\u3089\u59cb\u3081\u3066\u5f90\u3005\u306b\u8907\u96d1\u306b\u3057\u3066\u3044\u304f - \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b9f\u9a13\u3057\u3066\u307f\u308b - \u6700\u65b0\u306e\u7814\u7a76\u8ad6\u6587\u3092\u8aad\u3093\u3067\u65b0\u3057\u3044\u624b\u6cd5\u3092\u8a66\u3059 - \u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u30b3\u30fc\u30c9\u3092\u53c2\u8003\u306b\u3057\u306a\u304c\u3089\u81ea\u5206\u306a\u308a\u306e\u6539\u826f\u3092\u52a0\u3048\u308b</p> <p>\u9811\u5f35\u3063\u3066\u304f\u3060\u3055\u3044\uff01\ud83d\ude80</p>"},{"location":"part1/math-basics/","title":"\u5fc5\u8981\u306a\u6570\u5b66\u7684\u57fa\u790e","text":""},{"location":"part1/math-basics/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u305f\u3081\u306e\u6570\u5b66","text":"<p>\u300c\u6570\u5b66\u306f\u82e6\u624b...\u300d\u305d\u3093\u306a\u58f0\u304c\u805e\u3053\u3048\u3066\u304d\u305d\u3046\u3067\u3059\u304c\u3001\u5fc3\u914d\u306f\u3044\u308a\u307e\u305b\u3093\u3002\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3092\u5b9f\u88c5\u3067\u304d\u308b\u3042\u306a\u305f\u306f\u3001\u3059\u3067\u306b\u591a\u304f\u306e\u6570\u5b66\u7684\u6982\u5ff5\u3092\u4f7f\u3044\u3053\u306a\u3057\u3066\u3044\u307e\u3059\u3002\u518d\u5e30\u3001\u30b0\u30e9\u30d5\u7406\u8ad6\u3001\u8a08\u7b97\u91cf\u89e3\u6790...\u3053\u308c\u3089\u306f\u3059\u3079\u3066\u6570\u5b66\u3067\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001Transformer\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6570\u5b66\u3092\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u8996\u70b9\u304b\u3089\u89e3\u8aac\u3057\u307e\u3059\u3002\u62bd\u8c61\u7684\u306a\u5b9a\u7406\u3067\u306f\u306a\u304f\u3001\u5b9f\u88c5\u53ef\u80fd\u306a\u30b3\u30fc\u30c9\u3068\u3057\u3066\u6570\u5b66\u3092\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"part1/math-basics/#31","title":"3.1 \u7dda\u5f62\u4ee3\u6570\u306e\u672c\u8cea\u7684\u7406\u89e3","text":""},{"location":"part1/math-basics/#_3","title":"\u30d9\u30af\u30c8\u30eb\uff1a\u30c7\u30fc\u30bf\u306e\u57fa\u672c\u5358\u4f4d","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067\u306f\u914d\u5217\u3084\u30ea\u30b9\u30c8\u3092\u65e5\u5e38\u7684\u306b\u6271\u3044\u307e\u3059\u3002\u30d9\u30af\u30c8\u30eb\u306f\u3001\u3053\u308c\u3089\u306b\u300c\u5e7e\u4f55\u5b66\u7684\u306a\u610f\u5473\u300d\u3092\u4e0e\u3048\u305f\u3082\u306e\u3067\u3059\uff1a</p> <p>```python import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import torch import torch.nn as nn from typing import List, Tuple, Optional</p> <p>class VectorBasics:     \"\"\"\u30d9\u30af\u30c8\u30eb\u306e\u57fa\u672c\u6982\u5ff5\u3092\u5b9f\u88c5\u3067\u7406\u89e3\"\"\"</p> <pre><code>def __init__(self):\n    self.examples = {\n        \"word_embedding\": \"\u5358\u8a9e\u306e\u610f\u5473\u3092\u30d9\u30af\u30c8\u30eb\u3067\u8868\u73fe\",\n        \"program_state\": \"\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u72b6\u614b\u3092\u30d9\u30af\u30c8\u30eb\u3067\u8868\u73fe\",\n        \"feature_vector\": \"\u7279\u5fb4\u91cf\u3092\u30d9\u30af\u30c8\u30eb\u3067\u8868\u73fe\"\n    }\n\ndef vector_as_array(self):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u306f\u5358\u306a\u308b\u6570\u5024\u306e\u914d\u5217\"\"\"\n    # \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306b\u3068\u3063\u3066\u99b4\u67d3\u307f\u306e\u3042\u308b\u8868\u73fe\n    array = [1.0, 2.0, 3.0]\n\n    # NumPy\u30d9\u30af\u30c8\u30eb\n    np_vector = np.array(array)\n\n    # PyTorch\u30c6\u30f3\u30bd\u30eb\n    torch_vector = torch.tensor(array)\n\n    print(\"=== \u30d9\u30af\u30c8\u30eb\u306e\u8868\u73fe ===\")\n    print(f\"Python list: {array}\")\n    print(f\"NumPy array: {np_vector}\")\n    print(f\"PyTorch tensor: {torch_vector}\")\n\n    return array, np_vector, torch_vector\n\ndef vector_as_point(self):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u306f\u7a7a\u9593\u4e0a\u306e\u70b9\"\"\"\n    fig = plt.figure(figsize=(15, 5))\n\n    # 2\u6b21\u5143\u30d9\u30af\u30c8\u30eb\n    ax1 = fig.add_subplot(131)\n    vectors_2d = [\n        ([0, 0], [3, 4], 'v1=(3,4)'),\n        ([0, 0], [5, 2], 'v2=(5,2)'),\n        ([0, 0], [-2, 3], 'v3=(-2,3)')\n    ]\n\n    for start, end, label in vectors_2d:\n        ax1.arrow(start[0], start[1], \n                 end[0]-start[0], end[1]-start[1],\n                 head_width=0.3, head_length=0.2, \n                 fc='blue', ec='blue')\n        ax1.text(end[0]+0.2, end[1]+0.2, label)\n\n    ax1.set_xlim(-3, 6)\n    ax1.set_ylim(-1, 5)\n    ax1.grid(True)\n    ax1.set_title('2\u6b21\u5143\u30d9\u30af\u30c8\u30eb')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n\n    # 3\u6b21\u5143\u30d9\u30af\u30c8\u30eb\n    ax2 = fig.add_subplot(132, projection='3d')\n    vectors_3d = [\n        ([0, 0, 0], [3, 4, 2], 'v1'),\n        ([0, 0, 0], [5, 2, 4], 'v2'),\n        ([0, 0, 0], [-2, 3, 1], 'v3')\n    ]\n\n    for start, end, label in vectors_3d:\n        ax2.quiver(start[0], start[1], start[2],\n                  end[0], end[1], end[2],\n                  arrow_length_ratio=0.1)\n        ax2.text(end[0], end[1], end[2], label)\n\n    ax2.set_xlim(-3, 6)\n    ax2.set_ylim(-1, 5)\n    ax2.set_zlim(0, 5)\n    ax2.set_title('3\u6b21\u5143\u30d9\u30af\u30c8\u30eb')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_zlabel('z')\n\n    # \u9ad8\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e\u53ef\u8996\u5316\uff08\u6b21\u5143\u524a\u6e1b\uff09\n    ax3 = fig.add_subplot(133)\n\n    # \u4eee\u60f3\u7684\u306a100\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u30922\u6b21\u5143\u306b\u6295\u5f71\n    np.random.seed(42)\n    high_dim_vectors = np.random.randn(50, 100)\n\n    # PCA\u98a8\u306e\u6b21\u5143\u524a\u6e1b\uff08\u7c21\u6613\u7248\uff09\n    projection_matrix = np.random.randn(100, 2)\n    projected = high_dim_vectors @ projection_matrix\n\n    ax3.scatter(projected[:, 0], projected[:, 1], alpha=0.6)\n    ax3.set_title('100\u6b21\u5143\u30d9\u30af\u30c8\u30eb\u306e2\u6b21\u5143\u6295\u5f71')\n    ax3.set_xlabel('\u7b2c1\u4e3b\u6210\u5206')\n    ax3.set_ylabel('\u7b2c2\u4e3b\u6210\u5206')\n    ax3.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\ndef vector_operations(self):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u306e\u610f\u5473\"\"\"\n\n    # \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u4f8b\n    word_vectors = {\n        \"king\": np.array([1.0, 0.5, 0.2, 0.8]),\n        \"queen\": np.array([0.9, 0.6, 0.8, 0.7]),\n        \"man\": np.array([0.8, 0.3, 0.1, 0.9]),\n        \"woman\": np.array([0.7, 0.4, 0.7, 0.8]),\n        \"prince\": np.array([0.9, 0.4, 0.15, 0.75]),\n        \"princess\": np.array([0.85, 0.45, 0.65, 0.65])\n    }\n\n    print(\"=== \u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u306e\u610f\u5473 ===\")\n\n    # \u52a0\u6cd5\uff1a\u6982\u5ff5\u306e\u7d44\u307f\u5408\u308f\u305b\n    print(\"\\n1. \u30d9\u30af\u30c8\u30eb\u306e\u52a0\u6cd5\uff08\u6982\u5ff5\u306e\u7d44\u307f\u5408\u308f\u305b\uff09\")\n    result = word_vectors[\"king\"] - word_vectors[\"man\"] + word_vectors[\"woman\"]\n    print(f\"'king' - 'man' + 'woman' = {result}\")\n    print(f\"'queen' = {word_vectors['queen']}\")\n    similarity = np.dot(result, word_vectors[\"queen\"]) / (np.linalg.norm(result) * np.linalg.norm(word_vectors[\"queen\"]))\n    print(f\"\u985e\u4f3c\u5ea6: {similarity:.3f}\")\n\n    # \u30b9\u30ab\u30e9\u30fc\u500d\uff1a\u5f37\u5ea6\u306e\u8abf\u6574\n    print(\"\\n2. \u30b9\u30ab\u30e9\u30fc\u500d\uff08\u5f37\u5ea6\u306e\u8abf\u6574\uff09\")\n    strong_king = 2.0 * word_vectors[\"king\"]\n    weak_king = 0.5 * word_vectors[\"king\"]\n    print(f\"\u901a\u5e38\u306e'king': {word_vectors['king']}\")\n    print(f\"\u5f37\u3044'king' (2x): {strong_king}\")\n    print(f\"\u5f31\u3044'king' (0.5x): {weak_king}\")\n\n    # \u5185\u7a4d\uff1a\u985e\u4f3c\u5ea6\u306e\u8a08\u7b97\n    print(\"\\n3. \u5185\u7a4d\uff08\u985e\u4f3c\u5ea6\u306e\u8a08\u7b97\uff09\")\n    for word1 in [\"king\", \"queen\", \"man\"]:\n        for word2 in [\"queen\", \"woman\", \"prince\"]:\n            if word1 != word2:\n                dot_product = np.dot(word_vectors[word1], word_vectors[word2])\n                print(f\"'{word1}' \u00b7 '{word2}' = {dot_product:.3f}\")\n\n    # \u53ef\u8996\u5316\n    self.visualize_vector_operations(word_vectors)\n\ndef visualize_vector_operations(self, word_vectors):\n    \"\"\"\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u306e\u53ef\u8996\u5316\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    # 2\u6b21\u5143\u306b\u6295\u5f71\uff08\u6700\u521d\u306e2\u6210\u5206\uff09\n    words = list(word_vectors.keys())\n    vectors = np.array([word_vectors[w][:2] for w in words])\n\n    # 1. \u30d9\u30af\u30c8\u30eb\u7a7a\u9593\u3067\u306e\u5358\u8a9e\u306e\u914d\u7f6e\n    ax = axes[0]\n    for i, (word, vec) in enumerate(zip(words, vectors)):\n        ax.arrow(0, 0, vec[0], vec[1], \n                head_width=0.05, head_length=0.05,\n                fc=f'C{i}', ec=f'C{i}')\n        ax.text(vec[0]+0.05, vec[1]+0.05, word, fontsize=10)\n\n    ax.set_xlim(-0.2, 1.2)\n    ax.set_ylim(-0.1, 0.8)\n    ax.grid(True)\n    ax.set_title('\u5358\u8a9e\u30d9\u30af\u30c8\u30eb')\n    ax.set_xlabel('\u6b21\u51431')\n    ax.set_ylabel('\u6b21\u51432')\n\n    # 2. \u30d9\u30af\u30c8\u30eb\u6f14\u7b97\n    ax = axes[1]\n\n    # king - man + woman \u2248 queen\n    king = word_vectors[\"king\"][:2]\n    man = word_vectors[\"man\"][:2]\n    woman = word_vectors[\"woman\"][:2]\n    queen = word_vectors[\"queen\"][:2]\n\n    # \u6f14\u7b97\u306e\u53ef\u8996\u5316\n    ax.arrow(0, 0, king[0], king[1], \n            head_width=0.05, head_length=0.05,\n            fc='red', ec='red', label='king')\n\n    # king - man\n    diff = king - man\n    ax.arrow(king[0], king[1], -man[0], -man[1],\n            head_width=0.05, head_length=0.05,\n            fc='blue', ec='blue', linestyle='--', alpha=0.5)\n\n    # + woman\n    result = diff + woman\n    ax.arrow(diff[0], diff[1], woman[0], woman[1],\n            head_width=0.05, head_length=0.05,\n            fc='green', ec='green', alpha=0.5)\n\n    # \u7d50\u679c\u3068queen\u306e\u6bd4\u8f03\n    ax.arrow(0, 0, result[0], result[1],\n            head_width=0.05, head_length=0.05,\n            fc='purple', ec='purple', linewidth=2, label='result')\n    ax.arrow(0, 0, queen[0], queen[1],\n            head_width=0.05, head_length=0.05,\n            fc='orange', ec='orange', linestyle=':', linewidth=2, label='queen')\n\n    ax.set_xlim(-0.5, 1.5)\n    ax.set_ylim(-0.5, 1.0)\n    ax.grid(True)\n    ax.set_title('king - man + woman \u2248 queen')\n    ax.legend()\n\n    # 3. \u5185\u7a4d\u3068\u89d2\u5ea6\n    ax = axes[2]\n\n    # \u3044\u304f\u3064\u304b\u306e\u30d9\u30af\u30c8\u30eb\u30da\u30a2\u306e\u89d2\u5ea6\u3092\u53ef\u8996\u5316\n    pairs = [(\"king\", \"queen\"), (\"king\", \"woman\"), (\"man\", \"woman\")]\n\n    for i, (w1, w2) in enumerate(pairs):\n        v1 = word_vectors[w1][:2]\n        v2 = word_vectors[w2][:2]\n\n        # \u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n        ax.arrow(0, 0, v1[0], v1[1],\n                head_width=0.05, head_length=0.05,\n                fc=f'C{i*2}', ec=f'C{i*2}', alpha=0.7)\n        ax.arrow(0, 0, v2[0], v2[1],\n                head_width=0.05, head_length=0.05,\n                fc=f'C{i*2+1}', ec=f'C{i*2+1}', alpha=0.7)\n\n        # \u89d2\u5ea6\u3092\u8a08\u7b97\n        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n        angle = np.arccos(np.clip(cos_angle, -1, 1))\n\n        # \u89d2\u5ea6\u3092\u8868\u793a\n        ax.text(0.1, 0.7 - i*0.15, \n               f'{w1}-{w2}: {np.degrees(angle):.1f}\u00b0',\n               fontsize=10)\n\n    ax.set_xlim(-0.2, 1.2)\n    ax.set_ylim(-0.1, 0.8)\n    ax.grid(True)\n    ax.set_title('\u30d9\u30af\u30c8\u30eb\u9593\u306e\u89d2\u5ea6\uff08\u985e\u4f3c\u5ea6\uff09')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class MatrixOperations:     \"\"\"\u884c\u5217\u6f14\u7b97\u306e\u76f4\u611f\u7684\u7406\u89e3\"\"\"</p> <pre><code>def __init__(self):\n    self.examples = {\n        \"linear_transform\": \"\u7dda\u5f62\u5909\u63db\",\n        \"weight_matrix\": \"\u91cd\u307f\u884c\u5217\",\n        \"attention_matrix\": \"\u6ce8\u610f\u884c\u5217\"\n    }\n\ndef matrix_as_transformation(self):\n    \"\"\"\u884c\u5217\u3092\u5909\u63db\u3068\u3057\u3066\u7406\u89e3\"\"\"\n\n    print(\"=== \u884c\u5217\u306f\u5909\u63db ===\")\n\n    # \u57fa\u672c\u7684\u306a\u5909\u63db\u884c\u5217\n    transformations = {\n        \"\u6052\u7b49\u5909\u63db\": np.array([[1, 0], [0, 1]]),\n        \"\u62e1\u5927\": np.array([[2, 0], [0, 2]]),\n        \"\u56de\u8ee2(45\u00b0)\": np.array([[np.cos(np.pi/4), -np.sin(np.pi/4)],\n                               [np.sin(np.pi/4), np.cos(np.pi/4)]]),\n        \"\u305b\u3093\u65ad\": np.array([[1, 0.5], [0, 1]]),\n        \"\u53cd\u5c04\": np.array([[1, 0], [0, -1]])\n    }\n\n    # \u5143\u306e\u30d9\u30af\u30c8\u30eb\u96c6\u5408\uff08\u6b63\u65b9\u5f62\uff09\n    square = np.array([\n        [0, 0], [1, 0], [1, 1], [0, 1], [0, 0]\n    ]).T\n\n    # \u53ef\u8996\u5316\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    for i, (name, matrix) in enumerate(transformations.items()):\n        ax = axes[i]\n\n        # \u5143\u306e\u5f62\n        ax.plot(square[0], square[1], 'b-', linewidth=2, \n               label='\u5143\u306e\u5f62', alpha=0.5)\n\n        # \u5909\u63db\u5f8c\n        transformed = matrix @ square\n        ax.plot(transformed[0], transformed[1], 'r-', linewidth=2,\n               label='\u5909\u63db\u5f8c')\n\n        # \u884c\u5217\u3092\u8868\u793a\n        ax.text(0.5, 1.5, f'{name}\\n{matrix}', \n               ha='center', va='bottom', fontsize=10,\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n\n        ax.set_xlim(-2, 2)\n        ax.set_ylim(-2, 2)\n        ax.grid(True, alpha=0.3)\n        ax.axhline(y=0, color='k', linewidth=0.5)\n        ax.axvline(x=0, color='k', linewidth=0.5)\n        ax.legend()\n        ax.set_aspect('equal')\n\n    # \u5408\u6210\u5909\u63db\n    ax = axes[5]\n\n    # \u56de\u8ee2\u3057\u3066\u304b\u3089\u62e1\u5927\n    rotate = transformations[\"\u56de\u8ee2(45\u00b0)\"]\n    scale = transformations[\"\u62e1\u5927\"]\n    composed = scale @ rotate  # \u884c\u5217\u306e\u7a4d\u306f\u5909\u63db\u306e\u5408\u6210\n\n    transformed = composed @ square\n    ax.plot(square[0], square[1], 'b-', linewidth=2, \n           label='\u5143\u306e\u5f62', alpha=0.5)\n    ax.plot(transformed[0], transformed[1], 'g-', linewidth=2,\n           label='\u56de\u8ee2\u2192\u62e1\u5927')\n\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='k', linewidth=0.5)\n    ax.axvline(x=0, color='k', linewidth=0.5)\n    ax.legend()\n    ax.set_aspect('equal')\n    ax.set_title('\u5408\u6210\u5909\u63db')\n\n    plt.tight_layout()\n    plt.show()\n\ndef matrix_multiplication_intuition(self):\n    \"\"\"\u884c\u5217\u7a4d\u306e\u76f4\u611f\u7684\u7406\u89e3\"\"\"\n\n    print(\"\\n=== \u884c\u5217\u7a4d\u306e\u610f\u5473 ===\")\n\n    # \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6587\u8108\u3067\n    class SimpleLayer:\n        def __init__(self, input_dim, output_dim):\n            # \u91cd\u307f\u884c\u5217\n            self.W = np.random.randn(output_dim, input_dim) * 0.1\n            self.b = np.zeros(output_dim)\n\n        def forward(self, x):\n            \"\"\"\n            x: [batch_size, input_dim]\n            W: [output_dim, input_dim]\n            output: [batch_size, output_dim]\n            \"\"\"\n            # \u884c\u5217\u7a4d\u306e\u5404\u8981\u7d20\u306f\u5185\u7a4d\n            output = x @ self.W.T + self.b\n            return output\n\n        def visualize_computation(self, x):\n            \"\"\"\u8a08\u7b97\u904e\u7a0b\u3092\u53ef\u8996\u5316\"\"\"\n            batch_size = x.shape[0]\n\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n            # 1. \u5165\u529b\n            ax = axes[0]\n            im1 = ax.imshow(x, cmap='Blues', aspect='auto')\n            ax.set_title('\u5165\u529b x')\n            ax.set_ylabel('\u30d0\u30c3\u30c1')\n            ax.set_xlabel('\u5165\u529b\u6b21\u5143')\n            plt.colorbar(im1, ax=ax)\n\n            # 2. \u91cd\u307f\u884c\u5217\n            ax = axes[1]\n            im2 = ax.imshow(self.W, cmap='RdBu', aspect='auto')\n            ax.set_title('\u91cd\u307f\u884c\u5217 W')\n            ax.set_ylabel('\u51fa\u529b\u6b21\u5143')\n            ax.set_xlabel('\u5165\u529b\u6b21\u5143')\n            plt.colorbar(im2, ax=ax)\n\n            # 3. \u51fa\u529b\n            ax = axes[2]\n            output = self.forward(x)\n            im3 = ax.imshow(output, cmap='Greens', aspect='auto')\n            ax.set_title('\u51fa\u529b y = xW^T + b')\n            ax.set_ylabel('\u30d0\u30c3\u30c1')\n            ax.set_xlabel('\u51fa\u529b\u6b21\u5143')\n            plt.colorbar(im3, ax=ax)\n\n            plt.tight_layout()\n            plt.show()\n\n            # 1\u3064\u306e\u51fa\u529b\u8981\u7d20\u306e\u8a08\u7b97\u3092\u8a73\u7d30\u306b\u8868\u793a\n            print(\"\\n\u51fa\u529b\u306e1\u8981\u7d20\u306e\u8a08\u7b97\u8a73\u7d30:\")\n            print(f\"output[0,0] = \u03a3(x[0,i] * W[0,i]) + b[0]\")\n            dot_product = sum(x[0, i] * self.W[0, i] for i in range(x.shape[1]))\n            print(f\"= {dot_product:.3f} + {self.b[0]:.3f}\")\n            print(f\"= {dot_product + self.b[0]:.3f}\")\n            print(f\"\u5b9f\u969b\u306e\u5024: {output[0, 0]:.3f}\")\n\n    # \u30c7\u30e2\n    layer = SimpleLayer(input_dim=5, output_dim=3)\n    x = np.random.randn(4, 5)  # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba4\u3001\u5165\u529b\u6b21\u51435\n    layer.visualize_computation(x)\n\ndef eigenvalues_and_eigenvectors(self):\n    \"\"\"\u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff1a\u884c\u5217\u306e\u300c\u672c\u8cea\u300d\"\"\"\n\n    print(\"\\n=== \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb ===\")\n    print(\"\u56fa\u6709\u30d9\u30af\u30c8\u30eb v \u306b\u5bfe\u3057\u3066: Av = \u03bbv\")\n    print(\"\u3064\u307e\u308a\u3001\u884c\u5217A\u306f\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u3092\u5909\u3048\u306a\u3044\uff08\u5927\u304d\u3055\u3060\u3051\u5909\u3048\u308b\uff09\")\n\n    # \u4f8b\uff1a\u5171\u5206\u6563\u884c\u5217\uff08\u30c7\u30fc\u30bf\u306e\u4e3b\u8981\u306a\u65b9\u5411\u3092\u8868\u3059\uff09\n    # \u30c7\u30fc\u30bf\u751f\u6210\n    np.random.seed(42)\n    mean = [2, 3]\n    cov = [[2, 1.5], [1.5, 1]]\n    data = np.random.multivariate_normal(mean, cov, 1000)\n\n    # \u5171\u5206\u6563\u884c\u5217\u3092\u8a08\u7b97\n    data_centered = data - np.mean(data, axis=0)\n    cov_matrix = np.cov(data_centered.T)\n\n    # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n    # \u53ef\u8996\u5316\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # 1. \u30c7\u30fc\u30bf\u3068\u4e3b\u6210\u5206\n    ax = axes[0]\n    ax.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\n\n    # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a\uff08\u4e3b\u6210\u5206\u306e\u65b9\u5411\uff09\n    center = np.mean(data, axis=0)\n    for i in range(2):\n        # \u56fa\u6709\u5024\u306e\u5927\u304d\u3055\u306b\u6bd4\u4f8b\u3057\u305f\u9577\u3055\u3067\u8868\u793a\n        scale = np.sqrt(eigenvalues[i]) * 2\n        eigvec = eigenvectors[:, i]\n        ax.arrow(center[0], center[1],\n                eigvec[0] * scale, eigvec[1] * scale,\n                head_width=0.2, head_length=0.1,\n                fc=f'C{i}', ec=f'C{i}', linewidth=2,\n                label=f'\u56fa\u6709\u5024 {eigenvalues[i]:.2f}')\n\n    ax.set_title('\u30c7\u30fc\u30bf\u306e\u4e3b\u6210\u5206\uff08\u56fa\u6709\u30d9\u30af\u30c8\u30eb\uff09')\n    ax.legend()\n    ax.set_aspect('equal')\n    ax.grid(True, alpha=0.3)\n\n    # 2. \u56fa\u6709\u5024\u306e\u89e3\u91c8\n    ax = axes[1]\n    ax.bar([0, 1], eigenvalues)\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels(['\u7b2c1\u4e3b\u6210\u5206', '\u7b2c2\u4e3b\u6210\u5206'])\n    ax.set_ylabel('\u56fa\u6709\u5024\uff08\u5206\u6563\uff09')\n    ax.set_title('\u5404\u4e3b\u6210\u5206\u306e\u91cd\u8981\u5ea6')\n\n    # \u5bc4\u4e0e\u7387\u3092\u8868\u793a\n    total_var = sum(eigenvalues)\n    for i, (x, y) in enumerate(zip([0, 1], eigenvalues)):\n        ratio = y / total_var * 100\n        ax.text(x, y + 0.05, f'{ratio:.1f}%', ha='center')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"\\n\u5171\u5206\u6563\u884c\u5217:\\n{cov_matrix}\")\n    print(f\"\\n\u56fa\u6709\u5024: {eigenvalues}\")\n    print(f\"\u56fa\u6709\u30d9\u30af\u30c8\u30eb:\\n{eigenvectors}\")\n    print(f\"\\n\u7b2c1\u4e3b\u6210\u5206\u306e\u5bc4\u4e0e\u7387: {eigenvalues[0]/sum(eigenvalues)*100:.1f}%\")\n</code></pre> <p>class AttentionMathematics:     \"\"\"Attention\u6a5f\u69cb\u306e\u6570\u5b66\u7684\u57fa\u790e\"\"\"</p> <pre><code>def __init__(self, d_model=64):\n    self.d_model = d_model\n    self.scale = np.sqrt(d_model)\n\ndef dot_product_attention(self):\n    \"\"\"\u5185\u7a4d\u6ce8\u610f\u306e\u6570\u5b66\"\"\"\n\n    print(\"=== \u5185\u7a4d\u6ce8\u610f\uff08Dot Product Attention\uff09===\")\n\n    # \u7c21\u5358\u306a\u4f8b\u3067\u8aac\u660e\n    seq_len = 5\n    d_k = 4\n\n    # Query, Key, Value\n    Q = np.random.randn(seq_len, d_k)\n    K = np.random.randn(seq_len, d_k)\n    V = np.random.randn(seq_len, d_k)\n\n    # \u30b9\u30c6\u30c3\u30d71: Q\u3068K\u306e\u5185\u7a4d\u3067\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\n    scores = Q @ K.T  # [seq_len, seq_len]\n    print(f\"1. \u30b9\u30b3\u30a2\u884c\u5217\u306e\u5f62\u72b6: {scores.shape}\")\n    print(f\"   scores[i,j] = Q[i] \u00b7 K[j] \uff08\u30af\u30a8\u30eai\u3068\u30ad\u30fcj\u306e\u985e\u4f3c\u5ea6\uff09\")\n\n    # \u30b9\u30c6\u30c3\u30d72: \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n    scaled_scores = scores / self.scale\n    print(f\"\\n2. \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0: \u9664\u7b97 by sqrt({d_k}) = {self.scale:.2f}\")\n    print(f\"   \u7406\u7531: \u5185\u7a4d\u306e\u5024\u304c\u5927\u304d\u304f\u306a\u308a\u3059\u304e\u308b\u306e\u3092\u9632\u3050\")\n\n    # \u30b9\u30c6\u30c3\u30d73: Softmax\n    attention_weights = self.softmax(scaled_scores)\n    print(f\"\\n3. Softmax: \u5404\u884c\u306e\u548c\u304c1\u306b\u306a\u308b\u78ba\u7387\u5206\u5e03\u306b\u5909\u63db\")\n    print(f\"   attention_weights[i] = softmax(scaled_scores[i])\")\n\n    # \u30b9\u30c6\u30c3\u30d74: \u91cd\u307f\u4ed8\u304d\u548c\n    output = attention_weights @ V\n    print(f\"\\n4. \u51fa\u529b: \u91cd\u307f\u4ed8\u304d\u548c\")\n    print(f\"   output[i] = \u03a3_j (attention_weights[i,j] * V[j])\")\n\n    # \u53ef\u8996\u5316\n    self.visualize_attention_computation(Q, K, V, scores, attention_weights, output)\n\n    return output, attention_weights\n\ndef softmax(self, x):\n    \"\"\"Softmax\u95a2\u6570\u306e\u5b9f\u88c5\"\"\"\n    # \u6570\u5024\u7684\u5b89\u5b9a\u6027\u306e\u305f\u3081\u6700\u5927\u5024\u3092\u5f15\u304f\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef visualize_attention_computation(self, Q, K, V, scores, weights, output):\n    \"\"\"Attention\u8a08\u7b97\u306e\u53ef\u8996\u5316\"\"\"\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n    # Query\n    ax = axes[0, 0]\n    im = ax.imshow(Q, cmap='Blues', aspect='auto')\n    ax.set_title('Query (Q)')\n    ax.set_ylabel('\u4f4d\u7f6e')\n    ax.set_xlabel('\u6b21\u5143')\n    plt.colorbar(im, ax=ax)\n\n    # Key\n    ax = axes[0, 1]\n    im = ax.imshow(K, cmap='Oranges', aspect='auto')\n    ax.set_title('Key (K)')\n    ax.set_ylabel('\u4f4d\u7f6e')\n    ax.set_xlabel('\u6b21\u5143')\n    plt.colorbar(im, ax=ax)\n\n    # Value\n    ax = axes[0, 2]\n    im = ax.imshow(V, cmap='Greens', aspect='auto')\n    ax.set_title('Value (V)')\n    ax.set_ylabel('\u4f4d\u7f6e')\n    ax.set_xlabel('\u6b21\u5143')\n    plt.colorbar(im, ax=ax)\n\n    # \u30b9\u30b3\u30a2\u884c\u5217\n    ax = axes[1, 0]\n    im = ax.imshow(scores, cmap='RdBu', aspect='auto')\n    ax.set_title('\u30b9\u30b3\u30a2 (Q\u00b7K^T)')\n    ax.set_ylabel('Query\u4f4d\u7f6e')\n    ax.set_xlabel('Key\u4f4d\u7f6e')\n    plt.colorbar(im, ax=ax)\n\n    # Attention\u91cd\u307f\n    ax = axes[1, 1]\n    im = ax.imshow(weights, cmap='hot', aspect='auto')\n    ax.set_title('Attention\u91cd\u307f (Softmax\u5f8c)')\n    ax.set_ylabel('Query\u4f4d\u7f6e')\n    ax.set_xlabel('Key\u4f4d\u7f6e')\n    plt.colorbar(im, ax=ax)\n\n    # \u51fa\u529b\n    ax = axes[1, 2]\n    im = ax.imshow(output, cmap='Purples', aspect='auto')\n    ax.set_title('\u51fa\u529b (\u91cd\u307f\u4ed8\u304dValue)')\n    ax.set_ylabel('\u4f4d\u7f6e')\n    ax.set_xlabel('\u6b21\u5143')\n    plt.colorbar(im, ax=ax)\n\n    plt.tight_layout()\n    plt.show()\n\ndef scaled_attention_importance(self):\n    \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306e\u91cd\u8981\u6027\u3092\u5b9f\u8a3c\"\"\"\n\n    print(\"\\n=== \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306e\u91cd\u8981\u6027 ===\")\n\n    d_k_values = [4, 64, 512]\n    fig, axes = plt.subplots(1, len(d_k_values), figsize=(15, 4))\n\n    for idx, d_k in enumerate(d_k_values):\n        ax = axes[idx]\n\n        # \u30e9\u30f3\u30c0\u30e0\u306a\u30d9\u30af\u30c8\u30eb\n        q = np.random.randn(d_k)\n        k = np.random.randn(d_k)\n\n        # \u5185\u7a4d\n        dot_product = np.dot(q, k)\n\n        # Softmax\u306e\u5165\u529b\u5024\u306e\u7bc4\u56f2\n        x_range = np.linspace(-20, 20, 1000)\n\n        # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306a\u3057\n        scores_no_scale = x_range\n        probs_no_scale = self.softmax(np.array([dot_product, 0]))\n\n        # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3042\u308a\n        scale = np.sqrt(d_k)\n        scores_scaled = x_range / scale\n        probs_scaled = self.softmax(np.array([dot_product / scale, 0]))\n\n        # Softmax\u95a2\u6570\u3092\u30d7\u30ed\u30c3\u30c8\n        y_no_scale = np.exp(x_range) / (np.exp(x_range) + 1)\n        y_scaled = np.exp(x_range / scale) / (np.exp(x_range / scale) + 1)\n\n        ax.plot(x_range, y_no_scale, 'r-', label='\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306a\u3057', alpha=0.7)\n        ax.plot(x_range, y_scaled, 'b-', label=f'\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3042\u308a (\u00f7\u221a{d_k})', alpha=0.7)\n\n        # \u5b9f\u969b\u306e\u5185\u7a4d\u5024\u3067\u306e\u78ba\u7387\u3092\u8868\u793a\n        ax.axvline(x=dot_product, color='red', linestyle='--', alpha=0.5)\n        ax.axvline(x=dot_product/scale, color='blue', linestyle='--', alpha=0.5)\n\n        ax.set_title(f'd_k = {d_k}')\n        ax.set_xlabel('\u30b9\u30b3\u30a2')\n        ax.set_ylabel('Softmax\u51fa\u529b')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # \u78ba\u7387\u5024\u3092\u8868\u793a\n        ax.text(0.5, 0.9, f'\u5185\u7a4d: {dot_product:.2f}', \n               transform=ax.transAxes, ha='center')\n        ax.text(0.5, 0.85, f'P(\u30b9\u30b1\u30fc\u30eb\u306a\u3057): {probs_no_scale[0]:.3f}', \n               transform=ax.transAxes, ha='center', color='red')\n        ax.text(0.5, 0.8, f'P(\u30b9\u30b1\u30fc\u30eb\u3042\u308a): {probs_scaled[0]:.3f}', \n               transform=ax.transAxes, ha='center', color='blue')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u89b3\u5bdf:\")\n    print(\"- d_k\u304c\u5927\u304d\u3044\u307b\u3069\u3001\u5185\u7a4d\u306e\u5024\u304c\u5927\u304d\u304f\u306a\u308b\u50be\u5411\")\n    print(\"- \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306a\u3057\u3067\u306f\u3001Softmax\u304c\u98fd\u548c\u3057\u3066\u52fe\u914d\u6d88\u5931\")\n    print(\"- \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306b\u3088\u308a\u3001\u9069\u5207\u306a\u52fe\u914d\u304c\u4fdd\u305f\u308c\u308b\")\n</code></pre>"},{"location":"part1/math-basics/#32","title":"3.2 \u78ba\u7387\u30fb\u7d71\u8a08\u306e\u5b9f\u8df5\u7684\u7406\u89e3","text":""},{"location":"part1/math-basics/#_4","title":"\u78ba\u7387\u5206\u5e03\uff1a\u4e0d\u78ba\u5b9f\u6027\u306e\u8868\u73fe","text":"<p>```python class ProbabilityDistributions:     \"\"\"\u78ba\u7387\u5206\u5e03\u306e\u76f4\u611f\u7684\u7406\u89e3\"\"\"</p> <pre><code>def __init__(self):\n    self.examples = {\n        \"discrete\": \"\u96e2\u6563\u5206\u5e03\uff08\u5358\u8a9e\u306e\u51fa\u73fe\u78ba\u7387\u306a\u3069\uff09\",\n        \"continuous\": \"\u9023\u7d9a\u5206\u5e03\uff08\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5206\u5e03\u306a\u3069\uff09\",\n        \"multivariate\": \"\u591a\u5909\u91cf\u5206\u5e03\uff08\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u306e\u5206\u5e03\u306a\u3069\uff09\"\n    }\n\ndef discrete_distributions(self):\n    \"\"\"\u96e2\u6563\u78ba\u7387\u5206\u5e03\"\"\"\n\n    print(\"=== \u96e2\u6563\u78ba\u7387\u5206\u5e03 ===\")\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. \u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\uff08\u5358\u8a9e\u306e\u51fa\u73fe\u78ba\u7387\uff09\n    ax = axes[0, 0]\n    words = ['the', 'is', 'a', 'of', 'and', 'to', 'in', 'that']\n    probs = [0.25, 0.15, 0.12, 0.10, 0.08, 0.08, 0.07, 0.05]\n    remaining = 1 - sum(probs)\n    words.append('others')\n    probs.append(remaining)\n\n    ax.bar(words, probs, color='skyblue')\n    ax.set_title('\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\uff08\u5358\u8a9e\u306e\u51fa\u73fe\u78ba\u7387\uff09')\n    ax.set_ylabel('\u78ba\u7387')\n    ax.set_xticklabels(words, rotation=45)\n\n    # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\n    entropy = -sum(p * np.log(p) for p in probs if p &gt; 0)\n    ax.text(0.7, 0.9, f'\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc: {entropy:.2f}', \n           transform=ax.transAxes,\n           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n\n    # 2. \u4e8c\u9805\u5206\u5e03\uff08\u6210\u529f/\u5931\u6557\u306e\u56de\u6570\uff09\n    ax = axes[0, 1]\n    n = 20  # \u8a66\u884c\u56de\u6570\n    p = 0.3  # \u6210\u529f\u78ba\u7387\n    x = np.arange(0, n+1)\n\n    from scipy import stats\n    pmf = stats.binom.pmf(x, n, p)\n\n    ax.bar(x, pmf, color='lightgreen')\n    ax.set_title(f'\u4e8c\u9805\u5206\u5e03 (n={n}, p={p})')\n    ax.set_xlabel('\u6210\u529f\u56de\u6570')\n    ax.set_ylabel('\u78ba\u7387')\n\n    # \u671f\u5f85\u5024\u3068\u5206\u6563\n    mean = n * p\n    var = n * p * (1 - p)\n    ax.axvline(x=mean, color='red', linestyle='--', \n              label=f'\u671f\u5f85\u5024: {mean:.1f}')\n    ax.legend()\n\n    # 3. \u30dd\u30a2\u30bd\u30f3\u5206\u5e03\uff08\u7a00\u306a\u4e8b\u8c61\u306e\u767a\u751f\u56de\u6570\uff09\n    ax = axes[1, 0]\n    lambda_ = 3  # \u5e73\u5747\u767a\u751f\u7387\n    x = np.arange(0, 15)\n    pmf = stats.poisson.pmf(x, lambda_)\n\n    ax.bar(x, pmf, color='lightcoral')\n    ax.set_title(f'\u30dd\u30a2\u30bd\u30f3\u5206\u5e03 (\u03bb={lambda_})')\n    ax.set_xlabel('\u767a\u751f\u56de\u6570')\n    ax.set_ylabel('\u78ba\u7387')\n\n    # 4. \u5b9f\u969b\u306e\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u5358\u8a9e\u9577\u5206\u5e03\n    ax = axes[1, 1]\n    text = \"\"\"Transformers have revolutionized natural language processing \n              by introducing self-attention mechanisms that capture long-range \n              dependencies in text without recurrence or convolution.\"\"\"\n\n    words = text.split()\n    lengths = [len(word) for word in words]\n\n    unique_lengths, counts = np.unique(lengths, return_counts=True)\n    probs = counts / len(lengths)\n\n    ax.bar(unique_lengths, probs, color='mediumpurple')\n    ax.set_title('\u5b9f\u969b\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u5358\u8a9e\u9577\u5206\u5e03')\n    ax.set_xlabel('\u5358\u8a9e\u9577')\n    ax.set_ylabel('\u76f8\u5bfe\u983b\u5ea6')\n\n    # \u7d71\u8a08\u91cf\n    mean_length = np.mean(lengths)\n    ax.axvline(x=mean_length, color='red', linestyle='--',\n              label=f'\u5e73\u5747: {mean_length:.1f}')\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef continuous_distributions(self):\n    \"\"\"\u9023\u7d9a\u78ba\u7387\u5206\u5e03\"\"\"\n\n    print(\"\\n=== \u9023\u7d9a\u78ba\u7387\u5206\u5e03 ===\")\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n    # 1. \u6b63\u898f\u5206\u5e03\uff08\u30ac\u30a6\u30b9\u5206\u5e03\uff09\n    ax = axes[0, 0]\n    x = np.linspace(-4, 4, 100)\n\n    # \u7570\u306a\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u306e\u6b63\u898f\u5206\u5e03\n    params = [(0, 1, '\u03bc=0, \u03c3=1'), (0, 2, '\u03bc=0, \u03c3=2'), (2, 1, '\u03bc=2, \u03c3=1')]\n\n    for mu, sigma, label in params:\n        y = stats.norm.pdf(x, mu, sigma)\n        ax.plot(x, y, label=label, linewidth=2)\n\n    ax.set_title('\u6b63\u898f\u5206\u5e03')\n    ax.set_xlabel('x')\n    ax.set_ylabel('\u78ba\u7387\u5bc6\u5ea6')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # 2. \u30d1\u30e9\u30e1\u30fc\u30bf\u521d\u671f\u5316\u3067\u306e\u5229\u7528\n    ax = axes[0, 1]\n\n    # Xavier/He\u521d\u671f\u5316\u306e\u6bd4\u8f03\n    fan_in = 100\n    fan_out = 50\n\n    # Xavier\u521d\u671f\u5316\uff08\u6d3b\u6027\u5316\u95a2\u6570: tanh, sigmoid\uff09\n    xavier_std = np.sqrt(2 / (fan_in + fan_out))\n\n    # He\u521d\u671f\u5316\uff08\u6d3b\u6027\u5316\u95a2\u6570: ReLU\uff09\n    he_std = np.sqrt(2 / fan_in)\n\n    x = np.linspace(-0.5, 0.5, 1000)\n\n    xavier_dist = stats.norm.pdf(x, 0, xavier_std)\n    he_dist = stats.norm.pdf(x, 0, he_std)\n    uniform_dist = stats.uniform.pdf(x, -0.5, 1)\n\n    ax.plot(x, xavier_dist, label=f'Xavier (\u03c3={xavier_std:.3f})')\n    ax.plot(x, he_dist, label=f'He (\u03c3={he_std:.3f})')\n    ax.plot(x, uniform_dist * 2, label='\u4e00\u69d8\u5206\u5e03', linestyle='--')\n\n    ax.set_title('\u30d1\u30e9\u30e1\u30fc\u30bf\u521d\u671f\u5316\u306e\u5206\u5e03')\n    ax.set_xlabel('\u91cd\u307f\u306e\u5024')\n    ax.set_ylabel('\u78ba\u7387\u5bc6\u5ea6')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # 3. t\u5206\u5e03\uff08\u30ed\u30d0\u30b9\u30c8\u306a\u63a8\u5b9a\uff09\n    ax = axes[1, 0]\n    x = np.linspace(-4, 4, 100)\n\n    # \u81ea\u7531\u5ea6\u306b\u3088\u308b\u5f62\u72b6\u306e\u5909\u5316\n    dfs = [1, 3, 10, 30]\n\n    for df in dfs:\n        y = stats.t.pdf(x, df)\n        ax.plot(x, y, label=f'df={df}', linewidth=2)\n\n    # \u6b63\u898f\u5206\u5e03\u3068\u6bd4\u8f03\n    y_norm = stats.norm.pdf(x, 0, 1)\n    ax.plot(x, y_norm, 'k--', label='\u6b63\u898f\u5206\u5e03', linewidth=2)\n\n    ax.set_title('t\u5206\u5e03\uff08\u88fe\u304c\u539a\u3044\u5206\u5e03\uff09')\n    ax.set_xlabel('x')\n    ax.set_ylabel('\u78ba\u7387\u5bc6\u5ea6')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # 4. \u5b9f\u969b\u306e\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5206\u5e03\n    ax = axes[1, 1]\n\n    # \u4eee\u60f3\u7684\u306a\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    np.random.seed(42)\n\n    # \u521d\u671f\u5316\u6642\n    initial_params = np.random.normal(0, 0.1, 10000)\n\n    # \u5b66\u7fd2\u5f8c\uff08\u4eee\u60f3\u7684\uff09\n    trained_params = np.concatenate([\n        np.random.normal(-0.5, 0.05, 3000),  # \u8ca0\u306e\u91cd\u307f\n        np.random.normal(0, 0.02, 4000),     # \u30bc\u30ed\u4ed8\u8fd1\n        np.random.normal(0.5, 0.05, 3000)    # \u6b63\u306e\u91cd\u307f\n    ])\n\n    ax.hist(initial_params, bins=50, alpha=0.5, density=True, \n           label='\u521d\u671f\u5316\u6642', color='blue')\n    ax.hist(trained_params, bins=50, alpha=0.5, density=True,\n           label='\u5b66\u7fd2\u5f8c', color='red')\n\n    ax.set_title('\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u5206\u5e03\u5909\u5316')\n    ax.set_xlabel('\u30d1\u30e9\u30e1\u30fc\u30bf\u5024')\n    ax.set_ylabel('\u5bc6\u5ea6')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\ndef softmax_as_probability(self):\n    \"\"\"Softmax\u95a2\u6570\u3068\u78ba\u7387\u5206\u5e03\"\"\"\n\n    print(\"\\n=== Softmax\u95a2\u6570\uff1a\u30b9\u30b3\u30a2\u304b\u3089\u78ba\u7387\u3078 ===\")\n\n    def softmax(x, temperature=1.0):\n        \"\"\"\u6e29\u5ea6\u4ed8\u304dSoftmax\"\"\"\n        x = x / temperature\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / np.sum(exp_x)\n\n    # \u30ed\u30b8\u30c3\u30c8\uff08\u30b9\u30b3\u30a2\uff09\n    logits = np.array([2.0, 1.0, 0.1, -1.0, -2.0])\n    labels = ['very_pos', 'pos', 'neutral', 'neg', 'very_neg']\n\n    # \u7570\u306a\u308b\u6e29\u5ea6\u3067\u306eSoftmax\n    temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n\n    fig, axes = plt.subplots(1, len(temperatures), figsize=(20, 4))\n\n    for ax, temp in zip(axes, temperatures):\n        probs = softmax(logits, temperature=temp)\n\n        bars = ax.bar(labels, probs, color='lightblue')\n        ax.set_title(f'Temperature = {temp}')\n        ax.set_ylabel('\u78ba\u7387')\n        ax.set_ylim(0, 1)\n\n        # \u5024\u3092\u8868\u793a\n        for bar, prob in zip(bars, probs):\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                   f'{prob:.3f}', ha='center', va='bottom')\n\n        # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\n        entropy = -np.sum(probs * np.log(probs + 1e-10))\n        ax.text(0.5, 0.9, f'H = {entropy:.2f}',\n               transform=ax.transAxes, ha='center',\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u52b9\u679c:\")\n    print(\"- \u4f4e\u6e29\uff08T &lt; 1\uff09: \u3088\u308a\u78ba\u4fe1\u7684\u306a\u5206\u5e03\uff08\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u4f4e\uff09\")\n    print(\"- \u9ad8\u6e29\uff08T &gt; 1\uff09: \u3088\u308a\u4e00\u69d8\u306a\u5206\u5e03\uff08\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u9ad8\uff09\")\n    print(\"- T = 1: \u6a19\u6e96\u7684\u306aSoftmax\")\n\ndef cross_entropy_loss(self):\n    \"\"\"\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u7406\u89e3\"\"\"\n\n    print(\"\\n=== \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 ===\")\n\n    # \u771f\u306e\u5206\u5e03\uff08one-hot\uff09\n    true_dist = np.array([0, 0, 1, 0, 0])  # \u30af\u30e9\u30b92\u304c\u6b63\u89e3\n\n    # \u4e88\u6e2c\u5206\u5e03\u306e\u4f8b\n    predictions = [\n        np.array([0.1, 0.1, 0.6, 0.1, 0.1]),  # \u826f\u3044\u4e88\u6e2c\n        np.array([0.2, 0.2, 0.2, 0.2, 0.2]),  # \u4e0d\u78ba\u5b9f\u306a\u4e88\u6e2c\n        np.array([0.6, 0.1, 0.1, 0.1, 0.1]),  # \u60aa\u3044\u4e88\u6e2c\n    ]\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    for ax, pred in zip(axes, predictions):\n        # \u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\n        ce_loss = -np.sum(true_dist * np.log(pred + 1e-10))\n\n        # \u53ef\u8996\u5316\n        x = np.arange(len(true_dist))\n        width = 0.35\n\n        bars1 = ax.bar(x - width/2, true_dist, width, \n                       label='\u771f\u306e\u5206\u5e03', alpha=0.7)\n        bars2 = ax.bar(x + width/2, pred, width,\n                       label='\u4e88\u6e2c\u5206\u5e03', alpha=0.7)\n\n        ax.set_title(f'\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc: {ce_loss:.3f}')\n        ax.set_xlabel('\u30af\u30e9\u30b9')\n        ax.set_ylabel('\u78ba\u7387')\n        ax.set_xticks(x)\n        ax.legend()\n\n        # \u6b63\u89e3\u30af\u30e9\u30b9\u306e\u4e88\u6e2c\u78ba\u7387\u3092\u5f37\u8abf\n        correct_class = np.argmax(true_dist)\n        ax.text(correct_class, pred[correct_class] + 0.05,\n               f'{pred[correct_class]:.2f}',\n               ha='center', fontweight='bold', color='red')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u306e\u5f0f:\")\n    print(\"H(p, q) = -\u03a3 p(x) log q(x)\")\n    print(\"- p: \u771f\u306e\u5206\u5e03\uff08\u901a\u5e38one-hot\uff09\")\n    print(\"- q: \u4e88\u6e2c\u5206\u5e03\")\n    print(\"- \u6b63\u89e3\u30af\u30e9\u30b9\u306e\u4e88\u6e2c\u78ba\u7387\u304c\u9ad8\u3044\u307b\u3069\u640d\u5931\u304c\u5c0f\u3055\u3044\")\n</code></pre> <p>class StatisticalConcepts:     \"\"\"\u7d71\u8a08\u7684\u6982\u5ff5\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    pass\n\ndef expectation_and_variance(self):\n    \"\"\"\u671f\u5f85\u5024\u3068\u5206\u6563\"\"\"\n\n    print(\"=== \u671f\u5f85\u5024\u3068\u5206\u6563 ===\")\n\n    # Layer Normalization\u3067\u306e\u4f7f\u7528\u4f8b\n    class LayerNorm:\n        def __init__(self, eps=1e-6):\n            self.eps = eps\n\n        def forward(self, x):\n            \"\"\"\n            x: [batch_size, seq_len, d_model]\n            \"\"\"\n            # \u6700\u5f8c\u306e\u6b21\u5143\u3067\u7d71\u8a08\u91cf\u3092\u8a08\u7b97\n            mean = np.mean(x, axis=-1, keepdims=True)\n            var = np.var(x, axis=-1, keepdims=True)\n\n            # \u6b63\u898f\u5316\n            x_normalized = (x - mean) / np.sqrt(var + self.eps)\n\n            return x_normalized, mean, var\n\n        def visualize_normalization(self, x):\n            \"\"\"\u6b63\u898f\u5316\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\"\"\"\n            x_norm, mean, var = self.forward(x)\n\n            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n            # \u5143\u306e\u5206\u5e03\n            ax = axes[0, 0]\n            ax.hist(x.flatten(), bins=50, alpha=0.7, density=True)\n            ax.set_title('\u5143\u306e\u5206\u5e03')\n            ax.set_xlabel('\u5024')\n            ax.set_ylabel('\u5bc6\u5ea6')\n\n            orig_mean = np.mean(x)\n            orig_std = np.std(x)\n            ax.axvline(x=orig_mean, color='red', linestyle='--',\n                      label=f'\u03bc={orig_mean:.2f}')\n            ax.axvline(x=orig_mean + orig_std, color='green', linestyle='--',\n                      label=f'\u03c3={orig_std:.2f}')\n            ax.axvline(x=orig_mean - orig_std, color='green', linestyle='--')\n            ax.legend()\n\n            # \u6b63\u898f\u5316\u5f8c\u306e\u5206\u5e03\n            ax = axes[0, 1]\n            ax.hist(x_norm.flatten(), bins=50, alpha=0.7, density=True)\n            ax.set_title('\u6b63\u898f\u5316\u5f8c\u306e\u5206\u5e03')\n            ax.set_xlabel('\u5024')\n            ax.set_ylabel('\u5bc6\u5ea6')\n\n            norm_mean = np.mean(x_norm)\n            norm_std = np.std(x_norm)\n            ax.axvline(x=norm_mean, color='red', linestyle='--',\n                      label=f'\u03bc={norm_mean:.2f}')\n            ax.axvline(x=norm_mean + norm_std, color='green', linestyle='--',\n                      label=f'\u03c3={norm_std:.2f}')\n            ax.axvline(x=norm_mean - norm_std, color='green', linestyle='--')\n            ax.legend()\n\n            # \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7d71\u8a08\u91cf\n            ax = axes[1, 0]\n            sample_means = mean.flatten()\n            sample_vars = var.flatten()\n\n            scatter = ax.scatter(sample_means, sample_vars, alpha=0.6)\n            ax.set_xlabel('\u5e73\u5747')\n            ax.set_ylabel('\u5206\u6563')\n            ax.set_title('\u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7d71\u8a08\u91cf')\n            ax.grid(True, alpha=0.3)\n\n            # \u6b63\u898f\u5316\u306e\u5b89\u5b9a\u6027\n            ax = axes[1, 1]\n\n            # \u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u306e\u30c7\u30fc\u30bf\u3067\u6bd4\u8f03\n            scales = [0.1, 1.0, 10.0, 100.0]\n            colors = plt.cm.viridis(np.linspace(0, 1, len(scales)))\n\n            for scale, color in zip(scales, colors):\n                x_scaled = x * scale\n                x_scaled_norm, _, _ = self.forward(x_scaled)\n\n                ax.hist(x_scaled_norm.flatten(), bins=30, alpha=0.5,\n                       density=True, color=color,\n                       label=f'scale={scale}')\n\n            ax.set_title('\u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u3067\u3082\u540c\u3058\u5206\u5e03\u306b\u6b63\u898f\u5316')\n            ax.set_xlabel('\u6b63\u898f\u5316\u5f8c\u306e\u5024')\n            ax.set_ylabel('\u5bc6\u5ea6')\n            ax.legend()\n\n            plt.tight_layout()\n            plt.show()\n\n    # \u30c7\u30e2\n    ln = LayerNorm()\n\n    # \u30d0\u30c3\u30c1\u30c7\u30fc\u30bf\uff08\u7570\u306a\u308b\u7d71\u8a08\u91cf\u3092\u6301\u3064\uff09\n    batch_size, seq_len, d_model = 32, 10, 64\n    x = np.random.randn(batch_size, seq_len, d_model)\n\n    # \u4e00\u90e8\u306e\u30b5\u30f3\u30d7\u30eb\u306b\u7570\u306a\u308b\u30b9\u30b1\u30fc\u30eb\u3092\u9069\u7528\n    x[10:15] *= 5.0  # \u5927\u304d\u306a\u5024\n    x[20:25] *= 0.1  # \u5c0f\u3055\u306a\u5024\n\n    ln.visualize_normalization(x)\n\ndef correlation_and_covariance(self):\n    \"\"\"\u76f8\u95a2\u3068\u5171\u5206\u6563\"\"\"\n\n    print(\"\\n=== \u76f8\u95a2\u3068\u5171\u5206\u6563 ===\")\n\n    # Attention \u3067\u306e\u76f8\u95a2\u306e\u91cd\u8981\u6027\n    def attention_as_correlation():\n        \"\"\"Attention\u3092\u76f8\u95a2\u3068\u3057\u3066\u7406\u89e3\"\"\"\n\n        # \u4f8b\uff1a\u6587\u4e2d\u306e\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\n        sentence = \"The cat sat on the mat\"\n        words = sentence.split()\n        n_words = len(words)\n\n        # \u4eee\u60f3\u7684\u306a\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\n        np.random.seed(42)\n        d_model = 64\n        embeddings = np.random.randn(n_words, d_model)\n\n        # \u7279\u5b9a\u306e\u5358\u8a9e\u3092\u985e\u4f3c\u3055\u305b\u308b\n        embeddings[1] *= 0.8  # cat\n        embeddings[5] *= 0.8  # mat\uff08cat\u3068\u97fb\u3092\u8e0f\u3080\uff09\n\n        # \u76f8\u95a2\u884c\u5217\u3092\u8a08\u7b97\n        correlation_matrix = np.corrcoef(embeddings)\n\n        # Attention \u30b9\u30b3\u30a2\uff08\u6b63\u898f\u5316\u524d\uff09\n        attention_scores = embeddings @ embeddings.T\n\n        # \u53ef\u8996\u5316\n        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u76f8\u95a2\u884c\u5217\n        ax = axes[0]\n        im = ax.imshow(correlation_matrix, cmap='RdBu', vmin=-1, vmax=1)\n        ax.set_xticks(range(n_words))\n        ax.set_yticks(range(n_words))\n        ax.set_xticklabels(words, rotation=45)\n        ax.set_yticklabels(words)\n        ax.set_title('\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u76f8\u95a2\u884c\u5217')\n        plt.colorbar(im, ax=ax)\n\n        # Attention\u30b9\u30b3\u30a2\n        ax = axes[1]\n        im = ax.imshow(attention_scores, cmap='hot')\n        ax.set_xticks(range(n_words))\n        ax.set_yticks(range(n_words))\n        ax.set_xticklabels(words, rotation=45)\n        ax.set_yticklabels(words)\n        ax.set_title('Attention\u30b9\u30b3\u30a2\uff08\u5185\u7a4d\uff09')\n        plt.colorbar(im, ax=ax)\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"\u89b3\u5bdf:\")\n        print(\"- \u76f8\u95a2\u304c\u9ad8\u3044\u5358\u8a9e\u30da\u30a2\u306f\u3001Attention\u30b9\u30b3\u30a2\u3082\u9ad8\u3044\")\n        print(\"- 'cat'\u3068'mat'\u306e\u3088\u3046\u306b\u97fb\u3092\u8e0f\u3080\u5358\u8a9e\u306f\u76f8\u95a2\u304c\u9ad8\u3044\")\n        print(\"- Attention\u306f\u5358\u8a9e\u9593\u306e\u610f\u5473\u7684\u985e\u4f3c\u6027\u3092\u6349\u3048\u308b\")\n\n    attention_as_correlation()\n</code></pre>"},{"location":"part1/math-basics/#33","title":"3.3 \u5fae\u5206\u3068\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3","text":""},{"location":"part1/math-basics/#_5","title":"\u52fe\u914d\uff1a\u95a2\u6570\u306e\u5909\u5316\u306e\u65b9\u5411","text":"<p>```python class GradientConcepts:     \"\"\"\u52fe\u914d\u306e\u6982\u5ff5\u3092\u5b9f\u88c5\u3067\u7406\u89e3\"\"\"</p> <pre><code>def __init__(self):\n    self.history = []\n\ndef gradient_visualization(self):\n    \"\"\"\u52fe\u914d\u306e\u53ef\u8996\u5316\"\"\"\n\n    print(\"=== \u52fe\u914d\uff1a\u6700\u9069\u5316\u306e\u65b9\u5411 ===\")\n\n    # 1\u6b21\u5143\u306e\u4f8b\n    def f1d(x):\n        return x**2 - 4*x + 3\n\n    def df1d_dx(x):\n        return 2*x - 4\n\n    # 2\u6b21\u5143\u306e\u4f8b\uff08\u640d\u5931\u95a2\u6570\u306e\u666f\u89b3\uff09\n    def f2d(x, y):\n        return (x - 2)**2 + (y - 3)**2 + 0.5*x*y\n\n    def grad_f2d(x, y):\n        df_dx = 2*(x - 2) + 0.5*y\n        df_dy = 2*(y - 3) + 0.5*x\n        return df_dx, df_dy\n\n    fig = plt.figure(figsize=(15, 5))\n\n    # 1\u6b21\u5143\u95a2\u6570\u3068\u52fe\u914d\n    ax1 = fig.add_subplot(131)\n    x = np.linspace(-2, 6, 100)\n    y = f1d(x)\n    ax1.plot(x, y, 'b-', linewidth=2, label='f(x) = x\u00b2 - 4x + 3')\n\n    # \u3044\u304f\u3064\u304b\u306e\u70b9\u3067\u306e\u52fe\u914d\n    sample_points = [-1, 0, 2, 4, 5]\n    for x_point in sample_points:\n        y_point = f1d(x_point)\n        grad = df1d_dx(x_point)\n\n        # \u63a5\u7dda\u3092\u63cf\u753b\n        tangent_x = np.linspace(x_point - 0.5, x_point + 0.5, 10)\n        tangent_y = y_point + grad * (tangent_x - x_point)\n        ax1.plot(tangent_x, tangent_y, 'r-', linewidth=1, alpha=0.7)\n\n        # \u52fe\u914d\u306e\u65b9\u5411\u3092\u77e2\u5370\u3067\u8868\u793a\n        arrow_scale = 0.3\n        ax1.arrow(x_point, y_point, \n                 arrow_scale, arrow_scale * grad,\n                 head_width=0.1, head_length=0.05,\n                 fc='green', ec='green')\n\n        ax1.plot(x_point, y_point, 'ro', markersize=8)\n        ax1.text(x_point, y_point - 0.5, f'\u2207={grad:.1f}',\n                ha='center', fontsize=8)\n\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('f(x)')\n    ax1.set_title('1\u6b21\u5143\u95a2\u6570\u306e\u52fe\u914d')\n    ax1.grid(True, alpha=0.3)\n    ax1.legend()\n\n    # 2\u6b21\u5143\u95a2\u6570\u306e\u7b49\u9ad8\u7dda\u3068\u52fe\u914d\n    ax2 = fig.add_subplot(132)\n    x = np.linspace(-1, 5, 100)\n    y = np.linspace(-1, 7, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f2d(X, Y)\n\n    # \u7b49\u9ad8\u7dda\n    contour = ax2.contour(X, Y, Z, levels=20, alpha=0.6)\n    ax2.clabel(contour, inline=True, fontsize=8)\n\n    # \u52fe\u914d\u30d9\u30af\u30c8\u30eb\u5834\n    x_sparse = np.linspace(-1, 5, 10)\n    y_sparse = np.linspace(-1, 7, 10)\n    X_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\n\n    # \u5404\u70b9\u3067\u306e\u52fe\u914d\n    U = np.zeros_like(X_sparse)\n    V = np.zeros_like(Y_sparse)\n\n    for i in range(len(x_sparse)):\n        for j in range(len(y_sparse)):\n            grad_x, grad_y = grad_f2d(X_sparse[j, i], Y_sparse[j, i])\n            U[j, i] = -grad_x  # \u8ca0\u306e\u52fe\u914d\uff08\u964d\u4e0b\u65b9\u5411\uff09\n            V[j, i] = -grad_y\n\n    # \u52fe\u914d\u30d9\u30af\u30c8\u30eb\u3092\u63cf\u753b\n    ax2.quiver(X_sparse, Y_sparse, U, V, color='red', alpha=0.7)\n\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('2\u6b21\u5143\u95a2\u6570\u306e\u52fe\u914d\u30d9\u30af\u30c8\u30eb\u5834')\n    ax2.set_aspect('equal')\n\n    # \u6700\u9069\u5316\u306e\u8ecc\u8de1\n    ax3 = fig.add_subplot(133)\n\n    # \u52fe\u914d\u964d\u4e0b\u6cd5\u306e\u5b9f\u88c5\n    def gradient_descent(start_point, learning_rate=0.1, n_steps=50):\n        trajectory = [start_point]\n        point = np.array(start_point)\n\n        for _ in range(n_steps):\n            grad = np.array(grad_f2d(point[0], point[1]))\n            point = point - learning_rate * grad\n            trajectory.append(point.copy())\n\n        return np.array(trajectory)\n\n    # \u7570\u306a\u308b\u958b\u59cb\u70b9\u304b\u3089\u306e\u8ecc\u8de1\n    start_points = [(-0.5, 6), (4.5, 0), (4, 6)]\n    colors = ['blue', 'green', 'purple']\n\n    # \u7b49\u9ad8\u7dda\u3092\u518d\u63cf\u753b\n    contour = ax3.contour(X, Y, Z, levels=20, alpha=0.3)\n\n    for start, color in zip(start_points, colors):\n        trajectory = gradient_descent(start, learning_rate=0.1)\n        ax3.plot(trajectory[:, 0], trajectory[:, 1], \n                f'{color[0]}-', linewidth=2, label=f'\u958b\u59cb: {start}')\n        ax3.plot(trajectory[0, 0], trajectory[0, 1], \n                f'{color[0]}o', markersize=10)\n        ax3.plot(trajectory[-1, 0], trajectory[-1, 1], \n                f'{color[0]}*', markersize=15)\n\n    # \u6700\u5c0f\u5024\u306e\u4f4d\u7f6e\n    ax3.plot(2, 3, 'r*', markersize=20, label='\u6700\u5c0f\u5024')\n\n    ax3.set_xlabel('x')\n    ax3.set_ylabel('y')\n    ax3.set_title('\u52fe\u914d\u964d\u4e0b\u6cd5\u306e\u8ecc\u8de1')\n    ax3.legend()\n    ax3.set_aspect('equal')\n\n    plt.tight_layout()\n    plt.show()\n\ndef chain_rule_demonstration(self):\n    \"\"\"\u9023\u9396\u5f8b\u306e\u5b9f\u6f14\"\"\"\n\n    print(\"\\n=== \u9023\u9396\u5f8b\uff1a\u8907\u5408\u95a2\u6570\u306e\u5fae\u5206 ===\")\n\n    # \u7c21\u5358\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u8aac\u660e\n    class SimpleNetwork:\n        def __init__(self):\n            # \u30d1\u30e9\u30e1\u30fc\u30bf\n            self.W1 = np.array([[0.5, -0.3], [0.2, 0.8]])\n            self.b1 = np.array([0.1, -0.1])\n            self.W2 = np.array([[0.7], [-0.4]])\n            self.b2 = np.array([0.2])\n\n            # \u4e2d\u9593\u5024\u3092\u4fdd\u5b58\n            self.cache = {}\n\n        def relu(self, x):\n            return np.maximum(0, x)\n\n        def relu_derivative(self, x):\n            return (x &gt; 0).astype(float)\n\n        def forward(self, x):\n            \"\"\"\u9806\u4f1d\u64ad\"\"\"\n            # \u5c641: z1 = W1 @ x + b1\n            self.cache['x'] = x\n            self.cache['z1'] = self.W1 @ x + self.b1\n\n            # \u6d3b\u6027\u5316: a1 = ReLU(z1)\n            self.cache['a1'] = self.relu(self.cache['z1'])\n\n            # \u5c642: z2 = W2 @ a1 + b2\n            self.cache['z2'] = self.W2 @ self.cache['a1'] + self.b2\n\n            # \u51fa\u529b\uff08\u7dda\u5f62\uff09\n            y = self.cache['z2']\n\n            return y\n\n        def backward(self, y, y_true):\n            \"\"\"\u9006\u4f1d\u64ad\uff08\u9023\u9396\u5f8b\u3092\u4f7f\u7528\uff09\"\"\"\n            # \u640d\u5931: L = 0.5 * (y - y_true)^2\n\n            # \u2202L/\u2202y\n            dL_dy = y - y_true\n\n            # \u2202L/\u2202z2 = \u2202L/\u2202y * \u2202y/\u2202z2 = dL_dy * 1\n            dL_dz2 = dL_dy\n\n            # \u2202L/\u2202W2 = \u2202L/\u2202z2 * \u2202z2/\u2202W2 = dL_dz2 * a1^T\n            dL_dW2 = dL_dz2 @ self.cache['a1'].reshape(1, -1)\n\n            # \u2202L/\u2202b2 = \u2202L/\u2202z2 * \u2202z2/\u2202b2 = dL_dz2 * 1\n            dL_db2 = dL_dz2\n\n            # \u2202L/\u2202a1 = \u2202L/\u2202z2 * \u2202z2/\u2202a1 = W2^T @ dL_dz2\n            dL_da1 = self.W2.T @ dL_dz2\n\n            # \u2202L/\u2202z1 = \u2202L/\u2202a1 * \u2202a1/\u2202z1 = dL_da1 * ReLU'(z1)\n            dL_dz1 = dL_da1.flatten() * self.relu_derivative(self.cache['z1'])\n\n            # \u2202L/\u2202W1 = \u2202L/\u2202z1 * \u2202z1/\u2202W1 = dL_dz1 * x^T\n            dL_dW1 = np.outer(dL_dz1, self.cache['x'])\n\n            # \u2202L/\u2202b1 = \u2202L/\u2202z1 * \u2202z1/\u2202b1 = dL_dz1 * 1\n            dL_db1 = dL_dz1\n\n            gradients = {\n                'W1': dL_dW1,\n                'b1': dL_db1,\n                'W2': dL_dW2,\n                'b2': dL_db2\n            }\n\n            return gradients\n\n        def visualize_computation_graph(self):\n            \"\"\"\u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u53ef\u8996\u5316\"\"\"\n            import networkx as nx\n\n            G = nx.DiGraph()\n\n            # \u30ce\u30fc\u30c9\u3092\u8ffd\u52a0\n            nodes = ['x', 'W1', 'b1', 'z1', 'a1', 'W2', 'b2', 'z2', 'y', 'L']\n            node_colors = {\n                'x': 'lightblue',\n                'W1': 'lightgreen', 'b1': 'lightgreen',\n                'W2': 'lightgreen', 'b2': 'lightgreen',\n                'z1': 'lightyellow', 'a1': 'lightyellow',\n                'z2': 'lightyellow', 'y': 'lightyellow',\n                'L': 'lightcoral'\n            }\n\n            for node in nodes:\n                G.add_node(node)\n\n            # \u30a8\u30c3\u30b8\u3092\u8ffd\u52a0\uff08\u8a08\u7b97\u306e\u4f9d\u5b58\u95a2\u4fc2\uff09\n            edges = [\n                ('x', 'z1'), ('W1', 'z1'), ('b1', 'z1'),\n                ('z1', 'a1'),\n                ('a1', 'z2'), ('W2', 'z2'), ('b2', 'z2'),\n                ('z2', 'y'),\n                ('y', 'L')\n            ]\n\n            G.add_edges_from(edges)\n\n            # \u30ec\u30a4\u30a2\u30a6\u30c8\n            pos = {\n                'x': (0, 2),\n                'W1': (-1, 1), 'b1': (1, 1),\n                'z1': (0, 1),\n                'a1': (0, 0),\n                'W2': (-1, -1), 'b2': (1, -1),\n                'z2': (0, -1),\n                'y': (0, -2),\n                'L': (0, -3)\n            }\n\n            plt.figure(figsize=(10, 8))\n\n            # \u30ce\u30fc\u30c9\u3092\u63cf\u753b\n            for node in G.nodes():\n                nx.draw_networkx_nodes(G, pos, nodelist=[node],\n                                     node_color=node_colors[node],\n                                     node_size=1000)\n\n            # \u30a8\u30c3\u30b8\u3092\u63cf\u753b\n            nx.draw_networkx_edges(G, pos, edge_color='gray',\n                                 arrows=True, arrowsize=20)\n\n            # \u30e9\u30d9\u30eb\u3092\u8ffd\u52a0\n            nx.draw_networkx_labels(G, pos, font_size=12)\n\n            # \u52fe\u914d\u306e\u6d41\u308c\u3092\u8868\u793a\n            gradient_edges = [\n                ('L', 'y', '\u2202L/\u2202y'),\n                ('y', 'z2', '\u2202L/\u2202z2'),\n                ('z2', 'W2', '\u2202L/\u2202W2'),\n                ('z2', 'a1', '\u2202L/\u2202a1'),\n                ('a1', 'z1', '\u2202L/\u2202z1'),\n                ('z1', 'W1', '\u2202L/\u2202W1')\n            ]\n\n            for src, dst, label in gradient_edges:\n                # \u9006\u65b9\u5411\u306e\u77e2\u5370\u3067\u52fe\u914d\u3092\u8868\u793a\n                if src in pos and dst in pos:\n                    x1, y1 = pos[dst]\n                    x2, y2 = pos[src]\n                    plt.annotate('', xy=(x1, y1), xytext=(x2, y2),\n                               arrowprops=dict(arrowstyle='&lt;-',\n                                             color='red',\n                                             lw=2,\n                                             alpha=0.7))\n\n            plt.title('\u8a08\u7b97\u30b0\u30e9\u30d5\u3068\u52fe\u914d\u306e\u9006\u4f1d\u64ad')\n            plt.axis('off')\n            plt.tight_layout()\n            plt.show()\n\n    # \u5b9f\u6f14\n    net = SimpleNetwork()\n\n    # \u5165\u529b\n    x = np.array([1.0, 0.5])\n    y_true = np.array([0.8])\n\n    # \u9806\u4f1d\u64ad\n    y = net.forward(x)\n    print(f\"\u5165\u529b: {x}\")\n    print(f\"\u51fa\u529b: {y}\")\n    print(f\"\u76ee\u6a19: {y_true}\")\n\n    # \u9006\u4f1d\u64ad\n    gradients = net.backward(y, y_true)\n\n    print(\"\\n\u52fe\u914d:\")\n    for param, grad in gradients.items():\n        print(f\"\u2202L/\u2202{param} = \\n{grad}\")\n\n    # \u8a08\u7b97\u30b0\u30e9\u30d5\u3092\u53ef\u8996\u5316\n    net.visualize_computation_graph()\n\ndef automatic_differentiation(self):\n    \"\"\"\u81ea\u52d5\u5fae\u5206\u306e\u4ed5\u7d44\u307f\"\"\"\n\n    print(\"\\n=== \u81ea\u52d5\u5fae\u5206 ===\")\n\n    # PyTorch\u3067\u306e\u81ea\u52d5\u5fae\u5206\n    import torch\n\n    # \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9\u3068\u81ea\u52d5\u5fae\u5206\n    x = torch.tensor(2.0, requires_grad=True)\n    y = torch.tensor(3.0, requires_grad=True)\n\n    # \u8907\u96d1\u306a\u95a2\u6570\n    z = x ** 2 + 2 * x * y + y ** 2  # (x + y)^2\n    w = torch.sin(z) + torch.cos(x * y)\n    loss = w ** 2\n\n    # \u9006\u4f1d\u64ad\n    loss.backward()\n\n    print(\"\u81ea\u52d5\u5fae\u5206\u306e\u7d50\u679c:\")\n    print(f\"x = {x.item():.3f}, \u2202loss/\u2202x = {x.grad.item():.3f}\")\n    print(f\"y = {y.item():.3f}, \u2202loss/\u2202y = {y.grad.item():.3f}\")\n\n    # \u6570\u5024\u5fae\u5206\u3068\u306e\u6bd4\u8f03\n    def numerical_gradient(f, x, h=1e-5):\n        \"\"\"\u6570\u5024\u5fae\u5206\uff08\u6709\u9650\u5dee\u5206\u6cd5\uff09\"\"\"\n        return (f(x + h) - f(x - h)) / (2 * h)\n\n    # \u540c\u3058\u95a2\u6570\u3092\u901a\u5e38\u306ePython\u3067\u5b9a\u7fa9\n    def f(x_val, y_val):\n        z = x_val ** 2 + 2 * x_val * y_val + y_val ** 2\n        w = np.sin(z) + np.cos(x_val * y_val)\n        return w ** 2\n\n    # \u6570\u5024\u5fae\u5206\n    dx_numerical = numerical_gradient(lambda x: f(x, 3.0), 2.0)\n    dy_numerical = numerical_gradient(lambda y: f(2.0, y), 3.0)\n\n    print(f\"\\n\u6570\u5024\u5fae\u5206:\")\n    print(f\"\u2202loss/\u2202x \u2248 {dx_numerical:.3f}\")\n    print(f\"\u2202loss/\u2202y \u2248 {dy_numerical:.3f}\")\n\n    print(f\"\\n\u8aa4\u5dee:\")\n    print(f\"|\u81ea\u52d5\u5fae\u5206 - \u6570\u5024\u5fae\u5206|_x = {abs(x.grad.item() - dx_numerical):.6f}\")\n    print(f\"|\u81ea\u52d5\u5fae\u5206 - \u6570\u5024\u5fae\u5206|_y = {abs(y.grad.item() - dy_numerical):.6f}\")\n</code></pre>"},{"location":"part1/math-basics/#34","title":"3.4 \u5b9f\u8df5\uff1a\u6570\u5b66\u3092\u30b3\u30fc\u30c9\u3067\u78ba\u8a8d","text":""},{"location":"part1/math-basics/#transformer","title":"\u5b8c\u5168\u306aTransformer\u30d6\u30ed\u30c3\u30af\u306e\u6570\u5b66","text":"<p>```python class TransformerMathematics:     \"\"\"Transformer\u306e\u6570\u5b66\u7684\u69cb\u6210\u8981\u7d20\"\"\"</p> <pre><code>def __init__(self, d_model=512, n_heads=8, d_ff=2048):\n    self.d_model = d_model\n    self.n_heads = n_heads\n    self.d_ff = d_ff\n    self.d_k = d_model // n_heads\n\ndef complete_transformer_block(self):\n    \"\"\"\u5b8c\u5168\u306aTransformer\u30d6\u30ed\u30c3\u30af\u306e\u6570\u5b66\"\"\"\n\n    print(\"=== Transformer\u30d6\u30ed\u30c3\u30af\u306e\u6570\u5b66\u7684\u69cb\u6210 ===\")\n\n    # \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u6570\u5f0f\n    equations = {\n        \"Multi-Head Attention\": [\n            \"Q = XW_Q, K = XW_K, V = XW_V\",\n            \"head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\",\n            \"MultiHead(Q,K,V) = Concat(head_1,...,head_h)W_O\"\n        ],\n        \"Scaled Dot-Product Attention\": [\n            \"Attention(Q,K,V) = softmax(QK^T / \u221ad_k)V\"\n        ],\n        \"Position-wise Feed Forward\": [\n            \"FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\"\n        ],\n        \"Residual Connection\": [\n            \"output = LayerNorm(x + Sublayer(x))\"\n        ]\n    }\n\n    # \u6570\u5f0f\u3092\u8868\u793a\n    for component, eqs in equations.items():\n        print(f\"\\n{component}:\")\n        for eq in eqs:\n            print(f\"  {eq}\")\n\n    # \u5b9f\u969b\u306e\u8a08\u7b97\u3092\u30b9\u30c6\u30c3\u30d7\u30d0\u30a4\u30b9\u30c6\u30c3\u30d7\u3067\n    self.step_by_step_computation()\n\ndef step_by_step_computation(self):\n    \"\"\"\u30b9\u30c6\u30c3\u30d7\u30d0\u30a4\u30b9\u30c6\u30c3\u30d7\u306e\u8a08\u7b97\"\"\"\n\n    # \u5c0f\u3055\u306a\u4f8b\u3067\u8a08\u7b97\u904e\u7a0b\u3092\u8ffd\u8de1\n    batch_size = 2\n    seq_len = 4\n    d_model = 8  # \u5c0f\u3055\u304f\u3057\u3066\u898b\u3084\u3059\u304f\n    n_heads = 2\n    d_k = d_model // n_heads\n\n    print(\"\\n=== \u5177\u4f53\u4f8b\u3067\u306e\u8a08\u7b97 ===\")\n    print(f\"\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba: {batch_size}\")\n    print(f\"\u30b7\u30fc\u30b1\u30f3\u30b9\u9577: {seq_len}\")\n    print(f\"\u30e2\u30c7\u30eb\u6b21\u5143: {d_model}\")\n    print(f\"\u30d8\u30c3\u30c9\u6570: {n_heads}\")\n\n    # \u5165\u529b\n    X = torch.randn(batch_size, seq_len, d_model)\n\n    # \u91cd\u307f\u884c\u5217\n    W_Q = torch.randn(d_model, d_model)\n    W_K = torch.randn(d_model, d_model)\n    W_V = torch.randn(d_model, d_model)\n    W_O = torch.randn(d_model, d_model)\n\n    # Step 1: Linear projections\n    Q = X @ W_Q\n    K = X @ W_K\n    V = X @ W_V\n\n    print(f\"\\nStep 1: \u7dda\u5f62\u6295\u5f71\")\n    print(f\"Q shape: {Q.shape}\")\n    print(f\"K shape: {K.shape}\")\n    print(f\"V shape: {V.shape}\")\n\n    # Step 2: Reshape for multi-head\n    Q = Q.view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)\n    K = K.view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)\n    V = V.view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)\n\n    print(f\"\\nStep 2: Multi-head\u7528\u306b\u5f62\u72b6\u5909\u66f4\")\n    print(f\"Q shape: {Q.shape} [batch, heads, seq_len, d_k]\")\n\n    # Step 3: Scaled dot-product attention\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n    print(f\"\\nStep 3: \u30b9\u30b3\u30a2\u8a08\u7b97\")\n    print(f\"Scores shape: {scores.shape}\")\n    print(f\"Score[0,0] (\u7b2c1\u30d0\u30c3\u30c1\u3001\u7b2c1\u30d8\u30c3\u30c9):\")\n    print(scores[0, 0])\n\n    # Step 4: Softmax\n    attn_weights = F.softmax(scores, dim=-1)\n\n    print(f\"\\nStep 4: Softmax\")\n    print(f\"Attention weights[0,0]:\")\n    print(attn_weights[0, 0])\n    print(f\"\u5404\u884c\u306e\u548c: {attn_weights[0, 0].sum(dim=-1)}\")\n\n    # Step 5: Apply attention to values\n    context = torch.matmul(attn_weights, V)\n\n    print(f\"\\nStep 5: \u5024\u3078\u306e\u9069\u7528\")\n    print(f\"Context shape: {context.shape}\")\n\n    # Step 6: Concatenate heads\n    context = context.transpose(1, 2).contiguous().view(\n        batch_size, seq_len, d_model\n    )\n\n    print(f\"\\nStep 6: \u30d8\u30c3\u30c9\u306e\u7d50\u5408\")\n    print(f\"Context shape: {context.shape}\")\n\n    # Step 7: Output projection\n    output = context @ W_O\n\n    print(f\"\\nStep 7: \u51fa\u529b\u6295\u5f71\")\n    print(f\"Output shape: {output.shape}\")\n</code></pre>"},{"location":"part1/math-basics/#_6","title":"\u307e\u3068\u3081\uff1a\u6570\u5b66\u306f\u5b9f\u88c5\u3067\u7406\u89e3\u3059\u308b","text":"<p>\u3053\u306e\u7ae0\u3067\u306f\u3001Transformer\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6570\u5b66\u7684\u6982\u5ff5\u3092\u3001\u5b9f\u88c5\u3092\u901a\u3058\u3066\u5b66\u3073\u307e\u3057\u305f\uff1a</p> <ol> <li>\u7dda\u5f62\u4ee3\u6570</li> <li>\u30d9\u30af\u30c8\u30eb\uff1a\u30c7\u30fc\u30bf\u306e\u8868\u73fe</li> <li>\u884c\u5217\uff1a\u5909\u63db\u3068\u95a2\u4fc2\u6027</li> <li> <p>\u5185\u7a4d\uff1a\u985e\u4f3c\u5ea6\u306e\u8a08\u7b97</p> </li> <li> <p>\u78ba\u7387\u30fb\u7d71\u8a08</p> </li> <li>\u78ba\u7387\u5206\u5e03\uff1a\u4e0d\u78ba\u5b9f\u6027\u306e\u8868\u73fe</li> <li>Softmax\uff1a\u30b9\u30b3\u30a2\u304b\u3089\u78ba\u7387\u3078</li> <li> <p>\u671f\u5f85\u5024\u3068\u5206\u6563\uff1a\u6b63\u898f\u5316\u306e\u57fa\u790e</p> </li> <li> <p>\u5fae\u5206</p> </li> <li>\u52fe\u914d\uff1a\u6700\u9069\u5316\u306e\u65b9\u5411</li> <li>\u9023\u9396\u5f8b\uff1a\u8907\u96d1\u306a\u95a2\u6570\u306e\u5fae\u5206</li> <li>\u81ea\u52d5\u5fae\u5206\uff1a\u52b9\u7387\u7684\u306a\u8a08\u7b97</li> </ol> <p>\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u6b21\u7ae0\u3067\u5b66\u3076PyTorch\u3067\u306e\u5b9f\u88c5\u306b\u76f4\u63a5\u3064\u306a\u304c\u308a\u307e\u3059\u3002\u6570\u5b66\u306f\u62bd\u8c61\u7684\u306a\u7406\u8ad6\u3067\u306f\u306a\u304f\u3001\u5b9f\u969b\u306b\u52d5\u304f\u30b3\u30fc\u30c9\u3068\u3057\u3066\u7406\u89e3\u3059\u308b\u3053\u3068\u3067\u3001Transformer\u306e\u52d5\u4f5c\u539f\u7406\u304c\u3088\u308a\u660e\u78ba\u306b\u306a\u308b\u306f\u305a\u3067\u3059\u3002</p>"},{"location":"part1/math-basics/#_7","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li>\u30d9\u30af\u30c8\u30eb\u6f14\u7b97\u306e\u5b9f\u88c5</li> <li>2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u9593\u306e\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u3067\u300cking - man + woman \u2248 queen\u300d\u3092\u691c\u8a3c\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u884c\u5217\u5909\u63db\u306e\u53ef\u8996\u5316</p> </li> <li>\u4efb\u610f\u306e2\u00d72\u884c\u5217\u306b\u3088\u308b\u5909\u63db\u3092\u53ef\u8996\u5316\u3059\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u65b9\u5411\u304c\u5909\u63db\u3067\u4fdd\u5b58\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>Attention \u306e\u5b9f\u88c5</p> </li> <li>Scaled Dot-Product Attention\u3092\u4e00\u304b\u3089\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\u3057\u3001\u305d\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u52fe\u914d\u964d\u4e0b\u6cd5\u306e\u5b9f\u88c5</p> </li> <li>2\u6b21\u5143\u95a2\u6570\u306e\u52fe\u914d\u964d\u4e0b\u6cd5\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044</li> <li> <p>\u5b66\u7fd2\u7387\u306b\u3088\u308b\u53ce\u675f\u306e\u9055\u3044\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044</p> </li> <li> <p>\u81ea\u52d5\u5fae\u5206\u306e\u4ed5\u7d44\u307f</p> </li> <li>\u7c21\u5358\u306a\u8a08\u7b97\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3057\u3001\u624b\u52d5\u3067\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\u3092\u8a08\u7b97\u3057\u3066\u304f\u3060\u3055\u3044</li> <li>PyTorch\u306e\u81ea\u52d5\u5fae\u5206\u7d50\u679c\u3068\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044</li> </ol> <p>\u6b21\u7ae0\u3067\u306f\u3001PyTorch\u3092\u4f7f\u3063\u3066\u5b9f\u969b\u306b\u3053\u308c\u3089\u306e\u6570\u5b66\u7684\u6982\u5ff5\u3092\u5b9f\u88c5\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u7406\u8ad6\u3068\u5b9f\u88c5\u3092\u7d50\u3073\u3064\u3051\u308b\u3053\u3068\u3067\u3001Transformer\u306e\u7406\u89e3\u304c\u3055\u3089\u306b\u6df1\u307e\u308b\u3067\u3057\u3087\u3046\u3002</p>"},{"location":"part1/pytorch-basics/","title":"PyTorch\u306e\u6700\u5c0f\u9650\u306e\u4f7f\u3044\u65b9","text":""},{"location":"part1/pytorch-basics/#pytorch_1","title":"\u306f\u3058\u3081\u306b\uff1a\u306a\u305cPyTorch\u306a\u306e\u304b","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3092\u5b9f\u88c5\u3059\u308b\u969b\u3001\u3042\u306a\u305f\u306fC\u3084Rust\u306e\u3088\u3046\u306a\u4f4e\u30ec\u30d9\u30eb\u8a00\u8a9e\u3092\u9078\u3076\u3067\u3057\u3087\u3046\u3002\u901f\u5ea6\u3068\u5236\u5fa1\u6027\u3092\u91cd\u8996\u3059\u308b\u304b\u3089\u3067\u3059\u3002\u3057\u304b\u3057\u3001\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u4e16\u754c\u3067\u306f\u3001\u7570\u306a\u308b\u8981\u6c42\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ol> <li>\u81ea\u52d5\u5fae\u5206: \u624b\u52d5\u3067\u5fae\u5206\u3092\u8a08\u7b97\u3059\u308b\u306e\u306f\u975e\u73fe\u5b9f\u7684</li> <li>GPU\u5bfe\u5fdc: \u884c\u5217\u6f14\u7b97\u306e\u4e26\u5217\u5316\u304c\u4e0d\u53ef\u6b20</li> <li>\u8c4a\u5bcc\u306a\u30a8\u30b3\u30b7\u30b9\u30c6\u30e0: \u65e2\u5b58\u306e\u30e2\u30c7\u30eb\u3084\u30c4\u30fc\u30eb\u3068\u306e\u9023\u643a</li> </ol> <p>PyTorch\u306f\u3001\u3053\u308c\u3089\u3059\u3079\u3066\u3092\u63d0\u4f9b\u3057\u306a\u304c\u3089\u3001Python\u306e\u76f4\u611f\u7684\u306a\u6587\u6cd5\u3092\u7dad\u6301\u3057\u3066\u3044\u307e\u3059\u3002\u30b3\u30f3\u30d1\u30a4\u30e9\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u3001PyTorch\u306f\u300c\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u305f\u3081\u306eLLVM\u300d\u306e\u3088\u3046\u306a\u5b58\u5728\u3067\u3059\u3002</p>"},{"location":"part1/pytorch-basics/#41","title":"4.1 \u30c6\u30f3\u30bd\u30eb\u306e\u5b8c\u5168\u7406\u89e3","text":""},{"location":"part1/pytorch-basics/#gpu","title":"\u30c6\u30f3\u30bd\u30eb = \u591a\u6b21\u5143\u914d\u5217 + \u81ea\u52d5\u5fae\u5206 + GPU\u5bfe\u5fdc","text":"<p>\u307e\u305a\u3001\u306a\u305cNumPy\u3067\u306f\u30c0\u30e1\u306a\u306e\u304b\u3092\u7406\u89e3\u3057\u307e\u3057\u3087\u3046\uff1a</p> <pre><code>import numpy as np\nimport torch\nimport time\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List, Optional\n\nclass TensorVsNumPy:\n    \"\"\"NumPy\u3068PyTorch\u30c6\u30f3\u30bd\u30eb\u306e\u9055\u3044\u3092\u5b9f\u8a3c\"\"\"\n\n    def performance_comparison(self, size: int = 10000) -&gt; None:\n        \"\"\"\u5927\u898f\u6a21\u884c\u5217\u6f14\u7b97\u3067\u306e\u6027\u80fd\u6bd4\u8f03\"\"\"\n        # NumPy\n        np_a = np.random.randn(size, size).astype(np.float32)\n        np_b = np.random.randn(size, size).astype(np.float32)\n\n        start = time.time()\n        np_c = np.matmul(np_a, np_b)\n        numpy_time = time.time() - start\n\n        # PyTorch CPU\n        torch_a = torch.randn(size, size)\n        torch_b = torch.randn(size, size)\n\n        start = time.time()\n        torch_c = torch.matmul(torch_a, torch_b)\n        torch_cpu_time = time.time() - start\n\n        # PyTorch GPU\uff08\u5229\u7528\u53ef\u80fd\u306a\u5834\u5408\uff09\n        if torch.cuda.is_available():\n            torch_a_gpu = torch_a.cuda()\n            torch_b_gpu = torch_b.cuda()\n\n            # \u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\uff08GPU\u306e\u521d\u671f\u5316\uff09\n            _ = torch.matmul(torch_a_gpu, torch_b_gpu)\n            torch.cuda.synchronize()\n\n            start = time.time()\n            torch_c_gpu = torch.matmul(torch_a_gpu, torch_b_gpu)\n            torch.cuda.synchronize()  # GPU\u6f14\u7b97\u306e\u5b8c\u4e86\u3092\u5f85\u3064\n            torch_gpu_time = time.time() - start\n        else:\n            torch_gpu_time = float('inf')\n\n        print(f\"=== \u884c\u5217\u7a4d ({size}x{size}) \u306e\u6027\u80fd\u6bd4\u8f03 ===\")\n        print(f\"NumPy: {numpy_time:.3f}\u79d2\")\n        print(f\"PyTorch (CPU): {torch_cpu_time:.3f}\u79d2\")\n        if torch.cuda.is_available():\n            print(f\"PyTorch (GPU): {torch_gpu_time:.3f}\u79d2\")\n            print(f\"GPU\u9ad8\u901f\u5316: {torch_cpu_time/torch_gpu_time:.1f}\u500d\")\n\n    def gradient_capability(self) -&gt; None:\n        \"\"\"\u81ea\u52d5\u5fae\u5206\u306e\u80fd\u529b\"\"\"\n        # NumPy\u3067\u306f\u624b\u52d5\u8a08\u7b97\u304c\u5fc5\u8981\n        def manual_gradient():\n            # f(x, y) = x\u00b2y + xy\u00b2\n            x, y = 3.0, 4.0\n\n            # \u624b\u52d5\u3067\u504f\u5fae\u5206\u3092\u8a08\u7b97\n            df_dx = 2*x*y + y**2  # \u2202f/\u2202x = 2xy + y\u00b2\n            df_dy = x**2 + 2*x*y  # \u2202f/\u2202y = x\u00b2 + 2xy\n\n            return df_dx, df_dy\n\n        # PyTorch\u3067\u306f\u81ea\u52d5\u8a08\u7b97\n        def auto_gradient():\n            x = torch.tensor(3.0, requires_grad=True)\n            y = torch.tensor(4.0, requires_grad=True)\n\n            # \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9\n            f = x**2 * y + x * y**2\n\n            # \u81ea\u52d5\u5fae\u5206\n            f.backward()\n\n            return x.grad.item(), y.grad.item()\n\n        manual = manual_gradient()\n        auto = auto_gradient()\n\n        print(\"\\n=== \u81ea\u52d5\u5fae\u5206\u306e\u6bd4\u8f03 ===\")\n        print(f\"\u624b\u52d5\u8a08\u7b97: \u2202f/\u2202x = {manual[0]}, \u2202f/\u2202y = {manual[1]}\")\n        print(f\"\u81ea\u52d5\u5fae\u5206: \u2202f/\u2202x = {auto[0]}, \u2202f/\u2202y = {auto[1]}\")\n</code></pre>"},{"location":"part1/pytorch-basics/#_1","title":"\u30c6\u30f3\u30bd\u30eb\u306e\u4f5c\u6210\u3068\u57fa\u672c\u64cd\u4f5c","text":"<pre><code>class TensorBasics:\n    \"\"\"\u30c6\u30f3\u30bd\u30eb\u306e\u57fa\u672c\u64cd\u4f5c\u3092\u4f53\u7cfb\u7684\u306b\u5b66\u7fd2\"\"\"\n\n    def __init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9: {self.device}\")\n\n    def tensor_creation_methods(self) -&gt; dict:\n        \"\"\"\u69d8\u3005\u306a\u30c6\u30f3\u30bd\u30eb\u4f5c\u6210\u65b9\u6cd5\"\"\"\n        tensors = {}\n\n        # 1. Python\u30ea\u30b9\u30c8\u304b\u3089\n        tensors['from_list'] = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n        # 2. NumPy\u914d\u5217\u304b\u3089\uff08\u30e1\u30e2\u30ea\u5171\u6709\u306b\u6ce8\u610f\uff09\n        np_array = np.array([[1, 2], [3, 4]], dtype=np.float32)\n        tensors['from_numpy_shared'] = torch.from_numpy(np_array)  # \u30e1\u30e2\u30ea\u5171\u6709\n        tensors['from_numpy_copy'] = torch.tensor(np_array)  # \u30b3\u30d4\u30fc\n\n        # 3. \u7279\u6b8a\u306a\u30c6\u30f3\u30bd\u30eb\n        shape = (3, 4)\n        tensors['zeros'] = torch.zeros(shape)\n        tensors['ones'] = torch.ones(shape)\n        tensors['eye'] = torch.eye(4)  # \u5358\u4f4d\u884c\u5217\n        tensors['full'] = torch.full(shape, 3.14)  # \u5b9a\u6570\u3067\u57cb\u3081\u308b\n\n        # 4. \u30e9\u30f3\u30c0\u30e0\u30c6\u30f3\u30bd\u30eb\n        tensors['uniform'] = torch.rand(shape)  # [0, 1)\u306e\u4e00\u69d8\u5206\u5e03\n        tensors['normal'] = torch.randn(shape)  # \u6a19\u6e96\u6b63\u898f\u5206\u5e03\n        tensors['int_random'] = torch.randint(0, 10, shape)  # \u6574\u6570\u4e71\u6570\n\n        # 5. \u65e2\u5b58\u30c6\u30f3\u30bd\u30eb\u3068\u540c\u3058\u5c5e\u6027\n        reference = torch.randn(2, 3, dtype=torch.float64, device=self.device)\n        tensors['zeros_like'] = torch.zeros_like(reference)\n        tensors['ones_like'] = torch.ones_like(reference)\n        tensors['randn_like'] = torch.randn_like(reference)\n\n        # 6. \u7bc4\u56f2\u30c6\u30f3\u30bd\u30eb\n        tensors['arange'] = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n        tensors['linspace'] = torch.linspace(0, 1, 5)  # \u7b49\u9593\u9694\u306e5\u70b9\n        tensors['logspace'] = torch.logspace(0, 2, 5)  # \u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\n\n        return tensors\n\n    def tensor_properties_demo(self) -&gt; None:\n        \"\"\"\u30c6\u30f3\u30bd\u30eb\u306e\u5c5e\u6027\u3092\u8a73\u3057\u304f\u8abf\u67fb\"\"\"\n        t = torch.randn(2, 3, 4, dtype=torch.float32, requires_grad=True)\n\n        print(\"\\n=== \u30c6\u30f3\u30bd\u30eb\u306e\u5c5e\u6027 ===\")\n        print(f\"\u5f62\u72b6 (shape): {t.shape}\")\n        print(f\"\u30b5\u30a4\u30ba (size): {t.size()}\")  # shape\u3068\u540c\u3058\n        print(f\"\u6b21\u5143\u6570 (ndim): {t.ndim}\")\n        print(f\"\u8981\u7d20\u6570 (numel): {t.numel()}\")\n        print(f\"\u30c7\u30fc\u30bf\u578b (dtype): {t.dtype}\")\n        print(f\"\u30c7\u30d0\u30a4\u30b9 (device): {t.device}\")\n        print(f\"\u52fe\u914d\u8ffd\u8de1 (requires_grad): {t.requires_grad}\")\n        print(f\"\u52fe\u914d\u95a2\u6570 (grad_fn): {t.grad_fn}\")\n\n        # \u30e1\u30e2\u30ea\u30ec\u30a4\u30a2\u30a6\u30c8\n        print(f\"\\n--- \u30e1\u30e2\u30ea\u30ec\u30a4\u30a2\u30a6\u30c8 ---\")\n        print(f\"\u9023\u7d9a\u6027 (is_contiguous): {t.is_contiguous()}\")\n        print(f\"\u30b9\u30c8\u30e9\u30a4\u30c9 (stride): {t.stride()}\")\n        print(f\"\u30e1\u30e2\u30ea\u4e0a\u306e\u30d0\u30a4\u30c8\u6570: {t.element_size() * t.numel()}\")\n\n        # \u5f62\u72b6\u5909\u66f4\u5f8c\u306e\u9023\u7d9a\u6027\n        t_reshaped = t.transpose(0, 1)\n        print(f\"\\n\u8ee2\u7f6e\u5f8c\u306e\u9023\u7d9a\u6027: {t_reshaped.is_contiguous()}\")\n        t_contiguous = t_reshaped.contiguous()\n        print(f\"contiguous()\u5f8c: {t_contiguous.is_contiguous()}\")\n\n    def broadcasting_deep_dive(self) -&gt; None:\n        \"\"\"\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c6\u30a3\u30f3\u30b0\u306e\u8a73\u7d30\u7406\u89e3\"\"\"\n        print(\"\\n=== \u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c6\u30a3\u30f3\u30b0\u30eb\u30fc\u30eb ===\")\n\n        # \u30eb\u30fc\u30eb1: \u6b21\u5143\u6570\u3092\u63c3\u3048\u308b\uff08\u53f3\u5074\u304b\u3089\uff09\n        a = torch.ones(3, 4)      # shape: (3, 4)\n        b = torch.ones(4)         # shape: (4,) \u2192 (1, 4) \u2192 (3, 4)\n        c = a + b\n        print(f\"\u884c\u5217 + \u30d9\u30af\u30c8\u30eb: {a.shape} + {b.shape} \u2192 {c.shape}\")\n\n        # \u30eb\u30fc\u30eb2: \u30b5\u30a4\u30ba1\u306e\u6b21\u5143\u306f\u62e1\u5f35\u53ef\u80fd\n        a = torch.ones(3, 1, 5)   # shape: (3, 1, 5)\n        b = torch.ones(1, 4, 5)   # shape: (1, 4, 5)\n        c = a + b                 # shape: (3, 4, 5)\n        print(f\"\u6b21\u5143\u62e1\u5f35: {a.shape} + {b.shape} \u2192 {c.shape}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_broadcasting()\n\n        # \u6ce8\u610f\uff1a\u30e1\u30e2\u30ea\u52b9\u7387\n        print(\"\\n--- \u30e1\u30e2\u30ea\u52b9\u7387\u306e\u89b3\u70b9 ---\")\n        large_tensor = torch.randn(1000, 1000)\n        small_vector = torch.randn(1000)\n\n        # \u52b9\u7387\u7684\uff1a\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c6\u30a3\u30f3\u30b0\uff08\u30e1\u30e2\u30ea\u30b3\u30d4\u30fc\u306a\u3057\uff09\n        result_efficient = large_tensor + small_vector.unsqueeze(0)\n\n        # \u975e\u52b9\u7387\uff1a\u660e\u793a\u7684\u306a\u62e1\u5f35\uff08\u30e1\u30e2\u30ea\u30b3\u30d4\u30fc\u3042\u308a\uff09\n        expanded_vector = small_vector.unsqueeze(0).expand(1000, 1000)\n        result_inefficient = large_tensor + expanded_vector\n\n        print(f\"small_vector \u306e\u30e1\u30e2\u30ea: {small_vector.numel() * 4} bytes\")\n        print(f\"expanded_vector \u306e\u30e1\u30e2\u30ea: {expanded_vector.numel() * 4} bytes\")\n\n    def _visualize_broadcasting(self) -&gt; None:\n        \"\"\"\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c6\u30a3\u30f3\u30b0\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n        # \u30b1\u30fc\u30b91: \u30b9\u30ab\u30e9\u30fc \u00d7 \u884c\u5217\n        ax = axes[0, 0]\n        matrix = torch.ones(3, 4)\n        scalar = torch.tensor(2.0)\n        self._draw_broadcast(ax, matrix.shape, scalar.shape, \"\u30b9\u30ab\u30e9\u30fc \u00d7 \u884c\u5217\")\n\n        # \u30b1\u30fc\u30b92: \u30d9\u30af\u30c8\u30eb \u00d7 \u884c\u5217\uff08\u884c\u65b9\u5411\uff09\n        ax = axes[0, 1]\n        matrix = torch.ones(3, 4)\n        row_vector = torch.ones(1, 4)\n        self._draw_broadcast(ax, matrix.shape, row_vector.shape, \"\u884c\u30d9\u30af\u30c8\u30eb \u00d7 \u884c\u5217\")\n\n        # \u30b1\u30fc\u30b93: \u30d9\u30af\u30c8\u30eb \u00d7 \u884c\u5217\uff08\u5217\u65b9\u5411\uff09\n        ax = axes[0, 2]\n        matrix = torch.ones(3, 4)\n        col_vector = torch.ones(3, 1)\n        self._draw_broadcast(ax, matrix.shape, col_vector.shape, \"\u5217\u30d9\u30af\u30c8\u30eb \u00d7 \u884c\u5217\")\n\n        # \u30b1\u30fc\u30b94: 3\u6b21\u5143\u306e\u4f8b\n        ax = axes[1, 0]\n        tensor3d = torch.ones(2, 3, 4)\n        matrix = torch.ones(3, 4)\n        self._draw_broadcast(ax, tensor3d.shape, matrix.shape, \"3\u6b21\u5143 \u00d7 2\u6b21\u5143\")\n\n        # \u30b1\u30fc\u30b95: \u30a8\u30e9\u30fc\u30b1\u30fc\u30b9\n        ax = axes[1, 1]\n        ax.text(0.5, 0.5, \"\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u4e0d\u53ef\u80fd\u306a\u4f8b:\\n(3, 4) \u00d7 (3, 3)\\n\u2192 \u30a8\u30e9\u30fc\", \n                ha='center', va='center', fontsize=12)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n\n        # \u30b1\u30fc\u30b96: \u8907\u96d1\u306a\u4f8b\n        ax = axes[1, 2]\n        a = torch.ones(5, 1, 4, 1)\n        b = torch.ones(3, 1, 6)\n        # \u7d50\u679c: (5, 3, 4, 6)\n        ax.text(0.5, 0.5, f\"\u8907\u96d1\u306a\u4f8b:\\n{a.shape} \u00d7 {b.shape}\\n\u2192 (5, 3, 4, 6)\", \n                ha='center', va='center', fontsize=12)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def _draw_broadcast(self, ax, shape1, shape2, title):\n        \"\"\"\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c6\u30a3\u30f3\u30b0\u306e\u56f3\u793a\u88dc\u52a9\u95a2\u6570\"\"\"\n        ax.set_title(title)\n        ax.text(0.5, 0.5, f\"{shape1} \u00d7 {shape2}\", ha='center', va='center')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n</code></pre>"},{"location":"part1/pytorch-basics/#_2","title":"\u9ad8\u5ea6\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0","text":"<pre><code>class AdvancedIndexing:\n    \"\"\"Transformer\u3067\u5fc5\u8981\u3068\u306a\u308b\u9ad8\u5ea6\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u64cd\u4f5c\"\"\"\n\n    def basic_indexing(self) -&gt; None:\n        \"\"\"\u57fa\u672c\u7684\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u64cd\u4f5c\"\"\"\n        print(\"=== \u57fa\u672c\u7684\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9 ===\")\n\n        # 3\u6b21\u5143\u30c6\u30f3\u30bd\u30eb: (batch, sequence, features)\n        tensor = torch.randn(4, 5, 6)  # 4\u30d0\u30c3\u30c1\u30015\u30c8\u30fc\u30af\u30f3\u30016\u6b21\u5143\u7279\u5fb4\n\n        # \u30b9\u30e9\u30a4\u30b7\u30f3\u30b0\n        print(f\"\u5143\u306e\u5f62\u72b6: {tensor.shape}\")\n        print(f\"\u6700\u521d\u306e\u30d0\u30c3\u30c1: {tensor[0].shape}\")\n        print(f\"\u5168\u30d0\u30c3\u30c1\u306e\u6700\u521d\u306e\u30c8\u30fc\u30af\u30f3: {tensor[:, 0].shape}\")\n        print(f\"\u6700\u521d\u306e2\u30d0\u30c3\u30c1\u306e3-5\u30c8\u30fc\u30af\u30f3: {tensor[:2, 2:5].shape}\")\n\n        # \u7701\u7565\u8a18\u53f7\uff08...\uff09\u306e\u4f7f\u7528\n        print(f\"\u5168\u6b21\u5143\u306e\u6700\u5f8c\u306e\u7279\u5fb4: {tensor[..., -1].shape}\")\n        print(f\"\u6700\u521d\u3068\u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3: {tensor[:, [0, -1]].shape}\")\n\n    def advanced_indexing(self) -&gt; None:\n        \"\"\"\u9ad8\u5ea6\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u64cd\u4f5c\"\"\"\n        print(\"\\n=== \u9ad8\u5ea6\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9 ===\")\n\n        # \u6574\u6570\u914d\u5217\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        batch_size, seq_len, hidden_dim = 8, 10, 16\n        hidden_states = torch.randn(batch_size, seq_len, hidden_dim)\n\n        # \u7279\u5b9a\u306e\u4f4d\u7f6e\u306e\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\n        positions = torch.tensor([1, 3, 5, 7, 2, 4, 6, 8])  # \u5404\u30d0\u30c3\u30c1\u3067\u9078\u629e\u3059\u308b\u4f4d\u7f6e\n        batch_indices = torch.arange(batch_size)\n\n        selected = hidden_states[batch_indices, positions]\n        print(f\"\u9078\u629e\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3: {selected.shape}\")  # (8, 16)\n\n        # \u30d6\u30fc\u30eb\u30de\u30b9\u30af\n        mask = hidden_states &gt; 0.5\n        positive_values = hidden_states[mask]\n        print(f\"0.5\u3088\u308a\u5927\u304d\u3044\u5024\u306e\u6570: {positive_values.numel()}\")\n\n        # \u30de\u30b9\u30af\u3092\u4f7f\u3063\u305f\u5024\u306e\u7f6e\u63db\n        hidden_states_copy = hidden_states.clone()\n        hidden_states_copy[mask] = 1.0\n        print(f\"\u30de\u30b9\u30af\u9069\u7528\u5f8c\u306e\u5e73\u5747\u5024: {hidden_states_copy.mean():.3f}\")\n\n    def gather_scatter_operations(self) -&gt; None:\n        \"\"\"gather/scatter\u64cd\u4f5c\uff08Transformer\u3067\u91cd\u8981\uff09\"\"\"\n        print(\"\\n=== Gather/Scatter\u64cd\u4f5c ===\")\n\n        # Gather: \u7279\u5b9a\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u5024\u3092\u53ce\u96c6\n        # \u4f8b\uff1a\u5404\u30d0\u30c3\u30c1\u3067\u6700\u3082\u6ce8\u76ee\u3059\u3079\u304d\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\n        batch_size, seq_len, hidden_dim = 4, 6, 8\n        hidden_states = torch.randn(batch_size, seq_len, hidden_dim)\n\n        # \u5404\u30d0\u30c3\u30c1\u3067\u6ce8\u76ee\u3059\u3079\u304d\u30c8\u30fc\u30af\u30f3\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        attention_indices = torch.tensor([[2], [0], [4], [1]])  # shape: (4, 1)\n\n        # gather\u3067\u9078\u629e\n        attention_indices_expanded = attention_indices.unsqueeze(-1).expand(-1, -1, hidden_dim)\n        selected_tokens = torch.gather(hidden_states, 1, attention_indices_expanded)\n        print(f\"Gather\u7d50\u679c: {selected_tokens.shape}\")  # (4, 1, 8)\n\n        # Scatter: \u5024\u3092\u7279\u5b9a\u306e\u4f4d\u7f6e\u306b\u914d\u7f6e\n        # \u4f8b\uff1aone-hot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        num_classes = 10\n        batch_size = 5\n        labels = torch.tensor([3, 7, 1, 9, 0])\n\n        one_hot = torch.zeros(batch_size, num_classes)\n        one_hot.scatter_(1, labels.unsqueeze(1), 1.0)\n        print(f\"\\nOne-hot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0:\")\n        print(one_hot)\n\n        # Scatter_add: \u5024\u3092\u52a0\u7b97\n        # \u4f8b\uff1a\u30c8\u30fc\u30af\u30f3\u306e\u57cb\u3081\u8fbc\u307f\u3092\u4f4d\u7f6e\u3054\u3068\u306b\u96c6\u8a08\n        embeddings = torch.randn(10, 8)  # 10\u500b\u306e8\u6b21\u5143\u57cb\u3081\u8fbc\u307f\n        positions = torch.tensor([0, 1, 0, 2, 1, 2, 0, 1, 2, 0])  # \u5404\u57cb\u3081\u8fbc\u307f\u306e\u4f4d\u7f6e\n\n        aggregated = torch.zeros(3, 8)  # 3\u3064\u306e\u4f4d\u7f6e\n        aggregated.scatter_add_(0, positions.unsqueeze(1).expand(-1, 8), embeddings)\n        print(f\"\\nScatter_add\u7d50\u679c: {aggregated.shape}\")\n\n    def index_operations_visualization(self) -&gt; None:\n        \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u64cd\u4f5c\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # 1. \u57fa\u672c\u7684\u306a\u30b9\u30e9\u30a4\u30b7\u30f3\u30b0\n        ax = axes[0, 0]\n        tensor = torch.arange(20).reshape(4, 5)\n        ax.imshow(tensor, cmap='viridis')\n        ax.set_title(\"\u5143\u306e\u30c6\u30f3\u30bd\u30eb\")\n        for i in range(4):\n            for j in range(5):\n                ax.text(j, i, str(tensor[i, j].item()), ha='center', va='center')\n\n        # 2. \u9ad8\u5ea6\u306a\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        ax = axes[0, 1]\n        indices = torch.tensor([0, 2, 3])\n        selected = tensor[indices]\n        ax.imshow(selected, cmap='plasma')\n        ax.set_title(\"\u884c\u30a4\u30f3\u30c7\u30c3\u30af\u30b9 [0, 2, 3]\")\n        for i in range(3):\n            for j in range(5):\n                ax.text(j, i, str(selected[i, j].item()), ha='center', va='center')\n\n        # 3. \u30d6\u30fc\u30eb\u30de\u30b9\u30af\n        ax = axes[1, 0]\n        mask = tensor &gt; 10\n        masked = torch.where(mask, tensor, torch.tensor(-1))\n        ax.imshow(masked, cmap='RdBu')\n        ax.set_title(\"\u30d6\u30fc\u30eb\u30de\u30b9\u30af (&gt;10)\")\n        for i in range(4):\n            for j in range(5):\n                ax.text(j, i, str(masked[i, j].item()), ha='center', va='center')\n\n        # 4. Gather\u64cd\u4f5c\n        ax = axes[1, 1]\n        indices = torch.tensor([[0, 2, 4], [1, 3, 0], [2, 4, 1], [3, 0, 2]])\n        gathered = torch.gather(tensor, 1, indices)\n        ax.imshow(gathered, cmap='coolwarm')\n        ax.set_title(\"Gather\u64cd\u4f5c\")\n        for i in range(4):\n            for j in range(3):\n                ax.text(j, i, str(gathered[i, j].item()), ha='center', va='center')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part1/pytorch-basics/#42","title":"4.2 \u81ea\u52d5\u5fae\u5206\u306e\u4ed5\u7d44\u307f\u3068\u5b9f\u8df5","text":""},{"location":"part1/pytorch-basics/#_3","title":"\u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u7406\u89e3","text":"<pre><code>class AutogradDeepDive:\n    \"\"\"\u81ea\u52d5\u5fae\u5206\u306e\u4ed5\u7d44\u307f\u3092\u6df1\u304f\u7406\u89e3\"\"\"\n\n    def computational_graph_basics(self) -&gt; None:\n        \"\"\"\u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u57fa\u672c\"\"\"\n        print(\"=== \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9 ===\")\n\n        # \u5165\u529b\n        x = torch.tensor(2.0, requires_grad=True)\n        y = torch.tensor(3.0, requires_grad=True)\n\n        # \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u69cb\u7bc9\n        z = x * y\n        w = z + x\n        loss = w ** 2\n\n        # \u30b0\u30e9\u30d5\u60c5\u5831\u306e\u8868\u793a\n        print(f\"x.requires_grad: {x.requires_grad}\")\n        print(f\"z.grad_fn: {z.grad_fn}\")  # MulBackward\n        print(f\"w.grad_fn: {w.grad_fn}\")  # AddBackward\n        print(f\"loss.grad_fn: {loss.grad_fn}\")  # PowBackward\n\n        # \u9006\u4f1d\u64ad\n        loss.backward()\n\n        # \u52fe\u914d\u306e\u78ba\u8a8d\n        print(f\"\\n\u52fe\u914d:\")\n        print(f\"\u2202loss/\u2202x = {x.grad}\")\n        print(f\"\u2202loss/\u2202y = {y.grad}\")\n\n        # \u624b\u52d5\u8a08\u7b97\u3068\u306e\u6bd4\u8f03\n        # loss = (xy + x)\u00b2 = (2*3 + 2)\u00b2 = 64\n        # \u2202loss/\u2202x = 2(xy + x)(y + 1) = 2*8*4 = 64\n        # \u2202loss/\u2202y = 2(xy + x)x = 2*8*2 = 32\n        print(f\"\\n\u624b\u52d5\u8a08\u7b97:\")\n        print(f\"\u2202loss/\u2202x = 2*(x*y + x)*(y + 1) = {2*(2*3 + 2)*(3 + 1)}\")\n        print(f\"\u2202loss/\u2202y = 2*(x*y + x)*x = {2*(2*3 + 2)*2}\")\n\n    def gradient_accumulation_detailed(self) -&gt; None:\n        \"\"\"\u52fe\u914d\u7d2f\u7a4d\u306e\u8a73\u7d30\"\"\"\n        print(\"\\n=== \u52fe\u914d\u7d2f\u7a4d ===\")\n\n        x = torch.tensor(1.0, requires_grad=True)\n\n        # \u6700\u521d\u306e\u8a08\u7b97\n        y1 = x ** 2\n        y1.backward()\n        print(f\"\u6700\u521d\u306ebackward\u5f8c: x.grad = {x.grad}\")\n\n        # \u4e8c\u56de\u76ee\u306e\u8a08\u7b97\uff08\u52fe\u914d\u304c\u7d2f\u7a4d\u3055\u308c\u308b\uff09\n        y2 = x ** 3\n        y2.backward()\n        print(f\"\u4e8c\u56de\u76ee\u306ebackward\u5f8c: x.grad = {x.grad}\")\n        # \u671f\u5f85\u5024: 2*1 + 3*1\u00b2 = 5\n\n        # \u52fe\u914d\u306e\u30ea\u30bb\u30c3\u30c8\n        x.grad.zero_()\n        y3 = x ** 4\n        y3.backward()\n        print(f\"\u30ea\u30bb\u30c3\u30c8\u5f8c\u306ebackward: x.grad = {x.grad}\")\n\n    def gradient_flow_control(self) -&gt; None:\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u5236\u5fa1\"\"\"\n        print(\"\\n=== \u52fe\u914d\u30d5\u30ed\u30fc\u306e\u5236\u5fa1 ===\")\n\n        # 1. torch.no_grad()\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        x = torch.tensor(1.0, requires_grad=True)\n\n        with torch.no_grad():\n            y = x * 2\n            z = y * 3\n\n        print(f\"no_grad\u5185\u3067\u306e\u8a08\u7b97:\")\n        print(f\"y.requires_grad: {y.requires_grad}\")\n        print(f\"z.requires_grad: {z.requires_grad}\")\n\n        # 2. detach()\u30e1\u30bd\u30c3\u30c9\n        x = torch.tensor(1.0, requires_grad=True)\n        y = x * 2\n        z = y.detach() * 3  # \u3053\u3053\u3067\u52fe\u914d\u306e\u6d41\u308c\u3092\u5207\u65ad\n        w = z + x  # x\u304b\u3089\u306e\u52fe\u914d\u306f\u6d41\u308c\u308b\u304c\u3001y\u304b\u3089\u306f\u6d41\u308c\u306a\u3044\n\n        w.backward()\n        print(f\"\\ndetach\u4f7f\u7528\u6642:\")\n        print(f\"x.grad: {x.grad}\")  # 1.0\uff08w = z + x\u306ex\u306e\u9805\u304b\u3089\uff09\n\n        # 3. requires_grad_()\u30e1\u30bd\u30c3\u30c9\n        a = torch.tensor(1.0)\n        print(f\"\\n\u521d\u671f\u72b6\u614b: a.requires_grad = {a.requires_grad}\")\n        a.requires_grad_(True)\n        print(f\"requires_grad_()\u5f8c: a.requires_grad = {a.requires_grad}\")\n\n    def higher_order_derivatives(self) -&gt; None:\n        \"\"\"\u9ad8\u968e\u5fae\u5206\u306e\u8a08\u7b97\"\"\"\n        print(\"\\n=== \u9ad8\u968e\u5fae\u5206 ===\")\n\n        x = torch.tensor(2.0, requires_grad=True)\n\n        # f(x) = x\u2074\n        y = x ** 4\n\n        # \u4e00\u968e\u5fae\u5206: f'(x) = 4x\u00b3\n        first_grad = torch.autograd.grad(y, x, create_graph=True)[0]\n        print(f\"f'(2) = {first_grad}\")  # 32\n\n        # \u4e8c\u968e\u5fae\u5206: f''(x) = 12x\u00b2\n        second_grad = torch.autograd.grad(first_grad, x, create_graph=True)[0]\n        print(f\"f''(2) = {second_grad}\")  # 48\n\n        # \u4e09\u968e\u5fae\u5206: f'''(x) = 24x\n        third_grad = torch.autograd.grad(second_grad, x, create_graph=True)[0]\n        print(f\"f'''(2) = {third_grad}\")  # 48\n\n        # \u56db\u968e\u5fae\u5206: f''''(x) = 24\n        fourth_grad = torch.autograd.grad(third_grad, x)[0]\n        print(f\"f''''(2) = {fourth_grad}\")  # 24\n\n    def custom_autograd_function(self) -&gt; None:\n        \"\"\"\u30ab\u30b9\u30bf\u30e0\u81ea\u52d5\u5fae\u5206\u95a2\u6570\u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== \u30ab\u30b9\u30bf\u30e0\u81ea\u52d5\u5fae\u5206\u95a2\u6570 ===\")\n\n        class ReLUCustom(torch.autograd.Function):\n            @staticmethod\n            def forward(ctx, input):\n                # \u9006\u4f1d\u64ad\u3067\u5fc5\u8981\u306a\u60c5\u5831\u3092\u4fdd\u5b58\n                ctx.save_for_backward(input)\n                return input.clamp(min=0)\n\n            @staticmethod\n            def backward(ctx, grad_output):\n                input, = ctx.saved_tensors\n                grad_input = grad_output.clone()\n                grad_input[input &lt; 0] = 0\n                return grad_input\n\n        # \u4f7f\u7528\u4f8b\n        relu_custom = ReLUCustom.apply\n\n        x = torch.randn(5, requires_grad=True)\n        y = relu_custom(x)\n        loss = y.sum()\n        loss.backward()\n\n        print(f\"\u5165\u529b: {x.data}\")\n        print(f\"\u51fa\u529b: {y.data}\")\n        print(f\"\u52fe\u914d: {x.grad}\")\n</code></pre>"},{"location":"part1/pytorch-basics/#_4","title":"\u5b9f\u8df5\u7684\u306a\u52fe\u914d\u51e6\u7406","text":"<pre><code>class PracticalGradientHandling:\n    \"\"\"\u5b9f\u8df5\u7684\u306a\u52fe\u914d\u51e6\u7406\u30c6\u30af\u30cb\u30c3\u30af\"\"\"\n\n    def gradient_clipping_strategies(self) -&gt; None:\n        \"\"\"\u69d8\u3005\u306a\u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u6226\u7565\"\"\"\n        model = torch.nn.Sequential(\n            torch.nn.Linear(10, 20),\n            torch.nn.ReLU(),\n            torch.nn.Linear(20, 10)\n        )\n\n        # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u3067\u52fe\u914d\u3092\u751f\u6210\n        x = torch.randn(32, 10)\n        y = model(x).sum()\n        y.backward()\n\n        print(\"=== \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u6226\u7565 ===\")\n\n        # 1. \u30ce\u30eb\u30e0\u306b\u3088\u308b\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        print(\"\\n1. \u30ce\u30eb\u30e0\u30af\u30ea\u30c3\u30d4\u30f3\u30b0:\")\n        total_norm_before = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        print(f\"\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u524d\u306e\u30ce\u30eb\u30e0: {total_norm_before:.3f}\")\n\n        # 2. \u5024\u306b\u3088\u308b\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        print(\"\\n2. \u5024\u30af\u30ea\u30c3\u30d4\u30f3\u30b0:\")\n        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1)\n\n        # 3. \u9069\u5fdc\u7684\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        print(\"\\n3. \u9069\u5fdc\u7684\u30af\u30ea\u30c3\u30d4\u30f3\u30b0:\")\n        self._adaptive_clipping(model)\n\n    def _adaptive_clipping(self, model):\n        \"\"\"\u9069\u5fdc\u7684\u306a\u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\"\"\"\n        # \u5404\u5c64\u3054\u3068\u306b\u7570\u306a\u308b\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u5024\u3092\u8a2d\u5b9a\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                if 'weight' in name:\n                    max_norm = 1.0\n                else:  # bias\n                    max_norm = 0.5\n\n                grad_norm = param.grad.norm()\n                if grad_norm &gt; max_norm:\n                    param.grad.mul_(max_norm / grad_norm)\n\n                print(f\"{name}: norm = {grad_norm:.3f}\")\n\n    def gradient_checkpointing_example(self) -&gt; None:\n        \"\"\"\u52fe\u914d\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\uff08\u30e1\u30e2\u30ea\u7bc0\u7d04\uff09\"\"\"\n        import torch.utils.checkpoint as checkpoint\n\n        class DeepModel(torch.nn.Module):\n            def __init__(self, use_checkpoint=False):\n                super().__init__()\n                self.use_checkpoint = use_checkpoint\n                self.layers = torch.nn.ModuleList([\n                    torch.nn.Sequential(\n                        torch.nn.Linear(100, 100),\n                        torch.nn.ReLU(),\n                        torch.nn.Dropout(0.1)\n                    ) for _ in range(10)\n                ])\n\n            def forward(self, x):\n                for layer in self.layers:\n                    if self.use_checkpoint:\n                        x = checkpoint.checkpoint(layer, x)\n                    else:\n                        x = layer(x)\n                return x\n\n        # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\n        print(\"\\n=== \u52fe\u914d\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8 ===\")\n\n        # \u901a\u5e38\u306e\u30e2\u30c7\u30eb\n        model_normal = DeepModel(use_checkpoint=False)\n        x = torch.randn(100, 100, requires_grad=True)\n        y = model_normal(x).sum()\n\n        # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u63a8\u5b9a\n        memory_normal = sum(p.numel() * p.element_size() for p in model_normal.parameters())\n\n        # \u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u4f7f\u7528\n        model_checkpoint = DeepModel(use_checkpoint=True)\n        y_checkpoint = model_checkpoint(x).sum()\n\n        print(f\"\u901a\u5e38\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u30e1\u30e2\u30ea: {memory_normal / 1024:.1f} KB\")\n        print(\"\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u4f7f\u7528\u6642: \u4e2d\u9593\u6d3b\u6027\u5316\u306e\u30e1\u30e2\u30ea\u3092\u7bc0\u7d04\")\n</code></pre>"},{"location":"part1/pytorch-basics/#43","title":"4.3 \u5b9f\u8df5\u7684\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u7bc9","text":""},{"location":"part1/pytorch-basics/#_5","title":"\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8a2d\u8a08\u54f2\u5b66","text":"<pre><code>class ModuleDesignPhilosophy:\n    \"\"\"PyTorch\u30e2\u30b8\u30e5\u30fc\u30eb\u8a2d\u8a08\u306e\u54f2\u5b66\u3068\u5b9f\u8df5\"\"\"\n\n    def module_as_computation_unit(self) -&gt; None:\n        \"\"\"\u30e2\u30b8\u30e5\u30fc\u30eb = \u8a08\u7b97\u306e\u5358\u4f4d\"\"\"\n        print(\"=== \u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8a2d\u8a08\u54f2\u5b66 ===\")\n\n        # 1. \u5358\u4e00\u8cac\u4efb\u306e\u539f\u5247\n        class AttentionHead(torch.nn.Module):\n            \"\"\"\u5358\u4e00\u306e\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30d8\u30c3\u30c9\uff08\u5358\u4e00\u8cac\u4efb\uff09\"\"\"\n            def __init__(self, d_model: int, d_k: int):\n                super().__init__()\n                self.d_k = d_k\n                self.W_q = torch.nn.Linear(d_model, d_k, bias=False)\n                self.W_k = torch.nn.Linear(d_model, d_k, bias=False)\n                self.W_v = torch.nn.Linear(d_model, d_k, bias=False)\n\n            def forward(self, query, key, value, mask=None):\n                Q = self.W_q(query)\n                K = self.W_k(key)\n                V = self.W_v(value)\n\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                attention_weights = torch.softmax(scores, dim=-1)\n                context = torch.matmul(attention_weights, V)\n\n                return context, attention_weights\n\n        # 2. \u7d44\u307f\u5408\u308f\u305b\u53ef\u80fd\u6027\n        class MultiHeadAttention(torch.nn.Module):\n            \"\"\"\u8907\u6570\u306eAttentionHead\u3092\u7d44\u307f\u5408\u308f\u305b\"\"\"\n            def __init__(self, d_model: int, num_heads: int):\n                super().__init__()\n                assert d_model % num_heads == 0\n                self.d_k = d_model // num_heads\n                self.num_heads = num_heads\n\n                # \u8907\u6570\u306e\u30d8\u30c3\u30c9\u3092\u4f5c\u6210\n                self.heads = torch.nn.ModuleList([\n                    AttentionHead(d_model, self.d_k) \n                    for _ in range(num_heads)\n                ])\n                self.W_o = torch.nn.Linear(d_model, d_model)\n\n            def forward(self, query, key, value, mask=None):\n                # \u5404\u30d8\u30c3\u30c9\u3067\u8a08\u7b97\n                head_outputs = []\n                for head in self.heads:\n                    context, _ = head(query, key, value, mask)\n                    head_outputs.append(context)\n\n                # \u7d50\u5408\u3068\u7dda\u5f62\u5909\u63db\n                concatenated = torch.cat(head_outputs, dim=-1)\n                output = self.W_o(concatenated)\n\n                return output\n\n        # 3. \u518d\u5229\u7528\u6027\n        class TransformerBlock(torch.nn.Module):\n            \"\"\"\u518d\u5229\u7528\u53ef\u80fd\u306aTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n            def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n                super().__init__()\n                self.attention = MultiHeadAttention(d_model, num_heads)\n                self.norm1 = torch.nn.LayerNorm(d_model)\n                self.norm2 = torch.nn.LayerNorm(d_model)\n\n                self.feed_forward = torch.nn.Sequential(\n                    torch.nn.Linear(d_model, d_ff),\n                    torch.nn.ReLU(),\n                    torch.nn.Dropout(dropout),\n                    torch.nn.Linear(d_ff, d_model)\n                )\n                self.dropout = torch.nn.Dropout(dropout)\n\n            def forward(self, x, mask=None):\n                # Self-attention with residual\n                attn_output = self.attention(x, x, x, mask)\n                x = self.norm1(x + self.dropout(attn_output))\n\n                # Feed-forward with residual\n                ff_output = self.feed_forward(x)\n                x = self.norm2(x + self.dropout(ff_output))\n\n                return x\n\n    def initialization_strategies(self) -&gt; None:\n        \"\"\"\u521d\u671f\u5316\u6226\u7565\u306e\u91cd\u8981\u6027\"\"\"\n        print(\"\\n=== \u30d1\u30e9\u30e1\u30fc\u30bf\u521d\u671f\u5316\u6226\u7565 ===\")\n\n        class ProperlyInitializedModel(torch.nn.Module):\n            def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n                super().__init__()\n                self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n                self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n                self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n                self.relu = torch.nn.ReLU()\n\n                # \u9069\u5207\u306a\u521d\u671f\u5316\n                self._initialize_weights()\n\n            def _initialize_weights(self):\n                for module in self.modules():\n                    if isinstance(module, torch.nn.Linear):\n                        # Xavier/Glorot\u521d\u671f\u5316\uff08\u6d3b\u6027\u5316\u95a2\u6570\u306b\u5fdc\u3058\u3066\u9078\u629e\uff09\n                        if self._next_is_relu(module):\n                            # ReLU\u7528\u306eHe\u521d\u671f\u5316\n                            torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n                        else:\n                            # \u305d\u306e\u4ed6\u306e\u6d3b\u6027\u5316\u95a2\u6570\u7528\n                            torch.nn.init.xavier_normal_(module.weight)\n\n                        if module.bias is not None:\n                            torch.nn.init.constant_(module.bias, 0)\n\n            def _next_is_relu(self, module):\n                \"\"\"\u6b21\u306e\u5c64\u304cReLU\u304b\u3069\u3046\u304b\u3092\u5224\u5b9a\uff08\u7c21\u6613\u7248\uff09\"\"\"\n                return True  # \u3053\u306e\u4f8b\u3067\u306f\u5168\u3066ReLU\n\n            def forward(self, x):\n                x = self.relu(self.fc1(x))\n                x = self.relu(self.fc2(x))\n                x = self.fc3(x)\n                return x\n\n        # \u521d\u671f\u5316\u306e\u5f71\u97ff\u3092\u53ef\u8996\u5316\n        self._visualize_initialization_effects()\n\n    def _visualize_initialization_effects(self):\n        \"\"\"\u521d\u671f\u5316\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # \u7570\u306a\u308b\u521d\u671f\u5316\u65b9\u6cd5\n        init_methods = {\n            'Zero': lambda w: torch.nn.init.constant_(w, 0),\n            'Normal(0.01)': lambda w: torch.nn.init.normal_(w, std=0.01),\n            'Xavier': lambda w: torch.nn.init.xavier_normal_(w),\n            'He': lambda w: torch.nn.init.kaiming_normal_(w, nonlinearity='relu')\n        }\n\n        for idx, (name, init_fn) in enumerate(init_methods.items()):\n            ax = axes[idx // 2, idx % 2]\n\n            # \u30b7\u30f3\u30d7\u30eb\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n            model = torch.nn.Sequential(\n                torch.nn.Linear(100, 50),\n                torch.nn.ReLU(),\n                torch.nn.Linear(50, 10)\n            )\n\n            # \u521d\u671f\u5316\n            for layer in model:\n                if isinstance(layer, torch.nn.Linear):\n                    init_fn(layer.weight)\n\n            # \u6d3b\u6027\u5316\u306e\u5206\u5e03\u3092\u8abf\u67fb\n            x = torch.randn(1000, 100)\n            activations = []\n\n            def hook_fn(module, input, output):\n                activations.append(output.detach())\n\n            for layer in model:\n                if isinstance(layer, torch.nn.ReLU):\n                    layer.register_forward_hook(hook_fn)\n\n            with torch.no_grad():\n                _ = model(x)\n\n            # \u53ef\u8996\u5316\n            if activations:\n                act = activations[0].flatten().numpy()\n                ax.hist(act, bins=50, alpha=0.7, density=True)\n                ax.set_title(f'{name} \u521d\u671f\u5316')\n                ax.set_xlabel('\u6d3b\u6027\u5316\u5024')\n                ax.set_ylabel('\u5bc6\u5ea6')\n                ax.axvline(0, color='red', linestyle='--', alpha=0.5)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part1/pytorch-basics/#_6","title":"\u5b66\u7fd2\u30eb\u30fc\u30d7\u306e\u5b8c\u5168\u5b9f\u88c5","text":"<pre><code>class ComprehensiveTrainingLoop:\n    \"\"\"\u5305\u62ec\u7684\u306a\u5b66\u7fd2\u30eb\u30fc\u30d7\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, model: torch.nn.Module, device: str = 'cuda'):\n        self.model = model.to(device)\n        self.device = torch.device(device)\n        self.history = defaultdict(list)\n        self.best_model_state = None\n\n    def train_with_all_bells_and_whistles(\n        self,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        num_epochs: int,\n        learning_rate: float = 1e-3,\n        weight_decay: float = 1e-4,\n        gradient_clip: float = 1.0,\n        patience: int = 10,\n        warmup_steps: int = 1000\n    ):\n        \"\"\"\u5b8c\u5168\u306a\u6a5f\u80fd\u3092\u6301\u3064\u5b66\u7fd2\u30eb\u30fc\u30d7\"\"\"\n\n        # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306e\u8a2d\u5b9a\n        optimizer = self._setup_optimizer(learning_rate, weight_decay)\n\n        # \u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u306e\u8a2d\u5b9a\n        scheduler = self._setup_scheduler(optimizer, num_epochs, warmup_steps)\n\n        # \u640d\u5931\u95a2\u6570\n        criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n\n        # \u65e9\u671f\u7d42\u4e86\u306e\u8a2d\u5b9a\n        early_stopping = EarlyStopping(patience=patience)\n\n        # \u5b66\u7fd2\u30eb\u30fc\u30d7\n        for epoch in range(num_epochs):\n            print(f\"\\n{'='*50}\")\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n            print(f\"{'='*50}\")\n\n            # \u5b66\u7fd2\n            train_metrics = self._train_epoch(\n                train_loader, optimizer, criterion, scheduler, gradient_clip\n            )\n\n            # \u691c\u8a3c\n            val_metrics = self._validate_epoch(val_loader, criterion)\n\n            # \u5c65\u6b74\u306e\u8a18\u9332\n            self._update_history(train_metrics, val_metrics, optimizer)\n\n            # \u6700\u826f\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n            if val_metrics['accuracy'] &gt; self.best_val_accuracy:\n                self.best_val_accuracy = val_metrics['accuracy']\n                self.best_model_state = self.model.state_dict().copy()\n                print(f\"\u2713 \u6700\u826f\u30e2\u30c7\u30eb\u66f4\u65b0! Val Acc: {val_metrics['accuracy']:.4f}\")\n\n            # \u65e9\u671f\u7d42\u4e86\u306e\u30c1\u30a7\u30c3\u30af\n            if early_stopping(val_metrics['loss']):\n                print(f\"\\n\u65e9\u671f\u7d42\u4e86: {epoch+1}\u30a8\u30dd\u30c3\u30af\u3067\u505c\u6b62\")\n                break\n\n            # \u5b9a\u671f\u7684\u306a\u53ef\u8996\u5316\n            if (epoch + 1) % 5 == 0:\n                self._visualize_training_progress()\n\n    def _setup_optimizer(self, learning_rate: float, weight_decay: float):\n        \"\"\"\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306e\u8a2d\u5b9a\uff08\u5c64\u3054\u3068\u306b\u7570\u306a\u308b\u5b66\u7fd2\u7387\uff09\"\"\"\n        # \u5c64\u3054\u3068\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30b0\u30eb\u30fc\u30d7\u5316\n        param_groups = []\n\n        for name, param in self.model.named_parameters():\n            if not param.requires_grad:\n                continue\n\n            # \u5c64\u306e\u7a2e\u985e\u306b\u5fdc\u3058\u3066\u5b66\u7fd2\u7387\u3092\u8abf\u6574\n            if 'embedding' in name:\n                lr_scale = 0.1  # \u57cb\u3081\u8fbc\u307f\u5c64\u306f\u5c0f\u3055\u3044\u5b66\u7fd2\u7387\n            elif 'classifier' in name or 'output' in name:\n                lr_scale = 10.0  # \u51fa\u529b\u5c64\u306f\u5927\u304d\u3044\u5b66\u7fd2\u7387\n            else:\n                lr_scale = 1.0\n\n            # \u91cd\u307f\u6e1b\u8870\u306e\u8abf\u6574\uff08\u30d0\u30a4\u30a2\u30b9\u3068LayerNorm\u306b\u306f\u9069\u7528\u3057\u306a\u3044\uff09\n            wd = 0 if 'bias' in name or 'norm' in name else weight_decay\n\n            param_groups.append({\n                'params': param,\n                'lr': learning_rate * lr_scale,\n                'weight_decay': wd,\n                'name': name\n            })\n\n        return torch.optim.AdamW(param_groups)\n\n    def _setup_scheduler(self, optimizer, num_epochs: int, warmup_steps: int):\n        \"\"\"\u5b66\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u306e\u8a2d\u5b9a\"\"\"\n        # \u30ab\u30b9\u30bf\u30e0\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\uff1a\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7 + \u30b3\u30b5\u30a4\u30f3\u6e1b\u8870\n        def lr_lambda(step):\n            if step &lt; warmup_steps:\n                return step / warmup_steps\n            else:\n                progress = (step - warmup_steps) / (num_epochs * 1000 - warmup_steps)\n                return 0.5 * (1 + math.cos(math.pi * progress))\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    def _train_epoch(self, loader, optimizer, criterion, scheduler, gradient_clip):\n        \"\"\"1\u30a8\u30dd\u30c3\u30af\u306e\u5b66\u7fd2\"\"\"\n        self.model.train()\n\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        # \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\n        pbar = tqdm(loader, desc='Training', leave=False)\n\n        for batch_idx, (data, target) in enumerate(pbar):\n            data, target = data.to(self.device), target.to(self.device)\n\n            # Mixed Precision Training\n            with torch.cuda.amp.autocast():\n                output = self.model(data)\n                loss = criterion(output, target)\n\n            # \u9006\u4f1d\u64ad\n            optimizer.zero_grad()\n            loss.backward()\n\n            # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                self.model.parameters(), gradient_clip\n            )\n\n            # \u66f4\u65b0\n            optimizer.step()\n            scheduler.step()\n\n            # \u7d71\u8a08\n            total_loss += loss.item() * data.size(0)\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n            total += target.size(0)\n\n            # \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u66f4\u65b0\n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100. * correct / total:.2f}%',\n                'grad_norm': f'{grad_norm:.3f}',\n                'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n            })\n\n        return {\n            'loss': total_loss / total,\n            'accuracy': correct / total\n        }\n\n    def _visualize_training_progress(self):\n        \"\"\"\u5b66\u7fd2\u9032\u6357\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n        epochs = range(1, len(self.history['train_loss']) + 1)\n\n        # Loss\n        ax = axes[0, 0]\n        ax.plot(epochs, self.history['train_loss'], 'b-', label='Train')\n        ax.plot(epochs, self.history['val_loss'], 'r-', label='Val')\n        ax.set_title('Loss')\n        ax.set_xlabel('Epoch')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # Accuracy\n        ax = axes[0, 1]\n        ax.plot(epochs, self.history['train_acc'], 'b-', label='Train')\n        ax.plot(epochs, self.history['val_acc'], 'r-', label='Val')\n        ax.set_title('Accuracy')\n        ax.set_xlabel('Epoch')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # Learning Rate\n        ax = axes[0, 2]\n        ax.plot(epochs, self.history['lr'])\n        ax.set_title('Learning Rate')\n        ax.set_xlabel('Epoch')\n        ax.set_yscale('log')\n        ax.grid(True, alpha=0.3)\n\n        # Gradient Norm\n        ax = axes[1, 0]\n        ax.plot(epochs, self.history['grad_norm'])\n        ax.set_title('Gradient Norm')\n        ax.set_xlabel('Epoch')\n        ax.grid(True, alpha=0.3)\n\n        # \u904e\u5b66\u7fd2\u5ea6\n        ax = axes[1, 1]\n        overfit = np.array(self.history['train_acc']) - np.array(self.history['val_acc'])\n        ax.plot(epochs, overfit)\n        ax.axhline(y=0, color='k', linestyle='--')\n        ax.set_title('\u904e\u5b66\u7fd2\u5ea6 (Train - Val Acc)')\n        ax.set_xlabel('Epoch')\n        ax.grid(True, alpha=0.3)\n\n        # \u6700\u5f8c\u306e\u30a8\u30dd\u30c3\u30af\u306e\u8a73\u7d30\n        ax = axes[1, 2]\n        ax.text(0.1, 0.9, f\"\u6700\u7d42\u30a8\u30dd\u30c3\u30af\u7d71\u8a08:\", transform=ax.transAxes, fontsize=12, weight='bold')\n        stats_text = f\"\"\"\nTrain Loss: {self.history['train_loss'][-1]:.4f}\nVal Loss: {self.history['val_loss'][-1]:.4f}\nTrain Acc: {self.history['train_acc'][-1]:.4f}\nVal Acc: {self.history['val_acc'][-1]:.4f}\nBest Val Acc: {max(self.history['val_acc']):.4f}\nLearning Rate: {self.history['lr'][-1]:.6f}\n        \"\"\"\n        ax.text(0.1, 0.1, stats_text, transform=ax.transAxes, fontsize=10)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\nclass EarlyStopping:\n    \"\"\"\u65e9\u671f\u7d42\u4e86\u306e\u5b9f\u88c5\"\"\"\n    def __init__(self, patience: int = 10, min_delta: float = 0.0001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss: float) -&gt; bool:\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss &gt; self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\n        return self.early_stop\n</code></pre>"},{"location":"part1/pytorch-basics/#pytorch_2","title":"\u307e\u3068\u3081\uff1aPyTorch\u30de\u30b9\u30bf\u30fc\u3078\u306e\u9053","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060PyTorch\u306e\u57fa\u790e\u306f\u3001Transformer\u3092\u5b9f\u88c5\u3059\u308b\u4e0a\u3067\u306e\u571f\u53f0\u3068\u306a\u308a\u307e\u3059\uff1a</p> <ol> <li>\u30c6\u30f3\u30bd\u30eb\u64cd\u4f5c: NumPy\u611f\u899a\u3067\u4f7f\u3048\u308b\u304c\u3001\u81ea\u52d5\u5fae\u5206\u3068GPU\u5bfe\u5fdc\u3068\u3044\u3046\u5f37\u529b\u306a\u6a5f\u80fd\u3092\u6301\u3064</li> <li>\u81ea\u52d5\u5fae\u5206: \u8a08\u7b97\u30b0\u30e9\u30d5\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3057\u3001\u52fe\u914d\u306e\u6d41\u308c\u3092\u5236\u5fa1\u3067\u304d\u308b</li> <li>\u30e2\u30b8\u30e5\u30fc\u30eb\u8a2d\u8a08: \u518d\u5229\u7528\u53ef\u80fd\u3067\u7d44\u307f\u5408\u308f\u305b\u53ef\u80fd\u306a\u90e8\u54c1\u3068\u3057\u3066\u5b9f\u88c5</li> <li>\u5b66\u7fd2\u30eb\u30fc\u30d7: \u5b9f\u8df5\u7684\u306a\u6a5f\u80fd\uff08\u65e9\u671f\u7d42\u4e86\u3001\u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u3001\u30b9\u30b1\u30b8\u30e5\u30fc\u30ea\u30f3\u30b0\u7b49\uff09\u3092\u542b\u3080</li> </ol> <p>\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u77e5\u8b58\u3092\u6d3b\u7528\u3057\u3066\u3001\u3044\u3088\u3044\u3088Transformer\u306e\u4e2d\u6838\u3067\u3042\u308b\u300c\u5358\u8a9e\u306e\u6570\u5024\u8868\u73fe\u300d\u306b\u53d6\u308a\u7d44\u307f\u307e\u3059\u3002</p>"},{"location":"part1/pytorch-basics/#_7","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u30c6\u30f3\u30bd\u30eb\u64cd\u4f5c: \u30d0\u30c3\u30c1\u51e6\u7406\u3055\u308c\u305f\u30b7\u30fc\u30b1\u30f3\u30b9\u30c7\u30fc\u30bf\uff08shape: [batch, seq_len, features]\uff09\u304b\u3089\u3001\u5404\u30d0\u30c3\u30c1\u306e\u6700\u9577\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u3092\u52b9\u7387\u7684\u306b\u62bd\u51fa\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u81ea\u52d5\u5fae\u5206: \u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\u300cSwish (x * sigmoid(x))\u300d\u3092\u3001\u81ea\u52d5\u5fae\u5206\u5bfe\u5fdc\u3067\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u30e2\u30b8\u30e5\u30fc\u30eb\u8a2d\u8a08: \u6b8b\u5dee\u63a5\u7d9a\u3068\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u542b\u3080\u3001\u6df1\u3055\u53ef\u5909\u306eMLP\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5b66\u7fd2\u6700\u9069\u5316: \u5b66\u7fd2\u7387\u306e\u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\u3068\u518d\u30b9\u30bf\u30fc\u30c8\uff08SGDR\uff09\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u7b2c2\u90e8\uff1aTransformer\u3078\u306e\u9053\u306e\u308a - \u5358\u8a9e\u306e\u6570\u5024\u8868\u73fe\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part1/similarities/","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3068\u306e\u985e\u4f3c\u70b9","text":""},{"location":"part1/similarities/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u4e8c\u3064\u306e\u8a00\u8a9e\u51e6\u7406\u306e\u51fa\u4f1a\u3044","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u8a2d\u8a08\u8005\u3068\u3057\u3066\u3001\u3042\u306a\u305f\u306f\u300c\u8a00\u8a9e\u300d\u3092\u51e6\u7406\u3059\u308b\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3067\u3059\u3002\u5b57\u53e5\u89e3\u6790\u3001\u69cb\u6587\u89e3\u6790\u3001\u610f\u5473\u89e3\u6790\u3001\u6700\u9069\u5316...\u3053\u308c\u3089\u306e\u6982\u5ff5\u306f\u3001\u5b9f\u306fTransformer\u304c\u81ea\u7136\u8a00\u8a9e\u3092\u51e6\u7406\u3059\u308b\u65b9\u6cd5\u3068\u9a5a\u304f\u307b\u3069\u985e\u4f3c\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u3042\u306a\u305f\u304c\u3059\u3067\u306b\u6301\u3063\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u306e\u77e5\u8b58\u3092\u6d3b\u7528\u3057\u3066\u3001Transformer\u306e\u52d5\u4f5c\u539f\u7406\u3092\u6df1\u304f\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u30b3\u30f3\u30d1\u30a4\u30e9\u6280\u8853\u3068Transformer\u306e\u9593\u306b\u6a4b\u3092\u67b6\u3051\u3001\u65b0\u3057\u3044\u6280\u8853\u3092\u89aa\u3057\u307f\u3084\u3059\u304f\u3001\u305d\u3057\u3066\u672c\u8cea\u7684\u306b\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002</p>"},{"location":"part1/similarities/#21","title":"2.1 \u5b57\u53e5\u89e3\u6790\u3068\u30c8\u30fc\u30af\u30f3\u5316\u306e\u6df1\u3044\u5bfe\u6bd4","text":""},{"location":"part1/similarities/#_3","title":"\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5b57\u53e5\u89e3\u6790\u5668\u3092\u632f\u308a\u8fd4\u308b","text":"<p>\u307e\u305a\u3001\u5178\u578b\u7684\u306a\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5b57\u53e5\u89e3\u6790\u5668\uff08\u30ec\u30ad\u30b5\u30fc\uff09\u306e\u5b9f\u88c5\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <pre><code>import re\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\n\nclass TokenType(Enum):\n    # \u30ea\u30c6\u30e9\u30eb\n    NUMBER = auto()\n    STRING = auto()\n    IDENTIFIER = auto()\n\n    # \u30ad\u30fc\u30ef\u30fc\u30c9\n    IF = auto()\n    ELSE = auto()\n    WHILE = auto()\n    FUNCTION = auto()\n    RETURN = auto()\n\n    # \u6f14\u7b97\u5b50\n    PLUS = auto()\n    MINUS = auto()\n    MULTIPLY = auto()\n    DIVIDE = auto()\n    ASSIGN = auto()\n    EQUALS = auto()\n    NOT_EQUALS = auto()\n\n    # \u30c7\u30ea\u30df\u30bf\n    LPAREN = auto()\n    RPAREN = auto()\n    LBRACE = auto()\n    RBRACE = auto()\n    SEMICOLON = auto()\n    COMMA = auto()\n\n    # \u7279\u6b8a\n    EOF = auto()\n    WHITESPACE = auto()\n    COMMENT = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: str\n    line: int\n    column: int\n\nclass Lexer:\n    def __init__(self, source_code: str):\n        self.source = source_code\n        self.position = 0\n        self.line = 1\n        self.column = 1\n        self.tokens: List[Token] = []\n\n        # \u30c8\u30fc\u30af\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u5b9a\u7fa9\uff08\u512a\u5148\u9806\u4f4d\u9806\uff09\n        self.token_patterns = [\n            # \u7a7a\u767d\u3068\u30b3\u30e1\u30f3\u30c8\n            (r'[ \\t]+', TokenType.WHITESPACE),\n            (r'//[^\\n]*', TokenType.COMMENT),\n            (r'/\\*[\\s\\S]*?\\*/', TokenType.COMMENT),\n\n            # \u30ea\u30c6\u30e9\u30eb\n            (r'\\d+\\.?\\d*', TokenType.NUMBER),\n            (r'\"[^\"]*\"', TokenType.STRING),\n            (r\"'[^']*'\", TokenType.STRING),\n\n            # \u30ad\u30fc\u30ef\u30fc\u30c9\uff08\u8b58\u5225\u5b50\u3088\u308a\u5148\u306b\u30de\u30c3\u30c1\u3055\u305b\u308b\uff09\n            (r'\\bif\\b', TokenType.IF),\n            (r'\\belse\\b', TokenType.ELSE),\n            (r'\\bwhile\\b', TokenType.WHILE),\n            (r'\\bfunction\\b', TokenType.FUNCTION),\n            (r'\\breturn\\b', TokenType.RETURN),\n\n            # \u8b58\u5225\u5b50\n            (r'[a-zA-Z_][a-zA-Z0-9_]*', TokenType.IDENTIFIER),\n\n            # \u6f14\u7b97\u5b50\uff08\u9577\u3044\u3082\u306e\u304b\u3089\u5148\u306b\uff09\n            (r'==', TokenType.EQUALS),\n            (r'!=', TokenType.NOT_EQUALS),\n            (r'=', TokenType.ASSIGN),\n            (r'\\+', TokenType.PLUS),\n            (r'-', TokenType.MINUS),\n            (r'\\*', TokenType.MULTIPLY),\n            (r'/', TokenType.DIVIDE),\n\n            # \u30c7\u30ea\u30df\u30bf\n            (r'\\(', TokenType.LPAREN),\n            (r'\\)', TokenType.RPAREN),\n            (r'\\{', TokenType.LBRACE),\n            (r'\\}', TokenType.RBRACE),\n            (r';', TokenType.SEMICOLON),\n            (r',', TokenType.COMMA),\n        ]\n\n        # \u30d1\u30bf\u30fc\u30f3\u3092\u30b3\u30f3\u30d1\u30a4\u30eb\n        self.compiled_patterns = [\n            (re.compile(pattern), token_type) \n            for pattern, token_type in self.token_patterns\n        ]\n\n    def tokenize(self) -&gt; List[Token]:\n        \"\"\"\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u5168\u4f53\u3092\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        while self.position &lt; len(self.source):\n            # \u6539\u884c\u306e\u51e6\u7406\n            if self.source[self.position] == '\\n':\n                self.line += 1\n                self.column = 1\n                self.position += 1\n                continue\n\n            # \u30c8\u30fc\u30af\u30f3\u306e\u30de\u30c3\u30c1\u30f3\u30b0\n            matched = False\n            for pattern, token_type in self.compiled_patterns:\n                match = pattern.match(self.source, self.position)\n                if match:\n                    value = match.group(0)\n\n                    # \u7a7a\u767d\u3068\u30b3\u30e1\u30f3\u30c8\u306f\u30c8\u30fc\u30af\u30f3\u3068\u3057\u3066\u4fdd\u5b58\u3057\u306a\u3044\n                    if token_type not in [TokenType.WHITESPACE, TokenType.COMMENT]:\n                        token = Token(token_type, value, self.line, self.column)\n                        self.tokens.append(token)\n\n                    # \u4f4d\u7f6e\u3092\u66f4\u65b0\n                    self.position = match.end()\n                    self.column += len(value)\n                    matched = True\n                    break\n\n            if not matched:\n                raise SyntaxError(\n                    f\"Unexpected character '{self.source[self.position]}' \"\n                    f\"at line {self.line}, column {self.column}\"\n                )\n\n        # EOF \u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0\n        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))\n        return self.tokens\n\n    def print_tokens(self):\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u3092\u898b\u3084\u3059\u304f\u8868\u793a\"\"\"\n        for token in self.tokens:\n            print(f\"{token.type.name:12} | {token.value:10} | Line {token.line}:{token.column}\")\n\n# \u4f7f\u7528\u4f8b\nsource_code = \"\"\"\nfunction fibonacci(n) {\n    if (n &lt;= 1) {\n        return n;\n    }\n    return fibonacci(n - 1) + fibonacci(n - 2);\n}\n\"\"\"\n\nlexer = Lexer(source_code)\ntokens = lexer.tokenize()\nlexer.print_tokens()\n</code></pre>"},{"location":"part1/similarities/#_4","title":"\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\uff1a\u3088\u308a\u8907\u96d1\u306a\u4e16\u754c","text":"<p>\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3088\u308a\u3082\u306f\u308b\u304b\u306b\u8907\u96d1\u3067\u3059\u3002\u306a\u305c\u306a\u3089\uff1a</p> <ol> <li>\u660e\u78ba\u306a\u533a\u5207\u308a\u6587\u5b57\u304c\u306a\u3044\u5834\u5408\u304c\u3042\u308b\uff08\u7279\u306b\u65e5\u672c\u8a9e\u3084\u4e2d\u56fd\u8a9e\uff09</li> <li>\u540c\u3058\u6587\u5b57\u5217\u304c\u6587\u8108\u306b\u3088\u3063\u3066\u7570\u306a\u308b\u610f\u5473\u3092\u6301\u3064</li> <li>\u65b0\u3057\u3044\u5358\u8a9e\u304c\u5e38\u306b\u751f\u307e\u308c\u308b</li> <li>\u8aa4\u5b57\u8131\u5b57\u3084\u65b9\u8a00\u304c\u5b58\u5728\u3059\u308b</li> </ol> <pre><code>import torch\nimport torch.nn as nn\nfrom typing import Dict, List, Tuple, Optional\nfrom collections import Counter\nimport regex  # \u3088\u308a\u9ad8\u5ea6\u306a\u6b63\u898f\u8868\u73fe\u30b5\u30dd\u30fc\u30c8\n\nclass NaturalLanguageTokenizer:\n    \"\"\"\n    \u81ea\u7136\u8a00\u8a9e\u7528\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\n    BPE (Byte Pair Encoding) \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\n    \"\"\"\n    def __init__(self, vocab_size: int = 10000):\n        self.vocab_size = vocab_size\n        self.word_tokenizer = regex.compile(r\"\"\"\n            (?:[^\\s\\p{P}]+)  |  # \u5358\u8a9e\uff08\u53e5\u8aad\u70b9\u4ee5\u5916\uff09\n            (?:\\p{P})           # \u53e5\u8aad\u70b9\n        \"\"\", regex.VERBOSE)\n\n        # \u7279\u6b8a\u30c8\u30fc\u30af\u30f3\n        self.special_tokens = {\n            '&lt;PAD&gt;': 0,    # \u30d1\u30c7\u30a3\u30f3\u30b0\n            '&lt;UNK&gt;': 1,    # \u672a\u77e5\u8a9e\n            '&lt;BOS&gt;': 2,    # \u6587\u7ae0\u958b\u59cb\n            '&lt;EOS&gt;': 3,    # \u6587\u7ae0\u7d42\u4e86\n            '&lt;MASK&gt;': 4,   # \u30de\u30b9\u30af\uff08BERT\u7528\uff09\n        }\n\n        self.token_to_id = dict(self.special_tokens)\n        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n        self.merge_rules = {}  # BPE\u306e\u30de\u30fc\u30b8\u30eb\u30fc\u30eb\n\n    def train_bpe(self, corpus: List[str], min_frequency: int = 2):\n        \"\"\"\n        BPE\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u3092\u5b66\u7fd2\n        \u3053\u308c\u306f\u3001\u30b3\u30f3\u30d1\u30a4\u30e9\u3067\u306e\u300c\u5171\u901a\u90e8\u5206\u5f0f\u306e\u62bd\u51fa\u300d\u306b\u4f3c\u3066\u3044\u308b\n        \"\"\"\n        # \u30b9\u30c6\u30c3\u30d71: \u5358\u8a9e\u306e\u983b\u5ea6\u3092\u6570\u3048\u308b\n        word_freq = Counter()\n        for text in corpus:\n            words = self.word_tokenizer.findall(text.lower())\n            for word in words:\n                word_freq[word] += 1\n\n        # \u30b9\u30c6\u30c3\u30d72: \u5404\u5358\u8a9e\u3092\u6587\u5b57\u306b\u5206\u89e3\uff08\u521d\u671f\u5316\uff09\n        word_splits = {}\n        for word, freq in word_freq.items():\n            # \u5358\u8a9e\u3092\u6587\u5b57\u306e\u30ea\u30b9\u30c8\u306b\u5206\u89e3\u3057\u3001\u7d42\u7aef\u8a18\u53f7\u3092\u8ffd\u52a0\n            word_splits[word] = list(word) + ['&lt;/w&gt;']\n\n        # \u30b9\u30c6\u30c3\u30d73: \u30de\u30fc\u30b8\u3092\u7e70\u308a\u8fd4\u3059\n        num_merges = self.vocab_size - len(self.special_tokens) - 256  # \u57fa\u672c\u6587\u5b57\u5206\u3092\u5f15\u304f\n\n        for i in range(num_merges):\n            # \u96a3\u63a5\u3059\u308b\u30da\u30a2\u306e\u983b\u5ea6\u3092\u6570\u3048\u308b\n            pair_freq = Counter()\n            for word, splits in word_splits.items():\n                freq = word_freq[word]\n                for j in range(len(splits) - 1):\n                    pair = (splits[j], splits[j + 1])\n                    pair_freq[pair] += freq\n\n            # \u6700\u3082\u983b\u5ea6\u306e\u9ad8\u3044\u30da\u30a2\u3092\u9078\u629e\n            if not pair_freq:\n                break\n\n            best_pair = max(pair_freq, key=pair_freq.get)\n            if pair_freq[best_pair] &lt; min_frequency:\n                break\n\n            # \u30de\u30fc\u30b8\u30eb\u30fc\u30eb\u3092\u8a18\u9332\n            self.merge_rules[best_pair] = best_pair[0] + best_pair[1]\n\n            # \u5168\u3066\u306e\u5358\u8a9e\u3067\u30da\u30a2\u3092\u30de\u30fc\u30b8\n            new_word_splits = {}\n            for word, splits in word_splits.items():\n                new_splits = []\n                i = 0\n                while i &lt; len(splits):\n                    if (i &lt; len(splits) - 1 and \n                        (splits[i], splits[i + 1]) == best_pair):\n                        new_splits.append(best_pair[0] + best_pair[1])\n                        i += 2\n                    else:\n                        new_splits.append(splits[i])\n                        i += 1\n                new_word_splits[word] = new_splits\n\n            word_splits = new_word_splits\n\n            # \u9032\u6357\u8868\u793a\n            if (i + 1) % 1000 == 0:\n                print(f\"Merge {i + 1}/{num_merges}: {best_pair} -&gt; {best_pair[0] + best_pair[1]}\")\n\n        # \u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u306e\u69cb\u7bc9\n        self._build_vocab(word_splits, word_freq)\n\n    def _build_vocab(self, word_splits: Dict[str, List[str]], word_freq: Dict[str, int]):\n        \"\"\"\u5b66\u7fd2\u3057\u305fBPE\u304b\u3089\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\u3092\u69cb\u7bc9\"\"\"\n        # \u5168\u3066\u306e\u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u53ce\u96c6\n        subwords = Counter()\n        for word, splits in word_splits.items():\n            freq = word_freq[word]\n            for subword in splits:\n                subwords[subword] += freq\n\n        # \u983b\u5ea6\u9806\u306b\u30bd\u30fc\u30c8\u3057\u3066ID\u3092\u5272\u308a\u5f53\u3066\n        current_id = len(self.special_tokens)\n        for subword, _ in subwords.most_common(self.vocab_size - len(self.special_tokens)):\n            if subword not in self.token_to_id:\n                self.token_to_id[subword] = current_id\n                self.id_to_token[current_id] = subword\n                current_id += 1\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5206\u89e3\"\"\"\n        tokens = []\n        words = self.word_tokenizer.findall(text.lower())\n\n        for word in words:\n            # BPE\u30eb\u30fc\u30eb\u3092\u9069\u7528\u3057\u3066\u30b5\u30d6\u30ef\u30fc\u30c9\u306b\u5206\u89e3\n            word_tokens = self._apply_bpe(word)\n            tokens.extend(word_tokens)\n\n        return tokens\n\n    def _apply_bpe(self, word: str) -&gt; List[str]:\n        \"\"\"\u5358\u8a9e\u306bBPE\u30eb\u30fc\u30eb\u3092\u9069\u7528\"\"\"\n        # \u5358\u8a9e\u3092\u6587\u5b57\u306b\u5206\u89e3\n        splits = list(word) + ['&lt;/w&gt;']\n\n        # \u30de\u30fc\u30b8\u30eb\u30fc\u30eb\u3092\u9806\u756a\u306b\u9069\u7528\n        changed = True\n        while changed:\n            changed = False\n            new_splits = []\n            i = 0\n\n            while i &lt; len(splits):\n                if i &lt; len(splits) - 1:\n                    pair = (splits[i], splits[i + 1])\n                    if pair in self.merge_rules:\n                        new_splits.append(self.merge_rules[pair])\n                        i += 2\n                        changed = True\n                        continue\n\n                new_splits.append(splits[i])\n                i += 1\n\n            splits = new_splits\n\n        return splits\n\n    def encode(self, text: str) -&gt; List[int]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092ID\u306e\u30ea\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = self.tokenize(text)\n        ids = []\n\n        for token in tokens:\n            if token in self.token_to_id:\n                ids.append(self.token_to_id[token])\n            else:\n                ids.append(self.token_to_id['&lt;UNK&gt;'])\n\n        return ids\n\n    def decode(self, ids: List[int]) -&gt; str:\n        \"\"\"ID\u306e\u30ea\u30b9\u30c8\u3092\u30c6\u30ad\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = []\n        for id in ids:\n            if id in self.id_to_token:\n                token = self.id_to_token[id]\n                if token not in self.special_tokens:\n                    tokens.append(token)\n\n        # \u30b5\u30d6\u30ef\u30fc\u30c9\u3092\u7d50\u5408\n        text = ''.join(tokens)\n        text = text.replace('&lt;/w&gt;', ' ')\n        return text.strip()\n\n# \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6bd4\u8f03\u30c7\u30e2\ndef compare_tokenizers():\n    \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\u3092\u6bd4\u8f03\"\"\"\n\n    # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b\n    code = \"if (x &gt; 0) { return x * 2; }\"\n    code_lexer = Lexer(code)\n    code_tokens = code_lexer.tokenize()\n\n    print(\"=== \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316 ===\")\n    print(f\"\u5165\u529b: {code}\")\n    print(\"\u30c8\u30fc\u30af\u30f3:\")\n    for token in code_tokens[:-1]:  # EOF\u4ee5\u5916\n        print(f\"  {token.value}\")\n\n    # \u81ea\u7136\u8a00\u8a9e\u306e\u4f8b\n    text = \"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3068Transformer\u306e\u985e\u4f3c\u70b9\u3092\u7406\u89e3\u3059\u308b\"\n\n    # \u7c21\u5358\u306a\u4f8b\u306e\u305f\u3081\u3001\u4e8b\u524d\u5b9a\u7fa9\u3055\u308c\u305f\u30eb\u30fc\u30eb\u3092\u4f7f\u7528\n    nl_tokenizer = NaturalLanguageTokenizer()\n\n    # \u6587\u5b57\u30ec\u30d9\u30eb\u306e\u30c8\u30fc\u30af\u30f3\u5316\uff08\u30c7\u30e2\u7528\uff09\n    char_tokens = list(text)\n    print(\"\\n=== \u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\uff08\u6587\u5b57\u30ec\u30d9\u30eb\uff09===\")\n    print(f\"\u5165\u529b: {text}\")\n    print(f\"\u30c8\u30fc\u30af\u30f3: {char_tokens}\")\n\n    # \u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b\n    print(\"\\n=== \u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b ===\")\n    subword_example = \"understanding unbelievable preprocessing\"\n    # \u4eee\u60f3\u7684\u306aBPE\u7d50\u679c\n    subword_tokens = [\"under\", \"stand\", \"ing\", \" \", \"un\", \"believ\", \"able\", \" \", \"pre\", \"process\", \"ing\"]\n    print(f\"\u5165\u529b: {subword_example}\")\n    print(f\"\u30c8\u30fc\u30af\u30f3: {subword_tokens}\")\n\ncompare_tokenizers()\n</code></pre>"},{"location":"part1/similarities/#_5","title":"\u30c8\u30fc\u30af\u30f3\u5316\u306e\u672c\u8cea\u7684\u306a\u9055\u3044\u3068\u5171\u901a\u70b9","text":"<pre><code>class TokenizationComparison:\n    \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\u3092\u8a73\u7d30\u306b\u6bd4\u8f03\"\"\"\n\n    def __init__(self):\n        self.prog_lang_characteristics = {\n            \"deterministic\": True,      # \u6c7a\u5b9a\u7684\n            \"context_free\": True,       # \u6587\u8108\u81ea\u7531\n            \"fixed_vocabulary\": True,   # \u56fa\u5b9a\u8a9e\u5f59\n            \"strict_syntax\": True,      # \u53b3\u5bc6\u306a\u69cb\u6587\n            \"error_handling\": \"reject\", # \u30a8\u30e9\u30fc\u6642\u306f\u62d2\u5426\n        }\n\n        self.natural_lang_characteristics = {\n            \"deterministic\": False,     # \u975e\u6c7a\u5b9a\u7684\uff08\u66d6\u6627\u6027\u3042\u308a\uff09\n            \"context_free\": False,      # \u6587\u8108\u4f9d\u5b58\n            \"fixed_vocabulary\": False,  # \u958b\u653e\u8a9e\u5f59\n            \"strict_syntax\": False,     # \u67d4\u8edf\u306a\u69cb\u6587\n            \"error_handling\": \"robust\", # \u30a8\u30e9\u30fc\u306b\u5bdb\u5bb9\n        }\n\n    def demonstrate_ambiguity(self):\n        \"\"\"\u81ea\u7136\u8a00\u8a9e\u306e\u66d6\u6627\u6027\u3092\u793a\u3059\u4f8b\"\"\"\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\uff1a\u660e\u78ba\n        code_example = \"list.append(item)\"\n        print(\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b:\")\n        print(f\"  {code_example}\")\n        print(\"  \u89e3\u91c8: 'list'\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e'append'\u30e1\u30bd\u30c3\u30c9\u3092'item'\u5f15\u6570\u3067\u547c\u3073\u51fa\u3059\")\n        print(\"  \u66d6\u6627\u6027: \u306a\u3057\")\n\n        # \u81ea\u7136\u8a00\u8a9e\uff1a\u66d6\u6627\n        nl_example = \"I saw the man with the telescope\"\n        print(\"\\n\u81ea\u7136\u8a00\u8a9e\u306e\u4f8b:\")\n        print(f\"  {nl_example}\")\n        print(\"  \u89e3\u91c81: \u671b\u9060\u93e1\u3092\u4f7f\u3063\u3066\u7537\u3092\u898b\u305f\")\n        print(\"  \u89e3\u91c82: \u671b\u9060\u93e1\u3092\u6301\u3063\u3066\u3044\u308b\u7537\u3092\u898b\u305f\")\n        print(\"  \u66d6\u6627\u6027: \u3042\u308a\uff08\u69cb\u6587\u7684\u66d6\u6627\u6027\uff09\")\n\n        # Transformer\u306f\u3053\u306e\u66d6\u6627\u6027\u3092\u6587\u8108\u304b\u3089\u89e3\u6c7a\u3059\u308b\n        return self.show_context_resolution()\n\n    def show_context_resolution(self):\n        \"\"\"Transformer\u304c\u6587\u8108\u304b\u3089\u66d6\u6627\u6027\u3092\u89e3\u6c7a\u3059\u308b\u4ed5\u7d44\u307f\"\"\"\n\n        class ContextAwareTokenizer:\n            def __init__(self):\n                # \u4eee\u60f3\u7684\u306a\u6587\u8108\u8a8d\u8b58\u6a5f\u69cb\n                self.context_embeddings = {}\n\n            def tokenize_with_context(self, sentence: str, context: List[str]) -&gt; List[Tuple[str, str]]:\n                \"\"\"\n                \u6587\u8108\u3092\u8003\u616e\u3057\u305f\u30c8\u30fc\u30af\u30f3\u5316\n                \u8fd4\u308a\u5024: [(\u30c8\u30fc\u30af\u30f3, \u610f\u5473\u30bf\u30b0), ...]\n                \"\"\"\n                # \u4f8b: \"bank\"\u306e\u66d6\u6627\u6027\u89e3\u6c7a\n                if \"bank\" in sentence:\n                    # \u6587\u8108\u304b\u3089\u610f\u5473\u3092\u63a8\u5b9a\n                    financial_words = {\"money\", \"account\", \"loan\", \"deposit\"}\n                    river_words = {\"river\", \"water\", \"boat\", \"fishing\"}\n\n                    context_text = \" \".join(context).lower()\n\n                    if any(word in context_text for word in financial_words):\n                        meaning = \"financial_institution\"\n                    elif any(word in context_text for word in river_words):\n                        meaning = \"river_bank\"\n                    else:\n                        meaning = \"unknown\"\n\n                    return [(\"bank\", meaning)]\n\n                # \u7c21\u5358\u306e\u305f\u3081\u3001\u4ed6\u306e\u5358\u8a9e\u306f\u901a\u5e38\u306e\u30c8\u30fc\u30af\u30f3\u5316\n                return [(word, \"general\") for word in sentence.split()]\n\n        # \u30c7\u30e2\n        tokenizer = ContextAwareTokenizer()\n\n        # \u6587\u81081: \u91d1\u878d\u95a2\u9023\n        context1 = [\"I need to deposit money\", \"The interest rate is high\"]\n        sentence1 = \"I went to the bank\"\n        result1 = tokenizer.tokenize_with_context(sentence1, context1)\n        print(\"\\n\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b1:\")\n        print(f\"\u6587\u8108: {context1}\")\n        print(f\"\u6587: {sentence1}\")\n        print(f\"\u7d50\u679c: {result1}\")\n\n        # \u6587\u81082: \u5ddd\u95a2\u9023\n        context2 = [\"The river was flowing fast\", \"We took our fishing rods\"]\n        sentence2 = \"We sat on the bank\"\n        result2 = tokenizer.tokenize_with_context(sentence2, context2)\n        print(\"\\n\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b2:\")\n        print(f\"\u6587\u8108: {context2}\")\n        print(f\"\u6587: {sentence2}\")\n        print(f\"\u7d50\u679c: {result2}\")\n\n# \u5b9f\u884c\ncomparison = TokenizationComparison()\ncomparison.demonstrate_ambiguity()\n</code></pre>"},{"location":"part1/similarities/#_6","title":"\u7d71\u4e00\u7684\u306a\u8996\u70b9\uff1a\u30c8\u30fc\u30af\u30f3\u5316\u306e\u672c\u8cea","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\u3092\u7d71\u4e00\u7684\u306b\u7406\u89e3\u3059\u308b\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u69cb\u7bc9\u3057\u307e\u3057\u3087\u3046\uff1a</p> <pre><code>from abc import ABC, abstractmethod\nimport numpy as np\n\nclass UniversalTokenizer(ABC):\n    \"\"\"\n    \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u4e21\u65b9\u306b\u9069\u7528\u53ef\u80fd\u306a\n    \u7d71\u4e00\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\n    \"\"\"\n\n    @abstractmethod\n    def tokenize(self, input_text: str) -&gt; List[Token]:\n        \"\"\"\u5165\u529b\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5206\u89e3\"\"\"\n        pass\n\n    @abstractmethod\n    def embed(self, tokens: List[Token]) -&gt; np.ndarray:\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u3092\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306b\u5909\u63db\"\"\"\n        pass\n\n    @abstractmethod\n    def handle_unknown(self, unknown_text: str) -&gt; List[Token]:\n        \"\"\"\u672a\u77e5\u306e\u5165\u529b\u306e\u51e6\u7406\"\"\"\n        pass\n\nclass HybridTokenizer(UniversalTokenizer):\n    \"\"\"\n    \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u4e21\u65b9\u3092\u6271\u3048\u308b\n    \u30cf\u30a4\u30d6\u30ea\u30c3\u30c9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\n    \"\"\"\n\n    def __init__(self, vocab_size: int = 50000):\n        self.vocab_size = vocab_size\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u7528\u306e\u78ba\u5b9a\u7684\u30eb\u30fc\u30eb\n        self.prog_rules = {\n            'keywords': ['if', 'else', 'for', 'while', 'function', 'class', 'return'],\n            'operators': ['+', '-', '*', '/', '=', '==', '!=', '&lt;', '&gt;', '&lt;=', '&gt;='],\n            'delimiters': ['(', ')', '{', '}', '[', ']', ';', ',', '.'],\n        }\n\n        # \u81ea\u7136\u8a00\u8a9e\u7528\u306eBPE\u30eb\u30fc\u30eb\n        self.bpe_rules = {}\n\n        # \u7d71\u4e00\u7684\u306a\u8a9e\u5f59\n        self.unified_vocab = {}\n        self._initialize_vocab()\n\n        # \u57cb\u3081\u8fbc\u307f\u884c\u5217\n        self.embedding_dim = 512\n        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n\n    def _initialize_vocab(self):\n        \"\"\"\u7d71\u4e00\u8a9e\u5f59\u306e\u521d\u671f\u5316\"\"\"\n        current_id = 0\n\n        # \u7279\u6b8a\u30c8\u30fc\u30af\u30f3\n        special_tokens = ['&lt;PAD&gt;', '&lt;UNK&gt;', '&lt;BOS&gt;', '&lt;EOS&gt;', '&lt;CODE&gt;', '&lt;TEXT&gt;']\n        for token in special_tokens:\n            self.unified_vocab[token] = current_id\n            current_id += 1\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\n        for category in ['keywords', 'operators', 'delimiters']:\n            for token in self.prog_rules[category]:\n                if token not in self.unified_vocab:\n                    self.unified_vocab[token] = current_id\n                    current_id += 1\n\n        # \u57fa\u672c\u6587\u5b57\uff08ASCII\uff09\n        for i in range(256):\n            char = chr(i)\n            if char not in self.unified_vocab and char.isprintable():\n                self.unified_vocab[char] = current_id\n                current_id += 1\n\n    def detect_language_type(self, text: str) -&gt; str:\n        \"\"\"\u5165\u529b\u304c\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u304b\u81ea\u7136\u8a00\u8a9e\u304b\u3092\u5224\u5b9a\"\"\"\n        prog_indicators = 0\n        nl_indicators = 0\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u6307\u6a19\n        for keyword in self.prog_rules['keywords']:\n            if keyword in text:\n                prog_indicators += 2\n\n        for op in self.prog_rules['operators']:\n            if op in text:\n                prog_indicators += 1\n\n        # \u30bb\u30df\u30b3\u30ed\u30f3\u3084\u4e2d\u62ec\u5f27\u306e\u5b58\u5728\n        if ';' in text or '{' in text or '}' in text:\n            prog_indicators += 3\n\n        # \u81ea\u7136\u8a00\u8a9e\u306e\u6307\u6a19\n        # \u53e5\u8aad\u70b9\u306e\u4f7f\u7528\u30d1\u30bf\u30fc\u30f3\n        if '\u3002' in text or '\u3001' in text or '. ' in text or ', ' in text:\n            nl_indicators += 2\n\n        # \u5358\u8a9e\u306e\u9577\u3055\u306e\u5206\u5e03\n        words = text.split()\n        if words:\n            avg_word_length = sum(len(w) for w in words) / len(words)\n            if 3 &lt; avg_word_length &lt; 10:  # \u81ea\u7136\u8a00\u8a9e\u306e\u5178\u578b\u7684\u306a\u5358\u8a9e\u9577\n                nl_indicators += 2\n\n        return 'programming' if prog_indicators &gt; nl_indicators else 'natural'\n\n    def tokenize(self, input_text: str) -&gt; List[Token]:\n        \"\"\"\u7d71\u4e00\u7684\u306a\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        language_type = self.detect_language_type(input_text)\n\n        if language_type == 'programming':\n            return self._tokenize_programming(input_text)\n        else:\n            return self._tokenize_natural(input_text)\n\n    def _tokenize_programming(self, code: str) -&gt; List[Token]:\n        \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        tokens = []\n        # \u8a00\u8a9e\u30bf\u30a4\u30d7\u30de\u30fc\u30ab\u30fc\n        tokens.append(Token(TokenType.SPECIAL, '&lt;CODE&gt;', 0, 0))\n\n        # \u7c21\u7565\u5316\u3055\u308c\u305f\u5b9f\u88c5\n        # \u5b9f\u969b\u306b\u306f\u3088\u308a\u6d17\u7df4\u3055\u308c\u305f\u30ec\u30ad\u30b5\u30fc\u3092\u4f7f\u7528\n        import re\n\n        # \u30c8\u30fc\u30af\u30f3\u30d1\u30bf\u30fc\u30f3\n        token_pattern = r'(\\w+|[^\\w\\s]|\\s+)'\n        matches = re.findall(token_pattern, code)\n\n        for match in matches:\n            if match.strip():  # \u7a7a\u767d\u4ee5\u5916\n                if match in self.prog_rules['keywords']:\n                    token_type = TokenType.KEYWORD\n                elif match in self.prog_rules['operators']:\n                    token_type = TokenType.OPERATOR\n                elif match in self.prog_rules['delimiters']:\n                    token_type = TokenType.DELIMITER\n                elif match.isdigit():\n                    token_type = TokenType.NUMBER\n                elif match.isidentifier():\n                    token_type = TokenType.IDENTIFIER\n                else:\n                    token_type = TokenType.OTHER\n\n                tokens.append(Token(token_type, match, 0, 0))\n\n        return tokens\n\n    def _tokenize_natural(self, text: str) -&gt; List[Token]:\n        \"\"\"\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        tokens = []\n        # \u8a00\u8a9e\u30bf\u30a4\u30d7\u30de\u30fc\u30ab\u30fc\n        tokens.append(Token(TokenType.SPECIAL, '&lt;TEXT&gt;', 0, 0))\n\n        # BPE\u30c8\u30fc\u30af\u30f3\u5316\uff08\u7c21\u7565\u7248\uff09\n        # \u5b9f\u969b\u306b\u306f\u3088\u308a\u6d17\u7df4\u3055\u308c\u305fBPE\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\n        words = text.split()\n\n        for word in words:\n            # \u5358\u8a9e\u3092\u30b5\u30d6\u30ef\u30fc\u30c9\u306b\u5206\u89e3\n            subwords = self._apply_bpe_rules(word)\n            for subword in subwords:\n                tokens.append(Token(TokenType.SUBWORD, subword, 0, 0))\n\n        return tokens\n\n    def _apply_bpe_rules(self, word: str) -&gt; List[str]:\n        \"\"\"BPE\u30eb\u30fc\u30eb\u306e\u9069\u7528\uff08\u7c21\u7565\u7248\uff09\"\"\"\n        # \u5b9f\u969b\u306e\u5b9f\u88c5\u3067\u306f\u5b66\u7fd2\u3055\u308c\u305f\u30de\u30fc\u30b8\u30eb\u30fc\u30eb\u3092\u4f7f\u7528\n        # \u3053\u3053\u3067\u306f\u6587\u5b57\u5358\u4f4d\u306e\u5206\u89e3\u3092\u8fd4\u3059\n        return list(word) + ['&lt;/w&gt;']\n\n    def embed(self, tokens: List[Token]) -&gt; torch.Tensor:\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u3092\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\"\"\"\n        token_ids = []\n\n        for token in tokens:\n            if token.value in self.unified_vocab:\n                token_ids.append(self.unified_vocab[token.value])\n            else:\n                token_ids.append(self.unified_vocab['&lt;UNK&gt;'])\n\n        token_ids = torch.tensor(token_ids)\n        return self.embeddings(token_ids)\n\n    def handle_unknown(self, unknown_text: str) -&gt; List[Token]:\n        \"\"\"\u672a\u77e5\u306e\u5165\u529b\u306e\u51e6\u7406\"\"\"\n        # \u6587\u5b57\u30ec\u30d9\u30eb\u306b\u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\n        tokens = []\n        for char in unknown_text:\n            if char in self.unified_vocab:\n                tokens.append(Token(TokenType.OTHER, char, 0, 0))\n            else:\n                tokens.append(Token(TokenType.OTHER, '&lt;UNK&gt;', 0, 0))\n\n        return tokens\n\n    def visualize_tokenization(self, text: str):\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316\"\"\"\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as mpatches\n\n        tokens = self.tokenize(text)\n\n        # \u30c8\u30fc\u30af\u30f3\u30bf\u30a4\u30d7\u3054\u3068\u306e\u8272\n        colors = {\n            TokenType.KEYWORD: 'blue',\n            TokenType.IDENTIFIER: 'green',\n            TokenType.OPERATOR: 'red',\n            TokenType.NUMBER: 'orange',\n            TokenType.DELIMITER: 'purple',\n            TokenType.SUBWORD: 'brown',\n            TokenType.SPECIAL: 'gray',\n            TokenType.OTHER: 'black',\n        }\n\n        fig, ax = plt.subplots(figsize=(15, 3))\n\n        x_position = 0\n        for i, token in enumerate(tokens):\n            color = colors.get(token.type, 'black')\n\n            # \u30c8\u30fc\u30af\u30f3\u3092\u8868\u793a\n            rect = mpatches.Rectangle(\n                (x_position, 0), len(token.value) * 0.1, 1,\n                facecolor=color, edgecolor='black', alpha=0.7\n            )\n            ax.add_patch(rect)\n\n            # \u30c6\u30ad\u30b9\u30c8\u3092\u8ffd\u52a0\n            ax.text(\n                x_position + len(token.value) * 0.05, 0.5,\n                token.value, ha='center', va='center',\n                fontsize=8, rotation=0 if len(token.value) &lt; 5 else 45\n            )\n\n            x_position += len(token.value) * 0.1 + 0.05\n\n        ax.set_xlim(0, x_position)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n        ax.set_title(f'Tokenization Visualization: \"{text[:50]}...\"')\n\n        # \u51e1\u4f8b\n        legend_elements = [\n            mpatches.Patch(color=color, label=token_type.name)\n            for token_type, color in colors.items()\n            if any(t.type == token_type for t in tokens)\n        ]\n        ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.1, 1))\n\n        plt.tight_layout()\n        plt.show()\n\n# \u30c7\u30e2\uff1a\u30cf\u30a4\u30d6\u30ea\u30c3\u30c9\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u4f7f\u7528\nhybrid_tokenizer = HybridTokenizer()\n\n# \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b\ncode_example = \"function add(a, b) { return a + b; }\"\nprint(\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316:\")\ncode_tokens = hybrid_tokenizer.tokenize(code_example)\nfor token in code_tokens[:10]:  # \u6700\u521d\u306e10\u30c8\u30fc\u30af\u30f3\n    print(f\"  {token.type.name}: {token.value}\")\n\n# \u81ea\u7136\u8a00\u8a9e\u306e\u4f8b\ntext_example = \"Transformer\u306f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306b\u9769\u547d\u3092\u3082\u305f\u3089\u3057\u307e\u3057\u305f\u3002\"\nprint(\"\\n\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316:\")\ntext_tokens = hybrid_tokenizer.tokenize(text_example)\nfor token in text_tokens[:10]:  # \u6700\u521d\u306e10\u30c8\u30fc\u30af\u30f3\n    print(f\"  {token.type.name}: {token.value}\")\n\n# \u53ef\u8996\u5316\nhybrid_tokenizer.visualize_tokenization(code_example)\nhybrid_tokenizer.visualize_tokenization(text_example)\n</code></pre>"},{"location":"part1/similarities/#22","title":"2.2 \u69cb\u6587\u89e3\u6790\u3068\u6587\u69cb\u9020\u7406\u89e3\u306e\u5bfe\u6bd4","text":""},{"location":"part1/similarities/#ast","title":"\u62bd\u8c61\u69cb\u6587\u6728\uff08AST\uff09\u306e\u69cb\u7bc9","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u69cb\u6587\u89e3\u6790\u3067\u306f\u3001\u30c8\u30fc\u30af\u30f3\u306e\u5217\u304b\u3089\u968e\u5c64\u7684\u306a\u69cb\u9020\uff08AST\uff09\u3092\u69cb\u7bc9\u3057\u307e\u3059\uff1a</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import List, Optional, Union, Any\nfrom abc import ABC, abstractmethod\n\n# AST\u30ce\u30fc\u30c9\u306e\u57fa\u5e95\u30af\u30e9\u30b9\nclass ASTNode(ABC):\n    @abstractmethod\n    def accept(self, visitor):\n        \"\"\"Visitor\u30d1\u30bf\u30fc\u30f3\u306e\u5b9f\u88c5\"\"\"\n        pass\n\n    @abstractmethod\n    def to_dict(self) -&gt; dict:\n        \"\"\"\u30c7\u30d0\u30c3\u30b0\u7528\u306e\u8f9e\u66f8\u8868\u73fe\"\"\"\n        pass\n\n# \u5f0f\u30ce\u30fc\u30c9\n@dataclass\nclass BinaryOp(ASTNode):\n    operator: str\n    left: ASTNode\n    right: ASTNode\n\n    def accept(self, visitor):\n        return visitor.visit_binary_op(self)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'type': 'BinaryOp',\n            'operator': self.operator,\n            'left': self.left.to_dict(),\n            'right': self.right.to_dict()\n        }\n\n@dataclass\nclass UnaryOp(ASTNode):\n    operator: str\n    operand: ASTNode\n\n    def accept(self, visitor):\n        return visitor.visit_unary_op(self)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'type': 'UnaryOp',\n            'operator': self.operator,\n            'operand': self.operand.to_dict()\n        }\n\n@dataclass\nclass Number(ASTNode):\n    value: float\n\n    def accept(self, visitor):\n        return visitor.visit_number(self)\n\n    def to_dict(self) -&gt; dict:\n        return {'type': 'Number', 'value': self.value}\n\n@dataclass\nclass Identifier(ASTNode):\n    name: str\n\n    def accept(self, visitor):\n        return visitor.visit_identifier(self)\n\n    def to_dict(self) -&gt; dict:\n        return {'type': 'Identifier', 'name': self.name}\n\n# \u6587\u30ce\u30fc\u30c9\n@dataclass\nclass Assignment(ASTNode):\n    target: Identifier\n    value: ASTNode\n\n    def accept(self, visitor):\n        return visitor.visit_assignment(self)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'type': 'Assignment',\n            'target': self.target.to_dict(),\n            'value': self.value.to_dict()\n        }\n\n@dataclass\nclass IfStatement(ASTNode):\n    condition: ASTNode\n    then_branch: List[ASTNode]\n    else_branch: Optional[List[ASTNode]] = None\n\n    def accept(self, visitor):\n        return visitor.visit_if_statement(self)\n\n    def to_dict(self) -&gt; dict:\n        result = {\n            'type': 'IfStatement',\n            'condition': self.condition.to_dict(),\n            'then': [stmt.to_dict() for stmt in self.then_branch]\n        }\n        if self.else_branch:\n            result['else'] = [stmt.to_dict() for stmt in self.else_branch]\n        return result\n\n@dataclass\nclass FunctionDef(ASTNode):\n    name: str\n    parameters: List[str]\n    body: List[ASTNode]\n\n    def accept(self, visitor):\n        return visitor.visit_function_def(self)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'type': 'FunctionDef',\n            'name': self.name,\n            'parameters': self.parameters,\n            'body': [stmt.to_dict() for stmt in self.body]\n        }\n\n# \u30d1\u30fc\u30b5\u30fc\u306e\u5b9f\u88c5\nclass Parser:\n    \"\"\"\n    \u518d\u5e30\u4e0b\u964d\u30d1\u30fc\u30b5\u30fc\n    \u6587\u6cd5:\n        program     -&gt; statement*\n        statement   -&gt; assignment | if_stmt | function_def | expression\n        assignment  -&gt; IDENTIFIER \"=\" expression\n        if_stmt     -&gt; \"if\" \"(\" expression \")\" \"{\" statement* \"}\" [\"else\" \"{\" statement* \"}\"]\n        function_def-&gt; \"function\" IDENTIFIER \"(\" params \")\" \"{\" statement* \"}\"\n        expression  -&gt; term ((\"+\"|\"-\") term)*\n        term        -&gt; factor ((\"*\"|\"/\") factor)*\n        factor      -&gt; NUMBER | IDENTIFIER | \"(\" expression \")\" | unary\n        unary       -&gt; (\"-\"|\"!\") factor\n    \"\"\"\n\n    def __init__(self, tokens: List[Token]):\n        self.tokens = tokens\n        self.current = 0\n\n    def parse(self) -&gt; List[ASTNode]:\n        \"\"\"\u30d7\u30ed\u30b0\u30e9\u30e0\u5168\u4f53\u3092\u30d1\u30fc\u30b9\"\"\"\n        statements = []\n        while not self.is_at_end():\n            if self.peek().type == TokenType.EOF:\n                break\n            stmt = self.statement()\n            if stmt:\n                statements.append(stmt)\n        return statements\n\n    def statement(self) -&gt; Optional[ASTNode]:\n        \"\"\"\u6587\u3092\u30d1\u30fc\u30b9\"\"\"\n        # \u95a2\u6570\u5b9a\u7fa9\n        if self.match(TokenType.FUNCTION):\n            return self.function_def()\n\n        # if\u6587\n        if self.match(TokenType.IF):\n            return self.if_statement()\n\n        # \u4ee3\u5165\u6587\uff08\u5148\u8aad\u307f\u304c\u5fc5\u8981\uff09\n        if self.peek().type == TokenType.IDENTIFIER and self.peek_next() and self.peek_next().type == TokenType.ASSIGN:\n            return self.assignment()\n\n        # \u5f0f\u6587\n        expr = self.expression()\n        self.consume(TokenType.SEMICOLON, \"Expected ';' after expression\")\n        return expr\n\n    def assignment(self) -&gt; Assignment:\n        \"\"\"\u4ee3\u5165\u6587\u3092\u30d1\u30fc\u30b9\"\"\"\n        name = self.consume(TokenType.IDENTIFIER, \"Expected identifier\")\n        self.consume(TokenType.ASSIGN, \"Expected '='\")\n        value = self.expression()\n        self.consume(TokenType.SEMICOLON, \"Expected ';'\")\n        return Assignment(Identifier(name.value), value)\n\n    def if_statement(self) -&gt; IfStatement:\n        \"\"\"if\u6587\u3092\u30d1\u30fc\u30b9\"\"\"\n        self.consume(TokenType.LPAREN, \"Expected '(' after 'if'\")\n        condition = self.expression()\n        self.consume(TokenType.RPAREN, \"Expected ')' after condition\")\n\n        self.consume(TokenType.LBRACE, \"Expected '{'\")\n        then_branch = []\n        while not self.check(TokenType.RBRACE) and not self.is_at_end():\n            then_branch.append(self.statement())\n        self.consume(TokenType.RBRACE, \"Expected '}'\")\n\n        else_branch = None\n        if self.match(TokenType.ELSE):\n            self.consume(TokenType.LBRACE, \"Expected '{' after 'else'\")\n            else_branch = []\n            while not self.check(TokenType.RBRACE) and not self.is_at_end():\n                else_branch.append(self.statement())\n            self.consume(TokenType.RBRACE, \"Expected '}'\")\n\n        return IfStatement(condition, then_branch, else_branch)\n\n    def function_def(self) -&gt; FunctionDef:\n        \"\"\"\u95a2\u6570\u5b9a\u7fa9\u3092\u30d1\u30fc\u30b9\"\"\"\n        name = self.consume(TokenType.IDENTIFIER, \"Expected function name\")\n        self.consume(TokenType.LPAREN, \"Expected '('\")\n\n        parameters = []\n        if not self.check(TokenType.RPAREN):\n            parameters.append(self.consume(TokenType.IDENTIFIER, \"Expected parameter name\").value)\n            while self.match(TokenType.COMMA):\n                parameters.append(self.consume(TokenType.IDENTIFIER, \"Expected parameter name\").value)\n\n        self.consume(TokenType.RPAREN, \"Expected ')'\")\n        self.consume(TokenType.LBRACE, \"Expected '{'\")\n\n        body = []\n        while not self.check(TokenType.RBRACE) and not self.is_at_end():\n            body.append(self.statement())\n\n        self.consume(TokenType.RBRACE, \"Expected '}'\")\n\n        return FunctionDef(name.value, parameters, body)\n\n    def expression(self) -&gt; ASTNode:\n        \"\"\"\u5f0f\u3092\u30d1\u30fc\u30b9\uff08\u52a0\u6e1b\u7b97\uff09\"\"\"\n        left = self.term()\n\n        while self.match(TokenType.PLUS, TokenType.MINUS):\n            op = self.previous()\n            right = self.term()\n            left = BinaryOp(op.value, left, right)\n\n        return left\n\n    def term(self) -&gt; ASTNode:\n        \"\"\"\u9805\u3092\u30d1\u30fc\u30b9\uff08\u4e57\u9664\u7b97\uff09\"\"\"\n        left = self.factor()\n\n        while self.match(TokenType.MULTIPLY, TokenType.DIVIDE):\n            op = self.previous()\n            right = self.factor()\n            left = BinaryOp(op.value, left, right)\n\n        return left\n\n    def factor(self) -&gt; ASTNode:\n        \"\"\"\u56e0\u5b50\u3092\u30d1\u30fc\u30b9\"\"\"\n        # \u5358\u9805\u6f14\u7b97\u5b50\n        if self.match(TokenType.MINUS):\n            op = self.previous()\n            operand = self.factor()\n            return UnaryOp(op.value, operand)\n\n        # \u6570\u5024\n        if self.match(TokenType.NUMBER):\n            return Number(float(self.previous().value))\n\n        # \u8b58\u5225\u5b50\n        if self.match(TokenType.IDENTIFIER):\n            return Identifier(self.previous().value)\n\n        # \u62ec\u5f27\u3067\u56f2\u307e\u308c\u305f\u5f0f\n        if self.match(TokenType.LPAREN):\n            expr = self.expression()\n            self.consume(TokenType.RPAREN, \"Expected ')' after expression\")\n            return expr\n\n        raise self.error(\"Expected expression\")\n\n    # \u30d8\u30eb\u30d1\u30fc\u30e1\u30bd\u30c3\u30c9\n    def match(self, *types: TokenType) -&gt; bool:\n        \"\"\"\u73fe\u5728\u306e\u30c8\u30fc\u30af\u30f3\u304c\u6307\u5b9a\u3055\u308c\u305f\u578b\u306e\u3044\u305a\u308c\u304b\u306b\u30de\u30c3\u30c1\u3059\u308b\u304b\"\"\"\n        for token_type in types:\n            if self.check(token_type):\n                self.advance()\n                return True\n        return False\n\n    def check(self, token_type: TokenType) -&gt; bool:\n        \"\"\"\u73fe\u5728\u306e\u30c8\u30fc\u30af\u30f3\u304c\u6307\u5b9a\u3055\u308c\u305f\u578b\u304b\uff08\u9032\u3081\u306a\u3044\uff09\"\"\"\n        if self.is_at_end():\n            return False\n        return self.peek().type == token_type\n\n    def advance(self) -&gt; Token:\n        \"\"\"\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u306b\u9032\u3080\"\"\"\n        if not self.is_at_end():\n            self.current += 1\n        return self.previous()\n\n    def is_at_end(self) -&gt; bool:\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u306e\u7d42\u7aef\u306b\u9054\u3057\u305f\u304b\"\"\"\n        return self.current &gt;= len(self.tokens)\n\n    def peek(self) -&gt; Token:\n        \"\"\"\u73fe\u5728\u306e\u30c8\u30fc\u30af\u30f3\u3092\u8fd4\u3059\uff08\u9032\u3081\u306a\u3044\uff09\"\"\"\n        if self.is_at_end():\n            return self.tokens[-1]  # EOF\n        return self.tokens[self.current]\n\n    def peek_next(self) -&gt; Optional[Token]:\n        \"\"\"\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u8fd4\u3059\uff08\u9032\u3081\u306a\u3044\uff09\"\"\"\n        if self.current + 1 &gt;= len(self.tokens):\n            return None\n        return self.tokens[self.current + 1]\n\n    def previous(self) -&gt; Token:\n        \"\"\"\u524d\u306e\u30c8\u30fc\u30af\u30f3\u3092\u8fd4\u3059\"\"\"\n        return self.tokens[self.current - 1]\n\n    def consume(self, token_type: TokenType, message: str) -&gt; Token:\n        \"\"\"\u6307\u5b9a\u3055\u308c\u305f\u578b\u306e\u30c8\u30fc\u30af\u30f3\u3092\u6d88\u8cbb\u3059\u308b\"\"\"\n        if self.check(token_type):\n            return self.advance()\n\n        raise self.error(message)\n\n    def error(self, message: str):\n        \"\"\"\u30d1\u30fc\u30b9\u30a8\u30e9\u30fc\u3092\u767a\u751f\u3055\u305b\u308b\"\"\"\n        token = self.peek()\n        return SyntaxError(f\"{message} at line {token.line}:{token.column}\")\n\n# AST\u53ef\u8996\u5316\ndef visualize_ast(node: ASTNode, level: int = 0):\n    \"\"\"AST\u3092\u30c6\u30ad\u30b9\u30c8\u5f62\u5f0f\u3067\u53ef\u8996\u5316\"\"\"\n    indent = \"  \" * level\n\n    if isinstance(node, Number):\n        print(f\"{indent}Number({node.value})\")\n    elif isinstance(node, Identifier):\n        print(f\"{indent}Identifier({node.name})\")\n    elif isinstance(node, BinaryOp):\n        print(f\"{indent}BinaryOp({node.operator})\")\n        visualize_ast(node.left, level + 1)\n        visualize_ast(node.right, level + 1)\n    elif isinstance(node, UnaryOp):\n        print(f\"{indent}UnaryOp({node.operator})\")\n        visualize_ast(node.operand, level + 1)\n    elif isinstance(node, Assignment):\n        print(f\"{indent}Assignment\")\n        print(f\"{indent}  target:\")\n        visualize_ast(node.target, level + 2)\n        print(f\"{indent}  value:\")\n        visualize_ast(node.value, level + 2)\n    elif isinstance(node, IfStatement):\n        print(f\"{indent}IfStatement\")\n        print(f\"{indent}  condition:\")\n        visualize_ast(node.condition, level + 2)\n        print(f\"{indent}  then:\")\n        for stmt in node.then_branch:\n            visualize_ast(stmt, level + 2)\n        if node.else_branch:\n            print(f\"{indent}  else:\")\n            for stmt in node.else_branch:\n                visualize_ast(stmt, level + 2)\n    elif isinstance(node, FunctionDef):\n        print(f\"{indent}FunctionDef({node.name})\")\n        print(f\"{indent}  parameters: {node.parameters}\")\n        print(f\"{indent}  body:\")\n        for stmt in node.body:\n            visualize_ast(stmt, level + 2)\n\n# \u4f7f\u7528\u4f8b\ncode = \"\"\"\nfunction calculate(x, y) {\n    if (x &gt; 0) {\n        result = x + y;\n    } else {\n        result = x - y;\n    }\n    return result;\n}\n\"\"\"\n\nlexer = Lexer(code)\ntokens = lexer.tokenize()\nparser = Parser(tokens)\nast = parser.parse()\n\nprint(\"=== AST ===\")\nfor node in ast:\n    visualize_ast(node)\n</code></pre>"},{"location":"part1/similarities/#transformer","title":"Transformer\u306e\u968e\u5c64\u7684\u7406\u89e3\uff1a\u6697\u9ed9\u7684\u306a\u69cb\u6587\u6728","text":"<p>Transformer\u306f\u660e\u793a\u7684\u306aAST\u3092\u69cb\u7bc9\u3057\u307e\u305b\u3093\u304c\u3001Attention\u6a5f\u69cb\u3092\u901a\u3058\u3066\u6697\u9ed9\u7684\u306b\u968e\u5c64\u69cb\u9020\u3092\u5b66\u7fd2\u3057\u307e\u3059\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Optional\n\nclass SyntacticAttention(nn.Module):\n    \"\"\"\n    \u69cb\u6587\u7684\u306a\u95a2\u4fc2\u3092\u5b66\u7fd2\u3059\u308bAttention\u6a5f\u69cb\n    Multi-Head Attention\u306e\u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u69cb\u6587\u95a2\u4fc2\u3092\u6349\u3048\u308b\n    \"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        # \u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u69cb\u6587\u95a2\u4fc2\u3092\u5b66\u7fd2\n        self.head_names = [\n            \"\u5c40\u6240\u4f9d\u5b58\",      # \u96a3\u63a5\u3059\u308b\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\n            \"\u9577\u8ddd\u96e2\u4f9d\u5b58\",    # \u96e2\u308c\u305f\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\n            \"\u968e\u5c64\u69cb\u9020\",      # \u53e5\u3084\u7bc0\u306e\u968e\u5c64\n            \"\u4e3b\u8a9e-\u52d5\u8a5e\",     # \u6587\u6cd5\u7684\u95a2\u4fc2\n            \"\u4fee\u98fe\u95a2\u4fc2\",      # \u5f62\u5bb9\u8a5e-\u540d\u8a5e\u306a\u3069\n            \"\u4e26\u5217\u69cb\u9020\",      # and\u3084or\u3067\u7d50\u3070\u308c\u305f\u8981\u7d20\n            \"\u7167\u5fdc\u95a2\u4fc2\",      # \u4ee3\u540d\u8a5e\u3068\u5148\u884c\u8a5e\n            \"\u305d\u306e\u4ed6\"         # \u305d\u306e\u4ed6\u306e\u95a2\u4fc2\n        ]\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            mask: [batch_size, seq_len, seq_len]\n        Returns:\n            output: [batch_size, seq_len, d_model]\n            attention_weights: [batch_size, n_heads, seq_len, seq_len]\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n\n        # Query, Key, Value\u3092\u8a08\u7b97\n        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Attention\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n\n        # \u30de\u30b9\u30af\u3092\u9069\u7528\n        if mask is not None:\n            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Attention\u91cd\u307f\u3092\u8a08\u7b97\n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n\n        # \u91cd\u307f\u4ed8\u304d\u548c\u3092\u8a08\u7b97\n        context = torch.matmul(attention_weights, V)\n\n        # \u5f62\u72b6\u3092\u623b\u3059\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n\n        # \u51fa\u529b\u5909\u63db\n        output = self.W_o(context)\n\n        return output, attention_weights\n\n    def analyze_syntactic_patterns(self, attention_weights: torch.Tensor, tokens: List[str]):\n        \"\"\"\u5404\u30d8\u30c3\u30c9\u304c\u5b66\u7fd2\u3057\u305f\u69cb\u6587\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\"\"\"\n        # attention_weights: [n_heads, seq_len, seq_len]\n\n        analysis = {}\n        for head_idx in range(self.n_heads):\n            head_weights = attention_weights[head_idx].cpu().numpy()\n            head_name = self.head_names[head_idx]\n\n            # \u3053\u306e\u30d8\u30c3\u30c9\u304c\u6349\u3048\u3066\u3044\u308b\u4e3b\u8981\u306a\u30d1\u30bf\u30fc\u30f3\u3092\u7279\u5b9a\n            patterns = self._identify_patterns(head_weights, tokens)\n            analysis[head_name] = patterns\n\n        return analysis\n\n    def _identify_patterns(self, weights: np.ndarray, tokens: List[str]) -&gt; dict:\n        \"\"\"Attention\u91cd\u307f\u304b\u3089\u30d1\u30bf\u30fc\u30f3\u3092\u7279\u5b9a\"\"\"\n        patterns = {\n            \"diagonal\": 0,      # \u81ea\u5df1\u6ce8\u610f\n            \"previous\": 0,      # \u76f4\u524d\u306e\u5358\u8a9e\u3078\u306e\u6ce8\u610f\n            \"next\": 0,          # \u76f4\u5f8c\u306e\u5358\u8a9e\u3078\u306e\u6ce8\u610f\n            \"first\": 0,         # \u6587\u982d\u3078\u306e\u6ce8\u610f\n            \"last\": 0,          # \u6587\u672b\u3078\u306e\u6ce8\u610f\n            \"periodic\": 0,      # \u5468\u671f\u7684\u30d1\u30bf\u30fc\u30f3\n            \"sparse\": 0,        # \u7279\u5b9a\u306e\u5358\u8a9e\u3078\u306e\u96c6\u4e2d\n        }\n\n        seq_len = weights.shape[0]\n\n        for i in range(seq_len):\n            # \u5bfe\u89d2\u6210\u5206\uff08\u81ea\u5df1\u6ce8\u610f\uff09\n            patterns[\"diagonal\"] += weights[i, i]\n\n            # \u524d\u5f8c\u306e\u5358\u8a9e\n            if i &gt; 0:\n                patterns[\"previous\"] += weights[i, i-1]\n            if i &lt; seq_len - 1:\n                patterns[\"next\"] += weights[i, i+1]\n\n            # \u6587\u982d\u30fb\u6587\u672b\n            patterns[\"first\"] += weights[i, 0]\n            patterns[\"last\"] += weights[i, -1]\n\n            # \u30b9\u30d1\u30fc\u30b9\u6027\uff08\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3067\u6e2c\u5b9a\uff09\n            entropy = -np.sum(weights[i] * np.log(weights[i] + 1e-9))\n            if entropy &lt; 1.0:  # \u4f4e\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc = \u30b9\u30d1\u30fc\u30b9\n                patterns[\"sparse\"] += 1\n\n        # \u6b63\u898f\u5316\n        for key in patterns:\n            patterns[key] /= seq_len\n\n        return patterns\n\nclass DependencyParser:\n    \"\"\"\n    Transformer\u306e Attention\u91cd\u307f\u304b\u3089\u4f9d\u5b58\u95a2\u4fc2\u3092\u62bd\u51fa\n    \u6697\u9ed9\u7684\u306a\u69cb\u6587\u6728\u3092\u53ef\u8996\u5316\n    \"\"\"\n\n    def __init__(self, model: SyntacticAttention):\n        self.model = model\n\n    def extract_dependencies(self, sentence: str, tokenizer) -&gt; dict:\n        \"\"\"\u6587\u304b\u3089\u4f9d\u5b58\u95a2\u4fc2\u3092\u62bd\u51fa\"\"\"\n        # \u30c8\u30fc\u30af\u30f3\u5316\n        tokens = tokenizer.tokenize(sentence)\n        token_ids = tokenizer.encode(sentence)\n\n        # \u57cb\u3081\u8fbc\u307f\uff08\u4eee\u60f3\u7684\uff09\n        embeddings = torch.randn(1, len(tokens), self.model.d_model)\n\n        # Attention\u8a08\u7b97\n        with torch.no_grad():\n            output, attention_weights = self.model(embeddings)\n\n        # \u4f9d\u5b58\u95a2\u4fc2\u306e\u62bd\u51fa\n        dependencies = self._extract_from_attention(attention_weights[0], tokens)\n\n        return {\n            'tokens': tokens,\n            'dependencies': dependencies,\n            'attention_weights': attention_weights[0]\n        }\n\n    def _extract_from_attention(self, attention_weights: torch.Tensor, tokens: List[str]) -&gt; List[Tuple[int, int, str]]:\n        \"\"\"\n        Attention\u91cd\u307f\u304b\u3089\u4f9d\u5b58\u95a2\u4fc2\u3092\u62bd\u51fa\n        Returns: [(dependent_idx, head_idx, relation_type), ...]\n        \"\"\"\n        dependencies = []\n        n_heads, seq_len, _ = attention_weights.shape\n\n        # \u5404\u30c8\u30fc\u30af\u30f3\u306b\u3064\u3044\u3066\u3001\u6700\u3082\u5f37\u3044\u4f9d\u5b58\u95a2\u4fc2\u3092\u7279\u5b9a\n        for i in range(seq_len):\n            # \u5168\u30d8\u30c3\u30c9\u306e\u5e73\u5747\u3092\u53d6\u308b\n            avg_attention = attention_weights.mean(dim=0)\n\n            # \u81ea\u5df1\u4ee5\u5916\u3067\u6700\u3082\u6ce8\u610f\u3092\u5411\u3051\u3066\u3044\u308b\u5358\u8a9e\u3092\u898b\u3064\u3051\u308b\n            attention_to_others = avg_attention[i].clone()\n            attention_to_others[i] = 0  # \u81ea\u5df1\u6ce8\u610f\u3092\u9664\u5916\n\n            if attention_to_others.sum() &gt; 0:\n                head_idx = attention_to_others.argmax().item()\n                score = attention_to_others[head_idx].item()\n\n                if score &gt; 0.1:  # \u95be\u5024\n                    # \u95a2\u4fc2\u30bf\u30a4\u30d7\u3092\u63a8\u5b9a\n                    relation = self._infer_relation(i, head_idx, tokens)\n                    dependencies.append((i, head_idx, relation))\n\n        return dependencies\n\n    def _infer_relation(self, dependent: int, head: int, tokens: List[str]) -&gt; str:\n        \"\"\"\u4f9d\u5b58\u95a2\u4fc2\u306e\u30bf\u30a4\u30d7\u3092\u63a8\u5b9a\"\"\"\n        # \u7c21\u5358\u306a\u30eb\u30fc\u30eb\u30d9\u30fc\u30b9\u306e\u63a8\u5b9a\n        if head &lt; dependent:\n            if dependent - head == 1:\n                return \"next_to\"\n            else:\n                return \"backward_dep\"\n        else:\n            if head - dependent == 1:\n                return \"forward_next\"\n            else:\n                return \"forward_dep\"\n\n    def visualize_dependencies(self, result: dict):\n        \"\"\"\u4f9d\u5b58\u95a2\u4fc2\u3092\u53ef\u8996\u5316\"\"\"\n        import networkx as nx\n        from matplotlib import pyplot as plt\n\n        tokens = result['tokens']\n        dependencies = result['dependencies']\n\n        # \u30b0\u30e9\u30d5\u3092\u69cb\u7bc9\n        G = nx.DiGraph()\n\n        # \u30ce\u30fc\u30c9\u3092\u8ffd\u52a0\n        for i, token in enumerate(tokens):\n            G.add_node(i, label=token)\n\n        # \u30a8\u30c3\u30b8\u3092\u8ffd\u52a0\n        for dep, head, rel in dependencies:\n            G.add_edge(head, dep, relation=rel)\n\n        # \u30ec\u30a4\u30a2\u30a6\u30c8\u3092\u8a08\u7b97\n        pos = {}\n        for i in range(len(tokens)):\n            # \u5358\u8a9e\u3092\u6a2a\u4e00\u5217\u306b\u914d\u7f6e\n            pos[i] = (i, 0)\n\n        # \u30b0\u30e9\u30d5\u3092\u63cf\u753b\n        plt.figure(figsize=(15, 8))\n\n        # \u30ce\u30fc\u30c9\u3092\u63cf\u753b\n        nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=1000)\n\n        # \u30e9\u30d9\u30eb\u3092\u63cf\u753b\n        labels = {i: tokens[i] for i in range(len(tokens))}\n        nx.draw_networkx_labels(G, pos, labels, font_size=10)\n\n        # \u30a8\u30c3\u30b8\u3092\u63cf\u753b\uff08\u66f2\u7dda\u3067\uff09\n        for edge in G.edges():\n            head, dep = edge\n            rel = G[head][dep]['relation']\n\n            # \u8272\u3092\u95a2\u4fc2\u30bf\u30a4\u30d7\u306b\u3088\u3063\u3066\u5909\u3048\u308b\n            if 'next' in rel:\n                color = 'green'\n            elif 'backward' in rel:\n                color = 'red'\n            else:\n                color = 'blue'\n\n            # \u66f2\u7dda\u3092\u63cf\u753b\n            if head != dep:\n                connectionstyle = \"arc3,rad=0.3\" if head &lt; dep else \"arc3,rad=-0.3\"\n                plt.annotate('', xy=pos[dep], xytext=pos[head],\n                           arrowprops=dict(arrowstyle='-&gt;', color=color,\n                                         connectionstyle=connectionstyle,\n                                         linewidth=2))\n\n        plt.title('Transformer \u304c\u5b66\u7fd2\u3057\u305f\u4f9d\u5b58\u69cb\u9020\uff08\u6697\u9ed9\u7684\u306a\u69cb\u6587\u6728\uff09')\n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n        # Attention \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3082\u8868\u793a\n        self.visualize_attention_heatmap(result['attention_weights'], tokens)\n\n    def visualize_attention_heatmap(self, attention_weights: torch.Tensor, tokens: List[str]):\n        \"\"\"Attention\u91cd\u307f\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u8868\u793a\"\"\"\n        n_heads = attention_weights.shape[0]\n\n        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n        axes = axes.flatten()\n\n        for head_idx in range(n_heads):\n            ax = axes[head_idx]\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u63cf\u753b\n            im = ax.imshow(attention_weights[head_idx].cpu().numpy(), cmap='Blues', aspect='auto')\n\n            # \u8ef8\u30e9\u30d9\u30eb\n            ax.set_xticks(range(len(tokens)))\n            ax.set_yticks(range(len(tokens)))\n            ax.set_xticklabels(tokens, rotation=45, ha='right')\n            ax.set_yticklabels(tokens)\n\n            ax.set_title(f'Head {head_idx + 1}: {self.model.head_names[head_idx]}')\n            ax.set_xlabel('Attended to')\n            ax.set_ylabel('Attending from')\n\n            # \u30ab\u30e9\u30fc\u30d0\u30fc\n            plt.colorbar(im, ax=ax)\n\n        plt.tight_layout()\n        plt.show()\n\n# \u30c7\u30e2\uff1a\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u69cb\u6587\u89e3\u6790\u6bd4\u8f03\ndef compare_syntax_parsing():\n    \"\"\"AST\u3068Transformer\u306e\u69cb\u6587\u7406\u89e3\u3092\u6bd4\u8f03\"\"\"\n\n    # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b\n    code = \"if (x &gt; 0) { y = x * 2; } else { y = 0; }\"\n\n    print(\"=== \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u69cb\u6587\u89e3\u6790 ===\")\n    lexer = Lexer(code)\n    tokens = lexer.tokenize()\n    parser = Parser(tokens)\n    ast = parser.parse()\n\n    print(\"\u5165\u529b:\", code)\n    print(\"\\nAST:\")\n    for node in ast:\n        visualize_ast(node)\n\n    # \u81ea\u7136\u8a00\u8a9e\u306e\u4f8b\uff08Transformer\u306b\u3088\u308b\u89e3\u6790\uff09\n    print(\"\\n=== \u81ea\u7136\u8a00\u8a9e\u306e\u69cb\u6587\u89e3\u6790\uff08Transformer\uff09===\")\n    sentence = \"The quick brown fox jumps over the lazy dog\"\n\n    # \u30e2\u30c7\u30eb\u3068\u30d1\u30fc\u30b5\u30fc\u3092\u521d\u671f\u5316\n    model = SyntacticAttention(d_model=128, n_heads=8)\n    dep_parser = DependencyParser(model)\n\n    # \u7c21\u6613\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\n    class SimpleTokenizer:\n        def tokenize(self, text):\n            return text.split()\n        def encode(self, text):\n            tokens = self.tokenize(text)\n            return list(range(len(tokens)))\n\n    tokenizer = SimpleTokenizer()\n    result = dep_parser.extract_dependencies(sentence, tokenizer)\n\n    print(\"\u5165\u529b:\", sentence)\n    print(\"\\n\u62bd\u51fa\u3055\u308c\u305f\u4f9d\u5b58\u95a2\u4fc2:\")\n    for dep, head, rel in result['dependencies']:\n        print(f\"  {result['tokens'][dep]} &lt;- {result['tokens'][head]} ({rel})\")\n\n    # \u53ef\u8996\u5316\n    dep_parser.visualize_dependencies(result)\n\n# \u5b9f\u884c\ncompare_syntax_parsing()\n</code></pre>"},{"location":"part1/similarities/#_7","title":"\u69cb\u6587\u7684\u66d6\u6627\u6027\u306e\u89e3\u6c7a","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u306f\u66d6\u6627\u6027\u306f\u8a31\u3055\u308c\u307e\u305b\u3093\u304c\u3001\u81ea\u7136\u8a00\u8a9e\u3067\u306f\u6587\u8108\u306b\u3088\u3063\u3066\u89e3\u6c7a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>class AmbiguityResolution:\n    \"\"\"\n    \u69cb\u6587\u7684\u66d6\u6627\u6027\u3092Transformer\u304c\u3069\u306e\u3088\u3046\u306b\u89e3\u6c7a\u3059\u308b\u304b\u3092\u793a\u3059\u30c7\u30e2\n    \"\"\"\n\n    def __init__(self):\n        self.examples = {\n            \"PP attachment\": {\n                \"sentence\": \"I saw the man with the telescope\",\n                \"interpretations\": [\n                    \"I used the telescope to see the man\",\n                    \"I saw the man who had the telescope\"\n                ],\n                \"context_clues\": {\n                    \"instrumental\": [\"using\", \"through\", \"by means of\"],\n                    \"possessive\": [\"holding\", \"carrying\", \"who had\"]\n                }\n            },\n            \"coordination\": {\n                \"sentence\": \"old men and women\",\n                \"interpretations\": [\n                    \"[old men] and [women]\",\n                    \"[old [men and women]]\"\n                ],\n                \"context_clues\": {\n                    \"narrow_scope\": [\"young women\", \"elderly men\"],\n                    \"wide_scope\": [\"elderly people\", \"senior citizens\"]\n                }\n            }\n        }\n\n    def demonstrate_context_dependency(self):\n        \"\"\"\u6587\u8108\u306b\u3088\u308b\u66d6\u6627\u6027\u89e3\u6c7a\u306e\u30c7\u30e2\"\"\"\n\n        # PP attachment \u306e\u4f8b\n        example = self.examples[\"PP attachment\"]\n\n        print(\"=== \u69cb\u6587\u7684\u66d6\u6627\u6027\u306e\u4f8b\uff1aPP Attachment ===\")\n        print(f\"\u66d6\u6627\u306a\u6587: {example['sentence']}\")\n        print(\"\\n\u53ef\u80fd\u306a\u89e3\u91c8:\")\n        for i, interp in enumerate(example['interpretations']):\n            print(f\"  {i+1}. {interp}\")\n\n        # \u6587\u8108\u3092\u8ffd\u52a0\u3057\u3066\u66d6\u6627\u6027\u3092\u89e3\u6c7a\n        print(\"\\n\u6587\u8108\u306b\u3088\u308b\u89e3\u6c7a:\")\n\n        # \u6587\u81081\uff1a\u9053\u5177\u3068\u3057\u3066\n        context1 = \"I had borrowed a telescope from the observatory. \" + example['sentence']\n        print(f\"\\n\u6587\u81081: {context1}\")\n        print(\"\u2192 \u89e3\u91c8: \u671b\u9060\u93e1\u3092\u4f7f\u3063\u3066\u7537\u3092\u898b\u305f\")\n\n        # \u6587\u81082\uff1a\u6240\u6709\u3068\u3057\u3066\n        context2 = \"The man was an astronomer. \" + example['sentence']\n        print(f\"\\n\u6587\u81082: {context2}\")\n        print(\"\u2192 \u89e3\u91c8: \u671b\u9060\u93e1\u3092\u6301\u3063\u3066\u3044\u308b\u7537\u3092\u898b\u305f\")\n\n        # Transformer\u304c\u3069\u306e\u3088\u3046\u306b\u89e3\u6c7a\u3059\u308b\u304b\u3092\u53ef\u8996\u5316\n        self.visualize_ambiguity_resolution(example['sentence'], [context1, context2])\n\n    def visualize_ambiguity_resolution(self, ambiguous_sentence: str, contexts: List[str]):\n        \"\"\"\n        \u7570\u306a\u308b\u6587\u8108\u3067\u306eAttention\u30d1\u30bf\u30fc\u30f3\u306e\u9055\u3044\u3092\u53ef\u8996\u5316\n        \"\"\"\n        # \u4eee\u60f3\u7684\u306aAttention\u30d1\u30bf\u30fc\u30f3\n        # \u5b9f\u969b\u306b\u306f\u30e2\u30c7\u30eb\u304b\u3089\u53d6\u5f97\n\n        fig, axes = plt.subplots(1, len(contexts), figsize=(15, 5))\n\n        words = ambiguous_sentence.split()\n\n        for i, (context, ax) in enumerate(zip(contexts, axes)):\n            # \u6587\u8108\u306b\u5fdc\u3058\u305f\u4eee\u60f3\u7684\u306aAttention\u30d1\u30bf\u30fc\u30f3\n            if i == 0:  # \u9053\u5177\u7684\u89e3\u91c8\n                # \"with\" \u304c \"saw\" \u306b\u5f37\u304f\u7d50\u3073\u3064\u304f\n                attention_matrix = np.zeros((len(words), len(words)))\n                saw_idx = words.index(\"saw\")\n                with_idx = words.index(\"with\")\n                telescope_idx = words.index(\"telescope\")\n\n                attention_matrix[with_idx, saw_idx] = 0.8\n                attention_matrix[telescope_idx, with_idx] = 0.7\n\n            else:  # \u6240\u6709\u7684\u89e3\u91c8\n                # \"with\" \u304c \"man\" \u306b\u5f37\u304f\u7d50\u3073\u3064\u304f\n                attention_matrix = np.zeros((len(words), len(words)))\n                man_idx = words.index(\"man\")\n                with_idx = words.index(\"with\")\n                telescope_idx = words.index(\"telescope\")\n\n                attention_matrix[with_idx, man_idx] = 0.8\n                attention_matrix[telescope_idx, with_idx] = 0.7\n\n            # \u5bfe\u89d2\u6210\u5206\uff08\u81ea\u5df1\u6ce8\u610f\uff09\u3092\u8ffd\u52a0\n            np.fill_diagonal(attention_matrix, 0.3)\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u63cf\u753b\n            im = ax.imshow(attention_matrix, cmap='Blues', aspect='auto')\n\n            # \u8ef8\u30e9\u30d9\u30eb\n            ax.set_xticks(range(len(words)))\n            ax.set_yticks(range(len(words)))\n            ax.set_xticklabels(words, rotation=45, ha='right')\n            ax.set_yticklabels(words)\n\n            ax.set_title(f'\u6587\u8108{i+1}\u3067\u306eAttention\u30d1\u30bf\u30fc\u30f3')\n\n            # \u4e3b\u8981\u306a\u7d50\u5408\u3092\u77e2\u5370\u3067\u5f37\u8abf\n            if i == 0:\n                ax.annotate('', xy=(saw_idx, with_idx), xytext=(saw_idx, with_idx + 0.3),\n                          arrowprops=dict(arrowstyle='-&gt;', color='red', lw=2))\n            else:\n                ax.annotate('', xy=(man_idx, with_idx), xytext=(man_idx, with_idx + 0.3),\n                          arrowprops=dict(arrowstyle='-&gt;', color='red', lw=2))\n\n        plt.tight_layout()\n        plt.show()\n\n# \u30c7\u30e2\u5b9f\u884c\nambiguity_demo = AmbiguityResolution()\nambiguity_demo.demonstrate_context_dependency()\n</code></pre>"},{"location":"part1/similarities/#23","title":"2.3 \u610f\u5473\u89e3\u6790\u3068\u610f\u5473\u7406\u89e3\u306e\u5bfe\u6bd4","text":""},{"location":"part1/similarities/#_8","title":"\u578b\u63a8\u8ad6\u3068\u6587\u8108\u7406\u89e3\u306e\u985e\u4f3c\u6027","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u578b\u63a8\u8ad6\u30b7\u30b9\u30c6\u30e0\u3068Transformer\u306e\u6587\u8108\u7406\u89e3\u306b\u306f\u3001\u9a5a\u304f\u3079\u304d\u985e\u4f3c\u6027\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>from typing import Dict, List, Set, Union, Optional, Any\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\n\n# \u578b\u30b7\u30b9\u30c6\u30e0\u306e\u5b9a\u7fa9\n@dataclass\nclass Type:\n    \"\"\"\u57fa\u672c\u7684\u306a\u578b\u30af\u30e9\u30b9\"\"\"\n    pass\n\n@dataclass\nclass PrimitiveType(Type):\n    name: str  # int, float, string, bool\n\n@dataclass\nclass FunctionType(Type):\n    param_types: List[Type]\n    return_type: Type\n\n@dataclass\nclass ArrayType(Type):\n    element_type: Type\n\n@dataclass\nclass ObjectType(Type):\n    fields: Dict[str, Type]\n\nclass TypeInferenceEngine:\n    \"\"\"\n    \u578b\u63a8\u8ad6\u30a8\u30f3\u30b8\u30f3\n    Hindley-Milner\u578b\u63a8\u8ad6\u306e\u7c21\u7565\u7248\n    \"\"\"\n\n    def __init__(self):\n        self.type_env: Dict[str, Type] = {}\n        self.constraints: List[Tuple[Type, Type]] = []\n\n        # \u7d44\u307f\u8fbc\u307f\u95a2\u6570\u306e\u578b\n        self.builtin_types = {\n            'print': FunctionType([PrimitiveType('any')], PrimitiveType('void')),\n            'len': FunctionType([ArrayType(PrimitiveType('any'))], PrimitiveType('int')),\n            'str': FunctionType([PrimitiveType('any')], PrimitiveType('string')),\n        }\n\n    def infer_type(self, ast_node: ASTNode, env: Dict[str, Type] = None) -&gt; Type:\n        \"\"\"AST\u30ce\u30fc\u30c9\u304b\u3089\u578b\u3092\u63a8\u8ad6\"\"\"\n        if env is None:\n            env = self.type_env.copy()\n\n        if isinstance(ast_node, Number):\n            # \u6570\u5024\u30ea\u30c6\u30e9\u30eb\n            if '.' in str(ast_node.value):\n                return PrimitiveType('float')\n            else:\n                return PrimitiveType('int')\n\n        elif isinstance(ast_node, Identifier):\n            # \u5909\u6570\u53c2\u7167\n            if ast_node.name in env:\n                return env[ast_node.name]\n            elif ast_node.name in self.builtin_types:\n                return self.builtin_types[ast_node.name]\n            else:\n                # \u672a\u77e5\u306e\u5909\u6570\uff1a\u578b\u5909\u6570\u3092\u751f\u6210\n                type_var = self.create_type_variable()\n                env[ast_node.name] = type_var\n                return type_var\n\n        elif isinstance(ast_node, BinaryOp):\n            # \u4e8c\u9805\u6f14\u7b97\n            left_type = self.infer_type(ast_node.left, env)\n            right_type = self.infer_type(ast_node.right, env)\n\n            # \u6f14\u7b97\u5b50\u306b\u5fdc\u3058\u305f\u578b\u5236\u7d04\n            if ast_node.operator in ['+', '-', '*', '/']:\n                # \u6570\u5024\u6f14\u7b97\n                self.add_constraint(left_type, right_type)\n                if ast_node.operator == '/':\n                    return PrimitiveType('float')\n                else:\n                    return left_type\n\n            elif ast_node.operator in ['==', '!=', '&lt;', '&gt;', '&lt;=', '&gt;=']:\n                # \u6bd4\u8f03\u6f14\u7b97\n                self.add_constraint(left_type, right_type)\n                return PrimitiveType('bool')\n\n        elif isinstance(ast_node, Assignment):\n            # \u4ee3\u5165\u6587\n            value_type = self.infer_type(ast_node.value, env)\n            env[ast_node.target.name] = value_type\n            return value_type\n\n        # ... \u4ed6\u306e\u30ce\u30fc\u30c9\u30bf\u30a4\u30d7\u306e\u51e6\u7406\n\n        return PrimitiveType('any')\n\n    def create_type_variable(self) -&gt; Type:\n        \"\"\"\u65b0\u3057\u3044\u578b\u5909\u6570\u3092\u751f\u6210\"\"\"\n        # \u7c21\u7565\u5316\u306e\u305f\u3081\u3001\u3053\u3053\u3067\u306f any \u578b\u3092\u8fd4\u3059\n        return PrimitiveType('any')\n\n    def add_constraint(self, type1: Type, type2: Type):\n        \"\"\"\u578b\u5236\u7d04\u3092\u8ffd\u52a0\"\"\"\n        self.constraints.append((type1, type2))\n\n    def solve_constraints(self):\n        \"\"\"\u5236\u7d04\u3092\u89e3\u3044\u3066\u578b\u3092\u78ba\u5b9a\"\"\"\n        # \u7c21\u7565\u5316\u3055\u308c\u305f\u5b9f\u88c5\n        # \u5b9f\u969b\u306b\u306funification\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u4f7f\u7528\n        pass\n\nclass SemanticUnderstanding(nn.Module):\n    \"\"\"\n    Transformer\u306b\u3088\u308b\u610f\u5473\u7406\u89e3\n    \u578b\u63a8\u8ad6\u3068\u306e\u985e\u4f3c\u6027\u3092\u793a\u3059\u30e2\u30c7\u30eb\n    \"\"\"\n\n    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8):\n        super().__init__()\n        self.d_model = d_model\n\n        # \u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n\n        # \u300c\u578b\u300d\u57cb\u3081\u8fbc\u307f\uff08\u610f\u5473\u30ab\u30c6\u30b4\u30ea\uff09\n        self.semantic_types = [\n            \"entity\",      # \u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\uff08\u4eba\u3001\u7269\u3001\u5834\u6240\uff09\n            \"action\",      # \u52d5\u4f5c\u30fb\u884c\u70ba\n            \"attribute\",   # \u5c5e\u6027\u30fb\u6027\u8cea\n            \"relation\",    # \u95a2\u4fc2\n            \"quantity\",    # \u6570\u91cf\n            \"time\",        # \u6642\u9593\n            \"location\",    # \u5834\u6240\n            \"abstract\"     # \u62bd\u8c61\u6982\u5ff5\n        ]\n        self.type_embedding = nn.Embedding(len(self.semantic_types), d_model)\n\n        # Transformer\u5c64\n        self.attention = nn.MultiheadAttention(d_model, n_heads)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),\n            nn.ReLU(),\n            nn.Linear(d_model * 4, d_model)\n        )\n\n        # \u610f\u5473\u30bf\u30a4\u30d7\u5206\u985e\u5668\n        self.type_classifier = nn.Linear(d_model, len(self.semantic_types))\n\n        # \u95a2\u4fc2\u62bd\u51fa\u5668\n        self.relation_extractor = nn.Bilinear(d_model, d_model, d_model)\n\n    def forward(self, input_ids: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        \u5165\u529b\u304b\u3089\u610f\u5473\u8868\u73fe\u3092\u8a08\u7b97\n        \u578b\u63a8\u8ad6\u3068\u540c\u69d8\u306b\u3001\u5404\u30c8\u30fc\u30af\u30f3\u306e\u300c\u610f\u5473\u578b\u300d\u3092\u63a8\u8ad6\n        \"\"\"\n        # \u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\n        x = self.token_embedding(input_ids)\n\n        # Self-Attention\uff08\u6587\u8108\u3092\u8003\u616e\uff09\n        attn_output, attention_weights = self.attention(x, x, x)\n        x = x + attn_output  # \u6b8b\u5dee\u63a5\u7d9a\n\n        # Feed Forward\n        ff_output = self.feed_forward(x)\n        x = x + ff_output  # \u6b8b\u5dee\u63a5\u7d9a\n\n        # \u5404\u30c8\u30fc\u30af\u30f3\u306e\u610f\u5473\u30bf\u30a4\u30d7\u3092\u63a8\u8ad6\n        type_logits = self.type_classifier(x)\n        predicted_types = torch.argmax(type_logits, dim=-1)\n\n        # \u610f\u5473\u8868\u73fe\n        semantic_representation = x\n\n        return {\n            'representation': semantic_representation,\n            'type_logits': type_logits,\n            'predicted_types': predicted_types,\n            'attention_weights': attention_weights\n        }\n\n    def extract_relations(self, representation: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        \u30c8\u30fc\u30af\u30f3\u9593\u306e\u610f\u5473\u7684\u95a2\u4fc2\u3092\u62bd\u51fa\n        \u578b\u30b7\u30b9\u30c6\u30e0\u306e\u95a2\u6570\u578b\u306e\u3088\u3046\u306a\u95a2\u4fc2\u3092\u5b66\u7fd2\n        \"\"\"\n        batch_size, seq_len, d_model = representation.shape\n\n        # \u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u30da\u30a2\u9593\u306e\u95a2\u4fc2\u3092\u8a08\u7b97\n        relations = torch.zeros(batch_size, seq_len, seq_len, self.d_model)\n\n        for i in range(seq_len):\n            for j in range(seq_len):\n                if i != j:\n                    relations[:, i, j] = self.relation_extractor(\n                        representation[:, i],\n                        representation[:, j]\n                    )\n\n        return relations\n\ndef compare_type_inference_and_semantic_understanding():\n    \"\"\"\n    \u578b\u63a8\u8ad6\u3068\u610f\u5473\u7406\u89e3\u306e\u6bd4\u8f03\u30c7\u30e2\n    \"\"\"\n\n    print(\"=== \u578b\u63a8\u8ad6\u3068\u610f\u5473\u7406\u89e3\u306e\u6bd4\u8f03 ===\\n\")\n\n    # 1. \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u578b\u63a8\u8ad6\n    print(\"1. \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u578b\u63a8\u8ad6\")\n    code = \"\"\"\n    x = 42\n    y = 3.14\n    z = x + y\n    result = z &gt; 40\n    \"\"\"\n\n    # \u7c21\u6613\u7684\u306a\u578b\u63a8\u8ad6\n    type_engine = TypeInferenceEngine()\n    print(f\"\u30b3\u30fc\u30c9:\\n{code}\")\n    print(\"\\n\u63a8\u8ad6\u3055\u308c\u305f\u578b:\")\n    print(\"  x: int\")\n    print(\"  y: float\")\n    print(\"  z: float (int + float \u2192 float)\")\n    print(\"  result: bool (float &gt; int \u2192 bool)\")\n\n    # 2. \u81ea\u7136\u8a00\u8a9e\u306e\u610f\u5473\u7406\u89e3\n    print(\"\\n2. \u81ea\u7136\u8a00\u8a9e\u306e\u610f\u5473\u7406\u89e3\uff08Transformer\uff09\")\n    sentence = \"The cat sat on the mat\"\n    tokens = sentence.split()\n\n    # \u4eee\u60f3\u7684\u306a\u610f\u5473\u30bf\u30a4\u30d7\n    semantic_types = {\n        \"The\": \"determiner\",\n        \"cat\": \"entity\",\n        \"sat\": \"action\",\n        \"on\": \"relation\",\n        \"the\": \"determiner\",\n        \"mat\": \"entity\"\n    }\n\n    print(f\"\\n\u6587: {sentence}\")\n    print(\"\\n\u63a8\u8ad6\u3055\u308c\u305f\u610f\u5473\u30bf\u30a4\u30d7:\")\n    for token, sem_type in semantic_types.items():\n        print(f\"  {token}: {sem_type}\")\n\n    # 3. \u7d71\u4e00\u7684\u306a\u8996\u70b9\n    print(\"\\n3. \u7d71\u4e00\u7684\u306a\u8996\u70b9\uff1a\u5236\u7d04\u30d9\u30fc\u30b9\u306e\u63a8\u8ad6\")\n\n    class UnifiedInference:\n        \"\"\"\u578b\u63a8\u8ad6\u3068\u610f\u5473\u63a8\u8ad6\u306e\u7d71\u4e00\u30e2\u30c7\u30eb\"\"\"\n\n        def __init__(self):\n            self.constraints = []\n\n        def add_constraint(self, item1, item2, relation):\n            \"\"\"\u5236\u7d04\u3092\u8ffd\u52a0\"\"\"\n            self.constraints.append((item1, item2, relation))\n\n        def solve(self):\n            \"\"\"\u5236\u7d04\u3092\u89e3\u304f\"\"\"\n            # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b\n            print(\"\\n\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u5236\u7d04:\")\n            print(\"  - x + y \u306e\u578b\u306f x \u3068 y \u306e\u578b\u306e\u4e0a\u9650\")\n            print(\"  - \u6bd4\u8f03\u6f14\u7b97\u306e\u7d50\u679c\u306f bool\")\n\n            # \u81ea\u7136\u8a00\u8a9e\u306e\u4f8b\n            print(\"\\n\u81ea\u7136\u8a00\u8a9e\u306e\u5236\u7d04:\")\n            print(\"  - 'sat' \u306f\u4e3b\u8a9e\uff08entity\uff09\u3068\u5834\u6240\uff08location\uff09\u3092\u8981\u6c42\")\n            print(\"  - 'on' \u306f\uff12\u3064\u306eentity\u3092\u95a2\u4fc2\u3065\u3051\u308b\")\n\n    unified = UnifiedInference()\n    unified.solve()\n\n    # \u53ef\u8996\u5316\n    visualize_inference_process()\n\ndef visualize_inference_process():\n    \"\"\"\u63a8\u8ad6\u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316\"\"\"\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # \u578b\u63a8\u8ad6\u306e\u30b0\u30e9\u30d5\n    ax1.set_title(\"\u578b\u63a8\u8ad6\u306e\u30d7\u30ed\u30bb\u30b9\")\n\n    # \u30ce\u30fc\u30c9\uff08\u5909\u6570\u3068\u578b\uff09\n    variables = ['x', 'y', 'z', 'result']\n    types = ['int', 'float', 'float', 'bool']\n\n    # \u30b0\u30e9\u30d5\u306e\u63cf\u753b\n    import networkx as nx\n    G1 = nx.DiGraph()\n\n    # \u5909\u6570\u30ce\u30fc\u30c9\n    for i, var in enumerate(variables):\n        G1.add_node(f\"var_{var}\", label=var, pos=(0, -i))\n\n    # \u578b\u30ce\u30fc\u30c9\n    for i, type_name in enumerate(types):\n        G1.add_node(f\"type_{type_name}\", label=type_name, pos=(2, -i))\n\n    # \u30a8\u30c3\u30b8\uff08\u63a8\u8ad6\u95a2\u4fc2\uff09\n    edges = [\n        (\"var_x\", \"type_int\"),\n        (\"var_y\", \"type_float\"),\n        (\"var_z\", \"type_float\"),\n        (\"var_result\", \"type_bool\")\n    ]\n\n    G1.add_edges_from(edges)\n\n    pos1 = nx.get_node_attributes(G1, 'pos')\n    nx.draw(G1, pos1, ax=ax1, with_labels=True, node_color='lightblue',\n            node_size=1000, font_size=10, arrows=True)\n\n    # \u610f\u5473\u7406\u89e3\u306e\u30b0\u30e9\u30d5\n    ax2.set_title(\"\u610f\u5473\u7406\u89e3\u306e\u30d7\u30ed\u30bb\u30b9\")\n\n    # \u6587\u306e\u69cb\u9020\n    G2 = nx.DiGraph()\n\n    sentence_structure = {\n        \"The cat\": \"entity\",\n        \"sat\": \"action\",\n        \"on the mat\": \"location\"\n    }\n\n    # \u30ce\u30fc\u30c9\u3068\u30a8\u30c3\u30b8\n    prev_node = None\n    for i, (phrase, sem_type) in enumerate(sentence_structure.items()):\n        G2.add_node(phrase, label=f\"{phrase}\\n({sem_type})\", pos=(i, 0))\n        if prev_node:\n            G2.add_edge(prev_node, phrase)\n        prev_node = phrase\n\n    pos2 = nx.get_node_attributes(G2, 'pos')\n    nx.draw(G2, pos2, ax=ax2, with_labels=True, node_color='lightgreen',\n            node_size=2000, font_size=10, arrows=True)\n\n    plt.tight_layout()\n    plt.show()\n\n# \u30c7\u30e2\u5b9f\u884c\ncompare_type_inference_and_semantic_understanding()\n</code></pre>"},{"location":"part1/similarities/#_9","title":"\u30b9\u30b3\u30fc\u30d7\u89e3\u6c7a\u3068\u6587\u8108\u7a93","text":"<p>\u5909\u6570\u306e\u30b9\u30b3\u30fc\u30d7\u89e3\u6c7a\u3068\u3001Transformer\u306e\u6587\u8108\u7a93\u306b\u306f\u76f4\u63a5\u7684\u306a\u5bfe\u5fdc\u95a2\u4fc2\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>class ScopeAndContext:\n    \"\"\"\n    \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30b9\u30b3\u30fc\u30d7\u3068Transformer\u306e\u6587\u8108\u7a93\u306e\u6bd4\u8f03\n    \"\"\"\n\n    def __init__(self):\n        self.examples = []\n\n    def demonstrate_scope_resolution(self):\n        \"\"\"\u30b9\u30b3\u30fc\u30d7\u89e3\u6c7a\u306e\u30c7\u30e2\"\"\"\n\n        print(\"=== \u30b9\u30b3\u30fc\u30d7\u89e3\u6c7a\u306e\u4f8b ===\\n\")\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30b9\u30b3\u30fc\u30d7\n        code = '''\n        global_var = 100\n\n        def outer_function():\n            outer_var = 50\n\n            def inner_function():\n                inner_var = 10\n                print(inner_var)    # 10 (\u30ed\u30fc\u30ab\u30eb)\n                print(outer_var)    # 50 (\u5916\u5074\u306e\u95a2\u6570)\n                print(global_var)   # 100 (\u30b0\u30ed\u30fc\u30d0\u30eb)\n\n            inner_function()\n\n        outer_function()\n        '''\n\n        print(\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30b9\u30b3\u30fc\u30d7:\")\n        print(code)\n\n        # \u30b9\u30b3\u30fc\u30d7\u30c1\u30a7\u30fc\u30f3\u306e\u53ef\u8996\u5316\n        self.visualize_scope_chain()\n\n        # Transformer\u306e\u6587\u8108\u7a93\n        print(\"\\n\\nTransformer\u306e\u6587\u8108\u7a93:\")\n        self.demonstrate_context_window()\n\n    def visualize_scope_chain(self):\n        \"\"\"\u30b9\u30b3\u30fc\u30d7\u30c1\u30a7\u30fc\u30f3\u306e\u53ef\u8996\u5316\"\"\"\n\n        scopes = [\n            {\"name\": \"Global\", \"vars\": [\"global_var\"], \"level\": 0},\n            {\"name\": \"outer_function\", \"vars\": [\"outer_var\"], \"level\": 1},\n            {\"name\": \"inner_function\", \"vars\": [\"inner_var\"], \"level\": 2}\n        ]\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n        # \u30b9\u30b3\u30fc\u30d7\u3092\u5165\u308c\u5b50\u306e\u56db\u89d2\u5f62\u3067\u8868\u73fe\n        colors = ['lightblue', 'lightgreen', 'lightyellow']\n\n        for i, scope in enumerate(scopes):\n            # \u5916\u5074\u304b\u3089\u5185\u5074\u3078\n            margin = i * 0.1\n            rect = plt.Rectangle((margin, margin), \n                               1 - 2*margin, 1 - 2*margin,\n                               fill=True, facecolor=colors[i],\n                               edgecolor='black', linewidth=2)\n            ax.add_patch(rect)\n\n            # \u30e9\u30d9\u30eb\n            ax.text(0.5, 1 - margin - 0.05, scope[\"name\"],\n                   ha='center', va='top', fontsize=12, weight='bold')\n\n            # \u5909\u6570\n            var_text = \", \".join(scope[\"vars\"])\n            ax.text(0.5, 0.5 + margin, f\"\u5909\u6570: {var_text}\",\n                   ha='center', va='center', fontsize=10)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.axis('off')\n        ax.set_title('\u30b9\u30b3\u30fc\u30d7\u30c1\u30a7\u30fc\u30f3\uff08\u5185\u5074\u304b\u3089\u5916\u5074\u3092\u53c2\u7167\u53ef\u80fd\uff09')\n\n        plt.tight_layout()\n        plt.show()\n\n    def demonstrate_context_window(self):\n        \"\"\"Transformer\u306e\u6587\u8108\u7a93\u306e\u30c7\u30e2\"\"\"\n\n        class ContextWindow:\n            def __init__(self, max_length=512):\n                self.max_length = max_length\n                self.attention_type = \"causal\"  # or \"bidirectional\"\n\n            def create_attention_mask(self, seq_length: int) -&gt; torch.Tensor:\n                \"\"\"\n                Attention \u30de\u30b9\u30af\u3092\u4f5c\u6210\n                \u6587\u8108\u7a93\u306e\u5236\u9650\u3092\u5b9f\u88c5\n                \"\"\"\n                if self.attention_type == \"causal\":\n                    # \u56e0\u679c\u7684\u30de\u30b9\u30af\uff08\u672a\u6765\u3092\u898b\u306a\u3044\uff09\n                    mask = torch.tril(torch.ones(seq_length, seq_length))\n                else:\n                    # \u53cc\u65b9\u5411\u30de\u30b9\u30af\uff08\u5168\u3066\u898b\u308b\uff09\n                    mask = torch.ones(seq_length, seq_length)\n\n                # \u6587\u8108\u7a93\u306e\u5236\u9650\u3092\u9069\u7528\n                for i in range(seq_length):\n                    # max_length \u3088\u308a\u96e2\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306f\u30de\u30b9\u30af\n                    mask[i, :max(0, i - self.max_length)] = 0\n                    if i + self.max_length &lt; seq_length:\n                        mask[i, i + self.max_length:] = 0\n\n                return mask\n\n            def visualize_context_window(self):\n                \"\"\"\u6587\u8108\u7a93\u3092\u53ef\u8996\u5316\"\"\"\n                seq_lengths = [10, 50, 100]\n\n                fig, axes = plt.subplots(1, len(seq_lengths), figsize=(15, 5))\n\n                for ax, seq_len in zip(axes, seq_lengths):\n                    mask = self.create_attention_mask(seq_len)\n\n                    im = ax.imshow(mask, cmap='Blues', aspect='auto')\n                    ax.set_title(f'\u6587\u8108\u7a93 (\u9577\u3055={seq_len})')\n                    ax.set_xlabel('\u53c2\u7167\u3067\u304d\u308b\u4f4d\u7f6e')\n                    ax.set_ylabel('\u73fe\u5728\u306e\u4f4d\u7f6e')\n\n                    # \u6587\u8108\u7a93\u306e\u5883\u754c\u3092\u793a\u3059\u7dda\n                    if seq_len &gt; self.max_length:\n                        for i in range(seq_len):\n                            # \u5de6\u5883\u754c\n                            left_bound = max(0, i - self.max_length)\n                            ax.axvline(x=left_bound, color='red', linestyle='--', alpha=0.5)\n\n                            # \u53f3\u5883\u754c\uff08\u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u5834\u5408\u306f\u73fe\u5728\u4f4d\u7f6e\uff09\n                            right_bound = i if self.attention_type == \"causal\" else min(seq_len, i + self.max_length)\n                            ax.axvline(x=right_bound, color='red', linestyle='--', alpha=0.5)\n\n                plt.tight_layout()\n                plt.show()\n\n        # \u7570\u306a\u308b\u6587\u8108\u7a93\u30b5\u30a4\u30ba\u3067\u306e\u30c7\u30e2\n        for window_size in [8, 16, 512]:\n            print(f\"\\n\u6587\u8108\u7a93\u30b5\u30a4\u30ba: {window_size}\")\n            context = ContextWindow(max_length=window_size)\n            context.visualize_context_window()\n\n# \u5b9f\u884c\nscope_demo = ScopeAndContext()\nscope_demo.demonstrate_scope_resolution()\n</code></pre>"},{"location":"part1/similarities/#_10","title":"\u610f\u5473\u306e\u5408\u6210\u6027","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u5f0f\u306e\u8a55\u4fa1\u3068\u3001\u81ea\u7136\u8a00\u8a9e\u306e\u610f\u5473\u306e\u5408\u6210\u306b\u306f\u5171\u901a\u306e\u539f\u7406\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>class CompositionalSemantics:\n    \"\"\"\n    \u5408\u6210\u7684\u610f\u5473\u8ad6\uff1a\u90e8\u5206\u306e\u610f\u5473\u304b\u3089\u5168\u4f53\u306e\u610f\u5473\u3092\u69cb\u7bc9\n    \"\"\"\n\n    def __init__(self):\n        self.programming_example = {\n            \"expression\": \"f(g(x) + h(y))\",\n            \"evaluation_order\": [\n                \"1. g(x) \u3092\u8a55\u4fa1\",\n                \"2. h(y) \u3092\u8a55\u4fa1\",\n                \"3. g(x) + h(y) \u3092\u8a55\u4fa1\",\n                \"4. f(...) \u3092\u8a55\u4fa1\"\n            ]\n        }\n\n        self.natural_language_example = {\n            \"sentence\": \"The red car quickly overtook the blue truck\",\n            \"composition\": [\n                \"1. 'red' + 'car' \u2192 'red car' (\u5f62\u5bb9\u8a5e\u4fee\u98fe)\",\n                \"2. 'blue' + 'truck' \u2192 'blue truck' (\u5f62\u5bb9\u8a5e\u4fee\u98fe)\",\n                \"3. 'quickly' + 'overtook' \u2192 'quickly overtook' (\u526f\u8a5e\u4fee\u98fe)\",\n                \"4. 'The red car' + 'quickly overtook' + 'the blue truck' \u2192 \u5b8c\u5168\u306a\u6587\"\n            ]\n        }\n\n    def demonstrate_compositionality(self):\n        \"\"\"\u5408\u6210\u6027\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\"\"\"\n\n        print(\"=== \u610f\u5473\u306e\u5408\u6210\u6027 ===\\n\")\n\n        # \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u4f8b\n        print(\"1. \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u306e\u5408\u6210\u7684\u8a55\u4fa1\")\n        print(f\"\u5f0f: {self.programming_example['expression']}\")\n        print(\"\u8a55\u4fa1\u9806\u5e8f:\")\n        for step in self.programming_example['evaluation_order']:\n            print(f\"  {step}\")\n\n        # \u8a55\u4fa1\u6728\u306e\u69cb\u7bc9\n        self.build_evaluation_tree()\n\n        # \u81ea\u7136\u8a00\u8a9e\u306e\u4f8b\n        print(\"\\n2. \u81ea\u7136\u8a00\u8a9e\u3067\u306e\u610f\u5473\u306e\u5408\u6210\")\n        print(f\"\u6587: {self.natural_language_example['sentence']}\")\n        print(\"\u5408\u6210\u30d7\u30ed\u30bb\u30b9:\")\n        for step in self.natural_language_example['composition']:\n            print(f\"  {step}\")\n\n        # Transformer\u3067\u306e\u5408\u6210\n        print(\"\\n3. Transformer\u306b\u3088\u308b\u610f\u5473\u306e\u5408\u6210\")\n        self.transformer_composition()\n\n    def build_evaluation_tree(self):\n        \"\"\"\u8a55\u4fa1\u6728\u306e\u69cb\u7bc9\u3068\u53ef\u8996\u5316\"\"\"\n\n        # \u5f0f: f(g(x) + h(y))\n        class ExprNode:\n            def __init__(self, value, children=None):\n                self.value = value\n                self.children = children or []\n\n        # \u8a55\u4fa1\u6728\u3092\u69cb\u7bc9\n        tree = ExprNode(\"f(...)\", [\n            ExprNode(\"+\", [\n                ExprNode(\"g(x)\", [ExprNode(\"x\")]),\n                ExprNode(\"h(y)\", [ExprNode(\"y\")])\n            ])\n        ])\n\n        # \u53ef\u8996\u5316\n        self.visualize_tree(tree, \"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u8a55\u4fa1\u6728\")\n\n    def transformer_composition(self):\n        \"\"\"Transformer\u306b\u3088\u308b\u610f\u5473\u5408\u6210\u306e\u5b9f\u88c5\"\"\"\n\n        class CompositionTransformer(nn.Module):\n            def __init__(self, vocab_size, d_model=256):\n                super().__init__()\n                self.embedding = nn.Embedding(vocab_size, d_model)\n                self.position_encoding = nn.Embedding(512, d_model)\n\n                # \u5408\u6210\u306e\u305f\u3081\u306e\u5c64\n                self.composition_layers = nn.ModuleList([\n                    nn.TransformerEncoderLayer(d_model, nhead=8)\n                    for _ in range(6)\n                ])\n\n                # \u53e5\u69cb\u9020\u3092\u4e88\u6e2c\u3059\u308b\u30d8\u30c3\u30c9\n                self.phrase_predictor = nn.Linear(d_model, 2)  # \u53e5\u306e\u59cb\u307e\u308a/\u7d42\u308f\u308a\n\n            def forward(self, input_ids):\n                # \u57cb\u3081\u8fbc\u307f\n                x = self.embedding(input_ids)\n                positions = torch.arange(len(input_ids)).unsqueeze(0)\n                x = x + self.position_encoding(positions)\n\n                # \u5404\u5c64\u3067\u5f90\u3005\u306b\u610f\u5473\u3092\u5408\u6210\n                intermediate_representations = []\n\n                for i, layer in enumerate(self.composition_layers):\n                    x = layer(x)\n                    intermediate_representations.append(x.clone())\n\n                # \u53e5\u69cb\u9020\u306e\u4e88\u6e2c\n                phrase_boundaries = self.phrase_predictor(x)\n\n                return {\n                    'final_representation': x,\n                    'intermediate': intermediate_representations,\n                    'phrase_boundaries': phrase_boundaries\n                }\n\n        # \u30c7\u30e2\u7528\u306e\u53ef\u8996\u5316\n        self.visualize_composition_process()\n\n    def visualize_tree(self, root, title):\n        \"\"\"\u6728\u69cb\u9020\u306e\u53ef\u8996\u5316\"\"\"\n        import networkx as nx\n\n        G = nx.DiGraph()\n\n        def add_nodes(node, parent=None, pos_x=0, pos_y=0, layer=1):\n            node_id = f\"{node.value}_{pos_x}_{pos_y}\"\n            G.add_node(node_id, label=node.value, pos=(pos_x, -pos_y))\n\n            if parent:\n                G.add_edge(parent, node_id)\n\n            # \u5b50\u30ce\u30fc\u30c9\u306e\u914d\u7f6e\n            num_children = len(node.children)\n            if num_children &gt; 0:\n                spacing = 2 ** (3 - layer)\n                start_x = pos_x - (num_children - 1) * spacing / 2\n\n                for i, child in enumerate(node.children):\n                    child_x = start_x + i * spacing\n                    add_nodes(child, node_id, child_x, pos_y + 1, layer + 1)\n\n            return node_id\n\n        add_nodes(root)\n\n        plt.figure(figsize=(10, 6))\n        pos = nx.get_node_attributes(G, 'pos')\n        labels = nx.get_node_attributes(G, 'label')\n\n        nx.draw(G, pos, labels=labels, with_labels=True,\n                node_color='lightblue', node_size=1500,\n                font_size=10, arrows=True)\n\n        plt.title(title)\n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n\n    def visualize_composition_process(self):\n        \"\"\"\u5408\u6210\u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316\"\"\"\n\n        sentence = \"The red car quickly overtook the blue truck\"\n        words = sentence.split()\n\n        # \u5404\u5c64\u3067\u306e\u8868\u73fe\u306e\u5909\u5316\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\n        layers = 6\n\n        fig, axes = plt.subplots(1, layers, figsize=(20, 4))\n\n        for layer in range(layers):\n            ax = axes[layer]\n\n            # \u5c64\u304c\u6df1\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u3001\u3088\u308a\u5927\u304d\u306a\u5358\u4f4d\u3067\u7d50\u5408\n            if layer == 0:\n                # \u5358\u8a9e\u30ec\u30d9\u30eb\n                groups = [[w] for w in words]\n            elif layer == 1:\n                # \u5f62\u5bb9\u8a5e\u3068\u540d\u8a5e\n                groups = [[\"The\", \"red\", \"car\"], [\"quickly\"], [\"overtook\"], [\"the\", \"blue\", \"truck\"]]\n            elif layer == 2:\n                # \u540d\u8a5e\u53e5\n                groups = [[\"The red car\"], [\"quickly overtook\"], [\"the blue truck\"]]\n            elif layer &gt;= 3:\n                # \u5b8c\u5168\u306a\u6587\n                groups = [[sentence]]\n\n            # \u53ef\u8996\u5316\n            y_pos = 0\n            for group in groups:\n                text = \" \".join(group)\n                ax.text(0.5, y_pos, text, ha='center', va='center',\n                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'))\n                y_pos -= 0.2\n\n            ax.set_xlim(0, 1)\n            ax.set_ylim(-1, 0.5)\n            ax.axis('off')\n            ax.set_title(f'Layer {layer + 1}')\n\n        plt.suptitle('Transformer\u306b\u3088\u308b\u968e\u5c64\u7684\u306a\u610f\u5473\u5408\u6210')\n        plt.tight_layout()\n        plt.show()\n\n# \u30c7\u30e2\u5b9f\u884c\ncomp_semantics = CompositionalSemantics()\ncomp_semantics.demonstrate_compositionality()\n</code></pre>"},{"location":"part1/similarities/#24","title":"2.4 \u6700\u9069\u5316\u3068\u30e2\u30c7\u30eb\u6539\u5584","text":""},{"location":"part1/similarities/#_11","title":"\u30b3\u30f3\u30d1\u30a4\u30e9\u6700\u9069\u5316\u6280\u8853\u306e\u5fdc\u7528","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u6280\u8853\u306f\u3001Transformer\u30e2\u30c7\u30eb\u306e\u6700\u9069\u5316\u306b\u3082\u5fdc\u7528\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport time\nimport matplotlib.pyplot as plt\n\nclass OptimizationTechniques:\n    \"\"\"\n    \u30b3\u30f3\u30d1\u30a4\u30e9\u6700\u9069\u5316\u6280\u8853\u3068Transformer\u6700\u9069\u5316\u306e\u5bfe\u5fdc\n    \"\"\"\n\n    def __init__(self):\n        self.compiler_optimizations = {\n            \"constant_folding\": \"\u5b9a\u6570\u7573\u307f\u8fbc\u307f\",\n            \"dead_code_elimination\": \"\u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u9664\u53bb\",\n            \"loop_unrolling\": \"\u30eb\u30fc\u30d7\u5c55\u958b\",\n            \"function_inlining\": \"\u95a2\u6570\u306e\u30a4\u30f3\u30e9\u30a4\u30f3\u5316\",\n            \"common_subexpression_elimination\": \"\u5171\u901a\u90e8\u5206\u5f0f\u306e\u9664\u53bb\"\n        }\n\n        self.transformer_optimizations = {\n            \"weight_pruning\": \"\u91cd\u307f\u306e\u5208\u308a\u8fbc\u307f\",\n            \"quantization\": \"\u91cf\u5b50\u5316\",\n            \"knowledge_distillation\": \"\u77e5\u8b58\u84b8\u7559\",\n            \"attention_optimization\": \"Attention \u306e\u6700\u9069\u5316\",\n            \"model_fusion\": \"\u5c64\u306e\u878d\u5408\"\n        }\n\n    def demonstrate_constant_folding(self):\n        \"\"\"\u5b9a\u6570\u7573\u307f\u8fbc\u307f\u3068\u30e2\u30c7\u30eb\u306e\u4e8b\u524d\u8a08\u7b97\"\"\"\n\n        print(\"=== \u5b9a\u6570\u7573\u307f\u8fbc\u307f ===\")\n\n        # \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u4f8b\n        print(\"\\n1. \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5b9a\u6570\u7573\u307f\u8fbc\u307f:\")\n        print(\"  \u524d: x = 2 * 3 * 4\")\n        print(\"  \u5f8c: x = 24\")\n\n        # Transformer\u306e\u4f8b\n        print(\"\\n2. Transformer\u3067\u306e\u4e8b\u524d\u8a08\u7b97:\")\n\n        class PositionalEncoding(nn.Module):\n            def __init__(self, d_model, max_len=5000):\n                super().__init__()\n\n                # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4e8b\u524d\u8a08\u7b97\uff08\u5b9a\u6570\u7573\u307f\u8fbc\u307f\u306b\u76f8\u5f53\uff09\n                pe = torch.zeros(max_len, d_model)\n                position = torch.arange(0, max_len).unsqueeze(1).float()\n\n                div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                                   -(torch.log(torch.tensor(10000.0)) / d_model))\n\n                pe[:, 0::2] = torch.sin(position * div_term)\n                pe[:, 1::2] = torch.cos(position * div_term)\n\n                # \u30d0\u30c3\u30d5\u30a1\u3068\u3057\u3066\u767b\u9332\uff08\u5b66\u7fd2\u3057\u306a\u3044\u5b9a\u6570\uff09\n                self.register_buffer('pe', pe)\n\n            def forward(self, x):\n                # \u5b9f\u884c\u6642\u306f\u5358\u7d14\u306a\u52a0\u7b97\u306e\u307f\n                return x + self.pe[:x.size(1)]\n\n        # \u901f\u5ea6\u6bd4\u8f03\n        d_model = 512\n        seq_len = 100\n\n        # \u4e8b\u524d\u8a08\u7b97\u306a\u3057\u306e\u5834\u5408\n        def positional_encoding_naive(x, d_model, seq_len):\n            pe = torch.zeros(seq_len, d_model)\n            for pos in range(seq_len):\n                for i in range(0, d_model, 2):\n                    pe[pos, i] = torch.sin(pos / (10000 ** (i / d_model)))\n                    pe[pos, i + 1] = torch.cos(pos / (10000 ** (i / d_model)))\n            return x + pe\n\n        # \u4e8b\u524d\u8a08\u7b97\u3042\u308a\u306e\u5834\u5408\n        pe_optimized = PositionalEncoding(d_model)\n\n        x = torch.randn(1, seq_len, d_model)\n\n        # \u6642\u9593\u8a08\u6e2c\n        import timeit\n\n        time_naive = timeit.timeit(\n            lambda: positional_encoding_naive(x, d_model, seq_len),\n            number=100\n        )\n\n        time_optimized = timeit.timeit(\n            lambda: pe_optimized(x),\n            number=100\n        )\n\n        print(f\"\\n  \u4e8b\u524d\u8a08\u7b97\u306a\u3057: {time_naive:.4f}\u79d2\")\n        print(f\"  \u4e8b\u524d\u8a08\u7b97\u3042\u308a: {time_optimized:.4f}\u79d2\")\n        print(f\"  \u9ad8\u901f\u5316: {time_naive / time_optimized:.2f}\u500d\")\n\n    def demonstrate_dead_code_elimination(self):\n        \"\"\"\u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u9664\u53bb\u3068\u30e2\u30c7\u30eb\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\"\"\"\n\n        print(\"\\n=== \u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u9664\u53bb ===\")\n\n        # \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u4f8b\n        print(\"\\n1. \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u9664\u53bb:\")\n        print(\"  \u524d: if (false) { expensive_function(); }\")\n        print(\"  \u5f8c: // \u524a\u9664\")\n\n        # Transformer\u306e\u4f8b\n        print(\"\\n2. Transformer\u3067\u306e\u91cd\u307f\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0:\")\n\n        class PrunableLinear(nn.Module):\n            def __init__(self, in_features, out_features):\n                super().__init__()\n                self.weight = nn.Parameter(torch.randn(out_features, in_features))\n                self.bias = nn.Parameter(torch.zeros(out_features))\n                self.mask = torch.ones_like(self.weight)\n\n            def prune_weights(self, threshold=0.01):\n                \"\"\"\u5c0f\u3055\u3044\u91cd\u307f\u3092\u9664\u53bb\uff08\u30bc\u30ed\u306b\uff09\"\"\"\n                with torch.no_grad():\n                    # \u91cd\u307f\u306e\u7d76\u5bfe\u5024\u304c\u95be\u5024\u4ee5\u4e0b\u306e\u3082\u306e\u3092\u30de\u30b9\u30af\n                    self.mask = (self.weight.abs() &gt; threshold).float()\n                    pruned_ratio = 1 - self.mask.sum() / self.mask.numel()\n                    print(f\"  \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u7387: {pruned_ratio:.2%}\")\n                    return pruned_ratio\n\n            def forward(self, x):\n                # \u30de\u30b9\u30af\u3092\u9069\u7528\u3057\u305f\u91cd\u307f\u3067\u8a08\u7b97\n                masked_weight = self.weight * self.mask\n                return F.linear(x, masked_weight, self.bias)\n\n        # \u30c7\u30e2\n        layer = PrunableLinear(512, 256)\n        x = torch.randn(10, 512)\n\n        # \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u524d\n        output_before = layer(x)\n\n        # \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\n        pruned_ratio = layer.prune_weights(threshold=0.1)\n\n        # \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u5f8c\n        output_after = layer(x)\n\n        # \u51fa\u529b\u306e\u9055\u3044\u3092\u78ba\u8a8d\n        diff = (output_before - output_after).abs().mean()\n        print(f\"  \u51fa\u529b\u306e\u5e73\u5747\u5dee\u5206: {diff:.6f}\")\n\n    def demonstrate_loop_unrolling(self):\n        \"\"\"\u30eb\u30fc\u30d7\u5c55\u958b\u3068Attention\u306e\u6700\u9069\u5316\"\"\"\n\n        print(\"\\n=== \u30eb\u30fc\u30d7\u5c55\u958b ===\")\n\n        # \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u4f8b\n        print(\"\\n1. \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u30eb\u30fc\u30d7\u5c55\u958b:\")\n        print(\"  \u524d: for i in range(4): sum += arr[i]\")\n        print(\"  \u5f8c: sum += arr[0] + arr[1] + arr[2] + arr[3]\")\n\n        # Transformer\u306e\u4f8b\n        print(\"\\n2. Attention\u306eFlash Attention\u6700\u9069\u5316:\")\n\n        class StandardAttention(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.scale = d_model ** -0.5\n\n            def forward(self, q, k, v):\n                # \u6a19\u6e96\u7684\u306a\u5b9f\u88c5\n                scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n                attn_weights = torch.softmax(scores, dim=-1)\n                output = torch.matmul(attn_weights, v)\n                return output\n\n        class FlashAttention(nn.Module):\n            \"\"\"\n            Flash Attention: \u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u5b9f\u88c5\n            \u30eb\u30fc\u30d7\u3092\u30bf\u30a4\u30eb\u5316\u3057\u3066\u6700\u9069\u5316\n            \"\"\"\n            def __init__(self, d_model, block_size=64):\n                super().__init__()\n                self.scale = d_model ** -0.5\n                self.block_size = block_size\n\n            def forward(self, q, k, v):\n                batch_size, seq_len, d_model = q.shape\n\n                # \u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u3067\u51e6\u7406\uff08\u30eb\u30fc\u30d7\u5c55\u958b\u306e\u4e00\u7a2e\uff09\n                output = torch.zeros_like(q)\n\n                for i in range(0, seq_len, self.block_size):\n                    i_end = min(i + self.block_size, seq_len)\n                    q_block = q[:, i:i_end]\n\n                    # \u5404\u30d6\u30ed\u30c3\u30af\u3067\u8a08\u7b97\n                    scores_block = torch.matmul(q_block, k.transpose(-2, -1)) * self.scale\n                    attn_block = torch.softmax(scores_block, dim=-1)\n                    output[:, i:i_end] = torch.matmul(attn_block, v)\n\n                return output\n\n        # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\n        d_model = 512\n        seq_len = 1024\n        batch_size = 8\n\n        q = torch.randn(batch_size, seq_len, d_model)\n        k = torch.randn(batch_size, seq_len, d_model)\n        v = torch.randn(batch_size, seq_len, d_model)\n\n        # \u6a19\u6e96\u5b9f\u88c5\u3067\u306fO(seq_len^2)\u306e\u30e1\u30e2\u30ea\n        print(f\"\\n  \u6a19\u6e96Attention: \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf O({seq_len}\u00b2) = O({seq_len**2})\")\n        print(f\"  Flash Attention: \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf O({seq_len}) = O({seq_len})\")\n\n    def demonstrate_common_subexpression_elimination(self):\n        \"\"\"\u5171\u901a\u90e8\u5206\u5f0f\u306e\u9664\u53bb\u3068\u8a08\u7b97\u306e\u518d\u5229\u7528\"\"\"\n\n        print(\"\\n=== \u5171\u901a\u90e8\u5206\u5f0f\u306e\u9664\u53bb ===\")\n\n        # \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u4f8b\n        print(\"\\n1. \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5171\u901a\u90e8\u5206\u5f0f\u9664\u53bb:\")\n        print(\"  \u524d: a = b * c + d; e = b * c - d;\")\n        print(\"  \u5f8c: temp = b * c; a = temp + d; e = temp - d;\")\n\n        # Transformer\u306e\u4f8b\n        print(\"\\n2. Transformer\u3067\u306eKV\u30ad\u30e3\u30c3\u30b7\u30e5:\")\n\n        class TransformerWithKVCache(nn.Module):\n            def __init__(self, d_model, n_heads):\n                super().__init__()\n                self.attention = nn.MultiheadAttention(d_model, n_heads)\n                self.kv_cache = {}\n\n            def forward(self, query, key, value, use_cache=True):\n                if use_cache and 'key' in self.kv_cache:\n                    # \u30ad\u30e3\u30c3\u30b7\u30e5\u3055\u308c\u305fK, V\u3092\u518d\u5229\u7528\n                    cached_key = self.kv_cache['key']\n                    cached_value = self.kv_cache['value']\n\n                    # \u65b0\u3057\u3044\u90e8\u5206\u306e\u307f\u8a08\u7b97\n                    key = torch.cat([cached_key, key], dim=0)\n                    value = torch.cat([cached_value, value], dim=0)\n\n                # Attention\u3092\u8a08\u7b97\n                output, _ = self.attention(query, key, value)\n\n                # \u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u66f4\u65b0\n                if use_cache:\n                    self.kv_cache['key'] = key\n                    self.kv_cache['value'] = value\n\n                return output\n\n        print(\"\\n  \u5229\u70b9:\")\n        print(\"  - \u63a8\u8ad6\u6642\u306e\u8a08\u7b97\u91cf\u524a\u6e1b\")\n        print(\"  - \u7279\u306b\u81ea\u5df1\u56de\u5e30\u751f\u6210\u3067\u52b9\u679c\u7684\")\n        print(\"  - \u30e1\u30e2\u30ea\u3068\u8a08\u7b97\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\")\n\n    def demonstrate_quantization(self):\n        \"\"\"\u91cf\u5b50\u5316\uff1a\u7cbe\u5ea6\u3068\u52b9\u7387\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\"\"\"\n\n        print(\"\\n=== \u91cf\u5b50\u5316 ===\")\n\n        class QuantizedLinear(nn.Module):\n            def __init__(self, in_features, out_features, bits=8):\n                super().__init__()\n                self.in_features = in_features\n                self.out_features = out_features\n                self.bits = bits\n\n                # \u901a\u5e38\u306e\u91cd\u307f\uff08float32\uff09\n                self.weight = nn.Parameter(torch.randn(out_features, in_features))\n\n                # \u91cf\u5b50\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\n                self.register_buffer('scale', torch.tensor(1.0))\n                self.register_buffer('zero_point', torch.tensor(0))\n\n            def quantize_weights(self):\n                \"\"\"\u91cd\u307f\u3092\u91cf\u5b50\u5316\"\"\"\n                # \u91cd\u307f\u306e\u7bc4\u56f2\u3092\u8a08\u7b97\n                w_min = self.weight.min()\n                w_max = self.weight.max()\n\n                # \u30b9\u30b1\u30fc\u30eb\u3068\u30bc\u30ed\u70b9\u3092\u8a08\u7b97\n                qmin = -(2 ** (self.bits - 1))\n                qmax = 2 ** (self.bits - 1) - 1\n                self.scale = (w_max - w_min) / (qmax - qmin)\n                self.zero_point = qmin - w_min / self.scale\n\n                # \u91cf\u5b50\u5316\n                w_quantized = torch.round(self.weight / self.scale + self.zero_point)\n                w_quantized = torch.clamp(w_quantized, qmin, qmax)\n\n                return w_quantized\n\n            def forward(self, x):\n                # \u91cf\u5b50\u5316\u3055\u308c\u305f\u91cd\u307f\u3067\u8a08\u7b97\n                w_quantized = self.quantize_weights()\n                w_dequantized = (w_quantized - self.zero_point) * self.scale\n                return F.linear(x, w_dequantized)\n\n        # \u30b5\u30a4\u30ba\u3068\u30e1\u30e2\u30ea\u306e\u6bd4\u8f03\n        in_features, out_features = 1024, 512\n\n        # \u901a\u5e38\u306e\u5c64\n        normal_layer = nn.Linear(in_features, out_features)\n        normal_size = normal_layer.weight.numel() * 4  # float32 = 4 bytes\n\n        # \u91cf\u5b50\u5316\u5c64\uff088\u30d3\u30c3\u30c8\uff09\n        quantized_layer = QuantizedLinear(in_features, out_features, bits=8)\n        quantized_size = quantized_layer.weight.numel() * 1  # int8 = 1 byte\n\n        print(f\"\\n  \u901a\u5e38\u306e\u30e2\u30c7\u30eb: {normal_size / 1024:.1f} KB\")\n        print(f\"  \u91cf\u5b50\u5316\u30e2\u30c7\u30eb: {quantized_size / 1024:.1f} KB\")\n        print(f\"  \u5727\u7e2e\u7387: {normal_size / quantized_size:.1f}\u500d\")\n\n        # \u7cbe\u5ea6\u306e\u6bd4\u8f03\n        x = torch.randn(10, in_features)\n        output_normal = normal_layer(x)\n        output_quantized = quantized_layer(x)\n\n        mse = F.mse_loss(output_normal, output_quantized)\n        print(f\"  \u91cf\u5b50\u5316\u8aa4\u5dee (MSE): {mse:.6f}\")\n\n    def visualize_optimization_impact(self):\n        \"\"\"\u6700\u9069\u5316\u306e\u5f71\u97ff\u3092\u53ef\u8996\u5316\"\"\"\n\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # 1. \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u306e\u5f71\u97ff\n        ax = axes[0, 0]\n        pruning_rates = [0, 0.1, 0.3, 0.5, 0.7, 0.9]\n        accuracy = [0.95, 0.94, 0.93, 0.90, 0.85, 0.70]\n        model_size = [100, 90, 70, 50, 30, 10]\n\n        ax2 = ax.twinx()\n        line1 = ax.plot(pruning_rates, accuracy, 'b-o', label='\u7cbe\u5ea6')\n        line2 = ax2.plot(pruning_rates, model_size, 'r-s', label='\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba')\n\n        ax.set_xlabel('\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u7387')\n        ax.set_ylabel('\u7cbe\u5ea6', color='b')\n        ax2.set_ylabel('\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba (%)', color='r')\n        ax.set_title('\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u306e\u5f71\u97ff')\n\n        lines = line1 + line2\n        labels = [l.get_label() for l in lines]\n        ax.legend(lines, labels)\n\n        # 2. \u91cf\u5b50\u5316\u306e\u5f71\u97ff\n        ax = axes[0, 1]\n        bits = [32, 16, 8, 4, 2]\n        accuracy_q = [0.95, 0.94, 0.92, 0.85, 0.70]\n        size_ratio = [1.0, 0.5, 0.25, 0.125, 0.0625]\n\n        ax2 = ax.twinx()\n        ax.plot(bits, accuracy_q, 'b-o', label='\u7cbe\u5ea6')\n        ax2.plot(bits, size_ratio, 'r-s', label='\u30b5\u30a4\u30ba\u6bd4')\n\n        ax.set_xlabel('\u30d3\u30c3\u30c8\u6570')\n        ax.set_ylabel('\u7cbe\u5ea6', color='b')\n        ax2.set_ylabel('\u30b5\u30a4\u30ba\u6bd4', color='r')\n        ax.set_title('\u91cf\u5b50\u5316\u306e\u5f71\u97ff')\n        ax.invert_xaxis()\n\n        # 3. \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u901f\u5ea6\n        ax = axes[1, 0]\n        batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n        throughput = [10, 19, 36, 68, 120, 200, 280]\n\n        ax.plot(batch_sizes, throughput, 'g-o')\n        ax.set_xlabel('\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba')\n        ax.set_ylabel('\u30b9\u30eb\u30fc\u30d7\u30c3\u30c8 (samples/sec)')\n        ax.set_title('\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u51e6\u7406\u901f\u5ea6')\n        ax.set_xscale('log', base=2)\n\n        # 4. \u6700\u9069\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\n        ax = axes[1, 1]\n        methods = ['\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3', '\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0', '\u91cf\u5b50\u5316', '\u84b8\u7559', '\u5168\u3066\u9069\u7528']\n        speedup = [1.0, 1.5, 2.0, 1.8, 4.5]\n\n        bars = ax.bar(methods, speedup, color=['gray', 'blue', 'green', 'orange', 'red'])\n        ax.set_ylabel('\u9ad8\u901f\u5316\u500d\u7387')\n        ax.set_title('\u6700\u9069\u5316\u624b\u6cd5\u306e\u7d44\u307f\u5408\u308f\u305b\u52b9\u679c')\n        ax.axhline(y=1.0, color='k', linestyle='--', alpha=0.5)\n\n        # \u5024\u3092\u30d0\u30fc\u306e\u4e0a\u306b\u8868\u793a\n        for bar, value in zip(bars, speedup):\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                   f'{value:.1f}x', ha='center', va='bottom')\n\n        plt.tight_layout()\n        plt.show()\n\n# \u30c7\u30e2\u5b9f\u884c\nopt = OptimizationTechniques()\nopt.demonstrate_constant_folding()\nopt.demonstrate_dead_code_elimination()\nopt.demonstrate_loop_unrolling()\nopt.demonstrate_common_subexpression_elimination()\nopt.demonstrate_quantization()\nopt.visualize_optimization_impact()\n</code></pre>"},{"location":"part1/similarities/#_12","title":"\u307e\u3068\u3081\uff1a\u7d71\u4e00\u7684\u306a\u7406\u89e3\u3078","text":"<p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3068Transformer\u306e\u6df1\u3044\u985e\u4f3c\u6027\u3092\u63a2\u6c42\u3057\u307e\u3057\u305f\u3002\u4e3b\u8981\u306a\u5bfe\u5fdc\u95a2\u4fc2\u3092\u307e\u3068\u3081\u308b\u3068\uff1a</p> \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406 Transformer/\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u5b57\u53e5\u89e3\u6790\uff08Lexing\uff09 \u30c8\u30fc\u30af\u30f3\u5316\uff08Tokenization\uff09 \u69cb\u6587\u89e3\u6790\uff08Parsing\uff09 \u69cb\u9020\u7406\u89e3\uff08Attention\uff09 \u610f\u5473\u89e3\u6790\uff08Semantic Analysis\uff09 \u6587\u8108\u7406\u89e3\uff08Contextual Understanding\uff09 \u578b\u63a8\u8ad6\uff08Type Inference\uff09 \u610f\u5473\u63a8\u8ad6\uff08Semantic Inference\uff09 \u30b9\u30b3\u30fc\u30d7\u89e3\u6c7a \u6587\u8108\u7a93\uff08Context Window\uff09 \u6700\u9069\u5316\uff08Optimization\uff09 \u30e2\u30c7\u30eb\u5727\u7e2e\u30fb\u9ad8\u901f\u5316 <p>\u3053\u308c\u3089\u306e\u985e\u4f3c\u6027\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u3067\u3001Transformer\u306e\u52d5\u4f5c\u539f\u7406\u304c\u3088\u308a\u76f4\u611f\u7684\u306b\u7406\u89e3\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6982\u5ff5\u3092\u652f\u3048\u308b\u6570\u5b66\u7684\u57fa\u790e\u306b\u3064\u3044\u3066\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u8996\u70b9\u304b\u3089\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part1/similarities/#_13","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u30c8\u30fc\u30af\u30f3\u5316\u306e\u5b9f\u88c5: BPE\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3057\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u9069\u7528\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u901a\u5e38\u306e\u5b57\u53e5\u89e3\u6790\u3068\u306e\u9055\u3044\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>Attention\u53ef\u8996\u5316: \u7c21\u5358\u306aAttention\u6a5f\u69cb\u3092\u5b9f\u88c5\u3057\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u69cb\u6587\uff08\u4f8b\uff1a\u95a2\u6570\u547c\u3073\u51fa\u3057\uff09\u3067\u3069\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\u304c\u73fe\u308c\u308b\u304b\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u578b\u63a8\u8ad6\u3068\u306e\u5bfe\u5fdc: Transformer\u306e\u5404\u5c64\u304c\u51fa\u529b\u3059\u308b\u8868\u73fe\u3092\u300c\u578b\u300d\u3068\u3057\u3066\u89e3\u91c8\u3057\u3001\u5c64\u3092\u7d4c\u308b\u3054\u3068\u306b\u3069\u306e\u3088\u3046\u306b\u300c\u578b\u300d\u304c\u5909\u5316\u3059\u308b\u304b\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u6700\u9069\u5316\u306e\u5b9f\u88c5: \u91cd\u307f\u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u88c5\u3057\u3001\u7cbe\u5ea6\u3068\u901f\u5ea6\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u5b9f\u9a13\u7684\u306b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u30cf\u30a4\u30d6\u30ea\u30c3\u30c9\u30b7\u30b9\u30c6\u30e0: \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u4e21\u65b9\u3092\u6271\u3048\u308b\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u8a2d\u8a08\u30fb\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u3067\u306f\u3001Transformer\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u6570\u5b66\u7684\u57fa\u790e\u3092\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306b\u3068\u3063\u3066\u89aa\u3057\u307f\u3084\u3059\u3044\u5f62\u3067\u89e3\u8aac\u3057\u307e\u3059\u3002\u7dda\u5f62\u4ee3\u6570\u3001\u78ba\u7387\u30fb\u7d71\u8a08\u3001\u5fae\u5206\u306e\u57fa\u790e\u3092\u3001\u5b9f\u88c5\u3092\u901a\u3058\u3066\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002</p>"},{"location":"part1/why-transformer/","title":"\u306a\u305cTransformer\u304c\u91cd\u8981\u306a\u306e\u304b","text":""},{"location":"part1/why-transformer/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u4e16\u754c\u3092\u5909\u3048\u305f\u6280\u8853","text":"<p>2022\u5e7411\u670830\u65e5\u3001OpenAI\u304cChatGPT\u3092\u516c\u958b\u3057\u305f\u65e5\u3092\u899a\u3048\u3066\u3044\u307e\u3059\u304b\uff1f\u305d\u306e\u65e5\u304b\u3089\u3001\u79c1\u305f\u3061\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u4ed5\u4e8b\u306f\u5927\u304d\u304f\u5909\u308f\u308a\u59cb\u3081\u307e\u3057\u305f\u3002\u30b3\u30fc\u30c9\u306e\u81ea\u52d5\u751f\u6210\u3001\u30d0\u30b0\u306e\u691c\u51fa\u3001\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u306e\u63d0\u6848\u3001\u305d\u3057\u3066\u8907\u96d1\u306a\u6280\u8853\u7684\u8cea\u554f\u3078\u306e\u5373\u5ea7\u306e\u56de\u7b54\u3002\u3053\u308c\u3089\u3059\u3079\u3066\u306e\u80cc\u5f8c\u306b\u3042\u308b\u6280\u8853\u304c\u3001Transformer\u3067\u3059\u3002</p> <p>\u3057\u304b\u3057\u3001Transformer\u306f\u5358\u306a\u308b\u300c\u4fbf\u5229\u306a\u30c4\u30fc\u30eb\u300d\u3092\u751f\u307f\u51fa\u3057\u305f\u3060\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u305d\u308c\u306f\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u304c\u300c\u8a00\u8a9e\u300d\u3092\u7406\u89e3\u3059\u308b\u65b9\u6cd5\u306b\u9769\u547d\u3092\u3082\u305f\u3089\u3057\u305f\u3001\u6839\u672c\u7684\u306b\u65b0\u3057\u3044\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306a\u306e\u3067\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u306a\u305cTransformer\u304c\u3053\u308c\u307b\u3069\u307e\u3067\u306b\u91cd\u8981\u306a\u306e\u304b\u3001\u305d\u3057\u3066\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u5b9f\u88c5\u306e\u7d4c\u9a13\u3092\u6301\u3064\u3042\u306a\u305f\u306b\u3068\u3063\u3066\u3001\u306a\u305c\u3053\u306e\u6280\u8853\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u306a\u306e\u304b\u3092\u63a2\u3063\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part1/why-transformer/#11-aichatgpt","title":"1.1 \u73fe\u4ee3AI\u306e\u9769\u547d\uff1aChatGPT\u3068\u305d\u306e\u4ef2\u9593\u305f\u3061","text":""},{"location":"part1/why-transformer/#ai","title":"\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u65e5\u5e38\u3092\u5909\u3048\u305fAI\u30c4\u30fc\u30eb","text":"<p>\u307e\u305a\u3001\u5177\u4f53\u7684\u306a\u4f8b\u304b\u3089\u59cb\u3081\u307e\u3057\u3087\u3046\u3002\u4ee5\u4e0b\u306f\u3001GitHub Copilot\u304c\u5b9f\u969b\u306b\u3069\u306e\u3088\u3046\u306b\u30b3\u30fc\u30c9\u3092\u88dc\u5b8c\u3059\u308b\u304b\u306e\u4f8b\u3067\u3059\uff1a</p> <pre><code># \u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u304c\u66f8\u3044\u305f\u30b3\u30e1\u30f3\u30c8\n# \u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\n\n# Copilot\u304c\u751f\u6210\u3057\u305f\u30b3\u30fc\u30c9\ndef fibonacci(n):\n    \"\"\"\n    n\u756a\u76ee\u306e\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u3092\u8fd4\u3059\n\n    Args:\n        n: \u6c42\u3081\u305f\u3044\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u306e\u4f4d\u7f6e\uff080\u4ee5\u4e0a\u306e\u6574\u6570\uff09\n\n    Returns:\n        n\u756a\u76ee\u306e\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\n    \"\"\"\n    if n &lt;= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        # \u30e1\u30e2\u5316\u3092\u4f7f\u3063\u305f\u52b9\u7387\u7684\u306a\u5b9f\u88c5\n        fib_cache = {0: 0, 1: 1}\n        for i in range(2, n + 1):\n            fib_cache[i] = fib_cache[i-1] + fib_cache[i-2]\n        return fib_cache[n]\n</code></pre> <p>\u9a5a\u304f\u3079\u304d\u3053\u3068\u306b\u3001Copilot\u306f\u5358\u306b\u300c\u30d5\u30a3\u30dc\u30ca\u30c3\u30c1\u6570\u5217\u300d\u3068\u3044\u3046\u8a00\u8449\u304b\u3089\u3001\u4ee5\u4e0b\u3092\u7406\u89e3\u3057\u3066\u3044\u307e\u3059\uff1a</p> <ol> <li>\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u9078\u629e: \u5358\u7d14\u306a\u518d\u5e30\u3067\u306f\u306a\u304f\u3001\u52b9\u7387\u7684\u306a\u52d5\u7684\u8a08\u753b\u6cd5\u3092\u9078\u629e</li> <li>\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0: <code>n &lt;= 0</code> \u306e\u30b1\u30fc\u30b9\u3092\u9069\u5207\u306b\u51e6\u7406</li> <li>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3: \u9069\u5207\u306adocstring\u3092\u751f\u6210</li> <li>\u6700\u9069\u5316: \u30e1\u30e2\u5316\u306b\u3088\u308b\u52b9\u7387\u5316\u3092\u5b9f\u88c5</li> </ol> <p>\u3053\u308c\u306f\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u53ef\u80fd\u306b\u306a\u3063\u305f\u306e\u3067\u3057\u3087\u3046\u304b\uff1f</p>"},{"location":"part1/why-transformer/#transformer_1","title":"Transformer\u304c\u652f\u3048\u308b\u6280\u8853\u30b9\u30bf\u30c3\u30af","text":"<p>\u73fe\u5728\u306eAI\u9769\u547d\u3092\u652f\u3048\u308b\u4e3b\u8981\u306a\u6280\u8853\u3092\u6574\u7406\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <pre><code>graph TD\n    A[Transformer Architecture] --&gt; B[\u8a00\u8a9e\u30e2\u30c7\u30eb]\n    B --&gt; C[GPT-3/4]\n    B --&gt; D[Claude]\n    B --&gt; E[Gemini]\n    B --&gt; F[LLaMA]\n\n    C --&gt; G[ChatGPT]\n    C --&gt; H[GitHub Copilot]\n\n    A --&gt; I[\u753b\u50cf\u751f\u6210\u30e2\u30c7\u30eb]\n    I --&gt; J[DALL-E]\n    I --&gt; K[Stable Diffusion]\n\n    A --&gt; L[\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u30e2\u30c7\u30eb]\n    L --&gt; M[GPT-4V]\n    L --&gt; N[Gemini Vision]</code></pre> <p>\u3053\u308c\u3089\u3059\u3079\u3066\u306e\u30e2\u30c7\u30eb\u306e\u57fa\u76e4\u3068\u306a\u3063\u3066\u3044\u308b\u306e\u304c\u30012017\u5e74\u306b\u767a\u8868\u3055\u308c\u305f\u8ad6\u6587\u300cAttention is All You Need\u300d\u3067\u63d0\u6848\u3055\u308c\u305fTransformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u3059\u3002</p>"},{"location":"part1/why-transformer/#_2","title":"\u5b9f\u969b\u306e\u5f71\u97ff\uff1a\u958b\u767a\u73fe\u5834\u3067\u306e\u5909\u5316","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u73fe\u5834\u3067\u3001Transformer\u30d9\u30fc\u30b9\u306e\u30c4\u30fc\u30eb\u304c\u3082\u305f\u3089\u3057\u305f\u5909\u5316\u3092\u5177\u4f53\u7684\u306b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <pre><code># \u5f93\u6765\u306e\u958b\u767a\u30d5\u30ed\u30fc\ndef traditional_development():\n    \"\"\"\n    1. \u8981\u4ef6\u3092\u7406\u89e3\u3059\u308b\n    2. \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8003\u3048\u308b\n    3. \u30b3\u30fc\u30c9\u3092\u66f8\u304f\n    4. \u30c7\u30d0\u30c3\u30b0\u3059\u308b\n    5. \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u66f8\u304f\n    6. \u30c6\u30b9\u30c8\u3092\u66f8\u304f\n    7. \u30b3\u30fc\u30c9\u30ec\u30d3\u30e5\u30fc\u3092\u53d7\u3051\u308b\n    \"\"\"\n    pass\n\n# AI\u652f\u63f4\u306b\u3088\u308b\u65b0\u3057\u3044\u958b\u767a\u30d5\u30ed\u30fc\ndef ai_assisted_development():\n    \"\"\"\n    1. \u8981\u4ef6\u3092AI\u3068\u4e00\u7dd2\u306b\u6574\u7406\u3059\u308b\n    2. AI\u304c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5019\u88dc\u3092\u63d0\u6848\n    3. AI\u3068\u5354\u529b\u3057\u3066\u30b3\u30fc\u30c9\u3092\u66f8\u304f\n    4. AI\u304c\u30d0\u30b0\u306e\u53ef\u80fd\u6027\u3092\u6307\u6458\n    5. AI\u304c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u4e0b\u66f8\u304d\u3092\u751f\u6210\n    6. AI\u304c\u30c6\u30b9\u30c8\u30b1\u30fc\u30b9\u3092\u63d0\u6848\n    7. AI\u304c\u6f5c\u5728\u7684\u306a\u554f\u984c\u70b9\u3092\u4e8b\u524d\u306b\u6307\u6458\n    \"\"\"\n    # \u5b9f\u969b\u306e\u4f7f\u7528\u4f8b\n    prompt = \"\"\"\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u691c\u8a3c\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    - \u30e1\u30fc\u30eb\u30a2\u30c9\u30ec\u30b9\u306e\u5f62\u5f0f\u3092\u30c1\u30a7\u30c3\u30af\n    - SQL\u30a4\u30f3\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u5bfe\u7b56\n    - XSS\u5bfe\u7b56\n    \"\"\"\n\n    # AI\u304c\u751f\u6210\u3059\u308b\u30b3\u30fc\u30c9\n    import re\n    from html import escape\n\n    def validate_user_input(email: str, comment: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u691c\u8a3c\u3057\u3001\u5b89\u5168\u6027\u3092\u78ba\u4fdd\u3059\u308b\n\n        Returns:\n            (is_valid, error_message)\n        \"\"\"\n        # \u30e1\u30fc\u30eb\u30a2\u30c9\u30ec\u30b9\u306e\u691c\u8a3c\n        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        if not re.match(email_pattern, email):\n            return False, \"\u7121\u52b9\u306a\u30e1\u30fc\u30eb\u30a2\u30c9\u30ec\u30b9\u5f62\u5f0f\u3067\u3059\"\n\n        # SQL\u30a4\u30f3\u30b8\u30a7\u30af\u30b7\u30e7\u30f3\u5bfe\u7b56\uff1a\u5371\u967a\u306a\u6587\u5b57\u306e\u30c1\u30a7\u30c3\u30af\n        dangerous_patterns = [\n            r\"(\\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION|ALTER)\\b)\",\n            r\"(--|#|/\\*|\\*/)\",\n            r\"(;|\\||&amp;&amp;)\"\n        ]\n\n        for pattern in dangerous_patterns:\n            if re.search(pattern, comment, re.IGNORECASE):\n                return False, \"\u4e0d\u6b63\u306a\u6587\u5b57\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\"\n\n        # XSS\u5bfe\u7b56\uff1aHTML\u30a8\u30b9\u30b1\u30fc\u30d7\n        safe_comment = escape(comment)\n\n        return True, \"\"\n</code></pre> <p>\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u30bb\u30ad\u30e5\u30ea\u30c6\u30a3\u306e\u5c02\u9580\u77e5\u8b58\u3092\u542b\u3080\u8907\u96d1\u306a\u8981\u4ef6\u3092\u7406\u89e3\u3057\u3001\u9069\u5207\u306a\u5b9f\u88c5\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"part1/why-transformer/#12-transformer","title":"1.2 \u6b74\u53f2\u7684\u80cc\u666f\uff1a\u306a\u305cTransformer\u304c\u751f\u307e\u308c\u305f\u304b","text":""},{"location":"part1/why-transformer/#rnn","title":"RNN\u306e\u6642\u4ee3\u3068\u305d\u306e\u9650\u754c","text":"<p>Transformer\u304c\u767b\u5834\u3059\u308b\u524d\u3001\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u306e\u4e3b\u6d41\u306fRNN\uff08Recurrent Neural Network\uff09\u3067\u3057\u305f\u3002\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u51e6\u7406\u306b\u4f8b\u3048\u308b\u3068\u3001RNN\u306f\u6b21\u306e\u3088\u3046\u306a\u554f\u984c\u3092\u62b1\u3048\u3066\u3044\u307e\u3057\u305f\uff1a</p> <pre><code># RNN\u306e\u51e6\u7406\u30a4\u30e1\u30fc\u30b8\uff08\u7591\u4f3c\u30b3\u30fc\u30c9\uff09\nclass RNNCompiler:\n    def parse_code(self, source_code):\n        tokens = self.tokenize(source_code)\n        hidden_state = self.initial_state()\n\n        # \u554f\u984c1: \u9010\u6b21\u51e6\u7406\u3057\u304b\u3067\u304d\u306a\u3044\n        for token in tokens:\n            hidden_state = self.process_token(token, hidden_state)\n            # \u524d\u306e\u30c8\u30fc\u30af\u30f3\u306e\u51e6\u7406\u304c\u7d42\u308f\u308b\u307e\u3067\u6b21\u306b\u9032\u3081\u306a\u3044\uff01\n\n        return hidden_state\n\n    def process_token(self, token, prev_state):\n        # \u554f\u984c2: \u9577\u3044\u4f9d\u5b58\u95a2\u4fc2\u3092\u4fdd\u6301\u3067\u304d\u306a\u3044\n        if len(self.context) &gt; 100:\n            # \u53e4\u3044\u60c5\u5831\u304c\u5931\u308f\u308c\u3066\u3044\u304f\uff08\u52fe\u914d\u6d88\u5931\uff09\n            self.context = self.truncate(self.context)\n\n        return self.rnn_cell(token, prev_state)\n</code></pre> <p>\u3053\u308c\u306f\u3001\u9577\u3044\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u89e3\u6790\u3059\u308b\u969b\u306b\u3001\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u521d\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u5909\u6570\u3092\u3001\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u5f8c\u3067\u53c2\u7167\u3067\u304d\u306a\u3044\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002</p>"},{"location":"part1/why-transformer/#rnn_1","title":"RNN\u306e\u554f\u984c\u3092\u56f3\u89e3","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_rnn_problems():\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n    # \u554f\u984c1: \u9010\u6b21\u51e6\u7406\u306e\u53ef\u8996\u5316\n    sequence_length = 10\n    time_steps = range(sequence_length)\n\n    # RNN\u306e\u51e6\u7406\u6642\u9593\uff08\u7d2f\u7a4d\uff09\n    rnn_time = np.cumsum(np.ones(sequence_length))\n    # Transformer\u306e\u51e6\u7406\u6642\u9593\uff08\u4e26\u5217\uff09\n    transformer_time = np.ones(sequence_length)\n\n    ax1.plot(time_steps, rnn_time, 'r-', linewidth=2, label='RNN\uff08\u9010\u6b21\u51e6\u7406\uff09')\n    ax1.plot(time_steps, transformer_time, 'b-', linewidth=2, label='Transformer\uff08\u4e26\u5217\u51e6\u7406\uff09')\n    ax1.set_xlabel('\u30b7\u30fc\u30b1\u30f3\u30b9\u4f4d\u7f6e')\n    ax1.set_ylabel('\u51e6\u7406\u6642\u9593')\n    ax1.set_title('RNN vs Transformer: \u51e6\u7406\u6642\u9593\u306e\u6bd4\u8f03')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # \u554f\u984c2: \u52fe\u914d\u6d88\u5931\u306e\u53ef\u8996\u5316\n    distances = range(1, 21)\n    # \u8ddd\u96e2\u304c\u96e2\u308c\u308b\u307b\u3069\u52fe\u914d\u304c\u5c0f\u3055\u304f\u306a\u308b\n    rnn_gradient = 0.9 ** np.array(distances)\n    # Transformer\u306f\u8ddd\u96e2\u306b\u95a2\u4fc2\u306a\u304f\u4e00\u5b9a\n    transformer_gradient = np.ones(len(distances))\n\n    ax2.semilogy(distances, rnn_gradient, 'r-', linewidth=2, label='RNN\uff08\u52fe\u914d\u6d88\u5931\uff09')\n    ax2.semilogy(distances, transformer_gradient, 'b-', linewidth=2, label='Transformer\uff08\u4e00\u5b9a\uff09')\n    ax2.set_xlabel('\u30c8\u30fc\u30af\u30f3\u9593\u306e\u8ddd\u96e2')\n    ax2.set_ylabel('\u52fe\u914d\u306e\u5f37\u3055\uff08\u5bfe\u6570\u30b9\u30b1\u30fc\u30eb\uff09')\n    ax2.set_title('\u9577\u8ddd\u96e2\u4f9d\u5b58\u95a2\u4fc2\u306e\u5b66\u7fd2\u80fd\u529b')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n# \u30b0\u30e9\u30d5\u3092\u8868\u793a\nvisualize_rnn_problems()\n</code></pre>"},{"location":"part1/why-transformer/#lstmgru","title":"LSTM\u3068GRU\uff1a\u90e8\u5206\u7684\u306a\u89e3\u6c7a\u7b56","text":"<p>RNN\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3088\u3046\u3068\u3057\u3066\u3001LSTM\uff08Long Short-Term Memory\uff09\u3084GRU\uff08Gated Recurrent Unit\uff09\u304c\u958b\u767a\u3055\u308c\u307e\u3057\u305f\uff1a</p> <pre><code># LSTM\u306e\u30b2\u30fc\u30c8\u6a5f\u69cb\uff08\u6982\u5ff5\u7684\u306a\u5b9f\u88c5\uff09\nclass LSTMCell:\n    def __init__(self, input_dim, hidden_dim):\n        # 3\u3064\u306e\u30b2\u30fc\u30c8\uff1a\u5fd8\u5374\u3001\u5165\u529b\u3001\u51fa\u529b\n        self.forget_gate = self.create_gate(input_dim, hidden_dim)\n        self.input_gate = self.create_gate(input_dim, hidden_dim)\n        self.output_gate = self.create_gate(input_dim, hidden_dim)\n\n    def forward(self, x, prev_hidden, prev_cell):\n        # \u5fd8\u5374\u30b2\u30fc\u30c8\uff1a\u4f55\u3092\u5fd8\u308c\u308b\u304b\n        f = sigmoid(self.forget_gate(x, prev_hidden))\n\n        # \u5165\u529b\u30b2\u30fc\u30c8\uff1a\u4f55\u3092\u8a18\u61b6\u3059\u308b\u304b\n        i = sigmoid(self.input_gate(x, prev_hidden))\n        candidate = tanh(self.candidate_gate(x, prev_hidden))\n\n        # \u30bb\u30eb\u72b6\u614b\u306e\u66f4\u65b0\n        cell = f * prev_cell + i * candidate\n\n        # \u51fa\u529b\u30b2\u30fc\u30c8\uff1a\u4f55\u3092\u51fa\u529b\u3059\u308b\u304b\n        o = sigmoid(self.output_gate(x, prev_hidden))\n        hidden = o * tanh(cell)\n\n        return hidden, cell\n</code></pre> <p>\u3057\u304b\u3057\u3001\u3053\u308c\u3089\u3082\u6839\u672c\u7684\u306a\u554f\u984c\u306f\u89e3\u6c7a\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\uff1a - \u4f9d\u7136\u3068\u3057\u3066\u9010\u6b21\u51e6\u7406: \u4e26\u5217\u5316\u3067\u304d\u306a\u3044 - \u8a08\u7b97\u306e\u8907\u96d1\u3055: \u30b2\u30fc\u30c8\u6a5f\u69cb\u306b\u3088\u308b\u8a08\u7b97\u30b3\u30b9\u30c8\u5897\u5927 - \u9577\u8ddd\u96e2\u4f9d\u5b58\u306e\u9650\u754c: \u6539\u5584\u306f\u3055\u308c\u305f\u304c\u3001\u5b8c\u5168\u3067\u306f\u306a\u3044</p>"},{"location":"part1/why-transformer/#attention-is-all-you-need","title":"Attention is All You Need: \u30d1\u30e9\u30c0\u30a4\u30e0\u30b7\u30d5\u30c8","text":"<p>2017\u5e74\u3001Google\u306e\u7814\u7a76\u8005\u305f\u3061\u306f\u9769\u547d\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u3092\u63d0\u6848\u3057\u307e\u3057\u305f\uff1a</p> <p>\u300cRNN\u3092\u5b8c\u5168\u306b\u6392\u9664\u3057\u3001Attention\u6a5f\u69cb\u3060\u3051\u3067\u8a00\u8a9e\u3092\u51e6\u7406\u3067\u304d\u308b\u306e\u3067\u306f\u306a\u3044\u304b\uff1f\u300d</p> <p>\u3053\u308c\u306f\u3001\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u8a2d\u8a08\u306b\u4f8b\u3048\u308b\u3068\u3001\u6b21\u306e\u3088\u3046\u306a\u767a\u60f3\u306e\u8ee2\u63db\u3067\u3057\u305f\uff1a</p> <pre><code># \u5f93\u6765\u306e\u30b3\u30f3\u30d1\u30a4\u30e9\u8a2d\u8a08\nclass TraditionalCompiler:\n    def compile(self, source):\n        # \u6bb5\u968e\u7684\u306b\u51e6\u7406\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\uff09\n        tokens = self.lexer(source)          # \u5b57\u53e5\u89e3\u6790\n        ast = self.parser(tokens)            # \u69cb\u6587\u89e3\u6790\n        typed_ast = self.type_checker(ast)   # \u578b\u691c\u67fb\n        ir = self.ir_generator(typed_ast)    # \u4e2d\u9593\u8868\u73fe\u751f\u6210\n        code = self.code_generator(ir)       # \u30b3\u30fc\u30c9\u751f\u6210\n        return code\n\n# Transformer\u306e\u767a\u60f3\nclass TransformerCompiler:\n    def compile(self, source):\n        # \u3059\u3079\u3066\u306e\u60c5\u5831\u3092\u540c\u6642\u306b\u8003\u616e\n        tokens = self.tokenize(source)\n\n        # \u5404\u30c8\u30fc\u30af\u30f3\u304c\u4ed6\u306e\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u3068\u76f4\u63a5\u5bfe\u8a71\n        attention_matrix = self.compute_attention(tokens, tokens)\n\n        # \u4e26\u5217\u51e6\u7406\u3067\u4e00\u5ea6\u306b\u89e3\u6790\n        result = self.parallel_process(tokens, attention_matrix)\n        return result\n</code></pre>"},{"location":"part1/why-transformer/#transformer_2","title":"Transformer\u306e\u9769\u65b0\u6027","text":"<p>Transformer\u304c\u6301\u3064\u9769\u65b0\u7684\u306a\u7279\u5fb4\u3092\u6574\u7406\u3059\u308b\u3068\uff1a</p> <ol> <li> <p>\u5b8c\u5168\u306a\u4e26\u5217\u51e6\u7406 <pre><code># RNN: O(n) \u306e\u6642\u9593\u8a08\u7b97\u91cf\nfor i in range(len(sequence)):\n    hidden[i] = f(hidden[i-1], input[i])\n\n# Transformer: O(1) \u306e\u6642\u9593\u8a08\u7b97\u91cf\uff08\u4e26\u5217\u5316\u6642\uff09\nattention = compute_all_pairs(sequence, sequence)  # \u5168\u30da\u30a2\u3092\u4e00\u5ea6\u306b\u8a08\u7b97\n</code></pre></p> </li> <li> <p>\u9577\u8ddd\u96e2\u4f9d\u5b58\u306e\u76f4\u63a5\u30e2\u30c7\u30ea\u30f3\u30b0 <pre><code># \u4efb\u610f\u306e\u8ddd\u96e2\u306e\u30c8\u30fc\u30af\u30f3\u9593\u3067\u76f4\u63a5\u60c5\u5831\u4ea4\u63db\ndef attention(query_pos, key_pos):\n    # \u4f4d\u7f6e\u306b\u95a2\u4fc2\u306a\u304f\u3001\u95a2\u9023\u6027\u304c\u3042\u308c\u3070\u9ad8\u3044\u6ce8\u610f\u3092\u6255\u3046\n    return similarity(tokens[query_pos], tokens[key_pos])\n</code></pre></p> </li> <li> <p>\u8a08\u7b97\u52b9\u7387\u3068\u30b9\u30b1\u30fc\u30e9\u30d3\u30ea\u30c6\u30a3 <pre><code># GPU\u3067\u306e\u52b9\u7387\u7684\u306a\u884c\u5217\u6f14\u7b97\nQ = linear_transform(tokens)  # Query\nK = linear_transform(tokens)  # Key\nV = linear_transform(tokens)  # Value\n\n# \u4e00\u5ea6\u306e\u884c\u5217\u6f14\u7b97\u3067\u3059\u3079\u3066\u306e\u6ce8\u610f\u3092\u8a08\u7b97\nattention_scores = matmul(Q, K.T) / sqrt(d_k)\nattention_weights = softmax(attention_scores)\noutput = matmul(attention_weights, V)\n</code></pre></p> </li> </ol>"},{"location":"part1/why-transformer/#13","title":"1.3 \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3078\u306e\u6df1\u3044\u95a2\u9023","text":""},{"location":"part1/why-transformer/#_3","title":"\u30b3\u30fc\u30c9\u7406\u89e3\u30fb\u751f\u6210\u3078\u306e\u5fdc\u7528","text":"<p>Transformer\u304c\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u306b\u9769\u547d\u3092\u3082\u305f\u3089\u3057\u305f\u7406\u7531\u3092\u3001\u5177\u4f53\u4f8b\u3067\u898b\u3066\u307f\u307e\u3057\u3087\u3046\uff1a</p> <pre><code># \u4f8b\uff1aTransformer\u306b\u3088\u308b\u30b3\u30fc\u30c9\u88dc\u5b8c\nclass CodeTransformer:\n    def __init__(self):\n        self.context_window = 2048  # \u30c8\u30fc\u30af\u30f3\u6570\n\n    def analyze_code_context(self, partial_code):\n        \"\"\"\n        \u30b3\u30fc\u30c9\u306e\u6587\u8108\u3092\u7406\u89e3\u3057\u3001\u6b21\u306b\u6765\u308b\u3079\u304d\u30b3\u30fc\u30c9\u3092\u4e88\u6e2c\n        \"\"\"\n        # 1. \u5909\u6570\u306e\u30b9\u30b3\u30fc\u30d7\u3092\u7406\u89e3\n        variables = self.extract_variables(partial_code)\n\n        # 2. \u95a2\u6570\u306e\u30b7\u30b0\u30cd\u30c1\u30e3\u3092\u7406\u89e3\n        functions = self.extract_functions(partial_code)\n\n        # 3. \u30a4\u30f3\u30dd\u30fc\u30c8\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u7406\u89e3\n        imports = self.extract_imports(partial_code)\n\n        # 4. \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb\u3092\u5b66\u7fd2\n        style = self.analyze_style(partial_code)\n\n        return self.predict_next_tokens(\n            partial_code, \n            context={\n                'variables': variables,\n                'functions': functions,\n                'imports': imports,\n                'style': style\n            }\n        )\n\n# \u5b9f\u969b\u306e\u4f7f\u7528\u4f8b\ncode_so_far = '''\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_training_history(history):\n    \"\"\"\n    \u8a13\u7df4\u5c65\u6b74\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u95a2\u6570\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # \u640d\u5931\u306e\u30d7\u30ed\u30c3\u30c8\n    ax1.plot(history['train_loss'], label='Train')\n    ax1.plot(history['val_loss'], label='Validation')\n    ax1.set_title('Model Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n\n    # \u3053\u3053\u3067\u7cbe\u5ea6\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u305f\u3044\n'''\n\n# Transformer\u304c\u751f\u6210\u3059\u308b\u30b3\u30fc\u30c9\nsuggested_code = '''\n    # \u7cbe\u5ea6\u306e\u30d7\u30ed\u30c3\u30c8\n    ax2.plot(history['train_acc'], label='Train')\n    ax2.plot(history['val_acc'], label='Validation')\n    ax2.set_title('Model Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n'''\n</code></pre> <p>Transformer\u306f\u4ee5\u4e0b\u3092\u7406\u89e3\u3057\u3066\u3044\u307e\u3059\uff1a - <code>ax1</code>\u306e\u6b21\u306f<code>ax2</code>\u3092\u4f7f\u3046\u3079\u304d - \u640d\u5931\uff08loss\uff09\u306e\u6b21\u306f\u7cbe\u5ea6\uff08accuracy\uff09\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u6163\u7fd2 - <code>plt.tight_layout()</code>\u3068<code>plt.show()</code>\u3067\u7de0\u3081\u304f\u304f\u308b - \u4e00\u8cab\u3057\u305f\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb\u3092\u7dad\u6301</p>"},{"location":"part1/why-transformer/#_4","title":"\u30d7\u30ed\u30b0\u30e9\u30e0\u89e3\u6790\u3078\u306e\u5fdc\u7528","text":"<pre><code># \u30d0\u30b0\u691c\u51fa\u306e\u4f8b\ndef transformer_bug_detector(code):\n    \"\"\"\n    Transformer\u3092\u4f7f\u3063\u305f\u30d0\u30b0\u691c\u51fa\n    \"\"\"\n    # \u6f5c\u5728\u7684\u306a\u30d0\u30b0\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u6e08\u307f\n    bug_patterns = {\n        'null_pointer': '\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304cNone\u306e\u53ef\u80fd\u6027',\n        'index_error': '\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u7bc4\u56f2\u5916\u306e\u53ef\u80fd\u6027',\n        'type_error': '\u578b\u306e\u4e0d\u4e00\u81f4',\n        'resource_leak': '\u30ea\u30bd\u30fc\u30b9\u306e\u89e3\u653e\u5fd8\u308c',\n        'race_condition': '\u4e26\u884c\u51e6\u7406\u306e\u7af6\u5408\u72b6\u614b'\n    }\n\n    # \u5b9f\u969b\u306e\u30b3\u30fc\u30c9\u4f8b\n    suspicious_code = '''\n    def process_data(data_list):\n        result = []\n        for i in range(len(data_list) + 1):  # \u30d0\u30b0: \u7bc4\u56f2\u304c1\u3064\u591a\u3044\n            item = data_list[i]\n            if item &gt; 0:\n                result.append(item * 2)\n        return result\n    '''\n\n    # Transformer\u306e\u5206\u6790\u7d50\u679c\n    detected_issues = [\n        {\n            'line': 3,\n            'type': 'index_error',\n            'message': '\u30eb\u30fc\u30d7\u306e\u7bc4\u56f2\u304c\u914d\u5217\u306e\u30b5\u30a4\u30ba\u3092\u8d85\u3048\u3066\u3044\u307e\u3059\u3002`len(data_list)`\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002',\n            'severity': 'high',\n            'fix': 'for i in range(len(data_list)):'\n        }\n    ]\n\n    return detected_issues\n</code></pre>"},{"location":"part1/why-transformer/#_5","title":"\u30b3\u30f3\u30d1\u30a4\u30e9\u6280\u8853\u3068\u306e\u63a5\u70b9","text":"<p>Transformer\u306e\u6982\u5ff5\u306f\u3001\u5b9f\u306f\u30b3\u30f3\u30d1\u30a4\u30e9\u6280\u8853\u3068\u591a\u304f\u306e\u5171\u901a\u70b9\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code># \u30b7\u30f3\u30dc\u30eb\u30c6\u30fc\u30d6\u30eb\u3068Attention\u6a5f\u69cb\u306e\u985e\u4f3c\u6027\nclass SymbolTableVsAttention:\n    def compiler_symbol_resolution(self):\n        \"\"\"\n        \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u30b7\u30f3\u30dc\u30eb\u89e3\u6c7a\n        \"\"\"\n        symbol_table = {\n            'global': {'MAX_SIZE': 100, 'DEBUG': True},\n            'function_foo': {'x': 'int', 'y': 'string'},\n            'function_bar': {'z': 'float'}\n        }\n\n        # \u5909\u6570\u53c2\u7167\u306e\u89e3\u6c7a\n        def resolve(name, scope):\n            # \u5185\u5074\u306e\u30b9\u30b3\u30fc\u30d7\u304b\u3089\u5916\u5074\u3078\u63a2\u7d22\n            if name in symbol_table[scope]:\n                return symbol_table[scope][name]\n            elif name in symbol_table['global']:\n                return symbol_table['global'][name]\n            else:\n                raise NameError(f\"'{name}' is not defined\")\n\n    def transformer_attention(self):\n        \"\"\"\n        Transformer\u306eAttention\u6a5f\u69cb\n        \"\"\"\n        # \u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u304c\u4ed6\u306e\u3059\u3079\u3066\u306e\u30c8\u30fc\u30af\u30f3\u3092\u300c\u898b\u308b\u300d\n        def attention_mechanism(query_token, all_tokens):\n            scores = []\n            for key_token in all_tokens:\n                # \u30c8\u30fc\u30af\u30f3\u9593\u306e\u95a2\u9023\u6027\u3092\u8a08\u7b97\n                score = self.compute_relevance(query_token, key_token)\n                scores.append(score)\n\n            # \u95a2\u9023\u6027\u306e\u9ad8\u3044\u30c8\u30fc\u30af\u30f3\u306b\u6ce8\u76ee\n            attention_weights = softmax(scores)\n\n            # \u91cd\u307f\u4ed8\u304d\u548c\u3067\u60c5\u5831\u3092\u96c6\u7d04\n            context = sum(weight * token for weight, token in zip(attention_weights, all_tokens))\n            return context\n</code></pre> <p>\u4e21\u8005\u306e\u985e\u4f3c\u70b9\uff1a 1. \u6587\u8108\u4f9d\u5b58\u306e\u89e3\u6c7a: \u5909\u6570\u306e\u30b9\u30b3\u30fc\u30d7 vs \u30c8\u30fc\u30af\u30f3\u306e\u6587\u8108 2. \u9577\u8ddd\u96e2\u306e\u53c2\u7167: \u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570 vs \u96e2\u308c\u305f\u30c8\u30fc\u30af\u30f3 3. \u968e\u5c64\u7684\u306a\u51e6\u7406: \u30cd\u30b9\u30c8\u3057\u305f\u30b9\u30b3\u30fc\u30d7 vs Multi-head Attention</p>"},{"location":"part1/why-transformer/#_6","title":"\u6700\u9069\u5316\u6280\u8853\u306e\u5171\u901a\u6027","text":"<pre><code># \u30b3\u30f3\u30d1\u30a4\u30e9\u6700\u9069\u5316\u3068\u30e2\u30c7\u30eb\u6700\u9069\u5316\u306e\u5bfe\u6bd4\nclass OptimizationParallels:\n    def compiler_optimization(self):\n        \"\"\"\n        \u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u6280\u8853\n        \"\"\"\n        # \u5b9a\u6570\u7573\u307f\u8fbc\u307f\n        # Before: x = 2 * 3 * 4\n        # After:  x = 24\n\n        # \u30eb\u30fc\u30d7\u5c55\u958b\n        # Before: for i in range(4): sum += arr[i]\n        # After:  sum += arr[0] + arr[1] + arr[2] + arr[3]\n\n        # \u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u9664\u53bb\n        # Before: if False: expensive_function()\n        # After:  # removed\n\n    def transformer_optimization(self):\n        \"\"\"\n        Transformer\u306e\u6700\u9069\u5316\u6280\u8853\n        \"\"\"\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u884c\u5217\u306e\u52b9\u7387\u5316\n        # Flash Attention: O(n\u00b2) \u2192 O(n)\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n\n        # \u91cf\u5b50\u5316\n        # 32bit\u6d6e\u52d5\u5c0f\u6570\u70b9 \u2192 8bit\u6574\u6570\n\n        # \u30d7\u30eb\u30fc\u30cb\u30f3\u30b0\n        # \u91cd\u8981\u3067\u306a\u3044\u63a5\u7d9a\u3092\u524a\u9664\n\n        # \u77e5\u8b58\u84b8\u7559\n        # \u5927\u304d\u306a\u30e2\u30c7\u30eb\u304b\u3089\u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3078\u77e5\u8b58\u3092\u8ee2\u79fb\n</code></pre>"},{"location":"part1/why-transformer/#14","title":"1.4 \u672c\u66f8\u3067\u5b66\u3076\u3053\u3068\uff1a\u7406\u8ad6\u3068\u5b9f\u88c5\u306e\u878d\u5408","text":""},{"location":"part1/why-transformer/#_7","title":"\u5b66\u7fd2\u306e\u30ed\u30fc\u30c9\u30de\u30c3\u30d7","text":"<p>\u672c\u66f8\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u9806\u5e8f\u3067Transformer\u3092\u5b8c\u5168\u306b\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3059\uff1a</p> <pre><code>class TransformerLearningPath:\n    def __init__(self):\n        self.chapters = {\n            \"\u57fa\u790e\": {\n                \"\u6570\u5b66\": [\"\u7dda\u5f62\u4ee3\u6570\", \"\u78ba\u7387\u7d71\u8a08\", \"\u5fae\u5206\"],\n                \"PyTorch\": [\"\u30c6\u30f3\u30bd\u30eb\", \"\u81ea\u52d5\u5fae\u5206\", \"NN\u69cb\u7bc9\"]\n            },\n            \"\u30b3\u30a2\u6982\u5ff5\": {\n                \"Attention\": [\"\u5185\u7a4d\u6ce8\u610f\", \"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\", \"\u30de\u30b9\u30ad\u30f3\u30b0\"],\n                \"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\": [\"\u7d76\u5bfe\u4f4d\u7f6e\", \"\u76f8\u5bfe\u4f4d\u7f6e\", \"RoPE\"],\n                \"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\": [\"\u30a8\u30f3\u30b3\u30fc\u30c0\", \"\u30c7\u30b3\u30fc\u30c0\", \"\u63a5\u7d9a\"]\n            },\n            \"\u5b9f\u88c5\": {\n                \"\u30b9\u30af\u30e9\u30c3\u30c1\u5b9f\u88c5\": [\"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\", \"\u5168\u4f53\u7d71\u5408\"],\n                \"\u6700\u9069\u5316\": [\"\u30e1\u30e2\u30ea\u52b9\u7387\", \"\u8a08\u7b97\u52b9\u7387\", \"\u4e26\u5217\u5316\"],\n                \"\u5fdc\u7528\": [\"\u8a00\u8a9e\u30e2\u30c7\u30eb\", \"\u7ffb\u8a33\", \"\u30b3\u30fc\u30c9\u751f\u6210\"]\n            }\n        }\n\n    def learning_progression(self):\n        \"\"\"\n        \u6bb5\u968e\u7684\u306a\u5b66\u7fd2\u306e\u9032\u884c\n        \"\"\"\n        # Phase 1: \u7406\u8ad6\u306e\u7406\u89e3\n        theory = self.understand_mathematics()\n\n        # Phase 2: \u5c0f\u3055\u306a\u5b9f\u88c5\n        components = self.implement_components()\n\n        # Phase 3: \u7d71\u5408\n        full_model = self.integrate_components(components)\n\n        # Phase 4: \u5fdc\u7528\n        applications = self.build_applications(full_model)\n\n        return applications\n</code></pre>"},{"location":"part1/why-transformer/#_8","title":"\u5b9f\u88c5\u4e2d\u5fc3\u306e\u30a2\u30d7\u30ed\u30fc\u30c1","text":"<p>\u672c\u66f8\u306e\u7279\u5fb4\u306f\u3001\u3059\u3079\u3066\u306e\u6982\u5ff5\u3092\u5b9f\u969b\u306b\u52d5\u304f\u30b3\u30fc\u30c9\u3067\u8aac\u660e\u3059\u308b\u3053\u3068\u3067\u3059\uff1a</p> <pre><code># \u4f8b\uff1aAttention\u6a5f\u69cb\u306e\u6bb5\u968e\u7684\u5b9f\u88c5\nclass StepByStepAttention:\n    def step1_basic_dot_product(self):\n        \"\"\"\n        \u30b9\u30c6\u30c3\u30d71: \u57fa\u672c\u7684\u306a\u5185\u7a4d\u6ce8\u610f\n        \"\"\"\n        def dot_product_attention(query, key, value):\n            # \u6700\u3082\u30b7\u30f3\u30d7\u30eb\u306a\u5f62\n            score = torch.dot(query, key)\n            weight = torch.sigmoid(score)  # 0-1\u306e\u7bc4\u56f2\u306b\n            output = weight * value\n            return output\n\n    def step2_scaled_attention(self):\n        \"\"\"\n        \u30b9\u30c6\u30c3\u30d72: \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3092\u8ffd\u52a0\n        \"\"\"\n        def scaled_dot_product_attention(query, key, value):\n            d_k = key.size(-1)\n            score = torch.dot(query, key) / math.sqrt(d_k)\n            weight = torch.softmax(score, dim=-1)\n            output = weight * value\n            return output\n\n    def step3_matrix_attention(self):\n        \"\"\"\n        \u30b9\u30c6\u30c3\u30d73: \u884c\u5217\u6f14\u7b97\u306b\u62e1\u5f35\n        \"\"\"\n        def matrix_attention(Q, K, V):\n            d_k = K.size(-1)\n            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n            weights = torch.softmax(scores, dim=-1)\n            output = torch.matmul(weights, V)\n            return output, weights\n\n    def step4_multi_head(self):\n        \"\"\"\n        \u30b9\u30c6\u30c3\u30d74: \u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u5316\n        \"\"\"\n        class MultiHeadAttention(nn.Module):\n            def __init__(self, d_model, n_heads):\n                super().__init__()\n                self.d_model = d_model\n                self.n_heads = n_heads\n                self.d_k = d_model // n_heads\n\n                self.W_q = nn.Linear(d_model, d_model)\n                self.W_k = nn.Linear(d_model, d_model)\n                self.W_v = nn.Linear(d_model, d_model)\n                self.W_o = nn.Linear(d_model, d_model)\n\n            def forward(self, query, key, value, mask=None):\n                batch_size = query.size(0)\n\n                # \u7dda\u5f62\u5909\u63db\u3068\u5f62\u72b6\u5909\u66f4\n                Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n                K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n                V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n                # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                weights = torch.softmax(scores, dim=-1)\n                context = torch.matmul(weights, V)\n\n                # \u5f62\u72b6\u3092\u623b\u3059\n                context = context.transpose(1, 2).contiguous().view(\n                    batch_size, -1, self.d_model\n                )\n\n                output = self.W_o(context)\n                return output, weights\n</code></pre>"},{"location":"part1/why-transformer/#_9","title":"\u30c7\u30d0\u30c3\u30b0\u3068\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3","text":"<p>\u7406\u89e3\u3092\u6df1\u3081\u308b\u305f\u3081\u3001\u3059\u3079\u3066\u306e\u6982\u5ff5\u3092\u53ef\u8996\u5316\u3057\u307e\u3059\uff1a</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass AttentionVisualizer:\n    def visualize_attention_weights(self, text, attention_weights):\n        \"\"\"\n        Attention\u91cd\u307f\u306e\u53ef\u8996\u5316\n        \"\"\"\n        tokens = text.split()\n\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(\n            attention_weights,\n            xticklabels=tokens,\n            yticklabels=tokens,\n            cmap='Blues',\n            cbar_kws={'label': 'Attention Weight'}\n        )\n        plt.title('Attention Weights Visualization')\n        plt.xlabel('Keys (attended to)')\n        plt.ylabel('Queries (attending from)')\n\n        # \u7279\u5b9a\u306e\u30c8\u30fc\u30af\u30f3\u304c\u3069\u3053\u306b\u6ce8\u76ee\u3057\u3066\u3044\u308b\u304b\u30cf\u30a4\u30e9\u30a4\u30c8\n        for i in range(len(tokens)):\n            max_attention_idx = attention_weights[i].argmax()\n            plt.plot(max_attention_idx + 0.5, i + 0.5, 'r*', markersize=15)\n\n        plt.tight_layout()\n        plt.show()\n\n    def visualize_positional_encoding(self):\n        \"\"\"\n        \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u53ef\u8996\u5316\n        \"\"\"\n        d_model = 128\n        max_len = 100\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        plt.figure(figsize=(12, 8))\n        plt.imshow(pe.numpy(), aspect='auto', cmap='RdBu')\n        plt.colorbar(label='Value')\n        plt.xlabel('Dimension')\n        plt.ylabel('Position')\n        plt.title('Positional Encoding Visualization')\n        plt.show()\n</code></pre>"},{"location":"part1/why-transformer/#_10","title":"\u5fdc\u7528\u529b\u306e\u7372\u5f97","text":"<p>\u6700\u7d42\u7684\u306b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u5b9f\u7528\u7684\u306a\u30b9\u30ad\u30eb\u304c\u8eab\u306b\u3064\u304d\u307e\u3059\uff1a</p> <ol> <li> <p>\u72ec\u81ea\u30bf\u30b9\u30af\u3078\u306e\u9069\u7528 <pre><code># \u30b3\u30fc\u30c9\u8981\u7d04\u30bf\u30b9\u30af\u306e\u5b9f\u88c5\nclass CodeSummarizer(nn.Module):\n    def __init__(self, vocab_size, d_model=512):\n        super().__init__()\n        self.encoder = TransformerEncoder(vocab_size, d_model)\n        self.decoder = TransformerDecoder(vocab_size, d_model)\n\n    def forward(self, code_tokens, summary_tokens=None):\n        # \u30b3\u30fc\u30c9\u3092\u7406\u89e3\n        code_representation = self.encoder(code_tokens)\n\n        # \u8981\u7d04\u3092\u751f\u6210\n        if summary_tokens is None:\n            # \u63a8\u8ad6\u6642\uff1a\u8981\u7d04\u3092\u81ea\u5df1\u56de\u5e30\u7684\u306b\u751f\u6210\n            summary = self.generate_summary(code_representation)\n        else:\n            # \u8a13\u7df4\u6642\uff1a\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\n            summary = self.decoder(summary_tokens, code_representation)\n\n        return summary\n</code></pre></p> </li> <li> <p>\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u8df5 <pre><code># \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u7279\u5b9a\u30bf\u30b9\u30af\u3078\u306e\u9069\u5fdc\ndef finetune_for_bug_detection(pretrained_model, bug_dataset):\n    # \u6700\u7d42\u5c64\u3092\u7f6e\u304d\u63db\u3048\n    pretrained_model.classifier = nn.Linear(\n        pretrained_model.d_model, \n        num_bug_types\n    )\n\n    # \u5dee\u5206\u5b66\u7fd2\u7387\n    optimizer = torch.optim.AdamW([\n        {'params': pretrained_model.transformer.parameters(), 'lr': 1e-5},\n        {'params': pretrained_model.classifier.parameters(), 'lr': 1e-3}\n    ])\n\n    # \u8a13\u7df4\u30eb\u30fc\u30d7\n    for epoch in range(num_epochs):\n        for code, bug_label in bug_dataset:\n            loss = train_step(pretrained_model, code, bug_label)\n            optimizer.step()\n</code></pre></p> </li> <li> <p>\u30d7\u30ed\u30c0\u30af\u30b7\u30e7\u30f3\u74b0\u5883\u3067\u306e\u904b\u7528 <pre><code># \u63a8\u8ad6\u306e\u6700\u9069\u5316\nclass OptimizedInference:\n    def __init__(self, model_path):\n        self.model = torch.jit.load(model_path)\n        self.model.eval()\n\n        # KV\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u521d\u671f\u5316\n        self.kv_cache = {}\n\n    @torch.no_grad()\n    def generate(self, prompt, max_length=100):\n        # \u30d0\u30c3\u30c1\u63a8\u8ad6\u306e\u6e96\u5099\n        input_ids = self.tokenizer.encode(prompt)\n\n        for _ in range(max_length):\n            # KV\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u6d3b\u7528\u3057\u305f\u9ad8\u901f\u63a8\u8ad6\n            logits = self.model(input_ids, past_kv=self.kv_cache)\n\n            # \u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\n            next_token = self.sample(logits[-1])\n            input_ids.append(next_token)\n\n            if next_token == self.eos_token_id:\n                break\n\n        return self.tokenizer.decode(input_ids)\n</code></pre></p> </li> </ol>"},{"location":"part1/why-transformer/#transformer_3","title":"\u307e\u3068\u3081\uff1a\u306a\u305c\u4eca\u3001Transformer\u3092\u5b66\u3076\u3079\u304d\u304b","text":"<p>Transformer\u306f\u5358\u306a\u308b\u300c\u6d41\u884c\u308a\u306e\u6280\u8853\u300d\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u305d\u308c\u306f\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u304c\u8a00\u8a9e\u3092\u7406\u89e3\u3059\u308b\u65b9\u6cd5\u306e\u6839\u672c\u7684\u306a\u5909\u9769\u3067\u3042\u308a\u3001\u4eca\u5f8c\u306e\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u958b\u767a\u306b\u4e0d\u53ef\u6b20\u306a\u6280\u8853\u3067\u3059\u3002</p> <p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u5b9f\u88c5\u306e\u7d4c\u9a13\u3092\u6301\u3064\u3042\u306a\u305f\u306b\u306f\u3001Transformer\u3092\u6df1\u304f\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u7d20\u6674\u3089\u3057\u3044\u57fa\u790e\u304c\u3042\u308a\u307e\u3059\uff1a</p> <ul> <li>\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58: \u69cb\u6587\u89e3\u6790\u306e\u7d4c\u9a13\u306f\u3001Attention\u30e1\u30ab\u30cb\u30ba\u30e0\u306e\u7406\u89e3\u306b\u76f4\u7d50</li> <li>\u6700\u9069\u5316\u601d\u8003: \u30b3\u30f3\u30d1\u30a4\u30e9\u6700\u9069\u5316\u306e\u77e5\u8b58\u306f\u3001\u30e2\u30c7\u30eb\u6700\u9069\u5316\u306b\u5fdc\u7528\u53ef\u80fd</li> <li>\u30b7\u30b9\u30c6\u30e0\u8a2d\u8a08: \u5927\u898f\u6a21\u30b7\u30b9\u30c6\u30e0\u306e\u8a2d\u8a08\u7d4c\u9a13\u306f\u3001\u5927\u898f\u6a21\u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\u306b\u6d3b\u7528\u53ef\u80fd</li> </ul> <p>\u6b21\u306e\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u77e5\u8b58\u3092\u6d3b\u304b\u3057\u306a\u304c\u3089\u3001Transformer\u306e\u57fa\u790e\u3068\u306a\u308b\u6570\u5b66\u7684\u6982\u5ff5\u3092\u3001\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u306e\u8996\u70b9\u304b\u3089\u89e3\u8aac\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part1/why-transformer/#_11","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u6982\u5ff5\u7406\u89e3: RNN\u3068Transformer\u306e\u4e26\u5217\u51e6\u7406\u80fd\u529b\u306e\u9055\u3044\u3092\u3001\u5177\u4f53\u7684\u306a\u30b3\u30fc\u30c9\u4f8b\u3067\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5b9f\u88c5\u8ab2\u984c: \u7c21\u5358\u306aAttention\u6a5f\u69cb\u3092\u5b9f\u88c5\u3057\u3001\u5165\u529b\u6587\u306e\u5404\u5358\u8a9e\u304c\u3069\u306e\u5358\u8a9e\u306b\u6ce8\u76ee\u3057\u3066\u3044\u308b\u304b\u3092\u53ef\u8996\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5fdc\u7528\u601d\u8003: \u3042\u306a\u305f\u304c\u666e\u6bb5\u4f7f\u3063\u3066\u3044\u308b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30b3\u30f3\u30d1\u30a4\u30e9\u306b\u3001Transformer\u3092\u7d44\u307f\u8fbc\u3080\u3068\u3057\u305f\u3089\u3001\u3069\u306e\u90e8\u5206\u306b\u9069\u7528\u3059\u308b\u3068\u52b9\u679c\u7684\u3067\u3057\u3087\u3046\u304b\uff1f</p> </li> <li> <p>\u8abf\u67fb\u8ab2\u984c: \u6700\u65b0\u306e\u30b3\u30fc\u30c9\u751f\u6210AI\uff08GitHub Copilot\u3001CodeWhisperer\u7b49\uff09\u3092\u5b9f\u969b\u306b\u4f7f\u7528\u3057\u3001\u305d\u306e\u9577\u6240\u3068\u77ed\u6240\u3092\u307e\u3068\u3081\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol>"},{"location":"part1/why-transformer/#_12","title":"\u53c2\u8003\u6587\u732e","text":"<ol> <li>Vaswani, A., et al. (2017). \"Attention is All You Need\"</li> <li>Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers\"</li> <li>Brown, T., et al. (2020). \"Language Models are Few-Shot Learners\"</li> <li>Chen, M., et al. (2021). \"Evaluating Large Language Models Trained on Code\"</li> </ol> <p>\u6b21\u7ae0\u3067\u306f\u3001\u300c\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u3068\u306e\u985e\u4f3c\u70b9\u300d\u3092\u3055\u3089\u306b\u6df1\u304f\u6398\u308a\u4e0b\u3052\u3001\u30b3\u30f3\u30d1\u30a4\u30e9\u6280\u8853\u8005\u306e\u8996\u70b9\u304b\u3089Transformer\u3092\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part2/attention-intuition/","title":"\u6ce8\u610f\u6a5f\u69cb\u306e\u76f4\u611f\u7684\u7406\u89e3","text":""},{"location":"part2/attention-intuition/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u300c\u6ce8\u610f\u300d\u3068\u306f\u4f55\u304b","text":"<p>\u4eba\u9593\u304c\u6587\u7ae0\u3092\u8aad\u3080\u3068\u304d\u3001\u3059\u3079\u3066\u306e\u5358\u8a9e\u306b\u540c\u3058\u6ce8\u610f\u3092\u6255\u3046\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u91cd\u8981\u306a\u5358\u8a9e\u306b\u7126\u70b9\u3092\u5f53\u3066\u3001\u6587\u8108\u306b\u5fdc\u3058\u3066\u95a2\u9023\u3059\u308b\u90e8\u5206\u3092\u53c2\u7167\u3057\u306a\u304c\u3089\u7406\u89e3\u3092\u6df1\u3081\u3066\u3044\u304d\u307e\u3059\u3002</p> <p>\u30b3\u30f3\u30d1\u30a4\u30e9\u3067\u3082\u4f3c\u305f\u3053\u3068\u304c\u8d77\u304d\u3066\u3044\u307e\u3059\u3002\u5909\u6570\u306e\u53c2\u7167\u3092\u89e3\u6c7a\u3059\u308b\u3068\u304d\u3001\u30b9\u30b3\u30fc\u30d7\u5185\u306e\u3059\u3079\u3066\u306e\u5ba3\u8a00\u3092\u8abf\u3079\u307e\u3059\u304c\u3001\u540d\u524d\u304c\u4e00\u81f4\u3059\u308b\u3082\u306e\u306b\u300c\u6ce8\u76ee\u300d\u3057\u307e\u3059\u3002\u578b\u63a8\u8ad6\u3067\u306f\u3001\u95a2\u9023\u3059\u308b\u5236\u7d04\u306b\u300c\u6ce8\u610f\u300d\u3092\u5411\u3051\u3066\u3001\u77db\u76fe\u306e\u306a\u3044\u578b\u3092\u5c0e\u304d\u51fa\u3057\u307e\u3059\u3002</p> <p>Transformer\u306e\u300cAttention\uff08\u6ce8\u610f\u6a5f\u69cb\uff09\u300d\u306f\u3001\u3053\u306e\u4eba\u9593\u7684\u306a\u300c\u6ce8\u610f\u300d\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u6570\u5b66\u7684\u306b\u30e2\u30c7\u30eb\u5316\u3057\u305f\u3082\u306e\u3067\u3059\u3002\u9a5a\u304f\u3079\u304d\u3053\u3068\u306b\u3001\u3053\u306e\u5358\u7d14\u306a\u30a2\u30a4\u30c7\u30a2\u304c\u3001\u73fe\u4ee3\u306eAI\u9769\u547d\u306e\u4e2d\u6838\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"part2/attention-intuition/#61-rnn","title":"6.1 \u6ce8\u610f\u6a5f\u69cb\u306e\u5fc5\u8981\u6027\uff1a\u306a\u305cRNN\u3067\u306f\u4e0d\u5341\u5206\u3060\u3063\u305f\u306e\u304b","text":""},{"location":"part2/attention-intuition/#_3","title":"\u9577\u8ddd\u96e2\u4f9d\u5b58\u306e\u554f\u984c\u3092\u53ef\u8996\u5316","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional\nimport math\nfrom IPython.display import HTML\nimport matplotlib.animation as animation\n\nclass AttentionMotivation:\n    \"\"\"\u6ce8\u610f\u6a5f\u69cb\u304c\u5fc5\u8981\u306a\u7406\u7531\u3092\u5b9f\u4f8b\u3067\u793a\u3059\"\"\"\n\n    def __init__(self):\n        self.examples = self._create_examples()\n\n    def _create_examples(self) -&gt; List[Dict[str, str]]:\n        \"\"\"\u9577\u8ddd\u96e2\u4f9d\u5b58\u306e\u4f8b\u3092\u4f5c\u6210\"\"\"\n        return [\n            {\n                \"text\": \"The animal didn't cross the street because it was too tired.\",\n                \"question\": \"What does 'it' refer to?\",\n                \"answer\": \"animal\",\n                \"distance\": 7  # \"it\"\u304b\u3089\"animal\"\u307e\u3067\u306e\u8ddd\u96e2\n            },\n            {\n                \"text\": \"The animal didn't cross the street because it was too wide.\",\n                \"question\": \"What does 'it' refer to?\",\n                \"answer\": \"street\",\n                \"distance\": 4  # \"it\"\u304b\u3089\"street\"\u307e\u3067\u306e\u8ddd\u96e2\n            },\n            {\n                \"text\": \"In 1969, Neil Armstrong became the first person to walk on the moon, which was a giant leap for mankind.\",\n                \"question\": \"What was a giant leap?\",\n                \"answer\": \"walk on the moon\",\n                \"distance\": 12\n            }\n        ]\n\n    def visualize_dependency_problem(self):\n        \"\"\"\u4f9d\u5b58\u95a2\u4fc2\u306e\u554f\u984c\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n        # \u4f8b1: RNN\u306e\u60c5\u5831\u4f1d\u64ad\n        ax = axes[0]\n        self._plot_rnn_propagation(ax)\n\n        # \u4f8b2: \u5fc5\u8981\u306a\u63a5\u7d9a\n        ax = axes[1]\n        self._plot_ideal_connections(ax)\n\n        plt.tight_layout()\n        plt.show()\n\n    def _plot_rnn_propagation(self, ax):\n        \"\"\"RNN\u3067\u306e\u60c5\u5831\u4f1d\u64ad\u3092\u56f3\u793a\"\"\"\n        words = [\"The\", \"animal\", \"didn't\", \"cross\", \"the\", \"street\", \"because\", \"it\", \"was\", \"too\", \"tired\"]\n        positions = list(range(len(words)))\n\n        # RNN\u306e\u9010\u6b21\u7684\u306a\u63a5\u7d9a\n        for i in range(len(words) - 1):\n            ax.arrow(i, 0, 0.8, 0, head_width=0.1, head_length=0.1, \n                    fc='blue', ec='blue', alpha=0.5)\n\n        # \u5358\u8a9e\u3092\u8868\u793a\n        for i, word in enumerate(words):\n            ax.text(i, -0.5, word, ha='center', va='top', fontsize=10, \n                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n\n        # \"it\"\u304b\u3089\"animal\"\u3078\u306e\u5fc5\u8981\u306a\u63a5\u7d9a\uff08\u8584\u304f\u8868\u793a\uff09\n        ax.arrow(7, 0.2, -5.5, 0, head_width=0.1, head_length=0.1,\n                fc='red', ec='red', alpha=0.3, linestyle='--')\n        ax.text(4.5, 0.4, \"7 steps\", ha='center', color='red')\n\n        ax.set_xlim(-0.5, len(words) - 0.5)\n        ax.set_ylim(-1, 1)\n        ax.set_title(\"RNN: \u60c5\u5831\u306f\u9010\u6b21\u7684\u306b\u3057\u304b\u4f1d\u64ad\u3057\u306a\u3044\", fontsize=14)\n        ax.axis('off')\n\n    def _plot_ideal_connections(self, ax):\n        \"\"\"\u7406\u60f3\u7684\u306a\u63a5\u7d9a\u3092\u56f3\u793a\"\"\"\n        words = [\"The\", \"animal\", \"didn't\", \"cross\", \"the\", \"street\", \"because\", \"it\", \"was\", \"too\", \"tired\"]\n        positions = list(range(len(words)))\n\n        # \u3059\u3079\u3066\u306e\u5358\u8a9e\u3092\u914d\u7f6e\n        for i, word in enumerate(words):\n            ax.text(i, 0, word, ha='center', va='center', fontsize=10,\n                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n\n        # \u91cd\u8981\u306a\u63a5\u7d9a\u3092\u76f4\u63a5\u63cf\u753b\n        connections = [\n            (7, 1, \"refers to\"),    # it -&gt; animal\n            (7, 5, \"could refer\"),  # it -&gt; street\n            (10, 1, \"describes\"),   # tired -&gt; animal\n        ]\n\n        for start, end, label in connections:\n            # \u66f2\u7dda\u77e2\u5370\u3092\u63cf\u753b\n            ax.annotate('', xy=(end, 0), xytext=(start, 0),\n                       arrowprops=dict(arrowstyle='-&gt;', connectionstyle=\"arc3,rad=0.3\",\n                                     color='red' if start == 7 and end == 1 else 'gray',\n                                     linewidth=2 if start == 7 and end == 1 else 1))\n\n            # \u30e9\u30d9\u30eb\u3092\u8ffd\u52a0\n            mid_x = (start + end) / 2\n            ax.text(mid_x, 0.3, label, ha='center', fontsize=8,\n                   color='red' if start == 7 and end == 1 else 'gray')\n\n        ax.set_xlim(-0.5, len(words) - 0.5)\n        ax.set_ylim(-1, 1)\n        ax.set_title(\"\u7406\u60f3: \u4efb\u610f\u306e\u5358\u8a9e\u9593\u3067\u76f4\u63a5\u63a5\u7d9a\u304c\u53ef\u80fd\", fontsize=14)\n        ax.axis('off')\n</code></pre>"},{"location":"part2/attention-intuition/#_4","title":"\u8a08\u7b97\u52b9\u7387\u306e\u6bd4\u8f03","text":"<pre><code>class ComputationalEfficiency:\n    \"\"\"RNN\u3068Attention\u306e\u8a08\u7b97\u52b9\u7387\u3092\u6bd4\u8f03\"\"\"\n\n    def compare_complexity(self):\n        \"\"\"\u8a08\u7b97\u91cf\u306e\u6bd4\u8f03\"\"\"\n        sequence_lengths = [10, 50, 100, 500, 1000]\n\n        # \u7406\u8ad6\u7684\u306a\u8a08\u7b97\u91cf\n        rnn_sequential = sequence_lengths  # O(n) \u9010\u6b21\n        rnn_parallel = [1] * len(sequence_lengths)  # \u4e26\u5217\u5316\u4e0d\u53ef\n\n        attention_sequential = [n**2 for n in sequence_lengths]  # O(n\u00b2) \u9010\u6b21\n        attention_parallel = [1] * len(sequence_lengths)  # O(1) \u4e26\u5217\u6642\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u9010\u6b21\u8a08\u7b97\u6642\u9593\n        ax1.plot(sequence_lengths, rnn_sequential, 'b-o', label='RNN', linewidth=2)\n        ax1.plot(sequence_lengths, attention_sequential, 'r-s', label='Attention', linewidth=2)\n        ax1.set_xlabel('Sequence Length')\n        ax1.set_ylabel('Sequential Steps')\n        ax1.set_title('\u9010\u6b21\u8a08\u7b97\u30b9\u30c6\u30c3\u30d7\u6570')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_yscale('log')\n\n        # \u4e26\u5217\u8a08\u7b97\u306e\u53ef\u80fd\u6027\n        ax2.bar(['RNN', 'Attention'], [0, 100], color=['blue', 'red'], alpha=0.7)\n        ax2.set_ylabel('\u4e26\u5217\u5316\u53ef\u80fd\u7387 (%)')\n        ax2.set_title('\u4e26\u5217\u8a08\u7b97\u306e\u53ef\u80fd\u6027')\n        ax2.set_ylim(0, 120)\n\n        # \u6ce8\u91c8\u3092\u8ffd\u52a0\n        ax2.text(0, 10, 'Cannot\\nparallelize', ha='center', va='bottom', fontsize=12)\n        ax2.text(1, 105, 'Fully\\nparallelizable', ha='center', va='bottom', fontsize=12)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/attention-intuition/#62","title":"6.2 \u6ce8\u610f\u6a5f\u69cb\u306e\u76f4\u611f\u7684\u7406\u89e3","text":""},{"location":"part2/attention-intuition/#_5","title":"\u4eba\u9593\u306e\u6ce8\u610f\u30e1\u30ab\u30cb\u30ba\u30e0\u3068\u306e\u985e\u4f3c","text":"<pre><code>class HumanAttentionAnalogy:\n    \"\"\"\u4eba\u9593\u306e\u6ce8\u610f\u30e1\u30ab\u30cb\u30ba\u30e0\u3068\u306e\u985e\u4f3c\u6027\u3092\u793a\u3059\"\"\"\n\n    def __init__(self):\n        self.sentence = \"The quick brown fox jumps over the lazy dog\"\n        self.words = self.sentence.split()\n\n    def visualize_human_attention(self):\n        \"\"\"\u4eba\u9593\u304c\u3069\u306e\u3088\u3046\u306b\u6587\u3092\u8aad\u3080\u304b\u3092\u53ef\u8996\u5316\"\"\"\n        # \u30bf\u30b9\u30af\u3054\u3068\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\n        attention_patterns = {\n            \"\u52d5\u7269\u3092\u63a2\u3059\": {\n                \"fox\": 0.8, \"dog\": 0.7, \"brown\": 0.3, \"lazy\": 0.3,\n                \"The\": 0.1, \"quick\": 0.2, \"jumps\": 0.2, \"over\": 0.1, \"the\": 0.1\n            },\n            \"\u52d5\u4f5c\u3092\u63a2\u3059\": {\n                \"jumps\": 0.9, \"over\": 0.4,\n                \"quick\": 0.2, \"lazy\": 0.1,\n                \"The\": 0.1, \"brown\": 0.1, \"fox\": 0.2, \"the\": 0.1, \"dog\": 0.2\n            },\n            \"\u5f62\u5bb9\u8a5e\u3092\u63a2\u3059\": {\n                \"quick\": 0.8, \"brown\": 0.8, \"lazy\": 0.8,\n                \"The\": 0.1, \"fox\": 0.2, \"jumps\": 0.1, \"over\": 0.1, \"the\": 0.1, \"dog\": 0.2\n            }\n        }\n\n        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n\n        for idx, (task, attention) in enumerate(attention_patterns.items()):\n            ax = axes[idx]\n\n            # \u5358\u8a9e\u3054\u3068\u306e\u6ce8\u610f\u306e\u91cd\u307f\u3092\u53ef\u8996\u5316\n            x_pos = np.arange(len(self.words))\n            weights = [attention.get(word, 0.1) for word in self.words]\n\n            bars = ax.bar(x_pos, weights, alpha=0.7)\n\n            # \u8272\u3092\u91cd\u307f\u306b\u5fdc\u3058\u3066\u5909\u66f4\n            for bar, weight in zip(bars, weights):\n                bar.set_color(plt.cm.Reds(weight))\n\n            ax.set_xticks(x_pos)\n            ax.set_xticklabels(self.words, rotation=45, ha='right')\n            ax.set_ylabel('Attention Weight')\n            ax.set_title(f'\u30bf\u30b9\u30af: {task}')\n            ax.set_ylim(0, 1)\n\n            # \u91cd\u8981\u306a\u5358\u8a9e\u3092\u30cf\u30a4\u30e9\u30a4\u30c8\n            for i, (word, weight) in enumerate(zip(self.words, weights)):\n                if weight &gt; 0.5:\n                    ax.annotate(f'{weight:.1f}', xy=(i, weight), \n                              xytext=(i, weight + 0.05),\n                              ha='center', fontsize=10, color='red')\n\n        plt.tight_layout()\n        plt.show()\n\n    def attention_as_similarity(self):\n        \"\"\"\u6ce8\u610f\u3092\u985e\u4f3c\u6027\u3068\u3057\u3066\u7406\u89e3\"\"\"\n        print(\"=== \u6ce8\u610f\u6a5f\u69cb\u306e\u672c\u8cea\uff1a\u985e\u4f3c\u6027\u306e\u8a08\u7b97 ===\\n\")\n\n        # \u30af\u30a8\u30ea\uff08\u8cea\u554f\uff09\n        query = \"What animal?\"\n\n        # \u6587\u4e2d\u306e\u5404\u5358\u8a9e\uff08\u30ad\u30fc\uff09\n        keys = self.words\n\n        # \u7c21\u6613\u7684\u306a\u985e\u4f3c\u5ea6\u8a08\u7b97\uff08\u5b9f\u969b\u306f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\uff09\n        similarities = {\n            \"The\": 0.1,\n            \"quick\": 0.2,\n            \"brown\": 0.3,\n            \"fox\": 0.9,  # \u9ad8\u3044\u985e\u4f3c\u5ea6\n            \"jumps\": 0.2,\n            \"over\": 0.1,\n            \"the\": 0.1,\n            \"lazy\": 0.3,\n            \"dog\": 0.8   # \u9ad8\u3044\u985e\u4f3c\u5ea6\n        }\n\n        print(f\"Query (\u8cea\u554f): '{query}'\")\n        print(\"\\nKey (\u6587\u4e2d\u306e\u5358\u8a9e) \u3068\u306e\u985e\u4f3c\u5ea6:\")\n        for word, score in similarities.items():\n            bar = \"\u2588\" * int(score * 20)\n            print(f\"  {word:10s}: {bar} {score:.2f}\")\n\n        print(\"\\n\u2192 'fox'\u3068'dog'\u306b\u9ad8\u3044\u6ce8\u610f\u304c\u5411\u3051\u3089\u308c\u308b\")\n</code></pre>"},{"location":"part2/attention-intuition/#3query-key-value","title":"\u6ce8\u610f\u6a5f\u69cb\u306e3\u3064\u306e\u8981\u7d20\uff1aQuery, Key, Value","text":"<pre><code>class QKVExplanation:\n    \"\"\"Query, Key, Value\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n\n    def __init__(self, d_model: int = 64):\n        self.d_model = d_model\n\n    def explain_qkv_concept(self):\n        \"\"\"QKV\u306e\u6982\u5ff5\u3092\u5177\u4f53\u4f8b\u3067\u8aac\u660e\"\"\"\n        print(\"=== Query, Key, Value \u306e\u6982\u5ff5 ===\\n\")\n\n        # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306e\u30a2\u30ca\u30ed\u30b8\u30fc\n        print(\"1. \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u691c\u7d22\u306e\u30a2\u30ca\u30ed\u30b8\u30fc:\")\n        print(\"   - Query: \u691c\u7d22\u30af\u30a8\u30ea\uff08\u4f55\u3092\u63a2\u3057\u3066\u3044\u308b\u304b\uff09\")\n        print(\"   - Key: \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\uff08\u3069\u3053\u3092\u898b\u308b\u3079\u304d\u304b\uff09\")\n        print(\"   - Value: \u5b9f\u969b\u306e\u30c7\u30fc\u30bf\uff08\u53d6\u5f97\u3057\u305f\u3044\u60c5\u5831\uff09\")\n\n        # \u5177\u4f53\u4f8b\n        print(\"\\n2. \u8f9e\u66f8\u691c\u7d22\u306e\u4f8b:\")\n        dictionary = {\n            \"apple\": {\"meaning\": \"\u308a\u3093\u3054\", \"type\": \"noun\", \"color\": \"red\"},\n            \"run\": {\"meaning\": \"\u8d70\u308b\", \"type\": \"verb\", \"tense\": \"present\"},\n            \"happy\": {\"meaning\": \"\u5e78\u305b\u306a\", \"type\": \"adjective\", \"emotion\": \"positive\"}\n        }\n\n        query = \"fruit\"\n        print(f\"\\nQuery: '{query}'\u3092\u63a2\u3059\")\n\n        # \u5404\u5358\u8a9e\uff08Key\uff09\u3068\u306e\u95a2\u9023\u6027\u3092\u30c1\u30a7\u30c3\u30af\n        relevance = {\n            \"apple\": 0.9,   # \u30d5\u30eb\u30fc\u30c4\u306a\u306e\u3067\u9ad8\u3044\u95a2\u9023\u6027\n            \"run\": 0.1,     # \u52d5\u8a5e\u306a\u306e\u3067\u4f4e\u3044\u95a2\u9023\u6027\n            \"happy\": 0.1    # \u5f62\u5bb9\u8a5e\u306a\u306e\u3067\u4f4e\u3044\u95a2\u9023\u6027\n        }\n\n        print(\"\\nKey \u3068\u306e\u95a2\u9023\u6027:\")\n        for key, score in relevance.items():\n            print(f\"  {key}: {score:.1f}\")\n\n        # \u95a2\u9023\u6027\u306b\u57fa\u3065\u3044\u3066Value\u3092\u53d6\u5f97\n        print(\"\\nValue \u306e\u91cd\u307f\u4ed8\u304d\u53d6\u5f97:\")\n        for key, score in relevance.items():\n            if score &gt; 0.5:\n                print(f\"  {key}: {dictionary[key]} (weight: {score})\")\n\n    def implement_simple_attention(self):\n        \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u6ce8\u610f\u6a5f\u69cb\u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== \u30b7\u30f3\u30d7\u30eb\u306a\u6ce8\u610f\u6a5f\u69cb\u306e\u5b9f\u88c5 ===\")\n\n        # \u4f8b\uff1a3\u3064\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\n        words = [\"cat\", \"sat\", \"mat\"]\n        d_model = 4  # \u5c0f\u3055\u306a\u6b21\u5143\u3067\u4f8b\u793a\n\n        # \u30e9\u30f3\u30c0\u30e0\u306a\u57cb\u3081\u8fbc\u307f\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3055\u308c\u308b\uff09\n        embeddings = {\n            \"cat\": torch.tensor([0.7, 0.2, 0.1, 0.8]),\n            \"sat\": torch.tensor([0.1, 0.9, 0.3, 0.2]),\n            \"mat\": torch.tensor([0.6, 0.1, 0.2, 0.9])\n        }\n\n        # Query, Key, Value \u5909\u63db\u884c\u5217\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3055\u308c\u308b\uff09\n        W_q = torch.randn(d_model, d_model) * 0.1\n        W_k = torch.randn(d_model, d_model) * 0.1\n        W_v = torch.randn(d_model, d_model) * 0.1\n\n        # \"cat\"\u306b\u5bfe\u3059\u308b\u6ce8\u610f\u3092\u8a08\u7b97\n        query_word = \"cat\"\n        query_vector = embeddings[query_word]\n\n        # Query\u5909\u63db\n        Q = torch.matmul(query_vector, W_q)\n\n        print(f\"'{query_word}'\u306b\u5bfe\u3059\u308b\u6ce8\u610f\u306e\u8a08\u7b97:\")\n\n        attention_scores = {}\n        for word in words:\n            # Key\u5909\u63db\n            K = torch.matmul(embeddings[word], W_k)\n\n            # \u6ce8\u610f\u30b9\u30b3\u30a2 = Query \u00b7 Key\n            score = torch.dot(Q, K).item()\n            attention_scores[word] = score\n\n            print(f\"  {word}: score = {score:.3f}\")\n\n        # Softmax\u3067\u6b63\u898f\u5316\n        scores_tensor = torch.tensor(list(attention_scores.values()))\n        attention_weights = torch.softmax(scores_tensor, dim=0)\n\n        print(\"\\nSoftmax\u5f8c\u306e\u6ce8\u610f\u306e\u91cd\u307f:\")\n        for i, word in enumerate(words):\n            print(f\"  {word}: {attention_weights[i]:.3f}\")\n\n        # \u91cd\u307f\u4ed8\u304d\u548c\u3067\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97\n        context = torch.zeros(d_model)\n        for i, word in enumerate(words):\n            V = torch.matmul(embeddings[word], W_v)\n            context += attention_weights[i] * V\n\n        print(f\"\\n\u6700\u7d42\u7684\u306a\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb: {context}\")\n\n        return attention_weights.numpy()\n</code></pre>"},{"location":"part2/attention-intuition/#_6","title":"\u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u4ed5\u7d44\u307f","text":"<pre><code>class ScaledDotProductAttention:\n    \"\"\"\u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u8a73\u7d30\u306a\u5b9f\u88c5\u3068\u8aac\u660e\"\"\"\n\n    def __init__(self):\n        self.d_k = 64  # Key/Query\u306e\u6b21\u5143\n\n    def why_scaling_matters(self):\n        \"\"\"\u306a\u305c\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u304c\u5fc5\u8981\u304b\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306e\u91cd\u8981\u6027 ===\")\n\n        # \u7570\u306a\u308b\u6b21\u5143\u3067\u306e\u5185\u7a4d\u306e\u5206\u6563\u3092\u6bd4\u8f03\n        dimensions = [8, 32, 64, 128, 256, 512]\n        variances = []\n\n        for d in dimensions:\n            # \u30e9\u30f3\u30c0\u30e0\u306a\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210\n            q = torch.randn(1000, d)\n            k = torch.randn(1000, d)\n\n            # \u5185\u7a4d\u3092\u8a08\u7b97\n            dots = torch.sum(q * k, dim=1)\n            variances.append(dots.var().item())\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(dimensions, variances, 'b-o', linewidth=2, markersize=8)\n        plt.plot(dimensions, dimensions, 'r--', label='y = x (\u7406\u8ad6\u5024)', alpha=0.7)\n        plt.xlabel('Vector Dimension (d_k)')\n        plt.ylabel('Variance of Dot Product')\n        plt.title('\u5185\u7a4d\u306e\u5206\u6563\u306f\u6b21\u5143\u306b\u6bd4\u4f8b\u3057\u3066\u5897\u52a0')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n        print(\"\\n\u554f\u984c\uff1a\u6b21\u5143\u304c\u5927\u304d\u3044\u3068...\")\n        print(\"1. \u5185\u7a4d\u306e\u5024\u304c\u5927\u304d\u304f\u306a\u308b\")\n        print(\"2. Softmax\u304c\u6975\u7aef\u306a\u5024\uff080\u307e\u305f\u306f1\uff09\u306b\u306a\u308b\")\n        print(\"3. \u52fe\u914d\u6d88\u5931\u304c\u8d77\u304d\u3084\u3059\u3044\")\n        print(f\"\\n\u89e3\u6c7a\u7b56\uff1a\u221ad_k = \u221a{self.d_k} = {math.sqrt(self.d_k):.1f} \u3067\u5272\u308b\")\n\n    def implement_scaled_attention(self, \n                                 query: torch.Tensor, \n                                 key: torch.Tensor, \n                                 value: torch.Tensor,\n                                 mask: Optional[torch.Tensor] = None) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u5b9f\u88c5\"\"\"\n\n        # \u5165\u529b\u306e\u5f62\u72b6\n        batch_size, seq_len, d_k = query.shape\n\n        print(f\"=== \u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u8a08\u7b97 ===\")\n        print(f\"\u5165\u529b\u5f62\u72b6: batch_size={batch_size}, seq_len={seq_len}, d_k={d_k}\")\n\n        # \u30b9\u30c6\u30c3\u30d71: Q\u00b7K^T \u306e\u8a08\u7b97\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        print(f\"\\n1. \u6ce8\u610f\u30b9\u30b3\u30a2 (Q\u00b7K^T): shape = {scores.shape}\")\n\n        # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u524d\u306e\u5024\u3092\u8a18\u9332\n        pre_scale_max = scores.max().item()\n        pre_scale_std = scores.std().item()\n\n        # \u30b9\u30c6\u30c3\u30d72: \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n        scores = scores / math.sqrt(d_k)\n        print(f\"\\n2. \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u5f8c:\")\n        print(f\"   \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u524d: max={pre_scale_max:.2f}, std={pre_scale_std:.2f}\")\n        print(f\"   \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u5f8c: max={scores.max().item():.2f}, std={scores.std().item():.2f}\")\n\n        # \u30b9\u30c6\u30c3\u30d73: \u30de\u30b9\u30af\u9069\u7528\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            print(f\"\\n3. \u30de\u30b9\u30af\u9069\u7528\u6e08\u307f\")\n\n        # \u30b9\u30c6\u30c3\u30d74: Softmax\n        attention_weights = torch.softmax(scores, dim=-1)\n        print(f\"\\n4. Softmax\u5f8c\u306e\u6ce8\u610f\u306e\u91cd\u307f: shape = {attention_weights.shape}\")\n\n        # \u6ce8\u610f\u306e\u91cd\u307f\u306e\u7d71\u8a08\n        print(f\"   \u6700\u5927\u91cd\u307f: {attention_weights.max().item():.4f}\")\n        print(f\"   \u6700\u5c0f\u91cd\u307f: {attention_weights.min().item():.4f}\")\n        print(f\"   \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc: {-(attention_weights * attention_weights.log()).sum(-1).mean().item():.4f}\")\n\n        # \u30b9\u30c6\u30c3\u30d75: \u91cd\u307f\u4ed8\u304d\u548c\n        context = torch.matmul(attention_weights, value)\n        print(f\"\\n5. \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb: shape = {context.shape}\")\n\n        return context, attention_weights\n\n    def visualize_attention_computation(self):\n        \"\"\"\u6ce8\u610f\u8a08\u7b97\u306e\u5404\u30b9\u30c6\u30c3\u30d7\u3092\u53ef\u8996\u5316\"\"\"\n        # \u5c0f\u3055\u306a\u4f8b\u3067\u53ef\u8996\u5316\n        seq_len = 8\n        d_k = 4\n\n        # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n        torch.manual_seed(42)\n        Q = torch.randn(1, seq_len, d_k)\n        K = torch.randn(1, seq_len, d_k)\n        V = torch.randn(1, seq_len, d_k)\n\n        # \u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1))[0]\n        scaled_scores = scores / math.sqrt(d_k)\n        attention_weights = torch.softmax(scaled_scores, dim=-1)\n\n        # \u53ef\u8996\u5316\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # 1. \u751f\u306e\u30b9\u30b3\u30a2\n        ax = axes[0, 0]\n        im1 = ax.imshow(scores.detach(), cmap='RdBu', aspect='auto')\n        ax.set_title('1. \u6ce8\u610f\u30b9\u30b3\u30a2 (Q\u00b7K^T)')\n        ax.set_xlabel('Key positions')\n        ax.set_ylabel('Query positions')\n        plt.colorbar(im1, ax=ax)\n\n        # 2. \u30b9\u30b1\u30fc\u30eb\u5f8c\u306e\u30b9\u30b3\u30a2\n        ax = axes[0, 1]\n        im2 = ax.imshow(scaled_scores.detach(), cmap='RdBu', aspect='auto')\n        ax.set_title(f'2. \u30b9\u30b1\u30fc\u30eb\u5f8c (\u00f7\u221a{d_k})')\n        ax.set_xlabel('Key positions')\n        ax.set_ylabel('Query positions')\n        plt.colorbar(im2, ax=ax)\n\n        # 3. Softmax\u5f8c\n        ax = axes[1, 0]\n        im3 = ax.imshow(attention_weights.detach(), cmap='Blues', aspect='auto')\n        ax.set_title('3. Softmax\u5f8c\u306e\u6ce8\u610f\u306e\u91cd\u307f')\n        ax.set_xlabel('Key positions')\n        ax.set_ylabel('Query positions')\n        plt.colorbar(im3, ax=ax)\n\n        # 4. \u884c\u3054\u3068\u306e\u5206\u5e03\n        ax = axes[1, 1]\n        for i in range(min(4, seq_len)):  # \u6700\u521d\u306e4\u884c\u3092\u8868\u793a\n            ax.plot(attention_weights[i].detach(), label=f'Query pos {i}', marker='o')\n        ax.set_title('4. \u6ce8\u610f\u306e\u91cd\u307f\u5206\u5e03\uff08\u5404\u30af\u30a8\u30ea\u4f4d\u7f6e\uff09')\n        ax.set_xlabel('Key positions')\n        ax.set_ylabel('Attention weight')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/attention-intuition/#63","title":"6.3 \u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\uff1a\u6587\u304c\u81ea\u5206\u81ea\u8eab\u306b\u6ce8\u610f\u3092\u5411\u3051\u308b","text":""},{"location":"part2/attention-intuition/#self-attention","title":"Self-Attention\u306e\u52d5\u4f5c\u539f\u7406","text":"<pre><code>class SelfAttentionMechanism:\n    \"\"\"\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\u306e\u8a73\u7d30\u306a\u5b9f\u88c5\u3068\u89e3\u8aac\"\"\"\n\n    def __init__(self, d_model: int = 256, d_k: int = 64):\n        self.d_model = d_model\n        self.d_k = d_k\n\n        # \u7dda\u5f62\u5909\u63db\u5c64\n        self.W_q = nn.Linear(d_model, d_k, bias=False)\n        self.W_k = nn.Linear(d_model, d_k, bias=False)\n        self.W_v = nn.Linear(d_model, d_k, bias=False)\n\n    def explain_self_attention(self):\n        \"\"\"\u81ea\u5df1\u6ce8\u610f\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb\uff08Self-Attention\uff09===\\n\")\n\n        print(\"\u901a\u5e38\u306e\u6ce8\u610f\u6a5f\u69cb:\")\n        print(\"  - Query: \u8cea\u554f\u5074\u306e\u6587\")\n        print(\"  - Key/Value: \u53c2\u7167\u5074\u306e\u6587\")\n        print(\"  - \u4f8b: \u7ffb\u8a33\u3067\u306e\u539f\u6587\u3068\u8a33\u6587\u306e\u5bfe\u5fdc\")\n\n        print(\"\\n\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb:\")\n        print(\"  - Query, Key, Value: \u3059\u3079\u3066\u540c\u3058\u6587\u304b\u3089\u751f\u6210\")\n        print(\"  - \u6587\u4e2d\u306e\u5404\u5358\u8a9e\u304c\u3001\u540c\u3058\u6587\u306e\u4ed6\u306e\u5358\u8a9e\u306b\u6ce8\u610f\u3092\u5411\u3051\u308b\")\n        print(\"  - \u4f8b: \u6587\u4e2d\u306e\u4ee3\u540d\u8a5e\u304c\u4f55\u3092\u6307\u3059\u304b\u3092\u7406\u89e3\")\n\n        # \u5177\u4f53\u4f8b\u3067\u793a\u3059\n        self._demonstrate_self_attention()\n\n    def _demonstrate_self_attention(self):\n        \"\"\"\u81ea\u5df1\u6ce8\u610f\u306e\u5177\u4f53\u4f8b\"\"\"\n        sentence = \"The cat sat on the mat\"\n        words = sentence.split()\n\n        # \u4eee\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3067\u7372\u5f97\uff09\n        attention_matrix = torch.tensor([\n            # The  cat  sat  on  the  mat\n            [0.8, 0.1, 0.0, 0.0, 0.0, 0.1],  # The \u2192 \u81ea\u5206\u81ea\u8eab\u3068mat\n            [0.2, 0.5, 0.2, 0.0, 0.0, 0.1],  # cat \u2192 \u81ea\u5206\u81ea\u8eab\u3068sat\n            [0.1, 0.3, 0.4, 0.1, 0.0, 0.1],  # sat \u2192 cat\u3068\u81ea\u5206\u81ea\u8eab\n            [0.0, 0.0, 0.1, 0.6, 0.0, 0.3],  # on \u2192 \u81ea\u5206\u81ea\u8eab\u3068mat\n            [0.0, 0.0, 0.0, 0.0, 0.7, 0.3],  # the \u2192 \u81ea\u5206\u81ea\u8eab\u3068mat\n            [0.1, 0.2, 0.1, 0.2, 0.1, 0.3],  # mat \u2192 \u4ed6\u306e\u5358\u8a9e\u3092\u5747\u7b49\u306b\u53c2\u7167\n        ])\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(attention_matrix, \n                    xticklabels=words, \n                    yticklabels=words,\n                    cmap='Blues', \n                    cbar_kws={'label': 'Attention Weight'},\n                    annot=True, \n                    fmt='.1f')\n\n        plt.title('Self-Attention: \u5404\u5358\u8a9e\u304c\u6587\u4e2d\u306e\u4ed6\u306e\u5358\u8a9e\u306b\u5411\u3051\u308b\u6ce8\u610f')\n        plt.xlabel('Attended to (Key)')\n        plt.ylabel('Attending from (Query)')\n        plt.tight_layout()\n        plt.show()\n\n    def implement_self_attention_layer(self):\n        \"\"\"\u81ea\u5df1\u6ce8\u610f\u5c64\u306e\u5b8c\u5168\u306a\u5b9f\u88c5\"\"\"\n\n        class SelfAttentionLayer(nn.Module):\n            def __init__(self, d_model: int, d_k: int):\n                super().__init__()\n                self.d_k = d_k\n\n                # Q, K, V \u306e\u7dda\u5f62\u5909\u63db\n                self.W_q = nn.Linear(d_model, d_k, bias=False)\n                self.W_k = nn.Linear(d_model, d_k, bias=False)\n                self.W_v = nn.Linear(d_model, d_k, bias=False)\n\n                # \u521d\u671f\u5316\n                self._init_weights()\n\n            def _init_weights(self):\n                # Xavier\u521d\u671f\u5316\n                for module in [self.W_q, self.W_k, self.W_v]:\n                    nn.init.xavier_uniform_(module.weight)\n\n            def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n                \"\"\"\n                Args:\n                    x: [batch_size, seq_len, d_model]\n                    mask: [batch_size, seq_len, seq_len]\n                Returns:\n                    output: [batch_size, seq_len, d_k]\n                    attention_weights: [batch_size, seq_len, seq_len]\n                \"\"\"\n                batch_size, seq_len, d_model = x.shape\n\n                # 1. \u7dda\u5f62\u5909\u63db\u3067Q, K, V\u3092\u751f\u6210\n                Q = self.W_q(x)  # [batch_size, seq_len, d_k]\n                K = self.W_k(x)  # [batch_size, seq_len, d_k]\n                V = self.W_v(x)  # [batch_size, seq_len, d_k]\n\n                # 2. \u6ce8\u610f\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n                # 3. \u30de\u30b9\u30af\u306e\u9069\u7528\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                # 4. Softmax\n                attention_weights = torch.softmax(scores, dim=-1)\n\n                # 5. \u91cd\u307f\u4ed8\u304d\u548c\n                output = torch.matmul(attention_weights, V)\n\n                return output, attention_weights\n\n        return SelfAttentionLayer(self.d_model, self.d_k)\n\n    def analyze_attention_patterns(self, sentence: str):\n        \"\"\"\u5b9f\u969b\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\"\"\"\n        # \u30c8\u30fc\u30af\u30f3\u5316\uff08\u7c21\u6613\u7248\uff09\n        words = sentence.lower().split()\n        seq_len = len(words)\n\n        # \u30c0\u30df\u30fc\u306e\u57cb\u3081\u8fbc\u307f\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u6e08\u307f\u57cb\u3081\u8fbc\u307f\u3092\u4f7f\u7528\uff09\n        torch.manual_seed(42)\n        embeddings = torch.randn(1, seq_len, self.d_model)\n\n        # \u81ea\u5df1\u6ce8\u610f\u5c64\u3092\u4f5c\u6210\n        self_attention = self.implement_self_attention_layer()\n\n        # \u6ce8\u610f\u3092\u8a08\u7b97\n        with torch.no_grad():\n            output, attention_weights = self_attention(embeddings)\n\n        # \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\n        attention_matrix = attention_weights[0].numpy()\n\n        plt.figure(figsize=(10, 8))\n\n        # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        mask = np.zeros_like(attention_matrix)\n        np.fill_diagonal(mask, True)\n\n        sns.heatmap(attention_matrix, \n                    xticklabels=words,\n                    yticklabels=words,\n                    cmap='Blues',\n                    cbar_kws={'label': 'Attention Weight'},\n                    mask=mask,  # \u5bfe\u89d2\u7dda\u3092\u30de\u30b9\u30af\uff08\u898b\u3084\u3059\u3055\u306e\u305f\u3081\uff09\n                    linewidths=0.5)\n\n        # \u5bfe\u89d2\u7dda\u306f\u5225\u306e\u8272\u3067\u8868\u793a\n        for i in range(seq_len):\n            plt.scatter(i + 0.5, i + 0.5, s=attention_matrix[i, i] * 1000, \n                       c='red', marker='s', alpha=0.5)\n\n        plt.title(f'Self-Attention Pattern: \"{sentence}\"')\n        plt.xlabel('Attended to')\n        plt.ylabel('Attending from')\n\n        # \u91cd\u8981\u306a\u63a5\u7d9a\u3092\u30cf\u30a4\u30e9\u30a4\u30c8\n        threshold = attention_matrix.mean() + attention_matrix.std()\n        for i in range(seq_len):\n            for j in range(seq_len):\n                if i != j and attention_matrix[i, j] &gt; threshold:\n                    plt.annotate('', xy=(j + 0.5, i + 0.5), \n                               xytext=(i + 0.5, i + 0.5),\n                               arrowprops=dict(arrowstyle='-&gt;', \n                                             color='green', \n                                             alpha=0.5,\n                                             linewidth=2))\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/attention-intuition/#_7","title":"\u4f4d\u7f6e\u60c5\u5831\u306e\u91cd\u8981\u6027","text":"<pre><code>class PositionalInformation:\n    \"\"\"\u81ea\u5df1\u6ce8\u610f\u306b\u304a\u3051\u308b\u4f4d\u7f6e\u60c5\u5831\u306e\u91cd\u8981\u6027\"\"\"\n\n    def why_position_matters(self):\n        \"\"\"\u306a\u305c\u4f4d\u7f6e\u60c5\u5831\u304c\u5fc5\u8981\u304b\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== \u4f4d\u7f6e\u60c5\u5831\u306e\u91cd\u8981\u6027 ===\\n\")\n\n        # \u540c\u3058\u5358\u8a9e\u3001\u7570\u306a\u308b\u9806\u5e8f\u306e\u6587\n        sentences = [\n            \"The cat chased the mouse\",\n            \"The mouse chased the cat\"\n        ]\n\n        print(\"\u554f\u984c\uff1a\u81ea\u5df1\u6ce8\u610f\u306f\u9806\u5e8f\u3092\u8003\u616e\u3057\u306a\u3044\")\n        print(f\"\u65871: {sentences[0]}\")\n        print(f\"\u65872: {sentences[1]}\")\n        print(\"\\n\u4e21\u6587\u3067 'cat' \u3068 'mouse' \u306e\u95a2\u4fc2\u306f\u540c\u3058\u3088\u3046\u306b\u898b\u3048\u308b\uff01\")\n\n        # \u30d0\u30c3\u30b0\u30aa\u30d6\u30ef\u30fc\u30ba\u7684\u306a\u8868\u73fe\n        self._demonstrate_order_blindness()\n\n    def _demonstrate_order_blindness(self):\n        \"\"\"\u9806\u5e8f\u306e\u7121\u8996\u3092\u5b9f\u8a3c\"\"\"\n        # 2\u3064\u306e\u6587\u306e\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\uff08\u4eee\u60f3\u7684\uff09\n        words1 = [\"The\", \"cat\", \"chased\", \"the\", \"mouse\"]\n        words2 = [\"The\", \"mouse\", \"chased\", \"the\", \"cat\"]\n\n        # \u5358\u8a9e\u306e\u57cb\u3081\u8fbc\u307f\uff08\u4f4d\u7f6e\u60c5\u5831\u306a\u3057\uff09\n        word_embeddings = {\n            \"the\": torch.tensor([0.1, 0.2]),\n            \"cat\": torch.tensor([0.8, 0.3]),\n            \"mouse\": torch.tensor([0.7, 0.4]),\n            \"chased\": torch.tensor([0.2, 0.9])\n        }\n\n        # \u81ea\u5df1\u6ce8\u610f\u3067\u306e\u985e\u4f3c\u5ea6\u8a08\u7b97\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        for ax, words, title in [(ax1, words1, \"\u65871\"), (ax2, words2, \"\u65872\")]:\n            # \u985e\u4f3c\u5ea6\u884c\u5217\n            similarity_matrix = np.zeros((len(words), len(words)))\n\n            for i, word_i in enumerate(words):\n                for j, word_j in enumerate(words):\n                    emb_i = word_embeddings[word_i.lower()]\n                    emb_j = word_embeddings[word_j.lower()]\n                    similarity = torch.cosine_similarity(emb_i, emb_j, dim=0).item()\n                    similarity_matrix[i, j] = similarity\n\n            im = ax.imshow(similarity_matrix, cmap='Blues', aspect='auto')\n            ax.set_xticks(range(len(words)))\n            ax.set_yticks(range(len(words)))\n            ax.set_xticklabels(words, rotation=45)\n            ax.set_yticklabels(words)\n            ax.set_title(f'{title}: \u4f4d\u7f6e\u60c5\u5831\u306a\u3057\u306e\u985e\u4f3c\u5ea6')\n\n            # \u985e\u4f3c\u5ea6\u306e\u5024\u3092\u8868\u793a\n            for i in range(len(words)):\n                for j in range(len(words)):\n                    ax.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n                           ha='center', va='center')\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"\\n\u2192 \u4f4d\u7f6e\u60c5\u5831\u304c\u306a\u3044\u3068\u3001\u6587\u306e\u610f\u5473\u304c\u6b63\u3057\u304f\u7406\u89e3\u3067\u304d\u306a\u3044\uff01\")\n</code></pre>"},{"location":"part2/attention-intuition/#64","title":"6.4 \u6ce8\u610f\u6a5f\u69cb\u306e\u5b9f\u4f8b\u3068\u5fdc\u7528","text":""},{"location":"part2/attention-intuition/#_8","title":"\u7ffb\u8a33\u30bf\u30b9\u30af\u3067\u306e\u6ce8\u610f","text":"<pre><code>class AttentionInTranslation:\n    \"\"\"\u7ffb\u8a33\u30bf\u30b9\u30af\u306b\u304a\u3051\u308b\u6ce8\u610f\u6a5f\u69cb\u306e\u52d5\u4f5c\"\"\"\n\n    def __init__(self):\n        self.examples = [\n            {\n                \"source\": \"I love cats\",\n                \"target\": \"\u79c1\u306f\u732b\u304c\u5927\u597d\u304d\u3067\u3059\",\n                \"alignment\": {\n                    \"I\": [\"\u79c1\u306f\"],\n                    \"love\": [\"\u5927\u597d\u304d\u3067\u3059\"],\n                    \"cats\": [\"\u732b\u304c\"]\n                }\n            },\n            {\n                \"source\": \"The weather is beautiful today\",\n                \"target\": \"\u4eca\u65e5\u306e\u5929\u6c17\u306f\u7d20\u6674\u3089\u3057\u3044\u3067\u3059\",\n                \"alignment\": {\n                    \"The weather\": [\"\u5929\u6c17\u306f\"],\n                    \"is beautiful\": [\"\u7d20\u6674\u3089\u3057\u3044\u3067\u3059\"],\n                    \"today\": [\"\u4eca\u65e5\u306e\"]\n                }\n            }\n        ]\n\n    def visualize_translation_attention(self):\n        \"\"\"\u7ffb\u8a33\u3067\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\"\"\"\n        for example in self.examples:\n            source_words = example[\"source\"].split()\n            target_words = example[\"target\"]\n\n            # \u4eee\u60f3\u7684\u306a\u6ce8\u610f\u884c\u5217\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3067\u7372\u5f97\uff09\n            attention_matrix = self._create_attention_matrix(example)\n\n            plt.figure(figsize=(8, 6))\n            sns.heatmap(attention_matrix,\n                       xticklabels=source_words,\n                       yticklabels=list(target_words),\n                       cmap='Blues',\n                       cbar_kws={'label': 'Attention Weight'})\n\n            plt.title(f'\u7ffb\u8a33\u306e\u6ce8\u610f: \"{example[\"source\"]}\" \u2192 \"{example[\"target\"]}\"')\n            plt.xlabel('Source (English)')\n            plt.ylabel('Target (Japanese)')\n            plt.tight_layout()\n            plt.show()\n\n    def _create_attention_matrix(self, example):\n        \"\"\"\u30a2\u30e9\u30a4\u30f3\u30e1\u30f3\u30c8\u60c5\u5831\u304b\u3089\u6ce8\u610f\u884c\u5217\u3092\u4f5c\u6210\"\"\"\n        source_words = example[\"source\"].split()\n        target_chars = list(example[\"target\"])\n\n        matrix = np.zeros((len(target_chars), len(source_words)))\n\n        # \u30a2\u30e9\u30a4\u30f3\u30e1\u30f3\u30c8\u60c5\u5831\u306b\u57fa\u3065\u3044\u3066\u6ce8\u610f\u306e\u91cd\u307f\u3092\u8a2d\u5b9a\n        for src_word, tgt_phrases in example[\"alignment\"].items():\n            src_idx = source_words.index(src_word) if src_word in source_words else -1\n            if src_idx &gt;= 0:\n                for tgt_phrase in tgt_phrases:\n                    tgt_start = example[\"target\"].find(tgt_phrase)\n                    if tgt_start &gt;= 0:\n                        for i in range(tgt_start, tgt_start + len(tgt_phrase)):\n                            if i &lt; len(target_chars):\n                                matrix[i, src_idx] = 1.0\n\n        # \u6b63\u898f\u5316\n        matrix = matrix / (matrix.sum(axis=1, keepdims=True) + 1e-9)\n\n        return matrix\n</code></pre>"},{"location":"part2/attention-intuition/#_9","title":"\u6587\u66f8\u8981\u7d04\u3067\u306e\u6ce8\u610f","text":"<pre><code>class AttentionInSummarization:\n    \"\"\"\u6587\u66f8\u8981\u7d04\u306b\u304a\u3051\u308b\u6ce8\u610f\u6a5f\u69cb\u306e\u5f79\u5272\"\"\"\n\n    def demonstrate_extractive_attention(self):\n        \"\"\"\u62bd\u51fa\u578b\u8981\u7d04\u3067\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\"\"\"\n        document = \"\"\"\n        Artificial intelligence has made remarkable progress in recent years.\n        Deep learning models have achieved human-level performance in many tasks.\n        The transformer architecture revolutionized natural language processing.\n        Attention mechanisms allow models to focus on relevant information.\n        These advances have enabled applications like ChatGPT and GPT-4.\n        \"\"\"\n\n        sentences = [s.strip() for s in document.strip().split('.') if s.strip()]\n\n        # \u91cd\u8981\u5ea6\u30b9\u30b3\u30a2\uff08\u4eee\u60f3\u7684\uff09\n        importance_scores = [0.7, 0.8, 0.9, 0.8, 0.6]\n\n        # \u6587\u9593\u306e\u6ce8\u610f\u884c\u5217\uff08\u3069\u306e\u6587\u304c\u4ed6\u306e\u6587\u3092\u53c2\u7167\u3059\u308b\u304b\uff09\n        cross_sentence_attention = np.array([\n            [1.0, 0.3, 0.2, 0.3, 0.2],\n            [0.3, 1.0, 0.4, 0.5, 0.3],\n            [0.2, 0.4, 1.0, 0.6, 0.4],\n            [0.3, 0.5, 0.6, 1.0, 0.3],\n            [0.2, 0.3, 0.4, 0.3, 1.0]\n        ])\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u91cd\u8981\u5ea6\u30b9\u30b3\u30a2\n        ax1.barh(range(len(sentences)), importance_scores, color='skyblue')\n        ax1.set_yticks(range(len(sentences)))\n        ax1.set_yticklabels([f\"Sent {i+1}\" for i in range(len(sentences))])\n        ax1.set_xlabel('Importance Score')\n        ax1.set_title('\u6587\u306e\u91cd\u8981\u5ea6\u30b9\u30b3\u30a2')\n        ax1.set_xlim(0, 1)\n\n        # \u91cd\u8981\u306a\u6587\u3092\u30cf\u30a4\u30e9\u30a4\u30c8\n        for i, score in enumerate(importance_scores):\n            if score &gt; 0.75:\n                ax1.barh(i, score, color='orange')\n\n        # \u6587\u9593\u306e\u6ce8\u610f\n        im = ax2.imshow(cross_sentence_attention, cmap='Blues', aspect='auto')\n        ax2.set_xticks(range(len(sentences)))\n        ax2.set_yticks(range(len(sentences)))\n        ax2.set_xticklabels([f\"S{i+1}\" for i in range(len(sentences))])\n        ax2.set_yticklabels([f\"S{i+1}\" for i in range(len(sentences))])\n        ax2.set_title('\u6587\u9593\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3')\n        plt.colorbar(im, ax=ax2)\n\n        plt.tight_layout()\n        plt.show()\n\n        # \u8981\u7d04\u306e\u751f\u6210\n        print(\"=== \u62bd\u51fa\u578b\u8981\u7d04 ===\")\n        print(\"\\n\u5143\u306e\u6587\u66f8:\")\n        for i, sent in enumerate(sentences):\n            print(f\"{i+1}. {sent}.\")\n\n        print(\"\\n\u91cd\u8981\u306a\u6587\uff08\u30b9\u30b3\u30a2 &gt; 0.75\uff09:\")\n        for i, (sent, score) in enumerate(zip(sentences, importance_scores)):\n            if score &gt; 0.75:\n                print(f\"- {sent}. (score: {score})\")\n</code></pre>"},{"location":"part2/attention-intuition/#65","title":"6.5 \u6ce8\u610f\u6a5f\u69cb\u306e\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u30c7\u30e2","text":"<pre><code>class InteractiveAttentionDemo:\n    \"\"\"\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u6ce8\u610f\u6a5f\u69cb\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\"\"\"\n\n    def __init__(self):\n        self.d_model = 64\n        self.attention_layer = self._create_attention_layer()\n\n    def _create_attention_layer(self):\n        \"\"\"\u7c21\u5358\u306a\u6ce8\u610f\u5c64\u3092\u4f5c\u6210\"\"\"\n        return ScaledDotProductAttention()\n\n    def interactive_attention_explorer(self, sentence: str):\n        \"\"\"\u6587\u306b\u5bfe\u3059\u308b\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u63a2\u7d22\"\"\"\n        words = sentence.split()\n        seq_len = len(words)\n\n        # \u30b7\u30f3\u30d7\u30eb\u306a\u57cb\u3081\u8fbc\u307f\n        embeddings = self._get_simple_embeddings(words)\n\n        # Q, K, V\u3092\u751f\u6210\n        Q = embeddings\n        K = embeddings\n        V = embeddings\n\n        # \u6ce8\u610f\u3092\u8a08\u7b97\n        context, attention_weights = self.attention_layer.implement_scaled_attention(\n            Q.unsqueeze(0), K.unsqueeze(0), V.unsqueeze(0)\n        )\n\n        # \u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u53ef\u8996\u5316\n        self._create_interactive_visualization(words, attention_weights[0])\n\n    def _get_simple_embeddings(self, words: List[str]) -&gt; torch.Tensor:\n        \"\"\"\u5358\u8a9e\u306e\u7c21\u6613\u57cb\u3081\u8fbc\u307f\u3092\u751f\u6210\"\"\"\n        # \u5358\u8a9e\u30bf\u30a4\u30d7\u306b\u57fa\u3065\u304f\u7c21\u6613\u57cb\u3081\u8fbc\u307f\n        word_types = {\n            \"noun\": torch.tensor([1.0, 0.0, 0.0, 0.0]),\n            \"verb\": torch.tensor([0.0, 1.0, 0.0, 0.0]),\n            \"adj\": torch.tensor([0.0, 0.0, 1.0, 0.0]),\n            \"other\": torch.tensor([0.0, 0.0, 0.0, 1.0])\n        }\n\n        # \u7c21\u6613\u7684\u306a\u54c1\u8a5e\u63a8\u5b9a\n        embeddings = []\n        for word in words:\n            if word.lower() in [\"cat\", \"dog\", \"man\", \"woman\", \"car\"]:\n                embeddings.append(word_types[\"noun\"])\n            elif word.lower() in [\"run\", \"jump\", \"eat\", \"sleep\", \"is\", \"was\"]:\n                embeddings.append(word_types[\"verb\"])\n            elif word.lower() in [\"big\", \"small\", \"red\", \"blue\", \"happy\"]:\n                embeddings.append(word_types[\"adj\"])\n            else:\n                embeddings.append(word_types[\"other\"])\n\n        return torch.stack(embeddings) + torch.randn(len(words), 4) * 0.1\n\n    def _create_interactive_visualization(self, words: List[str], attention_weights: torch.Tensor):\n        \"\"\"\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u6ce8\u610f\u306e\u53ef\u8996\u5316\"\"\"\n        attention_matrix = attention_weights.detach().numpy()\n\n        fig, ax = plt.subplots(figsize=(10, 8))\n\n        # \u30d9\u30fc\u30b9\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        im = ax.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n\n        # \u8ef8\u306e\u8a2d\u5b9a\n        ax.set_xticks(range(len(words)))\n        ax.set_yticks(range(len(words)))\n        ax.set_xticklabels(words, rotation=45, ha='right')\n        ax.set_yticklabels(words)\n\n        # \u30b0\u30ea\u30c3\u30c9\u30e9\u30a4\u30f3\n        ax.set_xticks(np.arange(len(words) + 1) - 0.5, minor=True)\n        ax.set_yticks(np.arange(len(words) + 1) - 0.5, minor=True)\n        ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n\n        # \u5024\u3092\u8868\u793a\n        for i in range(len(words)):\n            for j in range(len(words)):\n                text = ax.text(j, i, f'{attention_matrix[i, j]:.2f}',\n                             ha='center', va='center', color='black' if attention_matrix[i, j] &lt; 0.5 else 'white')\n\n        plt.colorbar(im, ax=ax, label='Attention Weight')\n        plt.title('Interactive Attention Explorer')\n        plt.xlabel('Attended to (Key)')\n        plt.ylabel('Attending from (Query)')\n\n        # \u5404\u884c\u306e\u6700\u5927\u5024\u3092\u30cf\u30a4\u30e9\u30a4\u30c8\n        for i in range(len(words)):\n            max_idx = np.argmax(attention_matrix[i])\n            rect = plt.Rectangle((max_idx - 0.45, i - 0.45), 0.9, 0.9, \n                               fill=False, edgecolor='red', linewidth=3)\n            ax.add_patch(rect)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/attention-intuition/#_10","title":"\u307e\u3068\u3081\uff1a\u6ce8\u610f\u6a5f\u69cb\u306e\u672c\u8cea","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060\u6ce8\u610f\u6a5f\u69cb\u306e\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u52d5\u6a5f\uff1a\u9577\u8ddd\u96e2\u4f9d\u5b58\u3068\u4e26\u5217\u8a08\u7b97\u306e\u5fc5\u8981\u6027</li> <li>\u4ed5\u7d44\u307f\uff1aQuery-Key-Value\u306b\u3088\u308b\u985e\u4f3c\u5ea6\u30d9\u30fc\u30b9\u306e\u60c5\u5831\u96c6\u7d04</li> <li>\u81ea\u5df1\u6ce8\u610f\uff1a\u6587\u304c\u81ea\u5206\u81ea\u8eab\u306e\u7570\u306a\u308b\u90e8\u5206\u306b\u6ce8\u610f\u3092\u5411\u3051\u308b</li> <li>\u5fdc\u7528\uff1a\u7ffb\u8a33\u3001\u8981\u7d04\u3001\u8cea\u554f\u5fdc\u7b54\u306a\u3069\u5e45\u5e83\u3044\u30bf\u30b9\u30af\u3067\u6709\u52b9</li> </ol> <p>\u6ce8\u610f\u6a5f\u69cb\u306f\u3001\u307e\u3055\u306b\u300c\u95a2\u9023\u3059\u308b\u60c5\u5831\u3092\u898b\u3064\u3051\u3066\u91cd\u307f\u4ed8\u3051\u3059\u308b\u300d\u3068\u3044\u3046\u3001\u4eba\u9593\u306e\u8a8d\u77e5\u30d7\u30ed\u30bb\u30b9\u3092\u6a21\u5023\u3057\u305f\u30e1\u30ab\u30cb\u30ba\u30e0\u3067\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u306e\u6ce8\u610f\u6a5f\u69cb\u306b\u300c\u4f4d\u7f6e\u300d\u306e\u6982\u5ff5\u3092\u52a0\u3048\u308b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part2/attention-intuition/#_11","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1a\u30de\u30b9\u30af\u4ed8\u304d\u81ea\u5df1\u6ce8\u610f\u3092\u5b9f\u88c5\u3057\u3001\u6587\u306e\u5f8c\u534a\u304c\u524d\u534a\u3057\u304b\u898b\u3048\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1a\u4e0e\u3048\u3089\u308c\u305f\u6587\u306b\u5bfe\u3057\u3066\u3001\u3069\u306e\u5358\u8a9e\u30da\u30a2\u304c\u9ad8\u3044\u6ce8\u610f\u30b9\u30b3\u30a2\u3092\u6301\u3064\u304b\u4e88\u6e2c\u3057\u3001\u5b9f\u969b\u306b\u8a08\u7b97\u3057\u3066\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5fdc\u7528\u8ab2\u984c\uff1a\u6ce8\u610f\u6a5f\u69cb\u3092\u4f7f\u3063\u3066\u3001\u6587\u4e2d\u306e\u91cd\u8981\u306a\u5358\u8a9e\u3092\u62bd\u51fa\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8a2d\u8a08\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1a\u306a\u305c\u6ce8\u610f\u30b9\u30b3\u30a2\u3092\u221ad_k\u3067\u5272\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u304b\u3001\u6570\u5b66\u7684\u306b\u8a3c\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part2/layers-and-deep-learning/","title":"\u5c64\u306e\u6982\u5ff5\u3068\u6df1\u5c64\u5b66\u7fd2","text":""},{"location":"part2/layers-and-deep-learning/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u306a\u305c\u300c\u6df1\u3055\u300d\u304c\u91cd\u8981\u306a\u306e\u304b","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u30d1\u30b9\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u5358\u4e00\u306e\u30d1\u30b9\u3067\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u304b\u3089\u6700\u9069\u5316\u3055\u308c\u305f\u30de\u30b7\u30f3\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3057\u3087\u3046\u304b\uff1f\u7406\u8ad6\u7684\u306b\u306f\u53ef\u80fd\u3067\u3059\u304c\u3001\u5b9f\u969b\u306b\u306f\u8907\u6570\u306e\u30d1\u30b9\uff08\u5b57\u53e5\u89e3\u6790\u2192\u69cb\u6587\u89e3\u6790\u2192\u610f\u5473\u89e3\u6790\u2192\u6700\u9069\u5316\u2192\u30b3\u30fc\u30c9\u751f\u6210\uff09\u306b\u5206\u3051\u308b\u3053\u3068\u3067\u3001\u5404\u6bb5\u968e\u304c\u6271\u3044\u3084\u3059\u3044\u62bd\u8c61\u5ea6\u3067\u554f\u984c\u3092\u89e3\u6c7a\u3067\u304d\u307e\u3059\u3002</p> <p>\u6df1\u5c64\u5b66\u7fd2\u306e\u300c\u6df1\u3055\u300d\u3082\u540c\u3058\u539f\u7406\u3067\u3059\u3002\u8907\u96d1\u306a\u554f\u984c\u3092\u5c64\u3054\u3068\u306b\u6bb5\u968e\u7684\u306b\u89e3\u6c7a\u3059\u308b\u3053\u3068\u3067\u3001\u5404\u5c64\u306f\u6bd4\u8f03\u7684\u5358\u7d14\u306a\u5909\u63db\u3092\u5b66\u7fd2\u3059\u308c\u3070\u3088\u304f\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u306a\u305c\u6df1\u5c64\u5b66\u7fd2\u304c\u5f37\u529b\u306a\u306e\u304b\u3001\u305d\u3057\u3066Transformer\u304c\u3069\u306e\u3088\u3046\u306b\u6df1\u3055\u3092\u6d3b\u7528\u3057\u3066\u3044\u308b\u306e\u304b\u3092\u63a2\u308a\u307e\u3059\u3002</p>"},{"location":"part2/layers-and-deep-learning/#81","title":"8.1 \u6df1\u5c64\u5b66\u7fd2\u306e\u672c\u8cea\uff1a\u8868\u73fe\u5b66\u7fd2\u306e\u968e\u5c64","text":""},{"location":"part2/layers-and-deep-learning/#vs","title":"\u6d45\u3044 vs \u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Callable\nimport math\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom matplotlib.patches import Rectangle, FancyBboxPatch\nfrom matplotlib.patches import Circle\nfrom matplotlib.lines import Line2D\n\nclass DepthVsWidthComparison:\n    \"\"\"\u6df1\u3055\u3068\u5e45\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u5b9f\u8a3c\"\"\"\n\n    def __init__(self):\n        self.input_dim = 2\n        self.output_dim = 1\n        self.hidden_dim = 64\n\n    def create_shallow_wide_network(self, width: int) -&gt; nn.Module:\n        \"\"\"\u6d45\u304f\u5e83\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\"\"\"\n        return nn.Sequential(\n            nn.Linear(self.input_dim, width),\n            nn.ReLU(),\n            nn.Linear(width, self.output_dim)\n        )\n\n    def create_deep_narrow_network(self, depth: int, width: int) -&gt; nn.Module:\n        \"\"\"\u6df1\u304f\u72ed\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\"\"\"\n        layers = []\n\n        # \u5165\u529b\u5c64\n        layers.extend([\n            nn.Linear(self.input_dim, width),\n            nn.ReLU()\n        ])\n\n        # \u96a0\u308c\u5c64\n        for _ in range(depth - 2):\n            layers.extend([\n                nn.Linear(width, width),\n                nn.ReLU()\n            ])\n\n        # \u51fa\u529b\u5c64\n        layers.append(nn.Linear(width, self.output_dim))\n\n        return nn.Sequential(*layers)\n\n    def compare_expressiveness(self):\n        \"\"\"\u8868\u73fe\u529b\u306e\u6bd4\u8f03\"\"\"\n        print(\"=== \u6df1\u3055 vs \u5e45\uff1a\u8868\u73fe\u529b\u306e\u6bd4\u8f03 ===\\n\")\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u63c3\u3048\u305f\u6bd4\u8f03\n        shallow_width = 512  # 2\u5c64\u3001\u5e45512\n        deep_width = 64      # 8\u5c64\u3001\u5e4564\n        deep_depth = 8\n\n        shallow_net = self.create_shallow_wide_network(shallow_width)\n        deep_net = self.create_deep_narrow_network(deep_depth, deep_width)\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u8a08\u7b97\n        shallow_params = sum(p.numel() for p in shallow_net.parameters())\n        deep_params = sum(p.numel() for p in deep_net.parameters())\n\n        print(f\"\u6d45\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff082\u5c64\u3001\u5e45{shallow_width}\uff09: {shallow_params:,} \u30d1\u30e9\u30e1\u30fc\u30bf\")\n        print(f\"\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08{deep_depth}\u5c64\u3001\u5e45{deep_width}\uff09: {deep_params:,} \u30d1\u30e9\u30e1\u30fc\u30bf\")\n\n        # \u8907\u96d1\u306a\u95a2\u6570\u306e\u8fd1\u4f3c\u3092\u53ef\u8996\u5316\n        self._visualize_function_approximation(shallow_net, deep_net)\n\n    def _visualize_function_approximation(self, shallow_net, deep_net):\n        \"\"\"\u95a2\u6570\u8fd1\u4f3c\u80fd\u529b\u306e\u53ef\u8996\u5316\"\"\"\n        # \u8907\u96d1\u306a\u30bf\u30fc\u30b2\u30c3\u30c8\u95a2\u6570\n        def target_function(x, y):\n            return np.sin(5*x) * np.cos(5*y) + 0.5*np.sin(10*x*y)\n\n        # \u30b0\u30ea\u30c3\u30c9\u30c7\u30fc\u30bf\n        x = np.linspace(-1, 1, 100)\n        y = np.linspace(-1, 1, 100)\n        X, Y = np.meshgrid(x, y)\n        Z_target = target_function(X, Y)\n\n        # \u8a13\u7df4\u30c7\u30fc\u30bf\n        n_samples = 1000\n        x_train = torch.rand(n_samples, 2) * 2 - 1  # [-1, 1]\u306e\u7bc4\u56f2\n        y_train = target_function(x_train[:, 0], x_train[:, 1])\n        y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n\n        # \u7c21\u6613\u7684\u306a\u5b66\u7fd2\uff08\u30c7\u30e2\u7528\uff09\n        for net, name in [(shallow_net, \"Shallow\"), (deep_net, \"Deep\")]:\n            optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n\n            for epoch in range(100):\n                optimizer.zero_grad()\n                pred = net(x_train)\n                loss = F.mse_loss(pred, y_train)\n                loss.backward()\n                optimizer.step()\n\n        # \u4e88\u6e2c\u3092\u53ef\u8996\u5316\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        # \u30bf\u30fc\u30b2\u30c3\u30c8\u95a2\u6570\n        im = axes[0].contourf(X, Y, Z_target, levels=20, cmap='RdBu')\n        axes[0].set_title('Target Function')\n        axes[0].set_xlabel('x')\n        axes[0].set_ylabel('y')\n        plt.colorbar(im, ax=axes[0])\n\n        # \u5404\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\n        for idx, (net, name) in enumerate([(shallow_net, \"Shallow Network\"), \n                                          (deep_net, \"Deep Network\")]):\n            with torch.no_grad():\n                grid_points = torch.tensor(np.stack([X.ravel(), Y.ravel()], axis=1), \n                                         dtype=torch.float32)\n                Z_pred = net(grid_points).numpy().reshape(X.shape)\n\n            im = axes[idx+1].contourf(X, Y, Z_pred, levels=20, cmap='RdBu')\n            axes[idx+1].set_title(f'{name} Approximation')\n            axes[idx+1].set_xlabel('x')\n            axes[idx+1].set_ylabel('y')\n            plt.colorbar(im, ax=axes[idx+1])\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#_3","title":"\u968e\u5c64\u7684\u7279\u5fb4\u5b66\u7fd2","text":"<pre><code>class HierarchicalFeatureLearning:\n    \"\"\"\u968e\u5c64\u7684\u306a\u7279\u5fb4\u5b66\u7fd2\u306e\u53ef\u8996\u5316\"\"\"\n\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def demonstrate_feature_hierarchy(self):\n        \"\"\"\u7279\u5fb4\u306e\u968e\u5c64\u6027\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== \u968e\u5c64\u7684\u7279\u5fb4\u5b66\u7fd2 ===\\n\")\n\n        # \u753b\u50cf\u8a8d\u8b58\u3067\u306e\u4f8b\n        print(\"\u753b\u50cf\u8a8d\u8b58\u3067\u306e\u968e\u5c64:\")\n        hierarchy = [\n            (\"Layer 1\", \"\u30a8\u30c3\u30b8\u691c\u51fa\", [\"\u7e26\u7dda\", \"\u6a2a\u7dda\", \"\u659c\u3081\u7dda\"]),\n            (\"Layer 2\", \"\u5f62\u72b6\u691c\u51fa\", [\"\u89d2\", \"\u66f2\u7dda\", \"\u5186\"]),\n            (\"Layer 3\", \"\u90e8\u54c1\u691c\u51fa\", [\"\u76ee\", \"\u9f3b\", \"\u53e3\"]),\n            (\"Layer 4\", \"\u7269\u4f53\u8a8d\u8b58\", [\"\u9854\", \"\u8eca\", \"\u5efa\u7269\"])\n        ]\n\n        for layer, description, examples in hierarchy:\n            print(f\"{layer}: {description}\")\n            print(f\"  \u4f8b: {', '.join(examples)}\")\n\n        # \u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u306e\u4f8b\n        print(\"\\n\\n\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u306e\u968e\u5c64:\")\n        nlp_hierarchy = [\n            (\"Layer 1\", \"\u6587\u5b57/\u30b5\u30d6\u30ef\u30fc\u30c9\", [\"the\", \"##ing\", \"##ed\"]),\n            (\"Layer 2\", \"\u5358\u8a9e/\u53e5\", [\"running\", \"in the park\"]),\n            (\"Layer 3\", \"\u6587\u6cd5\u69cb\u9020\", [\"\u4e3b\u8a9e-\u52d5\u8a5e-\u76ee\u7684\u8a9e\", \"\u4fee\u98fe\u95a2\u4fc2\"]),\n            (\"Layer 4\", \"\u610f\u5473/\u6587\u8108\", [\"\u611f\u60c5\", \"\u610f\u56f3\", \"\u542b\u610f\"])\n        ]\n\n        for layer, description, examples in nlp_hierarchy:\n            print(f\"{layer}: {description}\")\n            print(f\"  \u4f8b: {', '.join(examples)}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_feature_hierarchy()\n\n    def _visualize_feature_hierarchy(self):\n        \"\"\"\u7279\u5fb4\u968e\u5c64\u306e\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        # \u5c64\u306e\u8a2d\u5b9a\n        layers = [\n            {\"name\": \"Input\", \"width\": 8, \"features\": [\"Raw\", \"Pixels\"]},\n            {\"name\": \"Layer 1\", \"width\": 6, \"features\": [\"Edges\", \"Colors\"]},\n            {\"name\": \"Layer 2\", \"width\": 5, \"features\": [\"Shapes\", \"Textures\"]},\n            {\"name\": \"Layer 3\", \"width\": 4, \"features\": [\"Parts\", \"Objects\"]},\n            {\"name\": \"Layer 4\", \"width\": 3, \"features\": [\"Concepts\"]},\n            {\"name\": \"Output\", \"width\": 2, \"features\": [\"Classes\"]}\n        ]\n\n        # \u5404\u5c64\u3092\u63cf\u753b\n        y_positions = np.linspace(0, 1, len(layers))\n\n        for i, (layer, y) in enumerate(zip(layers, y_positions)):\n            # \u5c64\u306e\u77e9\u5f62\n            rect = FancyBboxPatch(\n                (0.1, y - 0.05), layer[\"width\"] * 0.1, 0.08,\n                boxstyle=\"round,pad=0.01\",\n                facecolor=plt.cm.viridis(i / len(layers)),\n                edgecolor='black',\n                alpha=0.7\n            )\n            ax.add_patch(rect)\n\n            # \u5c64\u306e\u540d\u524d\n            ax.text(-0.05, y, layer[\"name\"], \n                   verticalalignment='center', \n                   horizontalalignment='right',\n                   fontsize=12, fontweight='bold')\n\n            # \u7279\u5fb4\u306e\u4f8b\n            feature_text = \", \".join(layer[\"features\"])\n            ax.text(0.1 + layer[\"width\"] * 0.05, y, feature_text,\n                   verticalalignment='center',\n                   horizontalalignment='center',\n                   fontsize=10, color='white')\n\n            # \u5c64\u9593\u306e\u63a5\u7d9a\n            if i &lt; len(layers) - 1:\n                # \u77e2\u5370\u3067\u63a5\u7d9a\n                ax.arrow(0.1 + layer[\"width\"] * 0.1 / 2, y + 0.04,\n                        0, y_positions[i+1] - y - 0.08,\n                        head_width=0.02, head_length=0.02,\n                        fc='gray', ec='gray', alpha=0.5)\n\n        ax.set_xlim(-0.2, 1.0)\n        ax.set_ylim(-0.1, 1.1)\n        ax.set_title('\u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u968e\u5c64\u7684\u7279\u5fb4\u5b66\u7fd2', fontsize=16)\n        ax.axis('off')\n\n        # \u8aac\u660e\u6587\n        ax.text(0.5, -0.05, '\u4e0b\u4f4d\u5c64\uff1a\u5358\u7d14\u306a\u7279\u5fb4 \u2192 \u4e0a\u4f4d\u5c64\uff1a\u8907\u96d1\u306a\u6982\u5ff5',\n               horizontalalignment='center', fontsize=12, style='italic')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#82","title":"8.2 \u6b8b\u5dee\u63a5\u7d9a\uff1a\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u53ef\u80fd\u306b\u3059\u308b\u6280\u8853","text":""},{"location":"part2/layers-and-deep-learning/#_4","title":"\u52fe\u914d\u6d88\u5931\u554f\u984c\u3068\u6b8b\u5dee\u63a5\u7d9a","text":"<pre><code>class ResidualConnections:\n    \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u7406\u89e3\u3068\u5b9f\u88c5\"\"\"\n\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def demonstrate_gradient_vanishing(self):\n        \"\"\"\u52fe\u914d\u6d88\u5931\u554f\u984c\u306e\u5b9f\u8a3c\"\"\"\n        print(\"=== \u52fe\u914d\u6d88\u5931\u554f\u984c ===\\n\")\n\n        # \u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08\u6b8b\u5dee\u63a5\u7d9a\u306a\u3057\uff09\n        class DeepNetworkWithoutResidual(nn.Module):\n            def __init__(self, depth: int):\n                super().__init__()\n                self.layers = nn.ModuleList([\n                    nn.Sequential(\n                        nn.Linear(10, 10),\n                        nn.ReLU()\n                    ) for _ in range(depth)\n                ])\n\n            def forward(self, x):\n                activations = [x]\n                for layer in self.layers:\n                    x = layer(x)\n                    activations.append(x)\n                return x, activations\n\n        # \u52fe\u914d\u306e\u6d41\u308c\u3092\u6e2c\u5b9a\n        depths = [5, 10, 20, 50]\n        gradient_norms = {d: [] for d in depths}\n\n        for depth in depths:\n            net = DeepNetworkWithoutResidual(depth)\n            x = torch.randn(32, 10, requires_grad=True)\n            y, activations = net(x)\n            loss = y.sum()\n            loss.backward()\n\n            # \u5404\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0\u3092\u8a18\u9332\n            for i, layer in enumerate(net.layers):\n                grad_norm = layer[0].weight.grad.norm().item()\n                gradient_norms[depth].append(grad_norm)\n\n        # \u53ef\u8996\u5316\n        self._plot_gradient_flow(gradient_norms)\n\n    def _plot_gradient_flow(self, gradient_norms):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u53ef\u8996\u5316\"\"\"\n        plt.figure(figsize=(10, 6))\n\n        for depth, norms in gradient_norms.items():\n            plt.semilogy(range(len(norms)), norms, \n                        marker='o', label=f'Depth={depth}')\n\n        plt.xlabel('Layer Index')\n        plt.ylabel('Gradient Norm (log scale)')\n        plt.title('\u52fe\u914d\u6d88\u5931\uff1a\u6df1\u3044\u5c64\u307b\u3069\u52fe\u914d\u304c\u5c0f\u3055\u304f\u306a\u308b')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n    def explain_residual_connection(self):\n        \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u4ed5\u7d44\u307f\u3092\u8aac\u660e\"\"\"\n        print(\"\\n=== \u6b8b\u5dee\u63a5\u7d9a\u306e\u4ed5\u7d44\u307f ===\\n\")\n\n        print(\"\u901a\u5e38\u306e\u5c64\uff1a\")\n        print(\"  y = F(x)\")\n        print(\"  \u554f\u984c\uff1aF\u304c\u6052\u7b49\u5199\u50cf\u3092\u5b66\u7fd2\u3059\u308b\u306e\u306f\u56f0\u96e3\")\n\n        print(\"\\n\u6b8b\u5dee\u63a5\u7d9a\uff1a\")\n        print(\"  y = F(x) + x\")\n        print(\"  \u5229\u70b9\uff1aF(x) = 0\u3092\u5b66\u7fd2\u3059\u308c\u3070\u6052\u7b49\u5199\u50cf\u306b\u306a\u308b\")\n\n        # \u56f3\u89e3\n        self._visualize_residual_connection()\n\n    def _visualize_residual_connection(self):\n        \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u56f3\u89e3\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n        # \u901a\u5e38\u306e\u63a5\u7d9a\n        ax1.set_title('\u901a\u5e38\u306e\u63a5\u7d9a', fontsize=14)\n        ax1.text(0.5, 0.8, 'x', ha='center', va='center', \n                bbox=dict(boxstyle='round', facecolor='lightblue'), fontsize=12)\n        ax1.text(0.5, 0.5, 'F(x)', ha='center', va='center',\n                bbox=dict(boxstyle='round', facecolor='lightgreen'), fontsize=12)\n        ax1.text(0.5, 0.2, 'y = F(x)', ha='center', va='center',\n                bbox=dict(boxstyle='round', facecolor='lightcoral'), fontsize=12)\n\n        # \u77e2\u5370\n        ax1.arrow(0.5, 0.75, 0, -0.15, head_width=0.03, head_length=0.02, fc='black')\n        ax1.arrow(0.5, 0.45, 0, -0.15, head_width=0.03, head_length=0.02, fc='black')\n\n        ax1.set_xlim(0, 1)\n        ax1.set_ylim(0, 1)\n        ax1.axis('off')\n\n        # \u6b8b\u5dee\u63a5\u7d9a\n        ax2.set_title('\u6b8b\u5dee\u63a5\u7d9a', fontsize=14)\n        ax2.text(0.5, 0.8, 'x', ha='center', va='center',\n                bbox=dict(boxstyle='round', facecolor='lightblue'), fontsize=12)\n        ax2.text(0.5, 0.5, 'F(x)', ha='center', va='center',\n                bbox=dict(boxstyle='round', facecolor='lightgreen'), fontsize=12)\n        ax2.text(0.5, 0.2, 'y = F(x) + x', ha='center', va='center',\n                bbox=dict(boxstyle='round', facecolor='lightcoral'), fontsize=12)\n\n        # \u77e2\u5370\n        ax2.arrow(0.5, 0.75, 0, -0.15, head_width=0.03, head_length=0.02, fc='black')\n        ax2.arrow(0.5, 0.45, 0, -0.15, head_width=0.03, head_length=0.02, fc='black')\n\n        # \u30b9\u30ad\u30c3\u30d7\u63a5\u7d9a\n        ax2.arrow(0.35, 0.8, -0.1, -0.5, head_width=0.03, head_length=0.02, \n                 fc='red', ec='red', linestyle='--', linewidth=2)\n        ax2.text(0.2, 0.55, 'Skip\\nConnection', ha='center', va='center', \n                color='red', fontsize=10)\n\n        ax2.set_xlim(0, 1)\n        ax2.set_ylim(0, 1)\n        ax2.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def implement_residual_block(self):\n        \"\"\"\u6b8b\u5dee\u30d6\u30ed\u30c3\u30af\u306e\u5b9f\u88c5\"\"\"\n        class ResidualBlock(nn.Module):\n            def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n                super().__init__()\n                self.linear1 = nn.Linear(d_model, d_ff)\n                self.linear2 = nn.Linear(d_ff, d_model)\n                self.dropout = nn.Dropout(dropout)\n                self.activation = nn.ReLU()\n\n                # \u521d\u671f\u5316\uff1a\u6700\u521d\u306f\u6052\u7b49\u5199\u50cf\u306b\u8fd1\u3044\u72b6\u614b\n                nn.init.xavier_uniform_(self.linear1.weight, gain=0.1)\n                nn.init.xavier_uniform_(self.linear2.weight, gain=0.1)\n                nn.init.zeros_(self.linear1.bias)\n                nn.init.zeros_(self.linear2.bias)\n\n            def forward(self, x):\n                # \u6b8b\u5dee\u63a5\u7d9a\n                residual = x\n\n                # \u5909\u63db\n                out = self.linear1(x)\n                out = self.activation(out)\n                out = self.dropout(out)\n                out = self.linear2(out)\n                out = self.dropout(out)\n\n                # \u6b8b\u5dee\u3092\u52a0\u7b97\n                out = out + residual\n\n                return out\n\n        # \u52d5\u4f5c\u78ba\u8a8d\n        print(\"\\n=== \u6b8b\u5dee\u30d6\u30ed\u30c3\u30af\u306e\u5b9f\u88c5 ===\")\n\n        block = ResidualBlock(d_model=256, d_ff=1024)\n        x = torch.randn(32, 10, 256)\n\n        with torch.no_grad():\n            y = block(x)\n\n            # \u521d\u671f\u72b6\u614b\u3067\u306f\u5165\u529b\u306b\u8fd1\u3044\u51fa\u529b\n            difference = (y - x).abs().mean().item()\n            print(f\"\u5165\u529b\u3068\u51fa\u529b\u306e\u5dee\uff08\u521d\u671f\u72b6\u614b\uff09: {difference:.6f}\")\n            print(\"\u2192 \u521d\u671f\u72b6\u614b\u3067\u306f\u6052\u7b49\u5199\u50cf\u306b\u8fd1\u3044\")\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#83","title":"8.3 \u5c64\u6b63\u898f\u5316\uff1a\u5b66\u7fd2\u306e\u5b89\u5b9a\u5316","text":""},{"location":"part2/layers-and-deep-learning/#_5","title":"\u306a\u305c\u6b63\u898f\u5316\u304c\u5fc5\u8981\u304b","text":"<pre><code>class LayerNormalization:\n    \"\"\"\u5c64\u6b63\u898f\u5316\u306e\u7406\u89e3\u3068\u5b9f\u88c5\"\"\"\n\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def demonstrate_internal_covariate_shift(self):\n        \"\"\"\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306e\u5b9f\u8a3c\"\"\"\n        print(\"=== \u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8 ===\\n\")\n        print(\"\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306f\u3001\u5404\u5c64\u306e\u5165\u529b\u5206\u5e03\u304c\u5b66\u7fd2\u4e2d\u306b\u5909\u5316\")\n        print(\"\u2192 \u4e0b\u4f4d\u5c64\u306e\u66f4\u65b0\u304c\u4e0a\u4f4d\u5c64\u306e\u5165\u529b\u5206\u5e03\u3092\u5909\u3048\u308b\")\n        print(\"\u2192 \u5b66\u7fd2\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\\n\")\n\n        # \u30b7\u30f3\u30d7\u30eb\u306a\u5b9f\u9a13\n        class UnstableNetwork(nn.Module):\n            def __init__(self, use_norm: bool = False):\n                super().__init__()\n                self.use_norm = use_norm\n                self.layers = nn.ModuleList()\n\n                for i in range(10):\n                    self.layers.append(nn.Linear(64, 64))\n                    if use_norm:\n                        self.layers.append(nn.LayerNorm(64))\n                    self.layers.append(nn.ReLU())\n\n            def forward(self, x):\n                activations_stats = []\n\n                for layer in self.layers:\n                    x = layer(x)\n\n                    # \u6d3b\u6027\u5316\u306e\u7d71\u8a08\u3092\u8a18\u9332\n                    if isinstance(layer, nn.ReLU):\n                        mean = x.mean().item()\n                        std = x.std().item()\n                        activations_stats.append((mean, std))\n\n                return x, activations_stats\n\n        # \u6bd4\u8f03\n        net_without_norm = UnstableNetwork(use_norm=False)\n        net_with_norm = UnstableNetwork(use_norm=True)\n\n        # \u30e9\u30f3\u30c0\u30e0\u5165\u529b\n        x = torch.randn(32, 64)\n\n        # \u5404\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u3059\n        _, stats_without = net_without_norm(x)\n        _, stats_with = net_with_norm(x)\n\n        # \u53ef\u8996\u5316\n        self._plot_activation_statistics(stats_without, stats_with)\n\n    def _plot_activation_statistics(self, stats_without, stats_with):\n        \"\"\"\u6d3b\u6027\u5316\u7d71\u8a08\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        layers = range(len(stats_without))\n\n        # \u5e73\u5747\n        means_without = [s[0] for s in stats_without]\n        means_with = [s[0] for s in stats_with]\n\n        ax1.plot(layers, means_without, 'r-o', label='Without LayerNorm')\n        ax1.plot(layers, means_with, 'b-o', label='With LayerNorm')\n        ax1.set_xlabel('Layer')\n        ax1.set_ylabel('Mean Activation')\n        ax1.set_title('\u6d3b\u6027\u5316\u306e\u5e73\u5747')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n\n        # \u6a19\u6e96\u504f\u5dee\n        stds_without = [s[1] for s in stats_without]\n        stds_with = [s[1] for s in stats_with]\n\n        ax2.plot(layers, stds_without, 'r-o', label='Without LayerNorm')\n        ax2.plot(layers, stds_with, 'b-o', label='With LayerNorm')\n        ax2.set_xlabel('Layer')\n        ax2.set_ylabel('Std Activation')\n        ax2.set_title('\u6d3b\u6027\u5316\u306e\u6a19\u6e96\u504f\u5dee')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n    def explain_layer_norm_vs_batch_norm(self):\n        \"\"\"LayerNorm vs BatchNorm\u306e\u8aac\u660e\"\"\"\n        print(\"\\n=== LayerNorm vs BatchNorm ===\\n\")\n\n        # \u9055\u3044\u3092\u53ef\u8996\u5316\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u30c7\u30fc\u30bf\u306e\u5f62\u72b6\u3092\u793a\u3059\n        batch_size, seq_len, d_model = 4, 6, 8\n\n        # BatchNorm\n        ax1.set_title('Batch Normalization', fontsize=14)\n        for i in range(batch_size):\n            for j in range(seq_len):\n                rect = Rectangle((j, i), 1, 1, \n                               facecolor=plt.cm.Blues((i+j)%2 * 0.3 + 0.3),\n                               edgecolor='black', linewidth=0.5)\n                ax1.add_patch(rect)\n\n        # \u6b63\u898f\u5316\u306e\u65b9\u5411\u3092\u793a\u3059\n        ax1.arrow(seq_len/2, -0.5, 0, batch_size + 0.5, \n                 head_width=0.3, head_length=0.2, \n                 fc='red', ec='red', linewidth=2)\n        ax1.text(seq_len/2 + 0.5, batch_size/2, 'Normalize\\nacross batch',\n                ha='left', va='center', color='red', fontsize=12)\n\n        ax1.set_xlim(0, seq_len)\n        ax1.set_ylim(-1, batch_size)\n        ax1.set_xlabel('Sequence Position')\n        ax1.set_ylabel('Batch')\n        ax1.invert_yaxis()\n\n        # LayerNorm\n        ax2.set_title('Layer Normalization', fontsize=14)\n        for i in range(batch_size):\n            for j in range(seq_len):\n                rect = Rectangle((j, i), 1, 1,\n                               facecolor=plt.cm.Greens((i+j)%2 * 0.3 + 0.3),\n                               edgecolor='black', linewidth=0.5)\n                ax2.add_patch(rect)\n\n        # \u6b63\u898f\u5316\u306e\u65b9\u5411\u3092\u793a\u3059\n        for i in range(batch_size):\n            ax2.arrow(-0.5, i + 0.5, seq_len + 0.5, 0,\n                     head_width=0.2, head_length=0.2,\n                     fc='blue', ec='blue', linewidth=1, alpha=0.7)\n\n        ax2.text(seq_len + 0.5, batch_size/2, 'Normalize\\nacross features',\n                ha='left', va='center', color='blue', fontsize=12)\n\n        ax2.set_xlim(-1, seq_len + 1)\n        ax2.set_ylim(-1, batch_size)\n        ax2.set_xlabel('Sequence Position')\n        ax2.set_ylabel('Batch')\n        ax2.invert_yaxis()\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"BatchNorm: \u30d0\u30c3\u30c1\u6b21\u5143\u3067\u6b63\u898f\u5316\uff08\u540c\u3058\u4f4d\u7f6e\u306e\u7570\u306a\u308b\u30b5\u30f3\u30d7\u30eb\uff09\")\n        print(\"  - \u5229\u70b9\uff1a\u30d0\u30c3\u30c1\u7d71\u8a08\u3092\u4f7f\u3063\u305f\u5f37\u529b\u306a\u6b63\u898f\u5316\")\n        print(\"  - \u6b20\u70b9\uff1a\u63a8\u8ad6\u6642\u306b\u30d0\u30c3\u30c1\u7d71\u8a08\u304c\u5fc5\u8981\u3001\u53ef\u5909\u9577\u7cfb\u5217\u3067\u554f\u984c\")\n\n        print(\"\\nLayerNorm: \u7279\u5fb4\u6b21\u5143\u3067\u6b63\u898f\u5316\uff08\u540c\u3058\u30b5\u30f3\u30d7\u30eb\u306e\u5168\u7279\u5fb4\uff09\")\n        print(\"  - \u5229\u70b9\uff1a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044\u3001\u7cfb\u5217\u9577\u306b\u67d4\u8edf\")\n        print(\"  - \u6b20\u70b9\uff1a\u30d0\u30c3\u30c1\u60c5\u5831\u3092\u6d3b\u7528\u3067\u304d\u306a\u3044\")\n        print(\"  - Transformer\u306b\u6700\u9069\uff01\")\n\n    def implement_layer_norm(self):\n        \"\"\"LayerNorm\u306e\u5b9f\u88c5\"\"\"\n        class CustomLayerNorm(nn.Module):\n            def __init__(self, d_model: int, eps: float = 1e-6):\n                super().__init__()\n                self.d_model = d_model\n                self.eps = eps\n\n                # \u5b66\u7fd2\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\n                self.gamma = nn.Parameter(torch.ones(d_model))\n                self.beta = nn.Parameter(torch.zeros(d_model))\n\n            def forward(self, x):\n                # x: [batch_size, seq_len, d_model]\n\n                # \u7279\u5fb4\u6b21\u5143\u3067\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\n                mean = x.mean(dim=-1, keepdim=True)\n                var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n                # \u6b63\u898f\u5316\n                x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n\n                # \u30b9\u30b1\u30fc\u30eb\u3068\u30b7\u30d5\u30c8\n                out = self.gamma * x_normalized + self.beta\n\n                return out\n\n        # \u52d5\u4f5c\u78ba\u8a8d\n        print(\"\\n=== LayerNorm\u306e\u5b9f\u88c5 ===\")\n\n        layer_norm = CustomLayerNorm(d_model=256)\n        x = torch.randn(32, 10, 256) * 5 + 2  # \u5e73\u57472\u3001\u6a19\u6e96\u504f\u5dee5\n\n        with torch.no_grad():\n            y = layer_norm(x)\n\n            print(f\"\u5165\u529b: mean={x.mean():.2f}, std={x.std():.2f}\")\n            print(f\"\u51fa\u529b: mean={y.mean():.2f}, std={y.std():.2f}\")\n\n            # \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7d71\u8a08\n            sample_mean = y[0].mean(dim=-1)\n            sample_std = y[0].std(dim=-1)\n            print(f\"\\n\u30b5\u30f3\u30d7\u30eb0\u306e\u5404\u4f4d\u7f6e\u3067\u306e\u7d71\u8a08:\")\n            print(f\"  \u5e73\u5747: {sample_mean[:5].tolist()}\")\n            print(f\"  \u6a19\u6e96\u504f\u5dee: {sample_std[:5].tolist()}\")\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#84-transformer","title":"8.4 Transformer\u30d6\u30ed\u30c3\u30af\uff1a\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u7d71\u5408","text":""},{"location":"part2/layers-and-deep-learning/#transformer","title":"\u6a19\u6e96\u7684\u306aTransformer\u30d6\u30ed\u30c3\u30af","text":"<pre><code>class TransformerBlock:\n    \"\"\"Transformer\u30d6\u30ed\u30c3\u30af\u306e\u5b8c\u5168\u306a\u5b9f\u88c5\"\"\"\n\n    def __init__(self, \n                 d_model: int = 512,\n                 n_heads: int = 8,\n                 d_ff: int = 2048,\n                 dropout: float = 0.1):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_ff = d_ff\n        self.dropout = dropout\n\n    def create_transformer_block(self) -&gt; nn.Module:\n        \"\"\"\u6a19\u6e96\u7684\u306aTransformer\u30d6\u30ed\u30c3\u30af\u3092\u4f5c\u6210\"\"\"\n\n        class StandardTransformerBlock(nn.Module):\n            def __init__(self, d_model, n_heads, d_ff, dropout):\n                super().__init__()\n\n                # Multi-Head Attention\n                self.self_attention = nn.MultiheadAttention(\n                    d_model, n_heads, dropout=dropout, batch_first=True\n                )\n\n                # Feed-Forward Network\n                self.feed_forward = nn.Sequential(\n                    nn.Linear(d_model, d_ff),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(d_ff, d_model),\n                    nn.Dropout(dropout)\n                )\n\n                # Layer Normalization\n                self.norm1 = nn.LayerNorm(d_model)\n                self.norm2 = nn.LayerNorm(d_model)\n\n                # Dropout\n                self.dropout = nn.Dropout(dropout)\n\n            def forward(self, x, mask=None):\n                # 1. Self-Attention with Residual\n                attn_output, _ = self.self_attention(x, x, x, attn_mask=mask)\n                x = self.norm1(x + self.dropout(attn_output))\n\n                # 2. Feed-Forward with Residual\n                ff_output = self.feed_forward(x)\n                x = self.norm2(x + ff_output)\n\n                return x\n\n        return StandardTransformerBlock(\n            self.d_model, self.n_heads, self.d_ff, self.dropout\n        )\n\n    def visualize_transformer_block(self):\n        \"\"\"Transformer\u30d6\u30ed\u30c3\u30af\u306e\u69cb\u9020\u3092\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 12))\n\n        # \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u4f4d\u7f6e\n        components = [\n            {\"name\": \"Input\", \"y\": 0, \"color\": \"lightblue\"},\n            {\"name\": \"Multi-Head\\nAttention\", \"y\": 0.15, \"color\": \"lightgreen\"},\n            {\"name\": \"Add &amp; Norm\", \"y\": 0.25, \"color\": \"lightyellow\"},\n            {\"name\": \"Feed Forward\", \"y\": 0.40, \"color\": \"lightcoral\"},\n            {\"name\": \"Add &amp; Norm\", \"y\": 0.50, \"color\": \"lightyellow\"},\n            {\"name\": \"Output\", \"y\": 0.65, \"color\": \"lightblue\"}\n        ]\n\n        # \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u63cf\u753b\n        box_width = 0.3\n        box_height = 0.08\n\n        for comp in components:\n            # \u30dc\u30c3\u30af\u30b9\n            rect = FancyBboxPatch(\n                (0.5 - box_width/2, comp[\"y\"] - box_height/2),\n                box_width, box_height,\n                boxstyle=\"round,pad=0.02\",\n                facecolor=comp[\"color\"],\n                edgecolor='black',\n                linewidth=2\n            )\n            ax.add_patch(rect)\n\n            # \u30c6\u30ad\u30b9\u30c8\n            ax.text(0.5, comp[\"y\"], comp[\"name\"],\n                   ha='center', va='center',\n                   fontsize=12, fontweight='bold')\n\n        # \u63a5\u7d9a\u7dda\n        connections = [\n            (0, 1, \"straight\"),      # Input \u2192 Attention\n            (1, 2, \"straight\"),      # Attention \u2192 Add&amp;Norm\n            (2, 3, \"straight\"),      # Add&amp;Norm \u2192 FF\n            (3, 4, \"straight\"),      # FF \u2192 Add&amp;Norm\n            (4, 5, \"straight\"),      # Add&amp;Norm \u2192 Output\n            (0, 2, \"residual\"),      # Input \u2192 Add&amp;Norm (residual)\n            (2, 4, \"residual\")       # Add&amp;Norm \u2192 Add&amp;Norm (residual)\n        ]\n\n        for start_idx, end_idx, conn_type in connections:\n            start_y = components[start_idx][\"y\"] + box_height/2\n            end_y = components[end_idx][\"y\"] - box_height/2\n\n            if conn_type == \"straight\":\n                ax.arrow(0.5, start_y, 0, end_y - start_y - 0.01,\n                        head_width=0.02, head_length=0.01,\n                        fc='black', ec='black')\n            else:  # residual\n                # \u66f2\u7dda\u3067\u6b8b\u5dee\u63a5\u7d9a\u3092\u8868\u73fe\n                x_offset = 0.15 if start_idx == 0 else -0.15\n                ax.annotate('', xy=(0.5 + x_offset, end_y),\n                           xytext=(0.5 + x_offset, start_y),\n                           arrowprops=dict(arrowstyle='-&gt;',\n                                         connectionstyle=\"arc3,rad=0\",\n                                         color='red', lw=2))\n\n        # \u6b8b\u5dee\u63a5\u7d9a\u306e\u30e9\u30d9\u30eb\n        ax.text(0.7, 0.125, 'Residual', color='red', fontsize=10, rotation=90)\n        ax.text(0.3, 0.325, 'Residual', color='red', fontsize=10, rotation=90)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(-0.1, 0.75)\n        ax.set_title('Transformer Block Architecture', fontsize=16)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def explain_component_roles(self):\n        \"\"\"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5f79\u5272\u3092\u8aac\u660e\"\"\"\n        print(\"=== Transformer\u30d6\u30ed\u30c3\u30af\u306e\u5404\u8981\u7d20 ===\\n\")\n\n        components = {\n            \"Multi-Head Attention\": {\n                \"\u5f79\u5272\": \"\u6587\u8108\u60c5\u5831\u306e\u7d71\u5408\",\n                \"\u8a73\u7d30\": \"\u5404\u4f4d\u7f6e\u304c\u4ed6\u306e\u3059\u3079\u3066\u306e\u4f4d\u7f6e\u306e\u60c5\u5831\u3092\u53c2\u7167\",\n                \"\u306a\u305c\u5fc5\u8981\": \"\u9577\u8ddd\u96e2\u4f9d\u5b58\u95a2\u4fc2\u306e\u6355\u6349\"\n            },\n            \"Feed-Forward Network\": {\n                \"\u5f79\u5272\": \"\u4f4d\u7f6e\u3054\u3068\u306e\u975e\u7dda\u5f62\u5909\u63db\",\n                \"\u8a73\u7d30\": \"\u5404\u4f4d\u7f6e\u3067\u72ec\u7acb\u306b\u9069\u7528\u3055\u308c\u308b2\u5c64MLP\",\n                \"\u306a\u305c\u5fc5\u8981\": \"\u8868\u73fe\u529b\u306e\u5411\u4e0a\"\n            },\n            \"Residual Connection\": {\n                \"\u5f79\u5272\": \"\u52fe\u914d\u306e\u6d41\u308c\u3092\u6539\u5584\",\n                \"\u8a73\u7d30\": \"\u5165\u529b\u3092\u51fa\u529b\u306b\u76f4\u63a5\u52a0\u7b97\",\n                \"\u306a\u305c\u5fc5\u8981\": \"\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b66\u7fd2\u3092\u5b89\u5b9a\u5316\"\n            },\n            \"Layer Normalization\": {\n                \"\u5f79\u5272\": \"\u6d3b\u6027\u5316\u306e\u6b63\u898f\u5316\",\n                \"\u8a73\u7d30\": \"\u5404\u4f4d\u7f6e\u3067\u7279\u5fb4\u3092\u6b63\u898f\u5316\",\n                \"\u306a\u305c\u5fc5\u8981\": \"\u5b66\u7fd2\u306e\u5b89\u5b9a\u5316\u3068\u9ad8\u901f\u5316\"\n            }\n        }\n\n        for name, info in components.items():\n            print(f\"{name}:\")\n            print(f\"  \u5f79\u5272: {info['\u5f79\u5272']}\")\n            print(f\"  \u8a73\u7d30: {info['\u8a73\u7d30']}\")\n            print(f\"  \u306a\u305c\u5fc5\u8981: {info['\u306a\u305c\u5fc5\u8981']}\")\n            print()\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#pre-ln-vs-post-ln","title":"Pre-LN vs Post-LN","text":"<pre><code>class NormalizationVariants:\n    \"\"\"\u6b63\u898f\u5316\u306e\u914d\u7f6e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\"\"\"\n\n    def compare_pre_ln_post_ln(self):\n        \"\"\"Pre-LN vs Post-LN\u306e\u6bd4\u8f03\"\"\"\n        print(\"=== Pre-LN vs Post-LN ===\\n\")\n\n        class PostLNBlock(nn.Module):\n            \"\"\"\u30aa\u30ea\u30b8\u30ca\u30eb\u306ePost-LN\u69cb\u6210\"\"\"\n            def __init__(self, d_model):\n                super().__init__()\n                self.attention = nn.MultiheadAttention(d_model, 8, batch_first=True)\n                self.norm1 = nn.LayerNorm(d_model)\n                self.feed_forward = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Linear(d_model * 4, d_model)\n                )\n                self.norm2 = nn.LayerNorm(d_model)\n\n            def forward(self, x):\n                # Attention \u2192 Add \u2192 Norm\n                attn_out, _ = self.attention(x, x, x)\n                x = self.norm1(x + attn_out)\n\n                # FF \u2192 Add \u2192 Norm\n                ff_out = self.feed_forward(x)\n                x = self.norm2(x + ff_out)\n\n                return x\n\n        class PreLNBlock(nn.Module):\n            \"\"\"\u6539\u826f\u3055\u308c\u305fPre-LN\u69cb\u6210\"\"\"\n            def __init__(self, d_model):\n                super().__init__()\n                self.norm1 = nn.LayerNorm(d_model)\n                self.attention = nn.MultiheadAttention(d_model, 8, batch_first=True)\n                self.norm2 = nn.LayerNorm(d_model)\n                self.feed_forward = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Linear(d_model * 4, d_model)\n                )\n\n            def forward(self, x):\n                # Norm \u2192 Attention \u2192 Add\n                attn_out, _ = self.attention(self.norm1(x), self.norm1(x), self.norm1(x))\n                x = x + attn_out\n\n                # Norm \u2192 FF \u2192 Add\n                ff_out = self.feed_forward(self.norm2(x))\n                x = x + ff_out\n\n                return x\n\n        # \u6bd4\u8f03\u8868\n        comparison = {\n            \"Post-LN\": {\n                \"\u5229\u70b9\": [\"\u30aa\u30ea\u30b8\u30ca\u30eb\u8ad6\u6587\u306e\u69cb\u6210\", \"\u7406\u8ad6\u7684\u306b\u89e3\u6790\u3057\u3084\u3059\u3044\"],\n                \"\u6b20\u70b9\": [\"\u6df1\u3044\u30e2\u30c7\u30eb\u3067\u4e0d\u5b89\u5b9a\", \"Warmup\u304c\u5fc5\u9808\"],\n                \"\u5f0f\": \"LN(x + Sublayer(x))\"\n            },\n            \"Pre-LN\": {\n                \"\u5229\u70b9\": [\"\u5b66\u7fd2\u304c\u5b89\u5b9a\", \"Warmup\u4e0d\u8981\", \"\u3088\u308a\u6df1\u3044\u30e2\u30c7\u30eb\u304c\u53ef\u80fd\"],\n                \"\u6b20\u70b9\": [\"\u6700\u7d42\u5c64\u306b\u8ffd\u52a0\u306eLN\u304c\u5fc5\u8981\"],\n                \"\u5f0f\": \"x + Sublayer(LN(x))\"\n            }\n        }\n\n        for variant, props in comparison.items():\n            print(f\"{variant}:\")\n            print(f\"  \u5f0f: {props['\u5f0f']}\")\n            print(f\"  \u5229\u70b9: {', '.join(props['\u5229\u70b9'])}\")\n            print(f\"  \u6b20\u70b9: {', '.join(props['\u6b20\u70b9'])}\")\n            print()\n\n        # \u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\u3092\u53ef\u8996\u5316\n        self._visualize_training_stability()\n\n    def _visualize_training_stability(self):\n        \"\"\"\u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\u3092\u53ef\u8996\u5316\"\"\"\n        # \u4eee\u60f3\u7684\u306a\u5b66\u7fd2\u66f2\u7dda\n        epochs = np.arange(100)\n\n        # Post-LN: \u4e0d\u5b89\u5b9a\u306a\u521d\u671f\n        post_ln_loss = np.exp(-epochs / 20) * (1 + 0.3 * np.sin(epochs / 5)) + 0.1\n        post_ln_loss[:10] = post_ln_loss[:10] * (1 + np.random.randn(10) * 0.2)\n\n        # Pre-LN: \u5b89\u5b9a\n        pre_ln_loss = np.exp(-epochs / 20) * (1 + 0.1 * np.sin(epochs / 5)) + 0.1\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(epochs, post_ln_loss, 'r-', label='Post-LN', linewidth=2)\n        plt.plot(epochs, pre_ln_loss, 'b-', label='Pre-LN', linewidth=2)\n\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('\u5b66\u7fd2\u306e\u5b89\u5b9a\u6027: Pre-LN vs Post-LN')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.ylim(0, 2)\n\n        # Warmup\u671f\u9593\u3092\u793a\u3059\n        plt.axvspan(0, 10, alpha=0.2, color='gray')\n        plt.text(5, 1.8, 'Warmup\u671f\u9593', ha='center', fontsize=12)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#85","title":"8.5 \u6df1\u3055\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247","text":""},{"location":"part2/layers-and-deep-learning/#_6","title":"\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3068\u6027\u80fd\u306e\u95a2\u4fc2","text":"<pre><code>class ScalingLaws:\n    \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u306e\u7406\u89e3\"\"\"\n\n    def demonstrate_scaling_laws(self):\n        \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u306e\u5b9f\u8a3c\"\"\"\n        print(\"=== \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247 ===\\n\")\n        print(\"Kaplan et al. (2020)\u306e\u767a\u898b:\")\n        print(\"Loss \u221d N^(-\u03b1)\")\n        print(\"  N: \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\")\n        print(\"  \u03b1 \u2248 0.076\")\n\n        # \u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3068\u6027\u80fd\u306e\u95a2\u4fc2\n        model_sizes = np.logspace(6, 11, 50)  # 1M to 100B parameters\n\n        # \u7406\u8ad6\u7684\u306a\u640d\u5931\u66f2\u7dda\n        alpha = 0.076\n        loss = 10 * model_sizes ** (-alpha)\n\n        # Chinchilla\u6700\u9069\u5316\n        chinchilla_optimal_data = model_sizes * 20  # 20\u30c8\u30fc\u30af\u30f3/\u30d1\u30e9\u30e1\u30fc\u30bf\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u640d\u5931 vs \u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\n        ax1.loglog(model_sizes, loss, 'b-', linewidth=2)\n\n        # \u5b9f\u969b\u306e\u30e2\u30c7\u30eb\u3092\u30d7\u30ed\u30c3\u30c8\n        real_models = {\n            'GPT-2': (1.5e9, 3.5),\n            'GPT-3': (175e9, 2.5),\n            'PaLM': (540e9, 2.2),\n            'GPT-4': (1e12, 1.8)  # \u63a8\u5b9a\n        }\n\n        for name, (size, loss_val) in real_models.items():\n            ax1.scatter(size, loss_val, s=100, label=name)\n            ax1.annotate(name, (size, loss_val), xytext=(5, 5), \n                        textcoords='offset points')\n\n        ax1.set_xlabel('Model Parameters')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247: Loss vs Model Size')\n        ax1.grid(True, alpha=0.3)\n        ax1.legend()\n\n        # \u30c7\u30fc\u30bf\u91cf\u306e\u6700\u9069\u914d\u5206\n        ax2.loglog(model_sizes, chinchilla_optimal_data, 'g-', linewidth=2)\n        ax2.fill_between(model_sizes, chinchilla_optimal_data * 0.5, \n                        chinchilla_optimal_data * 2, alpha=0.3, color='green')\n\n        ax2.set_xlabel('Model Parameters')\n        ax2.set_ylabel('Training Tokens')\n        ax2.set_title('Chinchilla\u6700\u9069\u5316: \u30c7\u30fc\u30bf\u91cf\u306e\u914d\u5206')\n        ax2.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"\\n\u91cd\u8981\u306a\u6d1e\u5bdf:\")\n        print(\"1. \u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u309210\u500d\u306b\u3059\u308b\u3068\u3001\u640d\u5931\u306f\u7d042\u500d\u6539\u5584\")\n        print(\"2. \u30c7\u30fc\u30bf\u91cf\u3082\u6bd4\u4f8b\u3057\u3066\u5897\u3084\u3059\u5fc5\u8981\u304c\u3042\u308b\uff08Chinchilla\uff09\")\n        print(\"3. \u8a08\u7b97\u91cf\u306f model_size \u00d7 data_size \u306b\u6bd4\u4f8b\")\n</code></pre>"},{"location":"part2/layers-and-deep-learning/#_7","title":"\u307e\u3068\u3081\uff1a\u6df1\u5c64\u5b66\u7fd2\u306e\u529b\u3092\u5f15\u304d\u51fa\u3059","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060\u6df1\u5c64\u5b66\u7fd2\u306e\u91cd\u8981\u306a\u8981\u7d20\uff1a</p> <ol> <li>\u6df1\u3055\u306e\u4fa1\u5024\uff1a</li> <li>\u968e\u5c64\u7684\u306a\u7279\u5fb4\u5b66\u7fd2</li> <li>\u6307\u6570\u7684\u306a\u8868\u73fe\u529b</li> <li> <p>\u5404\u5c64\u304c\u6271\u3044\u3084\u3059\u3044\u5909\u63db\u3092\u5b66\u7fd2</p> </li> <li> <p>\u5b89\u5b9a\u5316\u6280\u8853\uff1a</p> </li> <li>\u6b8b\u5dee\u63a5\u7d9a\uff1a\u52fe\u914d\u306e\u9ad8\u901f\u9053\u8def</li> <li>\u5c64\u6b63\u898f\u5316\uff1a\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306e\u6291\u5236</li> <li> <p>\u9069\u5207\u306a\u521d\u671f\u5316\uff1a\u5b66\u7fd2\u306e\u51fa\u767a\u70b9</p> </li> <li> <p>Transformer\u30d6\u30ed\u30c3\u30af\uff1a</p> </li> <li>\u3059\u3079\u3066\u306e\u8981\u7d20\u306e\u8abf\u548c</li> <li>Pre-LN\u69cb\u6210\u306b\u3088\u308b\u5b89\u5b9a\u6027</li> <li> <p>\u30b9\u30b1\u30fc\u30e9\u30d3\u30ea\u30c6\u30a3</p> </li> <li> <p>\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff1a</p> </li> <li>\u3088\u308a\u5927\u304d\u306a\u30e2\u30c7\u30eb = \u3088\u308a\u826f\u3044\u6027\u80fd</li> <li>\u305f\u3060\u3057\u3001\u30c7\u30fc\u30bf\u3068\u8a08\u7b97\u3082\u6bd4\u4f8b\u3057\u3066\u5fc5\u8981</li> </ol> <p>\u3053\u308c\u3089\u306e\u6280\u8853\u306b\u3088\u308a\u3001Transformer\u306f100\u5c64\u4ee5\u4e0a\u306e\u6df1\u3055\u3067\u3082\u5b89\u5b9a\u3057\u3066\u5b66\u7fd2\u3067\u304d\u3001\u9a5a\u7570\u7684\u306a\u6027\u80fd\u3092\u767a\u63ee\u3057\u307e\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3044\u3088\u3044\u3088\u3053\u308c\u3089\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u5b8c\u5168\u306aTransformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part2/layers-and-deep-learning/#_8","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1a\u6b8b\u5dee\u63a5\u7d9a\u3042\u308a\u3068\u306a\u3057\u306720\u5c64\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a13\u7df4\u3057\u3001\u52fe\u914d\u306e\u6d41\u308c\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1aPre-LN\u3068Post-LN\u306e\u69cb\u6210\u3067\u3001\u5c64\u3092\u91cd\u306d\u305f\u3068\u304d\u306e\u6d3b\u6027\u5316\u306e\u5206\u6563\u304c\u3069\u3046\u5909\u5316\u3059\u308b\u304b\u8abf\u3079\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u8a2d\u8a08\u8ab2\u984c\uff1a\u65b0\u3057\u3044\u6b63\u898f\u5316\u624b\u6cd5\uff08\u4f8b\uff1aRMSNorm\uff09\u3092\u5b9f\u88c5\u3057\u3001LayerNorm\u3068\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1a\u306a\u305c\u6b8b\u5dee\u63a5\u7d9a\u304c\u6052\u7b49\u5199\u50cf\u306e\u5b66\u7fd2\u3092\u5bb9\u6613\u306b\u3059\u308b\u304b\u3001\u6570\u5b66\u7684\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u7b2c3\u90e8\uff1aTransformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u8a73\u89e3\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part2/positional-encoding/","title":"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":""},{"location":"part2/positional-encoding/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u9806\u5e8f\u306e\u554f\u984c","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u306f\u3001\u30c8\u30fc\u30af\u30f3\u306e\u9806\u5e8f\u304c\u6c7a\u5b9a\u7684\u306b\u91cd\u8981\u3067\u3059\u3002<code>a = b</code> \u3068 <code>b = a</code> \u306f\u5168\u304f\u7570\u306a\u308b\u610f\u5473\u3092\u6301\u3061\u307e\u3059\u3002\u30d1\u30fc\u30b5\u30fc\u306f\u5404\u30c8\u30fc\u30af\u30f3\u306e\u4f4d\u7f6e\u3092\u5e38\u306b\u8ffd\u8de1\u3057\u3001\u6587\u6cd5\u898f\u5247\u306b\u5f93\u3063\u3066\u69cb\u6587\u6728\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002</p> <p>\u3057\u304b\u3057\u3001\u524d\u7ae0\u3067\u5b66\u3093\u3060\u6ce8\u610f\u6a5f\u69cb\u306b\u306f\u91cd\u5927\u306a\u554f\u984c\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u308c\u306f\u4f4d\u7f6e\u4e0d\u5909\u6027\u3067\u3059\u3002Self-Attention\u306f\u672c\u8cea\u7684\u306b\u96c6\u5408\u6f14\u7b97\u3067\u3042\u308a\u3001\u5165\u529b\u306e\u9806\u5e8f\u3092\u8003\u616e\u3057\u307e\u305b\u3093\u3002\u3053\u308c\u306f\u300c\u4e26\u5217\u51e6\u7406\u53ef\u80fd\u300d\u3068\u3044\u3046\u5229\u70b9\u306e\u88cf\u8fd4\u3057\u3067\u3082\u3042\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001Transformer\u304c\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u4f4d\u7f6e\u60c5\u5831\u3092\u6271\u3044\u3001\u9806\u5e8f\u3092\u7406\u89e3\u3059\u308b\u306e\u304b\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part2/positional-encoding/#71","title":"7.1 \u306a\u305c\u4f4d\u7f6e\u60c5\u5831\u304c\u5fc5\u8981\u304b","text":""},{"location":"part2/positional-encoding/#_3","title":"\u4f4d\u7f6e\u4e0d\u5909\u6027\u306e\u5b9f\u8a3c","text":"<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional\nimport math\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as mpatches\n\nclass PositionalProblemDemo:\n    \"\"\"\u4f4d\u7f6e\u60c5\u5831\u306e\u5fc5\u8981\u6027\u3092\u5b9f\u8a3c\"\"\"\n\n    def __init__(self):\n        self.d_model = 64\n        self.attention = self._create_simple_attention()\n\n    def _create_simple_attention(self):\n        \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u81ea\u5df1\u6ce8\u610f\u5c64\"\"\"\n        class SimpleAttention(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.d_model = d_model\n                self.W_q = nn.Linear(d_model, d_model)\n                self.W_k = nn.Linear(d_model, d_model)\n                self.W_v = nn.Linear(d_model, d_model)\n\n            def forward(self, x):\n                Q = self.W_q(x)\n                K = self.W_k(x)\n                V = self.W_v(x)\n\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n                weights = torch.softmax(scores, dim=-1)\n                output = torch.matmul(weights, V)\n\n                return output, weights\n\n        return SimpleAttention(self.d_model)\n\n    def demonstrate_position_blindness(self):\n        \"\"\"\u4f4d\u7f6e\u306e\u7121\u8996\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== \u4f4d\u7f6e\u4e0d\u5909\u6027\u306e\u554f\u984c ===\\n\")\n\n        # \u540c\u3058\u5358\u8a9e\u3001\u7570\u306a\u308b\u9806\u5e8f\n        sentences = [\n            [\"The\", \"cat\", \"ate\", \"the\", \"fish\"],\n            [\"The\", \"fish\", \"ate\", \"the\", \"cat\"],\n            [\"ate\", \"The\", \"cat\", \"the\", \"fish\"]  # \u6587\u6cd5\u7684\u306b\u5909\u3060\u304c\u3001\u30c7\u30e2\u306e\u305f\u3081\n        ]\n\n        # \u5358\u8a9e\u306e\u57cb\u3081\u8fbc\u307f\uff08\u4eee\u60f3\u7684\uff09\n        word_embeddings = {\n            \"The\": torch.randn(1, self.d_model),\n            \"the\": torch.randn(1, self.d_model),  # \u5927\u6587\u5b57\u5c0f\u6587\u5b57\u306f\u533a\u5225\n            \"cat\": torch.randn(1, self.d_model),\n            \"fish\": torch.randn(1, self.d_model),\n            \"ate\": torch.randn(1, self.d_model)\n        }\n\n        outputs = []\n\n        for sent_idx, sentence in enumerate(sentences):\n            # \u6587\u306e\u57cb\u3081\u8fbc\u307f\u3092\u4f5c\u6210\n            embeddings = []\n            for word in sentence:\n                embeddings.append(word_embeddings[word])\n\n            x = torch.cat(embeddings, dim=0).unsqueeze(0)  # [1, seq_len, d_model]\n\n            # \u81ea\u5df1\u6ce8\u610f\u3092\u9069\u7528\n            with torch.no_grad():\n                output, weights = self.attention(x)\n\n            outputs.append(output)\n\n            print(f\"\u6587{sent_idx + 1}: {' '.join(sentence)}\")\n            print(f\"\u51fa\u529b\u306e\u5e73\u5747: {output.mean().item():.4f}\")\n            print(f\"\u51fa\u529b\u306e\u6a19\u6e96\u504f\u5dee: {output.std().item():.4f}\\n\")\n\n        # \u51fa\u529b\u306e\u985e\u4f3c\u6027\u3092\u8a08\u7b97\n        print(\"\u51fa\u529b\u9593\u306e\u985e\u4f3c\u6027:\")\n        for i in range(len(outputs)):\n            for j in range(i + 1, len(outputs)):\n                similarity = torch.cosine_similarity(\n                    outputs[i].flatten(), \n                    outputs[j].flatten(), \n                    dim=0\n                ).item()\n                print(f\"\u6587{i+1} vs \u6587{j+1}: {similarity:.4f}\")\n\n        print(\"\\n\u2192 \u9806\u5e8f\u304c\u7570\u306a\u3063\u3066\u3082\u51fa\u529b\u304c\u4f3c\u3066\u3044\u308b\uff01\")\n\n        self._visualize_position_blindness(sentences, word_embeddings)\n\n    def _visualize_position_blindness(self, sentences, word_embeddings):\n        \"\"\"\u4f4d\u7f6e\u306e\u7121\u8996\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, (ax, sentence) in enumerate(zip(axes, sentences)):\n            # \u5404\u5358\u8a9e\u306e\u57cb\u3081\u8fbc\u307f\u30922\u6b21\u5143\u306b\u6295\u5f71\uff08PCA\u7684\u306a\u51e6\u7406\uff09\n            embeddings = []\n            for word in sentence:\n                embeddings.append(word_embeddings[word].squeeze().numpy()[:2])\n\n            embeddings = np.array(embeddings)\n\n            # \u6563\u5e03\u56f3\n            colors = plt.cm.tab10(np.arange(len(sentence)))\n\n            for i, (word, emb, color) in enumerate(zip(sentence, embeddings, colors)):\n                ax.scatter(emb[0], emb[1], c=[color], s=200, alpha=0.7)\n                ax.annotate(f\"{i}: {word}\", \n                           xy=(emb[0], emb[1]), \n                           xytext=(5, 5), \n                           textcoords='offset points',\n                           fontsize=10)\n\n            ax.set_title(f\"\u6587{idx + 1}: {' '.join(sentence)}\")\n            ax.set_xlabel(\"Dimension 1\")\n            ax.set_ylabel(\"Dimension 2\")\n            ax.grid(True, alpha=0.3)\n\n        plt.suptitle(\"\u4f4d\u7f6e\u60c5\u5831\u306a\u3057\uff1a\u5358\u8a9e\u306e\u9806\u5e8f\u304c\u5931\u308f\u308c\u308b\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/positional-encoding/#_4","title":"\u9806\u5e8f\u304c\u91cd\u8981\u306a\u4f8b","text":"<pre><code>class OrderImportanceExamples:\n    \"\"\"\u9806\u5e8f\u306e\u91cd\u8981\u6027\u3092\u793a\u3059\u69d8\u3005\u306a\u4f8b\"\"\"\n\n    def programming_language_examples(self):\n        \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u306e\u9806\u5e8f\u306e\u91cd\u8981\u6027\"\"\"\n        print(\"=== \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3067\u306e\u9806\u5e8f ===\\n\")\n\n        examples = [\n            {\n                \"correct\": \"result = function(arg1, arg2)\",\n                \"reordered\": \"function = result(arg1, arg2)\",\n                \"effect\": \"\u4ee3\u5165\u306e\u65b9\u5411\u304c\u9006\u8ee2\"\n            },\n            {\n                \"correct\": \"if (condition) { action(); }\",\n                \"reordered\": \"{ action(); } if (condition)\",\n                \"effect\": \"\u69cb\u6587\u30a8\u30e9\u30fc\"\n            },\n            {\n                \"correct\": \"array[index] = value\",\n                \"reordered\": \"value = array[index]\",\n                \"effect\": \"\u8aad\u307f\u53d6\u308a\u3068\u66f8\u304d\u8fbc\u307f\u304c\u9006\"\n            }\n        ]\n\n        for ex in examples:\n            print(f\"\u6b63\u3057\u3044\u9806\u5e8f: {ex['correct']}\")\n            print(f\"\u9806\u5e8f\u5909\u66f4\u5f8c: {ex['reordered']}\")\n            print(f\"\u5f71\u97ff: {ex['effect']}\\n\")\n\n    def natural_language_examples(self):\n        \"\"\"\u81ea\u7136\u8a00\u8a9e\u3067\u306e\u9806\u5e8f\u306e\u91cd\u8981\u6027\"\"\"\n        print(\"=== \u81ea\u7136\u8a00\u8a9e\u3067\u306e\u9806\u5e8f ===\\n\")\n\n        examples = [\n            {\n                \"sentences\": [\n                    \"Dog bites man\",\n                    \"Man bites dog\"\n                ],\n                \"difference\": \"\u4e3b\u8a9e\u3068\u76ee\u7684\u8a9e\u304c\u5165\u308c\u66ff\u308f\u308a\u3001\u610f\u5473\u304c\u9006\u8ee2\"\n            },\n            {\n                \"sentences\": [\n                    \"I saw the man with the telescope\",\n                    \"With the telescope, I saw the man\"\n                ],\n                \"difference\": \"\u4fee\u98fe\u95a2\u4fc2\u304c\u5909\u308f\u308a\u3001\u671b\u9060\u93e1\u3092\u6301\u3063\u3066\u3044\u308b\u306e\u304c\u8ab0\u304b\u4e0d\u660e\u78ba\u306b\"\n            },\n            {\n                \"sentences\": [\n                    \"Only I love you\",\n                    \"I only love you\",\n                    \"I love only you\"\n                ],\n                \"difference\": \"'only'\u306e\u4f4d\u7f6e\u3067\u610f\u5473\u304c\u5927\u304d\u304f\u5909\u5316\"\n            }\n        ]\n\n        for ex in examples:\n            print(\"\u6587\u306e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3:\")\n            for sent in ex[\"sentences\"]:\n                print(f\"  - {sent}\")\n            print(f\"\u9055\u3044: {ex['difference']}\\n\")\n\n    def visualize_order_impact(self):\n        \"\"\"\u9806\u5e8f\u306e\u5f71\u97ff\u3092\u53ef\u8996\u5316\"\"\"\n        # \u4f9d\u5b58\u95a2\u4fc2\u306e\u6728\u69cb\u9020\u3067\u8868\u73fe\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u65871: \"The cat chased the mouse\"\n        self._draw_dependency_tree(ax1, \n                                 [\"The\", \"cat\", \"chased\", \"the\", \"mouse\"],\n                                 [(0, 1), (1, 2), (3, 4), (4, 2)],\n                                 \"The cat chased the mouse\")\n\n        # \u65872: \"The mouse chased the cat\"\n        self._draw_dependency_tree(ax2,\n                                 [\"The\", \"mouse\", \"chased\", \"the\", \"cat\"],\n                                 [(0, 1), (1, 2), (3, 4), (4, 2)],\n                                 \"The mouse chased the cat\")\n\n        plt.suptitle(\"\u540c\u3058\u5358\u8a9e\u3001\u7570\u306a\u308b\u9806\u5e8f \u2192 \u7570\u306a\u308b\u610f\u5473\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n\n    def _draw_dependency_tree(self, ax, words, edges, title):\n        \"\"\"\u4f9d\u5b58\u95a2\u4fc2\u6728\u3092\u63cf\u753b\"\"\"\n        positions = [(i, 0) for i in range(len(words))]\n\n        # \u30ce\u30fc\u30c9\uff08\u5358\u8a9e\uff09\u3092\u63cf\u753b\n        for i, (x, y) in enumerate(positions):\n            circle = plt.Circle((x, y), 0.3, color='lightblue', ec='black')\n            ax.add_patch(circle)\n            ax.text(x, y, words[i], ha='center', va='center', fontsize=10)\n\n        # \u30a8\u30c3\u30b8\uff08\u4f9d\u5b58\u95a2\u4fc2\uff09\u3092\u63cf\u753b\n        for start, end in edges:\n            x1, y1 = positions[start]\n            x2, y2 = positions[end]\n\n            # \u66f2\u7dda\u77e2\u5370\n            ax.annotate('', xy=(x2, y2 + 0.3), xytext=(x1, y1 + 0.3),\n                       arrowprops=dict(arrowstyle='-&gt;', \n                                     connectionstyle=\"arc3,rad=0.3\",\n                                     color='red', lw=2))\n\n        ax.set_xlim(-0.5, len(words) - 0.5)\n        ax.set_ylim(-1, 2)\n        ax.set_title(title)\n        ax.axis('off')\n</code></pre>"},{"location":"part2/positional-encoding/#72","title":"7.2 \u7d76\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":""},{"location":"part2/positional-encoding/#_5","title":"\u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":"<pre><code>class SinusoidalPositionalEncoding:\n    \"\"\"\u30aa\u30ea\u30b8\u30ca\u30ebTransformer\u306e\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 5000):\n        self.d_model = d_model\n        self.max_len = max_len\n        self.pe = self._create_positional_encoding()\n\n    def _create_positional_encoding(self) -&gt; torch.Tensor:\n        \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u4f5c\u6210\"\"\"\n        pe = torch.zeros(self.max_len, self.d_model)\n        position = torch.arange(0, self.max_len).unsqueeze(1).float()\n\n        # \u6ce2\u9577\u3092\u8a08\u7b97\n        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * \n                            -(math.log(10000.0) / self.d_model))\n\n        # \u30b5\u30a4\u30f3\u3068\u30b3\u30b5\u30a4\u30f3\u3092\u4ea4\u4e92\u306b\u9069\u7528\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        return pe\n\n    def explain_formula(self):\n        \"\"\"\u6570\u5f0f\u306e\u8aac\u660e\"\"\"\n        print(\"=== \u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 ===\\n\")\n        print(\"\u4f4d\u7f6epos\u306e\u6b21\u5143i\u306b\u304a\u3051\u308b\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0:\")\n        print(\"PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\")\n        print(\"PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\n\")\n\n        print(\"\u306a\u305c\u3053\u306e\u6570\u5f0f\uff1f\")\n        print(\"1. \u5468\u671f\u6027: \u7570\u306a\u308b\u6b21\u5143\u3067\u7570\u306a\u308b\u5468\u671f\u3092\u6301\u3064\")\n        print(\"2. \u76f8\u5bfe\u4f4d\u7f6e: PE(pos+k)\u3092PE(pos)\u3068PE(k)\u306e\u7dda\u5f62\u5909\u63db\u3067\u8868\u73fe\u53ef\u80fd\")\n        print(\"3. \u5916\u633f\u6027: \u5b66\u7fd2\u6642\u3088\u308a\u9577\u3044\u7cfb\u5217\u306b\u3082\u5bfe\u5fdc\u53ef\u80fd\")\n\n        self._demonstrate_properties()\n\n    def _demonstrate_properties(self):\n        \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u6027\u8cea\u3092\u5b9f\u8a3c\"\"\"\n        # \u5c0f\u3055\u306a\u4f8b\u3067\u6027\u8cea\u3092\u793a\u3059\n        d_model = 8\n        positions = [0, 1, 2, 5, 10, 20]\n\n        pe_small = torch.zeros(30, d_model)\n        position = torch.arange(0, 30).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            -(math.log(10000.0) / d_model))\n\n        pe_small[:, 0::2] = torch.sin(position * div_term)\n        pe_small[:, 1::2] = torch.cos(position * div_term)\n\n        print(\"\\n\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5024\uff08\u6700\u521d\u306e4\u6b21\u5143\uff09:\")\n        for pos in positions:\n            print(f\"Position {pos:2d}: {pe_small[pos, :4].numpy()}\")\n\n    def visualize_encoding(self):\n        \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # 1. \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        ax = axes[0, 0]\n        im = ax.imshow(self.pe[:100, :64].T, cmap='RdBu', aspect='auto')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Dimension')\n        ax.set_title('\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7')\n        plt.colorbar(im, ax=ax)\n\n        # 2. \u500b\u5225\u6b21\u5143\u306e\u6ce2\u5f62\n        ax = axes[0, 1]\n        positions = np.arange(100)\n        dimensions = [0, 1, 4, 5, 10, 11]  # \u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\u306e\u30da\u30a2\n\n        for i, dim in enumerate(dimensions):\n            ax.plot(positions, self.pe[:100, dim], \n                   label=f'dim {dim} ({\"sin\" if dim % 2 == 0 else \"cos\"})',\n                   alpha=0.8)\n\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Value')\n        ax.set_title('\u7570\u306a\u308b\u6b21\u5143\u306e\u6ce2\u5f62')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # 3. \u5468\u6ce2\u6570\u30b9\u30da\u30af\u30c8\u30eb\n        ax = axes[1, 0]\n        for dim in range(0, min(10, self.d_model), 2):\n            wavelength = 10000 ** (dim / self.d_model)\n            frequency = 1 / wavelength\n            ax.scatter(dim, frequency, s=100)\n\n        ax.set_xlabel('Dimension')\n        ax.set_ylabel('Frequency')\n        ax.set_yscale('log')\n        ax.set_title('\u5404\u6b21\u5143\u306e\u5468\u6ce2\u6570')\n        ax.grid(True, alpha=0.3)\n\n        # 4. \u4f4d\u7f6e\u9593\u306e\u985e\u4f3c\u5ea6\n        ax = axes[1, 1]\n        positions_to_compare = [0, 10, 20, 30, 40]\n        similarity_matrix = np.zeros((len(positions_to_compare), len(positions_to_compare)))\n\n        for i, pos1 in enumerate(positions_to_compare):\n            for j, pos2 in enumerate(positions_to_compare):\n                similarity = torch.cosine_similarity(\n                    self.pe[pos1], self.pe[pos2], dim=0\n                ).item()\n                similarity_matrix[i, j] = similarity\n\n        im = ax.imshow(similarity_matrix, cmap='Blues', aspect='auto')\n        ax.set_xticks(range(len(positions_to_compare)))\n        ax.set_yticks(range(len(positions_to_compare)))\n        ax.set_xticklabels(positions_to_compare)\n        ax.set_yticklabels(positions_to_compare)\n        ax.set_title('\u4f4d\u7f6e\u9593\u306e\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6')\n\n        # \u5024\u3092\u8868\u793a\n        for i in range(len(positions_to_compare)):\n            for j in range(len(positions_to_compare)):\n                ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n                       ha='center', va='center')\n\n        plt.colorbar(im, ax=ax)\n        plt.tight_layout()\n        plt.show()\n\n    def demonstrate_relative_position(self):\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6027\u8cea\u3092\u5b9f\u8a3c\"\"\"\n        print(\"\\n=== \u76f8\u5bfe\u4f4d\u7f6e\u306e\u6027\u8cea ===\")\n\n        # PE(pos+k) \u2248 f(PE(pos), PE(k)) \u3092\u78ba\u8a8d\n        pos = 10\n        k = 5\n\n        pe_pos = self.pe[pos]\n        pe_k = self.pe[k]\n        pe_pos_plus_k = self.pe[pos + k]\n\n        # \u7dda\u5f62\u5909\u63db\u3067\u8fd1\u4f3c\u3067\u304d\u308b\u3053\u3068\u3092\u793a\u3059\n        # \uff08\u5b9f\u969b\u306b\u306f\u56de\u8ee2\u884c\u5217\u306b\u3088\u308b\u5909\u63db\uff09\n\n        print(f\"PE({pos}) \u306e\u6700\u521d\u306e4\u8981\u7d20: {pe_pos[:4].numpy()}\")\n        print(f\"PE({k}) \u306e\u6700\u521d\u306e4\u8981\u7d20: {pe_k[:4].numpy()}\")\n        print(f\"PE({pos+k}) \u306e\u6700\u521d\u306e4\u8981\u7d20: {pe_pos_plus_k[:4].numpy()}\")\n\n        # \u52a0\u6cd5\u5b9a\u7406\u3092\u4f7f\u3063\u305f\u7406\u8ad6\u5024\n        # sin(a+b) = sin(a)cos(b) + cos(a)sin(b)\n        # cos(a+b) = cos(a)cos(b) - sin(a)sin(b)\n\n        theoretical = torch.zeros_like(pe_pos_plus_k)\n        theoretical[0::2] = pe_pos[0::2] * pe_k[1::2] + pe_pos[1::2] * pe_k[0::2]\n        theoretical[1::2] = pe_pos[1::2] * pe_k[1::2] - pe_pos[0::2] * pe_k[0::2]\n\n        error = torch.abs(pe_pos_plus_k - theoretical).mean().item()\n        print(f\"\\n\u7406\u8ad6\u5024\u3068\u306e\u5e73\u5747\u7d76\u5bfe\u8aa4\u5dee: {error:.6f}\")\n        print(\"\u2192 \u76f8\u5bfe\u4f4d\u7f6e\u304c\u7dda\u5f62\u5909\u63db\u3067\u8868\u73fe\u53ef\u80fd\uff01\")\n</code></pre>"},{"location":"part2/positional-encoding/#_6","title":"\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f","text":"<pre><code>class LearnablePositionalEmbedding:\n    \"\"\"\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\uff08BERT\u65b9\u5f0f\uff09\"\"\"\n\n    def __init__(self, max_len: int, d_model: int):\n        self.max_len = max_len\n        self.d_model = d_model\n        self.position_embeddings = nn.Embedding(max_len, d_model)\n\n        # \u521d\u671f\u5316\n        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n\n    def compare_with_sinusoidal(self):\n        \"\"\"\u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\u65b9\u5f0f\u3068\u306e\u6bd4\u8f03\"\"\"\n        print(\"=== \u5b66\u7fd2\u53ef\u80fd vs \u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3 ===\\n\")\n\n        comparison = {\n            \"\u5b66\u7fd2\u53ef\u80fd\": {\n                \"\u9577\u6240\": [\n                    \"\u30bf\u30b9\u30af\u7279\u5316\u306e\u4f4d\u7f6e\u8868\u73fe\u3092\u5b66\u7fd2\",\n                    \"\u5b9f\u88c5\u304c\u30b7\u30f3\u30d7\u30eb\",\n                    \"\u77ed\u3044\u7cfb\u5217\u3067\u306f\u6027\u80fd\u304c\u826f\u3044\"\n                ],\n                \"\u77ed\u6240\": [\n                    \"\u5b66\u7fd2\u6642\u306e\u6700\u5927\u9577\u3092\u8d85\u3048\u3089\u308c\u306a\u3044\",\n                    \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u304c\u5897\u3048\u308b\",\n                    \"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u95a2\u4fc2\u304c\u4e0d\u660e\u77ad\"\n                ]\n            },\n            \"\u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\": {\n                \"\u9577\u6240\": [\n                    \"\u4efb\u610f\u306e\u9577\u3055\u306b\u5bfe\u5fdc\",\n                    \"\u30d1\u30e9\u30e1\u30fc\u30bf\u4e0d\u8981\",\n                    \"\u76f8\u5bfe\u4f4d\u7f6e\u3092\u81ea\u7136\u306b\u8868\u73fe\"\n                ],\n                \"\u77ed\u6240\": [\n                    \"\u56fa\u5b9a\u30d1\u30bf\u30fc\u30f3\u306e\u307f\",\n                    \"\u30bf\u30b9\u30af\u7279\u5316\u306e\u6700\u9069\u5316\u4e0d\u53ef\",\n                    \"\u7406\u8ad6\u7684\u306b\u8907\u96d1\"\n                ]\n            }\n        }\n\n        for method, props in comparison.items():\n            print(f\"{method}:\")\n            print(\"  \u9577\u6240:\")\n            for pro in props[\"\u9577\u6240\"]:\n                print(f\"    - {pro}\")\n            print(\"  \u77ed\u6240:\")\n            for con in props[\"\u77ed\u6240\"]:\n                print(f\"    - {con}\")\n            print()\n\n    def visualize_learned_patterns(self, num_positions: int = 50):\n        \"\"\"\u5b66\u7fd2\u3055\u308c\u305f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\"\"\"\n        # \u4eee\u60f3\u7684\u306a\u5b66\u7fd2\u6e08\u307f\u57cb\u3081\u8fbc\u307f\n        torch.manual_seed(42)\n\n        # \u4f4d\u7f6e\u306b\u3088\u308b\u6bb5\u968e\u7684\u306a\u5909\u5316\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\n        learned_embeddings = torch.zeros(num_positions, self.d_model)\n\n        for pos in range(num_positions):\n            # \u57fa\u672c\u30d1\u30bf\u30fc\u30f3 + \u30ce\u30a4\u30ba\n            base_pattern = torch.sin(torch.arange(self.d_model).float() * pos / 10)\n            noise = torch.randn(self.d_model) * 0.1\n            learned_embeddings[pos] = base_pattern + noise\n\n            # \u6b63\u898f\u5316\n            learned_embeddings[pos] = learned_embeddings[pos] / learned_embeddings[pos].norm() * math.sqrt(self.d_model)\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        im = ax1.imshow(learned_embeddings[:30].T, cmap='RdBu', aspect='auto')\n        ax1.set_xlabel('Position')\n        ax1.set_ylabel('Dimension')\n        ax1.set_title('\u5b66\u7fd2\u3055\u308c\u305f\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f')\n        plt.colorbar(im, ax=ax1)\n\n        # \u4f4d\u7f6e\u9593\u306e\u985e\u4f3c\u5ea6\n        similarity_matrix = torch.matmul(learned_embeddings, learned_embeddings.T)\n        im = ax2.imshow(similarity_matrix[:30, :30], cmap='Blues', aspect='auto')\n        ax2.set_xlabel('Position')\n        ax2.set_ylabel('Position')\n        ax2.set_title('\u4f4d\u7f6e\u9593\u306e\u985e\u4f3c\u5ea6')\n        plt.colorbar(im, ax=ax2)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/positional-encoding/#73","title":"7.3 \u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":""},{"location":"part2/positional-encoding/#_7","title":"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6982\u5ff5","text":"<pre><code>class RelativePositionalEncoding:\n    \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model: int, max_relative_position: int = 128):\n        self.d_model = d_model\n        self.max_relative_position = max_relative_position\n\n        # \u76f8\u5bfe\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\n        self.relative_positions_embeddings = nn.Embedding(\n            2 * max_relative_position + 1, d_model\n        )\n\n    def explain_concept(self):\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 ===\\n\")\n\n        print(\"\u7d76\u5bfe\u4f4d\u7f6e vs \u76f8\u5bfe\u4f4d\u7f6e:\")\n        print(\"- \u7d76\u5bfe\u4f4d\u7f6e: 'The' \u306f\u4f4d\u7f6e0\u3001'cat' \u306f\u4f4d\u7f6e1\")\n        print(\"- \u76f8\u5bfe\u4f4d\u7f6e: 'cat' \u306f 'The' \u304b\u3089\u898b\u3066+1\u306e\u4f4d\u7f6e\\n\")\n\n        print(\"\u5229\u70b9:\")\n        print(\"1. \u6587\u306e\u9577\u3055\u306b\u4f9d\u5b58\u3057\u306a\u3044\")\n        print(\"2. \u540c\u3058\u30d1\u30bf\u30fc\u30f3\u304c\u7570\u306a\u308b\u4f4d\u7f6e\u3067\u518d\u5229\u7528\u53ef\u80fd\")\n        print(\"3. \u3088\u308a\u81ea\u7136\u306a\u5e30\u7d0d\u30d0\u30a4\u30a2\u30b9\")\n\n        self._demonstrate_relative_positions()\n\n    def _demonstrate_relative_positions(self):\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u4f8b\u3092\u793a\u3059\"\"\"\n        sentence = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"]\n\n        print(\"\\n\u76f8\u5bfe\u4f4d\u7f6e\u884c\u5217:\")\n        print(\"    \", \"  \".join(f\"{w:&gt;6}\" for w in sentence))\n\n        for i, word_i in enumerate(sentence):\n            row = []\n            for j, word_j in enumerate(sentence):\n                relative_pos = j - i\n                row.append(f\"{relative_pos:6d}\")\n            print(f\"{word_i:&gt;6}\", \" \".join(row))\n\n    def compute_relative_positions(self, seq_len: int) -&gt; torch.Tensor:\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u884c\u5217\u3092\u8a08\u7b97\"\"\"\n        # \u5404\u4f4d\u7f6e\u30da\u30a2\u306e\u76f8\u5bfe\u8ddd\u96e2\n        positions = torch.arange(seq_len)\n        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)\n\n        # \u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        relative_positions = torch.clamp(\n            relative_positions, \n            -self.max_relative_position, \n            self.max_relative_position\n        )\n\n        # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\uff08\u8ca0\u306e\u5024\u306b\u5bfe\u5fdc\uff09\n        relative_positions = relative_positions + self.max_relative_position\n\n        return relative_positions\n\n    def visualize_relative_attention(self):\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u3092\u8003\u616e\u3057\u305f\u6ce8\u610f\u3092\u53ef\u8996\u5316\"\"\"\n        seq_len = 10\n        relative_positions = self.compute_relative_positions(seq_len)\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        # 1. \u76f8\u5bfe\u4f4d\u7f6e\u884c\u5217\n        ax = axes[0]\n        im = ax.imshow(relative_positions - self.max_relative_position, \n                      cmap='RdBu', aspect='auto')\n        ax.set_xlabel('Position j')\n        ax.set_ylabel('Position i')\n        ax.set_title('\u76f8\u5bfe\u4f4d\u7f6e (j - i)')\n        plt.colorbar(im, ax=ax)\n\n        # 2. \u8ddd\u96e2\u306b\u3088\u308b\u6e1b\u8870\n        ax = axes[1]\n        distances = torch.abs(torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1))\n        decay = 1.0 / (1.0 + distances.float())\n\n        im = ax.imshow(decay, cmap='Blues', aspect='auto')\n        ax.set_xlabel('Position j')\n        ax.set_ylabel('Position i')\n        ax.set_title('\u8ddd\u96e2\u306b\u3088\u308b\u6e1b\u8870')\n        plt.colorbar(im, ax=ax)\n\n        # 3. \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u4f8b\n        ax = axes[2]\n        # \u4eee\u60f3\u7684\u306a\u6ce8\u610f\u91cd\u307f\uff08\u76f8\u5bfe\u4f4d\u7f6e\u3092\u8003\u616e\uff09\n        attention = torch.softmax(-distances.float() / 2.0, dim=-1)\n\n        im = ax.imshow(attention, cmap='Blues', aspect='auto')\n        ax.set_xlabel('Attended to')\n        ax.set_ylabel('Attending from')\n        ax.set_title('\u76f8\u5bfe\u4f4d\u7f6e\u30d9\u30fc\u30b9\u306e\u6ce8\u610f')\n        plt.colorbar(im, ax=ax)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/positional-encoding/#transformer-xlrelative-position-encoding","title":"Transformer-XL\u3068Relative Position Encoding\u306e\u5b9f\u88c5","text":"<pre><code>class TransformerXLPositioning:\n    \"\"\"Transformer-XL\u65b9\u5f0f\u306e\u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n\n    def __init__(self, d_model: int, n_heads: int):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n\n        # \u76f8\u5bfe\u4f4d\u7f6e\u30d0\u30a4\u30a2\u30b9\n        self.r_w_bias = nn.Parameter(torch.randn(n_heads, self.d_head))\n        self.r_r_bias = nn.Parameter(torch.randn(n_heads, self.d_head))\n\n    def explain_mechanism(self):\n        \"\"\"Transformer-XL\u306e\u4ed5\u7d44\u307f\u3092\u8aac\u660e\"\"\"\n        print(\"=== Transformer-XL\u306e\u76f8\u5bfe\u4f4d\u7f6e ===\\n\")\n\n        print(\"\u6a19\u6e96\u306eAttention:\")\n        print(\"Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V\\n\")\n\n        print(\"Transformer-XL\u306eAttention:\")\n        print(\"\u5404\u30d8\u30c3\u30c9\u3067\u4ee5\u4e0b\u3092\u8a08\u7b97:\")\n        print(\"1. \u30b3\u30f3\u30c6\u30f3\u30c4\u30d9\u30fc\u30b9\u306e\u6ce8\u610f: q_i \u00b7 k_j\")\n        print(\"2. \u30b3\u30f3\u30c6\u30f3\u30c4-\u4f4d\u7f6e\u306e\u6ce8\u610f: q_i \u00b7 r_{i-j}\")\n        print(\"3. \u30b0\u30ed\u30fc\u30d0\u30eb\u30b3\u30f3\u30c6\u30f3\u30c4\u30d0\u30a4\u30a2\u30b9: u \u00b7 k_j\")\n        print(\"4. \u30b0\u30ed\u30fc\u30d0\u30eb\u4f4d\u7f6e\u30d0\u30a4\u30a2\u30b9: v \u00b7 r_{i-j}\")\n\n        print(\"\\n\u3053\u3053\u3067:\")\n        print(\"- r_{i-j}: \u76f8\u5bfe\u4f4d\u7f6ei-j\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\")\n        print(\"- u, v: \u5b66\u7fd2\u53ef\u80fd\u306a\u30d0\u30a4\u30a2\u30b9\u30d1\u30e9\u30e1\u30fc\u30bf\")\n\n    def compute_relative_attention(self, \n                                 query: torch.Tensor,\n                                 key: torch.Tensor,\n                                 value: torch.Tensor,\n                                 relative_embeddings: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u3092\u8003\u616e\u3057\u305f\u6ce8\u610f\u306e\u8a08\u7b97\"\"\"\n        batch_size, seq_len, _ = query.shape\n\n        # Multi-head\u306b\u5206\u5272\n        query = query.view(batch_size, seq_len, self.n_heads, self.d_head)\n        key = key.view(batch_size, seq_len, self.n_heads, self.d_head)\n\n        # \u30b3\u30f3\u30c6\u30f3\u30c4\u30d9\u30fc\u30b9\u306e\u6ce8\u610f\n        content_score = torch.einsum('bihd,bjhd-&gt;bhij', query, key)\n\n        # \u4f4d\u7f6e\u30d9\u30fc\u30b9\u306e\u6ce8\u610f\uff08\u7c21\u7565\u5316\u7248\uff09\n        # \u5b9f\u969b\u306e\u5b9f\u88c5\u306f\u3088\u308a\u8907\u96d1\n        position_score = self._compute_position_scores(query, relative_embeddings)\n\n        # \u5408\u8a08\u30b9\u30b3\u30a2\n        scores = content_score + position_score\n\n        # Softmax\u3068\u5024\u306e\u96c6\u7d04\n        attention_weights = torch.softmax(scores / math.sqrt(self.d_head), dim=-1)\n\n        return attention_weights\n\n    def _compute_position_scores(self, query, relative_embeddings):\n        \"\"\"\u4f4d\u7f6e\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\uff08\u7c21\u7565\u5316\uff09\"\"\"\n        # \u5b9f\u969b\u306e\u5b9f\u88c5\u3067\u306f\u3001\u76f8\u5bfe\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\u3068\u306e\u8907\u96d1\u306a\u8a08\u7b97\n        # \u3053\u3053\u3067\u306f\u6982\u5ff5\u3092\u793a\u3059\u305f\u3081\u306e\u7c21\u7565\u7248\n        batch_size, seq_len = query.shape[:2]\n        position_scores = torch.zeros(batch_size, self.n_heads, seq_len, seq_len)\n\n        return position_scores\n</code></pre>"},{"location":"part2/positional-encoding/#74","title":"7.4 \u6700\u65b0\u306e\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5","text":""},{"location":"part2/positional-encoding/#rotary-position-embedding-rope","title":"Rotary Position Embedding (RoPE)","text":"<pre><code>class RotaryPositionalEmbedding:\n    \"\"\"Rotary Position Embedding\uff08RoPE\uff09\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model: int, max_seq_len: int = 2048, base: int = 10000):\n        self.d_model = d_model\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # \u56de\u8ee2\u884c\u5217\u306e\u5468\u6ce2\u6570\u3092\u4e8b\u524d\u8a08\u7b97\n        self.inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n\n    def explain_rope(self):\n        \"\"\"RoPE\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== Rotary Position Embedding (RoPE) ===\\n\")\n\n        print(\"\u57fa\u672c\u30a2\u30a4\u30c7\u30a2:\")\n        print(\"- \u4f4d\u7f6e\u60c5\u5831\u3092\u56de\u8ee2\u3068\u3057\u3066\u8868\u73fe\")\n        print(\"- Query/Key\u30d9\u30af\u30c8\u30eb\u3092\u4f4d\u7f6e\u306b\u5fdc\u3058\u3066\u56de\u8ee2\")\n        print(\"- \u5185\u7a4d\u304c\u76f8\u5bfe\u4f4d\u7f6e\u306e\u307f\u306b\u4f9d\u5b58\\n\")\n\n        print(\"\u5229\u70b9:\")\n        print(\"1. \u76f8\u5bfe\u4f4d\u7f6e\u3092\u81ea\u7136\u306b\u8868\u73fe\")\n        print(\"2. \u4efb\u610f\u306e\u9577\u3055\u306b\u5916\u633f\u53ef\u80fd\")\n        print(\"3. \u8a08\u7b97\u52b9\u7387\u304c\u826f\u3044\")\n        print(\"4. \u7406\u8ad6\u7684\u306b\u7f8e\u3057\u3044\")\n\n        self._visualize_rotation_concept()\n\n    def _visualize_rotation_concept(self):\n        \"\"\"\u56de\u8ee2\u306e\u6982\u5ff5\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        # 1. 2\u6b21\u5143\u3067\u306e\u56de\u8ee2\n        ax = axes[0]\n        angles = np.linspace(0, 2*np.pi, 8, endpoint=False)\n\n        for i, angle in enumerate(angles):\n            x = np.cos(angle)\n            y = np.sin(angle)\n            ax.arrow(0, 0, x*0.8, y*0.8, head_width=0.1, head_length=0.1,\n                    fc=plt.cm.viridis(i/len(angles)), \n                    ec=plt.cm.viridis(i/len(angles)))\n            ax.text(x*1.1, y*1.1, f'pos={i}', ha='center', va='center')\n\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(-1.5, 1.5)\n        ax.set_aspect('equal')\n        ax.grid(True, alpha=0.3)\n        ax.set_title('2D\u56de\u8ee2\u306b\u3088\u308b\u4f4d\u7f6e\u8868\u73fe')\n\n        # 2. \u5468\u6ce2\u6570\u306e\u9055\u3044\n        ax = axes[1]\n        positions = np.arange(32)\n\n        for i, freq_idx in enumerate([0, 2, 4]):\n            freq = self.inv_freq[freq_idx].item()\n            angles = positions * freq\n            ax.plot(positions, np.sin(angles), \n                   label=f'dim {2*freq_idx}', alpha=0.8)\n\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Sin(position * freq)')\n        ax.set_title('\u7570\u306a\u308b\u6b21\u5143\u306e\u56de\u8ee2\u5468\u6ce2\u6570')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # 3. \u76f8\u5bfe\u4f4d\u7f6e\u306e\u4fdd\u5b58\n        ax = axes[2]\n        pos1, pos2 = 5, 8\n        relative_pos = pos2 - pos1\n\n        # \u5404\u6b21\u5143\u3067\u306e\u89d2\u5ea6\u5dee\n        dims = np.arange(0, 16, 2)\n        freqs = self.inv_freq[:len(dims)].numpy()\n        angle_diffs = relative_pos * freqs\n\n        ax.stem(dims, angle_diffs, basefmt=' ')\n        ax.set_xlabel('Dimension')\n        ax.set_ylabel('Angle difference')\n        ax.set_title(f'\u76f8\u5bfe\u4f4d\u7f6e {relative_pos} \u306e\u89d2\u5ea6\u5dee')\n        ax.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n    def apply_rope(self, x: torch.Tensor, position_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"RoPE\u3092\u9069\u7528\"\"\"\n        batch_size, seq_len, d_model = x.shape\n\n        # x\u3092\u5b9f\u90e8\u3068\u865a\u90e8\u306b\u5206\u5272\n        x_complex = x.view(batch_size, seq_len, -1, 2)\n        x_complex = torch.view_as_complex(x_complex)\n\n        # \u4f4d\u7f6e\u306b\u5fdc\u3058\u305f\u56de\u8ee2\u89d2\n        position_ids = position_ids.view(-1)\n        sinusoid = torch.einsum('i,j-&gt;ij', position_ids, self.inv_freq)\n\n        # \u8907\u7d20\u6570\u8868\u73fe\u3067\u306e\u56de\u8ee2\n        cos = sinusoid.cos()\n        sin = sinusoid.sin()\n\n        # \u56de\u8ee2\u3092\u9069\u7528\uff08\u7c21\u7565\u5316\u7248\uff09\n        # \u5b9f\u969b\u306e\u5b9f\u88c5\u3067\u306f\u3088\u308a\u52b9\u7387\u7684\u306a\u65b9\u6cd5\u3092\u4f7f\u7528\n\n        return x  # \u7c21\u7565\u5316\u306e\u305f\u3081\u5143\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u8fd4\u3059\n\n    def demonstrate_rope_properties(self):\n        \"\"\"RoPE\u306e\u6027\u8cea\u3092\u5b9f\u8a3c\"\"\"\n        print(\"\\n=== RoPE\u306e\u6027\u8cea ===\")\n\n        # \u5c0f\u3055\u306a\u4f8b\u3067\u6027\u8cea\u3092\u78ba\u8a8d\n        d_model = 4\n        q = torch.randn(1, 1, d_model)  # Query at position m\n        k = torch.randn(1, 1, d_model)  # Key at position n\n\n        # \u7570\u306a\u308b\u4f4d\u7f6e\u3067\u306eRoPE\u9069\u7528\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\n        positions = [0, 5, 10]\n\n        print(\"\\n\u5185\u7a4d\u306e\u5909\u5316:\")\n        for m in positions:\n            for n in positions:\n                # \u5b9f\u969b\u306e\u8a08\u7b97\u306f\u8907\u96d1\u306a\u306e\u3067\u3001\u6982\u5ff5\u7684\u306a\u7d50\u679c\u3092\u8868\u793a\n                relative = n - m\n                print(f\"Q(pos={m}) \u00b7 K(pos={n}) = f(relative_pos={relative})\")\n</code></pre>"},{"location":"part2/positional-encoding/#alibi-attention-with-linear-biases","title":"ALiBi (Attention with Linear Biases)","text":"<pre><code>class ALiBiPositioning:\n    \"\"\"ALiBi\uff08Attention with Linear Biases\uff09\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, n_heads: int):\n        self.n_heads = n_heads\n        self.slopes = self._get_slopes()\n\n    def _get_slopes(self) -&gt; torch.Tensor:\n        \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u30b9\u30ed\u30fc\u30d7\u3092\u8a08\u7b97\"\"\"\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-2 ** -(math.log2(n) - 3))\n            ratio = start\n            return [start * (ratio ** i) for i in range(n)]\n\n        if math.log2(self.n_heads).is_integer():\n            slopes = get_slopes_power_of_2(self.n_heads)\n        else:\n            # \u6700\u3082\u8fd1\u30442\u306e\u3079\u304d\u4e57\u304b\u3089\u88dc\u9593\n            closest_power_of_2 = 2 ** math.floor(math.log2(self.n_heads))\n            slopes = get_slopes_power_of_2(closest_power_of_2)\n            # \u8ffd\u52a0\u306e\u30b9\u30ed\u30fc\u30d7\u3092\u88dc\u9593\n            extra_slopes = get_slopes_power_of_2(2 * closest_power_of_2)[0::2][:self.n_heads - closest_power_of_2]\n            slopes.extend(extra_slopes)\n\n        return torch.tensor(slopes)\n\n    def explain_alibi(self):\n        \"\"\"ALiBi\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== ALiBi (Attention with Linear Biases) ===\\n\")\n\n        print(\"\u57fa\u672c\u30a2\u30a4\u30c7\u30a2:\")\n        print(\"- \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0\u305b\u305a\u3001\u6ce8\u610f\u30b9\u30b3\u30a2\u306b\u76f4\u63a5\u30d0\u30a4\u30a2\u30b9\u3092\u52a0\u3048\u308b\")\n        print(\"- \u30d0\u30a4\u30a2\u30b9 = -m * |i - j| \uff08m\u306f\u30d8\u30c3\u30c9\u3054\u3068\u306b\u7570\u306a\u308b\uff09\")\n        print(\"- \u30b7\u30f3\u30d7\u30eb\u3067\u52b9\u679c\u7684\\n\")\n\n        print(\"\u5229\u70b9:\")\n        print(\"1. \u5b9f\u88c5\u304c\u975e\u5e38\u306b\u30b7\u30f3\u30d7\u30eb\")\n        print(\"2. \u5b66\u7fd2\u6642\u3088\u308a\u9577\u3044\u7cfb\u5217\u306b\u81ea\u7136\u306b\u5916\u633f\")\n        print(\"3. \u30e1\u30e2\u30ea\u52b9\u7387\u304c\u826f\u3044\")\n        print(\"4. \u591a\u304f\u306e\u30bf\u30b9\u30af\u3067\u826f\u597d\u306a\u6027\u80fd\")\n\n    def create_alibi_bias(self, seq_len: int) -&gt; torch.Tensor:\n        \"\"\"ALiBi\u30d0\u30a4\u30a2\u30b9\u3092\u4f5c\u6210\"\"\"\n        # \u76f8\u5bfe\u4f4d\u7f6e\u884c\u5217\n        positions = torch.arange(seq_len)\n        relative_positions = positions.unsqueeze(0) - positions.unsqueeze(1)\n        relative_positions = -torch.abs(relative_positions).float()\n\n        # \u5404\u30d8\u30c3\u30c9\u306b\u7570\u306a\u308b\u30b9\u30ed\u30fc\u30d7\u3092\u9069\u7528\n        alibi_biases = relative_positions.unsqueeze(0) * self.slopes.view(-1, 1, 1)\n\n        return alibi_biases\n\n    def visualize_alibi(self):\n        \"\"\"ALiBi\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\"\"\"\n        seq_len = 20\n        alibi_biases = self.create_alibi_bias(seq_len)\n\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n        axes = axes.ravel()\n\n        # \u5404\u30d8\u30c3\u30c9\u306e\u30d0\u30a4\u30a2\u30b9\u30d1\u30bf\u30fc\u30f3\u3092\u8868\u793a\n        for head_idx in range(min(4, self.n_heads)):\n            ax = axes[head_idx]\n            im = ax.imshow(alibi_biases[head_idx], cmap='Blues_r', aspect='auto')\n            ax.set_xlabel('Key Position')\n            ax.set_ylabel('Query Position')\n            ax.set_title(f'Head {head_idx} (slope={self.slopes[head_idx]:.4f})')\n            plt.colorbar(im, ax=ax)\n\n        plt.suptitle('ALiBi: \u5404\u30d8\u30c3\u30c9\u306e\u8ddd\u96e2\u30da\u30ca\u30eb\u30c6\u30a3')\n        plt.tight_layout()\n        plt.show()\n\n    def compare_extrapolation(self):\n        \"\"\"\u5916\u633f\u80fd\u529b\u306e\u6bd4\u8f03\"\"\"\n        print(\"\\n=== \u5916\u633f\u80fd\u529b\u306e\u6bd4\u8f03 ===\")\n\n        train_length = 512\n        test_lengths = [512, 1024, 2048, 4096]\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n        # \u5404\u624b\u6cd5\u306e\u6027\u80fd\uff08\u4eee\u60f3\u7684\uff09\n        methods = {\n            'Sinusoidal': [100, 95, 85, 70],\n            'Learned': [100, 90, 60, 30],\n            'RoPE': [100, 98, 95, 90],\n            'ALiBi': [100, 99, 98, 95]\n        }\n\n        for method, scores in methods.items():\n            ax.plot(test_lengths, scores, marker='o', label=method, linewidth=2)\n\n        ax.axvline(x=train_length, color='red', linestyle='--', alpha=0.5)\n        ax.text(train_length + 50, 50, 'Training\\nLength', color='red')\n\n        ax.set_xlabel('Sequence Length')\n        ax.set_ylabel('Performance (%)')\n        ax.set_title('\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u306e\u5916\u633f\u6027\u80fd')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_xscale('log', base=2)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/positional-encoding/#75","title":"7.5 \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5b9f\u88c5\u3068\u7d71\u5408","text":""},{"location":"part2/positional-encoding/#_8","title":"\u5b8c\u5168\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30e2\u30b8\u30e5\u30fc\u30eb","text":"<pre><code>class PositionalEncodingModule(nn.Module):\n    \"\"\"\u5b9f\u7528\u7684\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30e2\u30b8\u30e5\u30fc\u30eb\"\"\"\n\n    def __init__(self, \n                 d_model: int,\n                 max_len: int = 5000,\n                 encoding_type: str = 'sinusoidal',\n                 dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.encoding_type = encoding_type\n        self.dropout = nn.Dropout(dropout)\n\n        if encoding_type == 'sinusoidal':\n            self.pos_encoding = self._create_sinusoidal_encoding(max_len, d_model)\n        elif encoding_type == 'learned':\n            self.pos_encoding = nn.Embedding(max_len, d_model)\n        elif encoding_type == 'rope':\n            self.rope = RotaryPositionalEmbedding(d_model, max_len)\n        elif encoding_type == 'alibi':\n            # ALiBi\u306f\u5225\u9014\u51e6\u7406\n            pass\n        else:\n            raise ValueError(f\"Unknown encoding type: {encoding_type}\")\n\n    def _create_sinusoidal_encoding(self, max_len: int, d_model: int) -&gt; torch.Tensor:\n        \"\"\"\u30b5\u30a4\u30f3\u30fb\u30b3\u30b5\u30a4\u30f3\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f5c\u6210\"\"\"\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                            -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)  # \u30d0\u30c3\u30c1\u6b21\u5143\u3092\u8ffd\u52a0\n        return pe\n\n    def forward(self, x: torch.Tensor, position_ids: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            position_ids: [batch_size, seq_len] (optional)\n        Returns:\n            x with positional encoding: [batch_size, seq_len, d_model]\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n\n        if self.encoding_type == 'sinusoidal':\n            # \u5fc5\u8981\u306a\u9577\u3055\u3060\u3051\u53d6\u5f97\n            pos_encoding = self.pos_encoding[:, :seq_len, :].to(x.device)\n            x = x + pos_encoding\n\n        elif self.encoding_type == 'learned':\n            if position_ids is None:\n                position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n\n            pos_embeddings = self.pos_encoding(position_ids)\n            x = x + pos_embeddings\n\n        elif self.encoding_type == 'rope':\n            # RoPE\u306f\u5225\u306e\u65b9\u6cd5\u3067\u9069\u7528\uff08Q, K\u306b\u5bfe\u3057\u3066\uff09\n            pass\n\n        return self.dropout(x)\n\n    def visualize_encoding_effect(self, sentence: str):\n        \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\"\"\"\n        words = sentence.split()\n        seq_len = len(words)\n\n        # \u30c0\u30df\u30fc\u306e\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\n        word_embeddings = torch.randn(1, seq_len, self.d_model)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u9069\u7528\n        with torch.no_grad():\n            encoded = self.forward(word_embeddings)\n\n        # \u5143\u306e\u57cb\u3081\u8fbc\u307f\u3068\u6bd4\u8f03\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        # \u5143\u306e\u57cb\u3081\u8fbc\u307f\n        ax = axes[0]\n        im = ax.imshow(word_embeddings[0, :, :32].T, cmap='RdBu', aspect='auto')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Dimension')\n        ax.set_title('\u5143\u306e\u5358\u8a9e\u57cb\u3081\u8fbc\u307f')\n        plt.colorbar(im, ax=ax)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        ax = axes[1]\n        if self.encoding_type == 'sinusoidal':\n            pe = self.pos_encoding[0, :seq_len, :32].T\n        else:\n            pe = encoded[0, :, :32].T - word_embeddings[0, :, :32].T\n\n        im = ax.imshow(pe, cmap='RdBu', aspect='auto')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Dimension')\n        ax.set_title('\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0')\n        plt.colorbar(im, ax=ax)\n\n        # \u5408\u8a08\n        ax = axes[2]\n        im = ax.imshow(encoded[0, :, :32].T, cmap='RdBu', aspect='auto')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Dimension')\n        ax.set_title('\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u9069\u7528\u5f8c')\n        plt.colorbar(im, ax=ax)\n\n        # \u5358\u8a9e\u3092\u8868\u793a\n        for ax in axes:\n            ax.set_xticks(range(seq_len))\n            ax.set_xticklabels(words, rotation=45, ha='right')\n\n        plt.suptitle(f'{self.encoding_type.capitalize()} Encoding \u306e\u52b9\u679c')\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/positional-encoding/#_9","title":"\u307e\u3068\u3081\uff1a\u4f4d\u7f6e\u60c5\u5831\u306e\u6271\u3044\u65b9","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u8981\u70b9\uff1a</p> <ol> <li>\u5fc5\u8981\u6027\uff1aSelf-Attention\u306e\u4f4d\u7f6e\u4e0d\u5909\u6027\u3092\u88dc\u5b8c</li> <li>\u624b\u6cd5\u306e\u9032\u5316\uff1a</li> <li>Sinusoidal\uff1a\u7406\u8ad6\u7684\u306b\u7f8e\u3057\u304f\u3001\u5916\u633f\u53ef\u80fd</li> <li>Learned\uff1a\u30bf\u30b9\u30af\u7279\u5316\u3060\u304c\u9577\u3055\u5236\u9650\u3042\u308a</li> <li>RoPE\uff1a\u76f8\u5bfe\u4f4d\u7f6e\u3092\u81ea\u7136\u306b\u8868\u73fe</li> <li>ALiBi\uff1a\u30b7\u30f3\u30d7\u30eb\u3067\u52b9\u679c\u7684</li> <li>\u9078\u629e\u57fa\u6e96\uff1a</li> <li>\u30bf\u30b9\u30af\u306e\u6027\u8cea\uff08\u56fa\u5b9a\u9577 vs \u53ef\u5909\u9577\uff09</li> <li>\u5916\u633f\u306e\u5fc5\u8981\u6027</li> <li>\u8a08\u7b97\u52b9\u7387</li> <li>\u5b9f\u88c5\u306e\u8907\u96d1\u3055</li> </ol> <p>\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306f\u3001Transformer\u304c\u300c\u9806\u5e8f\u300d\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u91cd\u8981\u306a\u8981\u7d20\u3067\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u6df1\u5c64\u5b66\u7fd2\u306e\u529b\u3092\u5f15\u304d\u51fa\u3059\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part2/positional-encoding/#_10","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1a2\u6b21\u5143\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff08\u753b\u50cf\u7528\uff09\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1a\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u6700\u5927\u9577\u304c100\u306e\u5834\u5408\u3001\u5404\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u304c\u9577\u3055200\u306e\u7cfb\u5217\u3067\u3069\u306e\u3088\u3046\u306b\u632f\u308b\u821e\u3046\u304b\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u6bd4\u8f03\u8ab2\u984c\uff1a\u540c\u3058\u30bf\u30b9\u30af\u3067\u7570\u306a\u308b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f7f\u3044\u3001\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1aRoPE\u304c\u306a\u305c\u76f8\u5bfe\u4f4d\u7f6e\u306e\u307f\u306b\u4f9d\u5b58\u3059\u308b\u3053\u3068\u3092\u6570\u5b66\u7684\u306b\u8a3c\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u5c64\u306e\u6982\u5ff5\u3068\u6df1\u5c64\u5b66\u7fd2\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part2/tokenization/","title":"\u5358\u8a9e\u306e\u6570\u5024\u8868\u73fe","text":""},{"location":"part2/tokenization/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u8a00\u8a9e\u3092\u6570\u5024\u306b\u5909\u63db\u3059\u308b\u6311\u6226","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u30b3\u30f3\u30d1\u30a4\u30e9\u3092\u4f5c\u308b\u969b\u3001\u6700\u521d\u306b\u884c\u3046\u306e\u306f\u5b57\u53e5\u89e3\u6790\uff08\u30ec\u30ad\u30b7\u30f3\u30b0\uff09\u3067\u3059\u3002\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3068\u3044\u3046\u6587\u5b57\u5217\u3092\u3001\u610f\u5473\u306e\u3042\u308b\u5358\u4f4d\uff08\u30c8\u30fc\u30af\u30f3\uff09\u306b\u5206\u5272\u3057\u307e\u3059\u3002\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u3067\u3082\u540c\u3058\u3053\u3068\u3092\u884c\u3044\u307e\u3059\u304c\u3001\u305d\u3053\u306b\u306f\u5927\u304d\u306a\u9055\u3044\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306f\u5f62\u5f0f\u8a00\u8a9e\u3067\u3059\u3002\u53b3\u5bc6\u306a\u6587\u6cd5\u898f\u5247\u304c\u3042\u308a\u3001\u66d6\u6627\u3055\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u4e00\u65b9\u3001\u81ea\u7136\u8a00\u8a9e\u306f\u66d6\u6627\u3055\u306e\u584a\u3067\u3059\u3002\u540c\u3058\u5358\u8a9e\u304c\u6587\u8108\u306b\u3088\u3063\u3066\u5168\u304f\u7570\u306a\u308b\u610f\u5473\u3092\u6301\u3061\u3001\u65b0\u3057\u3044\u5358\u8a9e\u304c\u65e5\u3005\u751f\u307e\u308c\u3001\u6587\u6cd5\u898f\u5247\u306b\u306f\u7121\u6570\u306e\u4f8b\u5916\u304c\u3042\u308a\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u3053\u306e\u6311\u6226\u7684\u306a\u554f\u984c\u306b\u5bfe\u3059\u308bTransformer\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u51e6\u7406\u306e\u77e5\u8b58\u3092\u6d3b\u304b\u3057\u306a\u304c\u3089\u7406\u89e3\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part2/tokenization/#51","title":"5.1 \u30c8\u30fc\u30af\u30f3\u5316\uff1a\u8a00\u8a9e\u306e\u539f\u5b50\u3092\u898b\u3064\u3051\u308b","text":""},{"location":"part2/tokenization/#vs","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e vs \u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316","text":"<pre><code>import re\nfrom typing import List, Dict, Tuple, Optional, Union\nfrom collections import Counter, defaultdict\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dataclasses import dataclass, field\n\nclass TokenizationComparison:\n    \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30f3\u5316\u306e\u6bd4\u8f03\"\"\"\n\n    def programming_language_tokenizer(self, code: str) -&gt; List[Tuple[str, str]]:\n        \"\"\"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u5b57\u53e5\u89e3\u6790\u5668\"\"\"\n        # \u30c8\u30fc\u30af\u30f3\u30d1\u30bf\u30fc\u30f3\u306e\u5b9a\u7fa9\n        token_specification = [\n            ('NUMBER',    r'\\d+(\\.\\d*)?'),                    # \u6570\u5024\n            ('IDENT',     r'[a-zA-Z_]\\w*'),                  # \u8b58\u5225\u5b50\n            ('STRING',    r'\"[^\"]*\"'),                        # \u6587\u5b57\u5217\n            ('COMMENT',   r'//[^\\n]*'),                       # \u30b3\u30e1\u30f3\u30c8\n            ('ASSIGN',    r'='),                              # \u4ee3\u5165\n            ('END',       r';'),                              # \u6587\u672b\n            ('OP',        r'[+\\-*/]'),                        # \u6f14\u7b97\u5b50\n            ('LPAREN',    r'\\('),                             # \u5de6\u62ec\u5f27\n            ('RPAREN',    r'\\)'),                             # \u53f3\u62ec\u5f27\n            ('LBRACE',    r'\\{'),                             # \u5de6\u6ce2\u62ec\u5f27\n            ('RBRACE',    r'\\}'),                             # \u53f3\u6ce2\u62ec\u5f27\n            ('SKIP',      r'[ \\t]+'),                         # \u30b9\u30da\u30fc\u30b9\n            ('NEWLINE',   r'\\n'),                             # \u6539\u884c\n            ('MISMATCH',  r'.'),                              # \u30a8\u30e9\u30fc\n        ]\n\n        tok_regex = '|'.join(f'(?P&lt;{name}&gt;{pattern})' for name, pattern in token_specification)\n        tokens = []\n\n        for match in re.finditer(tok_regex, code):\n            kind = match.lastgroup\n            value = match.group()\n            if kind not in ['SKIP', 'NEWLINE', 'COMMENT']:\n                tokens.append((kind, value))\n\n        return tokens\n\n    def natural_language_challenges(self) -&gt; None:\n        \"\"\"\u81ea\u7136\u8a00\u8a9e\u30c8\u30fc\u30af\u30f3\u5316\u306e\u8ab2\u984c\u3092\u5b9f\u6f14\"\"\"\n        print(\"=== \u81ea\u7136\u8a00\u8a9e\u30c8\u30fc\u30af\u30f3\u5316\u306e\u8ab2\u984c ===\")\n\n        # 1. \u5358\u8a9e\u5883\u754c\u306e\u66d6\u6627\u3055\n        examples = {\n            \"\u8907\u5408\u8a9e\": \"New York Times\",  # 1\u30c8\u30fc\u30af\u30f3\uff1f3\u30c8\u30fc\u30af\u30f3\uff1f\n            \"\u7e2e\u7d04\": \"don't\",             # \"do not\"\uff1f1\u30c8\u30fc\u30af\u30f3\uff1f\n            \"\u30cf\u30a4\u30d5\u30f3\": \"state-of-the-art\",  # \u3069\u3046\u5206\u5272\uff1f\n            \"\u6570\u5024\": \"$1,234.56\",         # \u901a\u8ca8\u8a18\u53f7\u3068\u6570\u5024\u3092\u5206\u3051\u308b\uff1f\n            \"\u7d75\u6587\u5b57\": \"Hello \ud83d\udc4b World\",   # \u7d75\u6587\u5b57\u306e\u6271\u3044\n            \"\u65e5\u672c\u8a9e\": \"\u79c1\u306f\u5b66\u751f\u3067\u3059\",      # \u30b9\u30da\u30fc\u30b9\u304c\u306a\u3044\u8a00\u8a9e\n        }\n\n        for category, text in examples.items():\n            print(f\"\\n{category}: '{text}'\")\n\n            # \u5358\u7d14\u306a\u30b9\u30da\u30fc\u30b9\u5206\u5272\n            simple_tokens = text.split()\n            print(f\"  \u30b9\u30da\u30fc\u30b9\u5206\u5272: {simple_tokens}\")\n\n            # \u3088\u308a\u6d17\u7df4\u3055\u308c\u305f\u5206\u5272\uff08\u5f8c\u8ff0\uff09\n\n    def why_subword_tokenization(self) -&gt; None:\n        \"\"\"\u306a\u305c\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\u5316\u304c\u5fc5\u8981\u304b\"\"\"\n\n        # \u554f\u984c1: \u8a9e\u5f59\u7206\u767a\n        print(\"\\n=== \u8a9e\u5f59\u7206\u767a\u306e\u554f\u984c ===\")\n\n        # \u82f1\u8a9e\u306e\u5358\u8a9e\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\n        word_variations = [\n            \"run\", \"runs\", \"running\", \"ran\", \"runner\", \"runners\",\n            \"runnable\", \"rerun\", \"overrun\", \"outrun\"\n        ]\n\n        print(\"'run'\u306e\u5909\u5316\u5f62:\", word_variations)\n        print(f\"\u5358\u8a9e\u30ec\u30d9\u30eb\u3067\u306f{len(word_variations)}\u500b\u306e\u7570\u306a\u308b\u30c8\u30fc\u30af\u30f3\u304c\u5fc5\u8981\")\n\n        # \u30b5\u30d6\u30ef\u30fc\u30c9\u5206\u5272\u306e\u4f8b\n        subword_splits = {\n            \"running\": [\"run\", \"##ning\"],\n            \"runners\": [\"run\", \"##ner\", \"##s\"],\n            \"unrunnable\": [\"un\", \"##run\", \"##nable\"],\n        }\n\n        print(\"\\n\u30b5\u30d6\u30ef\u30fc\u30c9\u5206\u5272:\")\n        for word, subwords in subword_splits.items():\n            print(f\"  {word} \u2192 {subwords}\")\n\n        # \u554f\u984c2: \u672a\u77e5\u8a9e\uff08OOV: Out-of-Vocabulary\uff09\n        print(\"\\n=== \u672a\u77e5\u8a9e\u306e\u554f\u984c ===\")\n\n        # \u8a13\u7df4\u6642\u306b\u898b\u305f\u3053\u3068\u304c\u306a\u3044\u5358\u8a9e\n        unknown_words = [\n            \"COVID-19\",      # \u65b0\u3057\u3044\u7528\u8a9e\n            \"Pneumonoultramicroscopicsilicovolcanoconiosis\",  # \u9577\u3044\u5c02\u9580\u7528\u8a9e\n            \"\ud83d\ude80\ud83c\udf1f\",          # \u7d75\u6587\u5b57\u306e\u7d44\u307f\u5408\u308f\u305b\n            \"supercalifragilisticexpialidocious\",  # \u9020\u8a9e\n        ]\n\n        print(\"\u672a\u77e5\u8a9e\u306e\u4f8b:\", unknown_words)\n        print(\"\u5358\u8a9e\u30ec\u30d9\u30eb\u3067\u306f\u5168\u3066[UNK]\u30c8\u30fc\u30af\u30f3\u306b\u306a\u3063\u3066\u3057\u307e\u3046\")\n</code></pre>"},{"location":"part2/tokenization/#byte-pair-encoding-bpe","title":"Byte-Pair Encoding (BPE) \u306e\u5b9f\u88c5","text":"<pre><code>@dataclass\nclass BPEToken:\n    \"\"\"BPE\u30c8\u30fc\u30af\u30f3\u306e\u30c7\u30fc\u30bf\u69cb\u9020\"\"\"\n    text: str\n    frequency: int = 0\n\nclass BytePairEncoding:\n    \"\"\"BPE\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size: int = 1000):\n        self.vocab_size = vocab_size\n        self.word_freq = Counter()\n        self.vocab = {}\n        self.merges = []\n\n    def train(self, corpus: List[str], verbose: bool = True) -&gt; None:\n        \"\"\"\u30b3\u30fc\u30d1\u30b9\u304b\u3089BPE\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\"\"\"\n        # \u30b9\u30c6\u30c3\u30d71: \u5358\u8a9e\u983b\u5ea6\u3092\u30ab\u30a6\u30f3\u30c8\n        for text in corpus:\n            words = text.lower().split()\n            for word in words:\n                # \u5358\u8a9e\u3092\u6587\u5b57\u5358\u4f4d\u306b\u5206\u5272\uff08\u7279\u6b8a\u306a\u7d42\u7aef\u8a18\u53f7\u3092\u8ffd\u52a0\uff09\n                word_tokens = list(word) + ['&lt;/w&gt;']\n                self.word_freq[tuple(word_tokens)] += 1\n\n        # \u521d\u671f\u8a9e\u5f59\uff08\u5168\u3066\u306e\u6587\u5b57\uff09\n        vocab = set()\n        for word_tokens, freq in self.word_freq.items():\n            for token in word_tokens:\n                vocab.add(token)\n\n        if verbose:\n            print(f\"\u521d\u671f\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(vocab)}\")\n            print(f\"\u521d\u671f\u8a9e\u5f59: {sorted(list(vocab))[:20]}...\")\n\n        # \u30b9\u30c6\u30c3\u30d72: \u30de\u30fc\u30b8\u3092\u7e70\u308a\u8fd4\u3059\n        num_merges = self.vocab_size - len(vocab)\n\n        for i in range(num_merges):\n            # \u6700\u3082\u983b\u5ea6\u306e\u9ad8\u3044\u30da\u30a2\u3092\u898b\u3064\u3051\u308b\n            pair_freq = self._get_pair_frequencies()\n\n            if not pair_freq:\n                break\n\n            best_pair = max(pair_freq, key=pair_freq.get)\n            self.merges.append(best_pair)\n\n            if verbose and i % 100 == 0:\n                print(f\"\\n\u30de\u30fc\u30b8 {i+1}: {best_pair} (\u983b\u5ea6: {pair_freq[best_pair]})\")\n\n            # \u8a9e\u5f59\u3092\u66f4\u65b0\n            self.word_freq = self._merge_pair(best_pair)\n\n            # \u65b0\u3057\u3044\u30c8\u30fc\u30af\u30f3\u3092\u8a9e\u5f59\u306b\u8ffd\u52a0\n            new_token = ''.join(best_pair)\n            vocab.add(new_token)\n\n        # \u6700\u7d42\u8a9e\u5f59\u3092\u4f5c\u6210\n        self.vocab = {token: idx for idx, token in enumerate(sorted(vocab))}\n\n        if verbose:\n            print(f\"\\n\u6700\u7d42\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n            print(f\"\u5b66\u7fd2\u3055\u308c\u305f\u30de\u30fc\u30b8\u6570: {len(self.merges)}\")\n\n    def _get_pair_frequencies(self) -&gt; Dict[Tuple[str, str], int]:\n        \"\"\"\u96a3\u63a5\u30c8\u30fc\u30af\u30f3\u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\"\"\"\n        pair_freq = defaultdict(int)\n\n        for word_tokens, freq in self.word_freq.items():\n            for i in range(len(word_tokens) - 1):\n                pair = (word_tokens[i], word_tokens[i + 1])\n                pair_freq[pair] += freq\n\n        return pair_freq\n\n    def _merge_pair(self, pair: Tuple[str, str]) -&gt; Counter:\n        \"\"\"\u6307\u5b9a\u3055\u308c\u305f\u30da\u30a2\u3092\u30de\u30fc\u30b8\"\"\"\n        new_word_freq = Counter()\n\n        for word_tokens, freq in self.word_freq.items():\n            new_word_tokens = []\n            i = 0\n\n            while i &lt; len(word_tokens):\n                # \u30da\u30a2\u304c\u898b\u3064\u304b\u3063\u305f\u3089\u30de\u30fc\u30b8\n                if i &lt; len(word_tokens) - 1 and \\\n                   word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:\n                    new_word_tokens.append(pair[0] + pair[1])\n                    i += 2\n                else:\n                    new_word_tokens.append(word_tokens[i])\n                    i += 1\n\n            new_word_freq[tuple(new_word_tokens)] = freq\n\n        return new_word_freq\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092BPE\u30c8\u30fc\u30af\u30f3\u306b\u5206\u5272\"\"\"\n        words = text.lower().split()\n        tokens = []\n\n        for word in words:\n            word_tokens = list(word) + ['&lt;/w&gt;']\n\n            # \u5b66\u7fd2\u3057\u305f\u30de\u30fc\u30b8\u3092\u9069\u7528\n            for pair in self.merges:\n                i = 0\n                new_word_tokens = []\n\n                while i &lt; len(word_tokens):\n                    if i &lt; len(word_tokens) - 1 and \\\n                       word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:\n                        new_word_tokens.append(pair[0] + pair[1])\n                        i += 2\n                    else:\n                        new_word_tokens.append(word_tokens[i])\n                        i += 1\n\n                word_tokens = new_word_tokens\n\n            tokens.extend(word_tokens)\n\n        return tokens\n\n    def visualize_tokenization(self, text: str) -&gt; None:\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u306e\u904e\u7a0b\u3092\u53ef\u8996\u5316\"\"\"\n        words = text.lower().split()\n\n        fig, axes = plt.subplots(len(words), 1, figsize=(12, 3 * len(words)))\n        if len(words) == 1:\n            axes = [axes]\n\n        for idx, word in enumerate(words):\n            ax = axes[idx]\n            word_tokens = list(word) + ['&lt;/w&gt;']\n\n            # \u5404\u30de\u30fc\u30b8\u30b9\u30c6\u30c3\u30d7\u3092\u8a18\u9332\n            steps = [word_tokens.copy()]\n\n            for pair in self.merges:\n                i = 0\n                new_word_tokens = []\n\n                while i &lt; len(word_tokens):\n                    if i &lt; len(word_tokens) - 1 and \\\n                       word_tokens[i] == pair[0] and word_tokens[i + 1] == pair[1]:\n                        new_word_tokens.append(pair[0] + pair[1])\n                        i += 2\n                    else:\n                        new_word_tokens.append(word_tokens[i])\n                        i += 1\n\n                if new_word_tokens != word_tokens:\n                    word_tokens = new_word_tokens\n                    steps.append(word_tokens.copy())\n\n            # \u53ef\u8996\u5316\n            y_labels = [f\"Step {i}\" for i in range(len(steps))]\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u7528\u306e\u30c7\u30fc\u30bf\u4f5c\u6210\n            max_len = max(len(step) for step in steps)\n            heatmap_data = np.zeros((len(steps), max_len))\n\n            for i, step in enumerate(steps):\n                for j, token in enumerate(step):\n                    heatmap_data[i, j] = len(token)\n\n            im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n\n            # \u30c8\u30fc\u30af\u30f3\u3092\u30c6\u30ad\u30b9\u30c8\u3068\u3057\u3066\u8868\u793a\n            for i, step in enumerate(steps):\n                for j, token in enumerate(step):\n                    ax.text(j, i, token, ha='center', va='center', fontsize=10)\n\n            ax.set_yticks(range(len(steps)))\n            ax.set_yticklabels(y_labels)\n            ax.set_xticks([])\n            ax.set_title(f\"'{word}'\u306e\u30c8\u30fc\u30af\u30f3\u5316\u904e\u7a0b\")\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#wordpiecesentencepiece","title":"WordPiece\u3068SentencePiece\u306e\u6bd4\u8f03","text":"<pre><code>class ModernTokenizers:\n    \"\"\"\u73fe\u4ee3\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6bd4\u8f03\"\"\"\n\n    def __init__(self):\n        self.tokenizers = {}\n\n    def compare_tokenization_methods(self, text: str) -&gt; None:\n        \"\"\"\u7570\u306a\u308b\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n        print(f\"=== \u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03 ===\")\n        print(f\"\u5165\u529b\u30c6\u30ad\u30b9\u30c8: '{text}'\")\n\n        # 1. \u5358\u8a9e\u30ec\u30d9\u30eb\n        word_tokens = text.split()\n        print(f\"\\n1. \u5358\u8a9e\u30ec\u30d9\u30eb: {word_tokens}\")\n        print(f\"   \u30c8\u30fc\u30af\u30f3\u6570: {len(word_tokens)}\")\n\n        # 2. \u6587\u5b57\u30ec\u30d9\u30eb\n        char_tokens = list(text)\n        print(f\"\\n2. \u6587\u5b57\u30ec\u30d9\u30eb: {char_tokens[:50]}...\")\n        print(f\"   \u30c8\u30fc\u30af\u30f3\u6570: {len(char_tokens)}\")\n\n        # 3. BPE\uff08\u7c21\u6613\u7248\uff09\n        bpe_tokens = self._simple_bpe_tokenize(text)\n        print(f\"\\n3. BPE: {bpe_tokens}\")\n        print(f\"   \u30c8\u30fc\u30af\u30f3\u6570: {len(bpe_tokens)}\")\n\n        # 4. WordPiece\uff08\u7c21\u6613\u7248\uff09\n        wordpiece_tokens = self._simple_wordpiece_tokenize(text)\n        print(f\"\\n4. WordPiece: {wordpiece_tokens}\")\n        print(f\"   \u30c8\u30fc\u30af\u30f3\u6570: {len(wordpiece_tokens)}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_tokenization_comparison(text, {\n            'Word': word_tokens,\n            'Character': char_tokens[:30],  # \u8868\u793a\u7528\u306b\u5236\u9650\n            'BPE': bpe_tokens,\n            'WordPiece': wordpiece_tokens\n        })\n\n    def _simple_bpe_tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\u7c21\u6613BPE\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        # \u5b9f\u969b\u306eBPE\u306f\u5b66\u7fd2\u304c\u5fc5\u8981\u3060\u304c\u3001\u3053\u3053\u3067\u306f\u7c21\u6613\u7248\n        tokens = []\n        for word in text.split():\n            if len(word) &gt; 6:\n                # \u9577\u3044\u5358\u8a9e\u306f\u5206\u5272\n                tokens.extend([word[:3], '##' + word[3:]])\n            else:\n                tokens.append(word)\n        return tokens\n\n    def _simple_wordpiece_tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\u7c21\u6613WordPiece\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        # \u5b9f\u969b\u306eWordPiece\u3082\u5b66\u7fd2\u304c\u5fc5\u8981\u3060\u304c\u3001\u3053\u3053\u3067\u306f\u7c21\u6613\u7248\n        common_prefixes = ['un', 're', 'pre', 'post', 'sub', 'over']\n        common_suffixes = ['ing', 'ed', 'er', 'est', 'ly', 'tion', 'ness']\n\n        tokens = []\n        for word in text.split():\n            tokenized = False\n\n            # \u63a5\u982d\u8f9e\u30c1\u30a7\u30c3\u30af\n            for prefix in common_prefixes:\n                if word.startswith(prefix) and len(word) &gt; len(prefix) + 2:\n                    tokens.extend([prefix, '##' + word[len(prefix):]])\n                    tokenized = True\n                    break\n\n            # \u63a5\u5c3e\u8f9e\u30c1\u30a7\u30c3\u30af\n            if not tokenized:\n                for suffix in common_suffixes:\n                    if word.endswith(suffix) and len(word) &gt; len(suffix) + 2:\n                        tokens.extend([word[:-len(suffix)], '##' + suffix])\n                        tokenized = True\n                        break\n\n            if not tokenized:\n                tokens.append(word)\n\n        return tokens\n\n    def _visualize_tokenization_comparison(self, text: str, tokenizations: Dict[str, List[str]]):\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\u3092\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 6))\n\n        methods = list(tokenizations.keys())\n        token_counts = [len(tokens) for tokens in tokenizations.values()]\n\n        bars = ax.bar(methods, token_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n\n        # \u5404\u30d0\u30fc\u306e\u4e0a\u306b\u30c8\u30fc\u30af\u30f3\u6570\u3092\u8868\u793a\n        for bar, count in zip(bars, token_counts):\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{count}', ha='center', va='bottom')\n\n        ax.set_xlabel('\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5')\n        ax.set_ylabel('\u30c8\u30fc\u30af\u30f3\u6570')\n        ax.set_title('\u7570\u306a\u308b\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u30c8\u30fc\u30af\u30f3\u6570\u6bd4\u8f03')\n\n        # \u7406\u60f3\u7684\u306a\u7bc4\u56f2\u3092\u8868\u793a\n        ax.axhspan(10, 30, alpha=0.2, color='green', label='\u7406\u60f3\u7684\u306a\u7bc4\u56f2')\n        ax.legend()\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#52","title":"5.2 \u5358\u8a9e\u57cb\u3081\u8fbc\u307f\uff1a\u610f\u5473\u3092\u6349\u3048\u308b\u30d9\u30af\u30c8\u30eb\u8868\u73fe","text":""},{"location":"part2/tokenization/#_3","title":"\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u9650\u754c","text":"<pre><code>class WordEmbeddings:\n    \"\"\"\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e\u6982\u5ff5\u3068\u5b9f\u88c5\"\"\"\n\n    def __init__(self, vocab_size: int = 10000, embedding_dim: int = 300):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.word_to_idx = {}\n        self.idx_to_word = {}\n        self.embeddings = None\n\n    def one_hot_problems(self) -&gt; None:\n        \"\"\"\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u554f\u984c\u70b9\"\"\"\n        print(\"=== \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u554f\u984c ===\")\n\n        # \u5c0f\u3055\u306a\u8a9e\u5f59\u3067\u306e\u4f8b\n        words = [\"cat\", \"dog\", \"animal\", \"car\", \"vehicle\"]\n        vocab_size = len(words)\n\n        # \u30ef\u30f3\u30db\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u4f5c\u6210\n        one_hot_vectors = {}\n        for idx, word in enumerate(words):\n            vector = np.zeros(vocab_size)\n            vector[idx] = 1\n            one_hot_vectors[word] = vector\n\n        print(\"\u30ef\u30f3\u30db\u30c3\u30c8\u30d9\u30af\u30c8\u30eb:\")\n        for word, vector in one_hot_vectors.items():\n            print(f\"{word}: {vector}\")\n\n        # \u554f\u984c1: \u30b9\u30d1\u30fc\u30b9\u6027\n        print(f\"\\n\u554f\u984c1: \u30b9\u30d1\u30fc\u30b9\u6027\")\n        print(f\"\u8a9e\u5f59\u30b5\u30a4\u30ba: {vocab_size}\")\n        print(f\"\u975e\u30bc\u30ed\u8981\u7d20: 1/{vocab_size} = {1/vocab_size:.1%}\")\n\n        # \u554f\u984c2: \u610f\u5473\u7684\u95a2\u4fc2\u306e\u6b20\u5982\n        print(f\"\\n\u554f\u984c2: \u610f\u5473\u7684\u95a2\u4fc2\u306e\u6b20\u5982\")\n\n        def cosine_similarity(v1, v2):\n            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n        similarities = {\n            \"cat vs dog\": cosine_similarity(one_hot_vectors[\"cat\"], one_hot_vectors[\"dog\"]),\n            \"cat vs animal\": cosine_similarity(one_hot_vectors[\"cat\"], one_hot_vectors[\"animal\"]),\n            \"car vs vehicle\": cosine_similarity(one_hot_vectors[\"car\"], one_hot_vectors[\"vehicle\"]),\n        }\n\n        for pair, sim in similarities.items():\n            print(f\"{pair}: {sim}\")\n        print(\"\u2192 \u5168\u3066\u306e\u5358\u8a9e\u9593\u306e\u985e\u4f3c\u5ea6\u304c0\uff08\u76f4\u4ea4\uff09\")\n\n        # \u554f\u984c3: \u30e1\u30e2\u30ea\u52b9\u7387\n        print(f\"\\n\u554f\u984c3: \u30e1\u30e2\u30ea\u52b9\u7387\")\n        real_vocab_size = 50000\n        print(f\"\u5b9f\u969b\u306e\u8a9e\u5f59\u30b5\u30a4\u30ba: {real_vocab_size:,}\")\n        memory_one_hot = real_vocab_size * real_vocab_size * 4  # float32\n        print(f\"\u5fc5\u8981\u30e1\u30e2\u30ea\uff08\u5168\u5358\u8a9e\uff09: {memory_one_hot / 1e9:.1f} GB\")\n\n    def dense_embeddings_intuition(self) -&gt; None:\n        \"\"\"\u5bc6\u306a\u57cb\u3081\u8fbc\u307f\u306e\u76f4\u611f\u7684\u7406\u89e3\"\"\"\n        print(\"\\n=== \u5bc6\u306a\u57cb\u3081\u8fbc\u307f\u306e\u5229\u70b9 ===\")\n\n        # \u4eee\u60f3\u7684\u306a\u57cb\u3081\u8fbc\u307f\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3067\u7372\u5f97\uff09\n        embeddings = {\n            \"cat\": np.array([0.2, 0.8, -0.1, 0.3]),\n            \"dog\": np.array([0.3, 0.7, -0.2, 0.4]),\n            \"animal\": np.array([0.25, 0.75, -0.15, 0.35]),\n            \"car\": np.array([-0.5, -0.3, 0.8, -0.2]),\n            \"vehicle\": np.array([-0.4, -0.35, 0.75, -0.15]),\n        }\n\n        embedding_dim = 4\n\n        print(f\"\u57cb\u3081\u8fbc\u307f\u6b21\u5143: {embedding_dim}\")\n        print(\"\\n\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb:\")\n        for word, vec in embeddings.items():\n            print(f\"{word}: {vec}\")\n\n        # \u610f\u5473\u7684\u985e\u4f3c\u5ea6\n        print(\"\\n\u610f\u5473\u7684\u985e\u4f3c\u5ea6\uff08\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\uff09:\")\n\n        def cosine_similarity(v1, v2):\n            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n        pairs = [\n            (\"cat\", \"dog\"),\n            (\"cat\", \"animal\"),\n            (\"car\", \"vehicle\"),\n            (\"cat\", \"car\"),\n        ]\n\n        for w1, w2 in pairs:\n            sim = cosine_similarity(embeddings[w1], embeddings[w2])\n            print(f\"{w1} vs {w2}: {sim:.3f}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_embeddings(embeddings)\n\n    def _visualize_embeddings(self, embeddings: Dict[str, np.ndarray]):\n        \"\"\"\u57cb\u3081\u8fbc\u307f\u306e\u53ef\u8996\u5316\uff082D\u6295\u5f71\uff09\"\"\"\n        from sklearn.decomposition import PCA\n\n        words = list(embeddings.keys())\n        vectors = np.array(list(embeddings.values()))\n\n        # PCA\u30672\u6b21\u5143\u306b\u524a\u6e1b\n        if vectors.shape[1] &gt; 2:\n            pca = PCA(n_components=2)\n            vectors_2d = pca.fit_transform(vectors)\n        else:\n            vectors_2d = vectors\n\n        plt.figure(figsize=(10, 8))\n\n        # \u5358\u8a9e\u3092\u30d7\u30ed\u30c3\u30c8\n        for i, word in enumerate(words):\n            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1], s=200)\n            plt.annotate(word, \n                        xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n                        xytext=(5, 5), \n                        textcoords='offset points',\n                        fontsize=12)\n\n        # \u985e\u4f3c\u3059\u308b\u5358\u8a9e\u3092\u7dda\u3067\u7d50\u3076\n        similar_pairs = [(\"cat\", \"dog\"), (\"cat\", \"animal\"), (\"car\", \"vehicle\")]\n        for w1, w2 in similar_pairs:\n            idx1 = words.index(w1)\n            idx2 = words.index(w2)\n            plt.plot([vectors_2d[idx1, 0], vectors_2d[idx2, 0]],\n                    [vectors_2d[idx1, 1], vectors_2d[idx2, 1]],\n                    'k--', alpha=0.3)\n\n        plt.xlabel('\u7b2c1\u4e3b\u6210\u5206')\n        plt.ylabel('\u7b2c2\u4e3b\u6210\u5206')\n        plt.title('\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306e2\u6b21\u5143\u53ef\u8996\u5316')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#_4","title":"\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u5b9f\u88c5\u3068\u5b66\u7fd2","text":"<pre><code>class EmbeddingLayer:\n    \"\"\"\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u5b9f\u88c5\u3068\u5b66\u7fd2\u904e\u7a0b\u306e\u7406\u89e3\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n\n        # \u57cb\u3081\u8fbc\u307f\u884c\u5217\u306e\u521d\u671f\u5316\n        self.embedding_matrix = torch.nn.Embedding(vocab_size, embedding_dim)\n\n        # \u521d\u671f\u5316\u65b9\u6cd5\u306e\u6bd4\u8f03\n        self._compare_initialization_methods()\n\n    def _compare_initialization_methods(self):\n        \"\"\"\u7570\u306a\u308b\u521d\u671f\u5316\u65b9\u6cd5\u306e\u6bd4\u8f03\"\"\"\n        methods = {\n            'uniform': lambda: torch.nn.init.uniform_(\n                torch.empty(self.vocab_size, self.embedding_dim), -0.1, 0.1\n            ),\n            'normal': lambda: torch.nn.init.normal_(\n                torch.empty(self.vocab_size, self.embedding_dim), 0, 0.02\n            ),\n            'xavier': lambda: torch.nn.init.xavier_uniform_(\n                torch.empty(self.vocab_size, self.embedding_dim)\n            ),\n        }\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, (name, init_fn) in enumerate(methods.items()):\n            matrix = init_fn()\n\n            ax = axes[idx]\n            im = ax.imshow(matrix[:50, :50], cmap='coolwarm', aspect='auto')\n            ax.set_title(f'{name.capitalize()} Initialization')\n            ax.set_xlabel('Embedding Dimension')\n            ax.set_ylabel('Word Index')\n            plt.colorbar(im, ax=ax)\n\n        plt.tight_layout()\n        plt.show()\n\n    def forward_pass_explained(self, word_indices: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\u57cb\u3081\u8fbc\u307f\u5c64\u306e\u9806\u4f1d\u64ad\u3092\u8a73\u3057\u304f\u8aac\u660e\"\"\"\n        print(\"=== \u57cb\u3081\u8fbc\u307f\u5c64\u306e\u9806\u4f1d\u64ad ===\")\n\n        # \u5165\u529b\u306e\u5f62\u72b6\n        print(f\"\u5165\u529b\uff08\u5358\u8a9e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\uff09: {word_indices}\")\n        print(f\"\u5165\u529b\u306e\u5f62\u72b6: {word_indices.shape}\")\n\n        # \u57cb\u3081\u8fbc\u307f\u884c\u5217\u306e\u5f62\u72b6\n        print(f\"\\n\u57cb\u3081\u8fbc\u307f\u884c\u5217\u306e\u5f62\u72b6: {self.embedding_matrix.weight.shape}\")\n        print(f\"  \u2192 {self.vocab_size} words \u00d7 {self.embedding_dim} dimensions\")\n\n        # \u30eb\u30c3\u30af\u30a2\u30c3\u30d7\u64cd\u4f5c\n        embeddings = self.embedding_matrix(word_indices)\n        print(f\"\\n\u51fa\u529b\u306e\u5f62\u72b6: {embeddings.shape}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_lookup_operation(word_indices, embeddings)\n\n        return embeddings\n\n    def _visualize_lookup_operation(self, indices: torch.Tensor, embeddings: torch.Tensor):\n        \"\"\"\u30eb\u30c3\u30af\u30a2\u30c3\u30d7\u64cd\u4f5c\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        # \u57cb\u3081\u8fbc\u307f\u884c\u5217\u5168\u4f53\n        ax = axes[0]\n        im = ax.imshow(self.embedding_matrix.weight.data[:20, :20], \n                       cmap='viridis', aspect='auto')\n        ax.set_title('\u57cb\u3081\u8fbc\u307f\u884c\u5217\uff08\u4e00\u90e8\uff09')\n        ax.set_xlabel('Dimension')\n        ax.set_ylabel('Word Index')\n\n        # \u9078\u629e\u3055\u308c\u305f\u884c\n        ax = axes[1]\n        selected_indices = indices.flatten().tolist()[:5]  # \u6700\u521d\u306e5\u3064\n        for i, idx in enumerate(selected_indices):\n            ax.axhline(y=idx, color='red', linewidth=2, alpha=0.7)\n        ax.set_ylim(-0.5, 19.5)\n        ax.set_title('\u9078\u629e\u3055\u308c\u305f\u5358\u8a9e')\n        ax.set_ylabel('Word Index')\n\n        # \u7d50\u679c\u306e\u57cb\u3081\u8fbc\u307f\n        ax = axes[2]\n        result = embeddings.reshape(-1, self.embedding_dim)[:5]\n        im = ax.imshow(result.detach(), cmap='viridis', aspect='auto')\n        ax.set_title('\u53d6\u5f97\u3055\u308c\u305f\u57cb\u3081\u8fbc\u307f')\n        ax.set_xlabel('Dimension')\n        ax.set_ylabel('Selected Words')\n\n        plt.tight_layout()\n        plt.show()\n\n    def gradient_flow_in_embeddings(self):\n        \"\"\"\u57cb\u3081\u8fbc\u307f\u5c64\u3067\u306e\u52fe\u914d\u306e\u6d41\u308c\"\"\"\n        print(\"\\n=== \u57cb\u3081\u8fbc\u307f\u5c64\u306e\u52fe\u914d\u66f4\u65b0 ===\")\n\n        # \u7c21\u5358\u306a\u4f8b\n        vocab_size = 10\n        embedding_dim = 4\n        embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n\n        # \u521d\u671f\u306e\u57cb\u3081\u8fbc\u307f\n        print(\"\u521d\u671f\u57cb\u3081\u8fbc\u307f\uff08word 3\uff09:\")\n        print(embedding.weight[3])\n\n        # \u9806\u4f1d\u64ad\n        word_indices = torch.tensor([3, 5, 3])  # word 3\u304c2\u56de\u51fa\u73fe\n        embedded = embedding(word_indices)\n\n        # \u4eee\u60f3\u7684\u306a\u640d\u5931\n        loss = embedded.sum()\n        loss.backward()\n\n        # \u52fe\u914d\u3092\u78ba\u8a8d\n        print(\"\\n\u52fe\u914d\uff08word 3\uff09:\")\n        print(embedding.weight.grad[3])\n        print(\"\u2192 word 3\u306f2\u56de\u4f7f\u308f\u308c\u305f\u306e\u3067\u3001\u52fe\u914d\u30822\u500d\")\n\n        # \u66f4\u65b0\n        with torch.no_grad():\n            embedding.weight -= 0.1 * embedding.weight.grad\n\n        print(\"\\n\u66f4\u65b0\u5f8c\u306e\u57cb\u3081\u8fbc\u307f\uff08word 3\uff09:\")\n        print(embedding.weight[3])\n</code></pre>"},{"location":"part2/tokenization/#_5","title":"\u4f4d\u7f6e\u3092\u8003\u616e\u3057\u305f\u57cb\u3081\u8fbc\u307f","text":"<pre><code>class PositionalAwareEmbeddings:\n    \"\"\"\u4f4d\u7f6e\u60c5\u5831\u3092\u8003\u616e\u3057\u305f\u57cb\u3081\u8fbc\u307f\"\"\"\n\n    def __init__(self, vocab_size: int, max_length: int, d_model: int):\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.d_model = d_model\n\n        # \u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\n        self.token_embedding = torch.nn.Embedding(vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\uff08\u5b66\u7fd2\u53ef\u80fd\uff09\n        self.position_embedding = torch.nn.Embedding(max_length, d_model)\n\n        # \u30bb\u30b0\u30e1\u30f3\u30c8\u57cb\u3081\u8fbc\u307f\uff08BERT\u30b9\u30bf\u30a4\u30eb\uff09\n        self.segment_embedding = torch.nn.Embedding(2, d_model)\n\n    def combined_embeddings(self, token_ids: torch.Tensor, \n                          position_ids: Optional[torch.Tensor] = None,\n                          segment_ids: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\u8907\u5408\u57cb\u3081\u8fbc\u307f\u306e\u8a08\u7b97\"\"\"\n        batch_size, seq_length = token_ids.shape\n\n        # \u30c8\u30fc\u30af\u30f3\u57cb\u3081\u8fbc\u307f\n        token_embeds = self.token_embedding(token_ids)\n\n        # \u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f\n        if position_ids is None:\n            position_ids = torch.arange(seq_length).expand(batch_size, -1)\n        position_embeds = self.position_embedding(position_ids)\n\n        # \u30bb\u30b0\u30e1\u30f3\u30c8\u57cb\u3081\u8fbc\u307f\n        if segment_ids is None:\n            segment_ids = torch.zeros_like(token_ids)\n        segment_embeds = self.segment_embedding(segment_ids)\n\n        # \u5408\u8a08\n        embeddings = token_embeds + position_embeds + segment_embeds\n\n        # \u53ef\u8996\u5316\n        self._visualize_embedding_components(\n            token_embeds[0], position_embeds[0], segment_embeds[0], embeddings[0]\n        )\n\n        return embeddings\n\n    def _visualize_embedding_components(self, token_emb, pos_emb, seg_emb, total_emb):\n        \"\"\"\u57cb\u3081\u8fbc\u307f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\n        components = [\n            (token_emb, 'Token Embeddings'),\n            (pos_emb, 'Position Embeddings'),\n            (seg_emb, 'Segment Embeddings'),\n            (total_emb, 'Combined Embeddings')\n        ]\n\n        for idx, (emb, title) in enumerate(components):\n            ax = axes[idx // 2, idx % 2]\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n            im = ax.imshow(emb[:10, :50].detach(), cmap='coolwarm', aspect='auto')\n            ax.set_title(title)\n            ax.set_xlabel('Embedding Dimension')\n            ax.set_ylabel('Position')\n            plt.colorbar(im, ax=ax)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#53","title":"5.3 \u5b9f\u8df5\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5","text":""},{"location":"part2/tokenization/#_6","title":"\u30ab\u30b9\u30bf\u30e0\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u69cb\u7bc9","text":"<pre><code>class CustomTokenizer:\n    \"\"\"\u5b9f\u7528\u7684\u306a\u30ab\u30b9\u30bf\u30e0\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\"\"\"\n\n    def __init__(self, vocab_size: int = 10000):\n        self.vocab_size = vocab_size\n        self.word_to_idx = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3, \"[MASK]\": 4}\n        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n        self.vocab_built = False\n\n    def build_vocab(self, texts: List[str], min_freq: int = 2) -&gt; None:\n        \"\"\"\u8a9e\u5f59\u3092\u69cb\u7bc9\"\"\"\n        print(\"=== \u8a9e\u5f59\u69cb\u7bc9 ===\")\n\n        # \u5358\u8a9e\u983b\u5ea6\u3092\u30ab\u30a6\u30f3\u30c8\n        word_freq = Counter()\n        for text in texts:\n            words = self._basic_tokenize(text)\n            word_freq.update(words)\n\n        print(f\"\u30e6\u30cb\u30fc\u30af\u306a\u5358\u8a9e\u6570: {len(word_freq)}\")\n        print(f\"\u6700\u983b\u51fa\u5358\u8a9e: {word_freq.most_common(10)}\")\n\n        # \u983b\u5ea6\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n        vocab_words = [word for word, freq in word_freq.items() if freq &gt;= min_freq]\n        vocab_words = vocab_words[:self.vocab_size - len(self.word_to_idx)]\n\n        # \u8a9e\u5f59\u306b\u8ffd\u52a0\n        for word in vocab_words:\n            if word not in self.word_to_idx:\n                idx = len(self.word_to_idx)\n                self.word_to_idx[word] = idx\n                self.idx_to_word[idx] = word\n\n        self.vocab_built = True\n        print(f\"\u6700\u7d42\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.word_to_idx)}\")\n\n    def _basic_tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\u57fa\u672c\u7684\u306a\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        # \u5c0f\u6587\u5b57\u5316\n        text = text.lower()\n\n        # \u53e5\u8aad\u70b9\u3092\u5206\u96e2\n        text = re.sub(r'([.!?,;:])', r' \\1 ', text)\n\n        # \u7a7a\u767d\u3067\u5206\u5272\n        tokens = text.split()\n\n        return tokens\n\n    def encode(self, text: str, max_length: Optional[int] = None, \n               padding: bool = True, truncation: bool = True) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\"\"\"\n        if not self.vocab_built:\n            raise ValueError(\"\u8a9e\u5f59\u304c\u69cb\u7bc9\u3055\u308c\u3066\u3044\u307e\u305b\u3093\u3002build_vocab()\u3092\u5148\u306b\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002\")\n\n        # \u30c8\u30fc\u30af\u30f3\u5316\n        tokens = self._basic_tokenize(text)\n\n        # CLS\u3068SEP\u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0\n        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n\n        # \u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\n        token_ids = []\n        for token in tokens:\n            token_ids.append(self.word_to_idx.get(token, self.word_to_idx[\"[UNK]\"]))\n\n        # \u5207\u308a\u8a70\u3081\u307e\u305f\u306f\u30d1\u30c7\u30a3\u30f3\u30b0\n        if max_length is not None:\n            if truncation and len(token_ids) &gt; max_length:\n                token_ids = token_ids[:max_length-1] + [self.word_to_idx[\"[SEP]\"]]\n\n            if padding and len(token_ids) &lt; max_length:\n                padding_length = max_length - len(token_ids)\n                token_ids = token_ids + [self.word_to_idx[\"[PAD]\"]] * padding_length\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u306e\u4f5c\u6210\n        attention_mask = [1 if token_id != self.word_to_idx[\"[PAD]\"] else 0 \n                         for token_id in token_ids]\n\n        return {\n            \"input_ids\": torch.tensor(token_ids),\n            \"attention_mask\": torch.tensor(attention_mask),\n            \"tokens\": tokens[:len(token_ids)]\n        }\n\n    def decode(self, token_ids: torch.Tensor, skip_special_tokens: bool = True) -&gt; str:\n        \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u30c6\u30ad\u30b9\u30c8\u306b\u5909\u63db\"\"\"\n        tokens = []\n\n        for token_id in token_ids:\n            token = self.idx_to_word.get(token_id.item(), \"[UNK]\")\n\n            if skip_special_tokens and token in [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:\n                continue\n\n            tokens.append(token)\n\n        return \" \".join(tokens)\n\n    def batch_encode(self, texts: List[str], max_length: int = 128) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\u30d0\u30c3\u30c1\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n        batch_encoding = {\n            \"input_ids\": [],\n            \"attention_mask\": []\n        }\n\n        for text in texts:\n            encoding = self.encode(text, max_length=max_length)\n            batch_encoding[\"input_ids\"].append(encoding[\"input_ids\"])\n            batch_encoding[\"attention_mask\"].append(encoding[\"attention_mask\"])\n\n        # \u30c6\u30f3\u30bd\u30eb\u306b\u5909\u63db\n        batch_encoding[\"input_ids\"] = torch.stack(batch_encoding[\"input_ids\"])\n        batch_encoding[\"attention_mask\"] = torch.stack(batch_encoding[\"attention_mask\"])\n\n        return batch_encoding\n\n    def visualize_tokenization(self, text: str, max_length: int = 20):\n        \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316\"\"\"\n        encoding = self.encode(text, max_length=max_length)\n\n        fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n\n        # \u30c8\u30fc\u30af\u30f3\n        ax = axes[0]\n        tokens = encoding[\"tokens\"]\n        y_pos = np.arange(len(tokens))\n        colors = ['red' if t.startswith('[') else 'blue' for t in tokens]\n        ax.barh(y_pos, [1]*len(tokens), color=colors, alpha=0.5)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(tokens)\n        ax.set_title('Tokens')\n        ax.set_xlim(0, 1)\n\n        # \u30c8\u30fc\u30af\u30f3ID\n        ax = axes[1]\n        token_ids = encoding[\"input_ids\"].tolist()\n        ax.barh(y_pos, [1]*len(token_ids), color='green', alpha=0.5)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(token_ids)\n        ax.set_title('Token IDs')\n        ax.set_xlim(0, 1)\n\n        # \u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30de\u30b9\u30af\n        ax = axes[2]\n        attention_mask = encoding[\"attention_mask\"].tolist()\n        colors = ['blue' if m == 1 else 'gray' for m in attention_mask]\n        ax.barh(y_pos, attention_mask, color=colors, alpha=0.5)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(attention_mask)\n        ax.set_title('Attention Mask')\n        ax.set_xlim(0, 1.1)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#_7","title":"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6027\u80fd\u8a55\u4fa1","text":"<pre><code>class TokenizerEvaluation:\n    \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6027\u80fd\u8a55\u4fa1\"\"\"\n\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def evaluate_coverage(self, test_texts: List[str]) -&gt; Dict[str, float]:\n        \"\"\"\u8a9e\u5f59\u30ab\u30d0\u30fc\u7387\u306e\u8a55\u4fa1\"\"\"\n        total_tokens = 0\n        unknown_tokens = 0\n\n        for text in test_texts:\n            tokens = self.tokenizer._basic_tokenize(text)\n            for token in tokens:\n                total_tokens += 1\n                if token not in self.tokenizer.word_to_idx:\n                    unknown_tokens += 1\n\n        coverage = 1 - (unknown_tokens / total_tokens)\n\n        return {\n            \"total_tokens\": total_tokens,\n            \"unknown_tokens\": unknown_tokens,\n            \"coverage\": coverage,\n            \"oov_rate\": unknown_tokens / total_tokens\n        }\n\n    def evaluate_compression(self, texts: List[str]) -&gt; Dict[str, float]:\n        \"\"\"\u5727\u7e2e\u7387\u306e\u8a55\u4fa1\"\"\"\n        original_chars = sum(len(text) for text in texts)\n\n        total_tokens = 0\n        for text in texts:\n            encoding = self.tokenizer.encode(text, padding=False)\n            total_tokens += len(encoding[\"input_ids\"])\n\n        compression_ratio = original_chars / total_tokens\n\n        return {\n            \"original_chars\": original_chars,\n            \"total_tokens\": total_tokens,\n            \"compression_ratio\": compression_ratio,\n            \"avg_chars_per_token\": compression_ratio\n        }\n\n    def visualize_evaluation(self, test_texts: List[str]):\n        \"\"\"\u8a55\u4fa1\u7d50\u679c\u306e\u53ef\u8996\u5316\"\"\"\n        coverage_stats = self.evaluate_coverage(test_texts)\n        compression_stats = self.evaluate_compression(test_texts)\n\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # \u30ab\u30d0\u30fc\u7387\n        ax = axes[0, 0]\n        ax.pie([coverage_stats[\"coverage\"], coverage_stats[\"oov_rate\"]], \n               labels=['Known', 'Unknown'], \n               autopct='%1.1f%%',\n               colors=['#2ecc71', '#e74c3c'])\n        ax.set_title('Vocabulary Coverage')\n\n        # \u30c8\u30fc\u30af\u30f3\u5206\u5e03\n        ax = axes[0, 1]\n        token_lengths = []\n        for text in test_texts[:100]:  # \u30b5\u30f3\u30d7\u30eb\n            encoding = self.tokenizer.encode(text, padding=False)\n            token_lengths.append(len(encoding[\"input_ids\"]))\n\n        ax.hist(token_lengths, bins=20, alpha=0.7, color='#3498db')\n        ax.set_xlabel('Number of Tokens')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Token Length Distribution')\n\n        # \u5727\u7e2e\u7d71\u8a08\n        ax = axes[1, 0]\n        metrics = ['Original\\nCharacters', 'Total\\nTokens', 'Compression\\nRatio']\n        values = [\n            compression_stats[\"original_chars\"] / 1000,  # \u30ad\u30ed\u5358\u4f4d\n            compression_stats[\"total_tokens\"] / 1000,\n            compression_stats[\"compression_ratio\"]\n        ]\n        bars = ax.bar(metrics, values, color=['#9b59b6', '#f39c12', '#1abc9c'])\n        ax.set_ylabel('Value (K) / Ratio')\n        ax.set_title('Compression Statistics')\n\n        # \u7d71\u8a08\u30b5\u30de\u30ea\u30fc\n        ax = axes[1, 1]\n        summary_text = f\"\"\"\nEvaluation Summary:\n==================\nCoverage: {coverage_stats['coverage']:.1%}\nOOV Rate: {coverage_stats['oov_rate']:.1%}\nUnknown Tokens: {coverage_stats['unknown_tokens']:,}\n\nCompression Ratio: {compression_stats['compression_ratio']:.2f}\nAvg Chars/Token: {compression_stats['avg_chars_per_token']:.2f}\n        \"\"\"\n        ax.text(0.1, 0.5, summary_text, transform=ax.transAxes, \n                fontsize=12, verticalalignment='center',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part2/tokenization/#_8","title":"\u307e\u3068\u3081\uff1a\u8a00\u8a9e\u3092\u6570\u5024\u306b\u5909\u63db\u3059\u308b\u6280\u8853","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060\u3053\u3068\u3092\u6574\u7406\u3057\u307e\u3057\u3087\u3046\uff1a</p> <ol> <li>\u30c8\u30fc\u30af\u30f3\u5316\u306e\u91cd\u8981\u6027</li> <li>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u5b57\u53e5\u89e3\u6790\u3068\u540c\u69d8\u306e\u5f79\u5272</li> <li>\u3057\u304b\u3057\u81ea\u7136\u8a00\u8a9e\u7279\u6709\u306e\u8ab2\u984c\uff08\u66d6\u6627\u6027\u3001\u65b0\u8a9e\u3001\u591a\u8a00\u8a9e\uff09\u3078\u306e\u5bfe\u5fdc\u304c\u5fc5\u8981</li> <li> <p>\u30b5\u30d6\u30ef\u30fc\u30c9\u30c8\u30fc\u30af\u30f3\u5316\uff08BPE\u3001WordPiece\uff09\u306b\u3088\u308b\u67d4\u8edf\u306a\u5bfe\u5fdc</p> </li> <li> <p>\u57cb\u3081\u8fbc\u307f\u306e\u672c\u8cea</p> </li> <li>\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u9650\u754c\u3092\u8d85\u3048\u308b</li> <li>\u610f\u5473\u7684\u306a\u95a2\u4fc2\u3092\u6349\u3048\u308b\u5bc6\u306a\u30d9\u30af\u30c8\u30eb\u8868\u73fe</li> <li> <p>\u5b66\u7fd2\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u3066\u306e\u57cb\u3081\u8fbc\u307f\u884c\u5217</p> </li> <li> <p>\u5b9f\u88c5\u4e0a\u306e\u8003\u616e\u70b9</p> </li> <li>\u7279\u6b8a\u30c8\u30fc\u30af\u30f3\uff08[CLS]\u3001[SEP]\u3001[PAD]\uff09\u306e\u6271\u3044</li> <li>\u30d0\u30c3\u30c1\u51e6\u7406\u306e\u305f\u3081\u306e\u7d71\u4e00\u7684\u306a\u9577\u3055</li> <li>\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u306b\u3088\u308b\u7121\u52b9\u9818\u57df\u306e\u7ba1\u7406</li> </ol> <p>\u6b21\u7ae0\u3067\u306f\u3001\u3053\u306e\u6570\u5024\u8868\u73fe\u3055\u308c\u305f\u8a00\u8a9e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u3001Transformer\u306e\u6838\u5fc3\u3067\u3042\u308b\u300c\u6ce8\u610f\u6a5f\u69cb\u300d\u304c\u3069\u306e\u3088\u3046\u306b\u50cd\u304f\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part2/tokenization/#_9","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>BPE\u5b9f\u88c5\u306e\u62e1\u5f35: \u65e5\u672c\u8a9e\u306e\u3088\u3046\u306a\u7a7a\u767d\u3067\u533a\u5207\u3089\u308c\u306a\u3044\u8a00\u8a9e\u306b\u5bfe\u5fdc\u3057\u305fBPE\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u57cb\u3081\u8fbc\u307f\u306e\u53ef\u8996\u5316: Word2Vec\u30b9\u30bf\u30a4\u30eb\u306e\u985e\u63a8\u30bf\u30b9\u30af\uff08king - man + woman = queen\uff09\u3092\u3001\u5b66\u7fd2\u6e08\u307f\u57cb\u3081\u8fbc\u307f\u3067\u691c\u8a3c\u3059\u308b\u30b3\u30fc\u30c9\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6700\u9069\u5316: \u4e0e\u3048\u3089\u308c\u305f\u30b3\u30fc\u30d1\u30b9\u306b\u5bfe\u3057\u3066\u3001\u6700\u9069\u306a\u8a9e\u5f59\u30b5\u30a4\u30ba\u3092\u81ea\u52d5\u7684\u306b\u6c7a\u5b9a\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u8a2d\u8a08\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u30de\u30eb\u30c1\u8a00\u8a9e\u5bfe\u5fdc: \u8907\u6570\u8a00\u8a9e\u3092\u540c\u6642\u306b\u6271\u3048\u308b\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3092\u5b9f\u88c5\u3057\u3001\u8a00\u8a9e\u9593\u3067\u306e\u30c8\u30fc\u30af\u30f3\u306e\u5171\u6709\u306b\u3064\u3044\u3066\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u6ce8\u610f\u6a5f\u69cb\u306e\u76f4\u611f\u7684\u7406\u89e3\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part3/encoder-decoder/","title":"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020","text":""},{"location":"part3/encoder-decoder/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u5909\u63db\u306e\u672c\u8cea","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\uff08\u5165\u529b\uff09\u3092\u6a5f\u68b0\u8a9e\uff08\u51fa\u529b\uff09\u306b\u5909\u63db\u3059\u308b\u904e\u7a0b\u3067\u3001\u307e\u305a\u5165\u529b\u3092\u5b8c\u5168\u306b\u89e3\u6790\u3057\u3066\u4e2d\u9593\u8868\u73fe\uff08AST\uff09\u3092\u69cb\u7bc9\u3057\u3001\u305d\u308c\u304b\u3089\u51fa\u529b\u3092\u751f\u6210\u3057\u307e\u3059\u3002\u3053\u306e\u300c\u7406\u89e3\u3057\u3066\u304b\u3089\u751f\u6210\u3059\u308b\u300d\u3068\u3044\u30462\u6bb5\u968e\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u3001Transformer\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u306e\u672c\u8cea\u3067\u3059\u3002</p> <p>\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306f\u5165\u529b\u5168\u4f53\u3092\u7406\u89e3\u3057\u3001\u8c4a\u304b\u306a\u5185\u90e8\u8868\u73fe\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002\u30c7\u30b3\u30fc\u30c0\u30fc\u306f\u305d\u306e\u8868\u73fe\u3092\u53c2\u7167\u3057\u306a\u304c\u3089\u3001\u4e00\u3064\u305a\u3064\u51fa\u529b\u3092\u751f\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u610f\u5473\u3092\u5b8c\u5168\u306b\u7406\u89e3\u3057\u3066\u304b\u3089\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u8a00\u8a9e\u306e\u30b3\u30fc\u30c9\u3092\u751f\u6210\u3059\u308b\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u52d5\u4f5c\u3068\u3088\u304f\u4f3c\u3066\u3044\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3068\u30c7\u30b3\u30fc\u30c0\u30fc\u304c\u3069\u306e\u3088\u3046\u306b\u5354\u8abf\u3057\u3066\u52d5\u4f5c\u3057\u3001\u306a\u305c\u3053\u306e\u69cb\u9020\u304c\u7ffb\u8a33\u306a\u3069\u306e\u30bf\u30b9\u30af\u306b\u52b9\u679c\u7684\u306a\u306e\u304b\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/encoder-decoder/#121","title":"12.1 \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u5f79\u5272","text":""},{"location":"part3/encoder-decoder/#_3","title":"\u5165\u529b\u306e\u5b8c\u5168\u306a\u7406\u89e3","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Any\nimport math\nfrom matplotlib.patches import Rectangle, FancyBboxPatch, Circle, FancyArrowPatch\nfrom matplotlib.patches import ConnectionPatch\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nclass TransformerEncoder:\n    \"\"\"Transformer\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u5b9f\u88c5\u3068\u89e3\u8aac\"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8, \n                 d_ff: int = 2048, n_layers: int = 6):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_ff = d_ff\n        self.n_layers = n_layers\n\n    def create_encoder_layer(self) -&gt; nn.Module:\n        \"\"\"\u5358\u4e00\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\u3092\u4f5c\u6210\"\"\"\n        class EncoderLayer(nn.Module):\n            def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n                super().__init__()\n\n                # Multi-Head Attention\n                self.self_attn = nn.MultiheadAttention(\n                    d_model, n_heads, dropout=dropout, batch_first=True\n                )\n\n                # Feed Forward Network\n                self.feed_forward = nn.Sequential(\n                    nn.Linear(d_model, d_ff),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(d_ff, d_model)\n                )\n\n                # Layer Normalization\n                self.norm1 = nn.LayerNorm(d_model)\n                self.norm2 = nn.LayerNorm(d_model)\n\n                # Dropout\n                self.dropout = nn.Dropout(dropout)\n\n            def forward(self, x, mask=None):\n                # Self-Attention with residual connection\n                attn_output, attn_weights = self.self_attn(\n                    x, x, x, attn_mask=mask\n                )\n                x = x + self.dropout(attn_output)\n                x = self.norm1(x)\n\n                # Feed Forward with residual connection\n                ff_output = self.feed_forward(x)\n                x = x + self.dropout(ff_output)\n                x = self.norm2(x)\n\n                return x, attn_weights\n\n        return EncoderLayer(self.d_model, self.n_heads, self.d_ff)\n\n    def create_full_encoder(self) -&gt; nn.Module:\n        \"\"\"\u5b8c\u5168\u306a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3092\u4f5c\u6210\"\"\"\n        class Encoder(nn.Module):\n            def __init__(self, n_layers, d_model, n_heads, d_ff, \n                        vocab_size, max_len=5000, dropout=0.1):\n                super().__init__()\n\n                # \u57cb\u3081\u8fbc\u307f\u5c64\n                self.embedding = nn.Embedding(vocab_size, d_model)\n                self.pos_encoding = self._create_positional_encoding(\n                    max_len, d_model\n                )\n\n                # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\u306e\u30b9\u30bf\u30c3\u30af\n                self.layers = nn.ModuleList([\n                    EncoderLayer(d_model, n_heads, d_ff, dropout)\n                    for _ in range(n_layers)\n                ])\n\n                self.dropout = nn.Dropout(dropout)\n                self.scale = math.sqrt(d_model)\n\n            def _create_positional_encoding(self, max_len, d_model):\n                \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u4f5c\u6210\"\"\"\n                pe = torch.zeros(max_len, d_model)\n                position = torch.arange(0, max_len).unsqueeze(1).float()\n\n                div_term = torch.exp(\n                    torch.arange(0, d_model, 2).float() * \n                    -(math.log(10000.0) / d_model)\n                )\n\n                pe[:, 0::2] = torch.sin(position * div_term)\n                pe[:, 1::2] = torch.cos(position * div_term)\n\n                return nn.Parameter(pe.unsqueeze(0), requires_grad=False)\n\n            def forward(self, src, mask=None):\n                # \u57cb\u3081\u8fbc\u307f\u3068\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n                seq_len = src.size(1)\n                x = self.embedding(src) * self.scale\n                x = x + self.pos_encoding[:, :seq_len]\n                x = self.dropout(x)\n\n                # \u5404\u5c64\u306e\u51fa\u529b\u3092\u4fdd\u5b58\uff08\u5206\u6790\u7528\uff09\n                layer_outputs = []\n                attention_weights = []\n\n                # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\u3092\u9806\u6b21\u9069\u7528\n                for layer in self.layers:\n                    x, attn = layer(x, mask)\n                    layer_outputs.append(x)\n                    attention_weights.append(attn)\n\n                return x, layer_outputs, attention_weights\n\n        return Encoder(\n            self.n_layers, self.d_model, self.n_heads, \n            self.d_ff, vocab_size=10000\n        )\n\n    def visualize_encoder_process(self):\n        \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u51e6\u7406\u904e\u7a0b\u3092\u53ef\u8996\u5316\"\"\"\n        print(\"=== \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u51e6\u7406\u904e\u7a0b ===\\n\")\n\n        # \u30b5\u30f3\u30d7\u30eb\u5165\u529b\n        sample_text = \"The cat sat on the mat\"\n        tokens = sample_text.split()\n        seq_len = len(tokens)\n\n        # \u30c0\u30df\u30fc\u306e\u30c8\u30fc\u30af\u30f3ID\n        token_ids = torch.randint(0, 1000, (1, seq_len))\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3092\u4f5c\u6210\u3057\u3066\u5b9f\u884c\n        encoder = self.create_full_encoder()\n        encoder.eval()\n\n        with torch.no_grad():\n            output, layer_outputs, attention_weights = encoder(token_ids)\n\n        # \u51e6\u7406\u904e\u7a0b\u306e\u53ef\u8996\u5316\n        self._plot_encoding_progression(tokens, layer_outputs)\n\n        print(\"\\n\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u7279\u5fb4:\")\n        print(\"\u2713 \u53cc\u65b9\u5411\u306e\u6587\u8108\u7406\u89e3\")\n        print(\"\u2713 \u4e26\u5217\u51e6\u7406\u53ef\u80fd\")\n        print(\"\u2713 \u5165\u529b\u5168\u4f53\u306e\u4fef\u77b0\u7684\u7406\u89e3\")\n        print(\"\u2713 \u968e\u5c64\u7684\u306a\u7279\u5fb4\u62bd\u51fa\")\n\n    def _plot_encoding_progression(self, tokens, layer_outputs):\n        \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u9032\u884c\u3092\u53ef\u8996\u5316\"\"\"\n        n_layers = len(layer_outputs)\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        axes = axes.flatten()\n\n        for layer_idx in range(min(n_layers, 6)):\n            ax = axes[layer_idx]\n\n            # \u5404\u5c64\u306e\u51fa\u529b\u306e\u6d3b\u6027\u5316\u30d1\u30bf\u30fc\u30f3\n            activations = layer_outputs[layer_idx][0].mean(dim=-1).numpy()\n\n            # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3068\u3057\u3066\u8868\u793a\n            im = ax.imshow(activations.reshape(-1, 1), cmap='RdBu_r', \n                          aspect='auto', vmin=-1, vmax=1)\n\n            ax.set_yticks(range(len(tokens)))\n            ax.set_yticklabels(tokens)\n            ax.set_xticks([])\n            ax.set_title(f'Layer {layer_idx + 1}')\n\n            # \u30ab\u30e9\u30fc\u30d0\u30fc\n            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n        plt.suptitle('\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5404\u5c64\u3067\u306e\u8868\u73fe\u306e\u5909\u5316', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\nclass TransformerDecoder:\n    \"\"\"Transformer\u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u5b9f\u88c5\u3068\u89e3\u8aac\"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8, \n                 d_ff: int = 2048, n_layers: int = 6):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_ff = d_ff\n        self.n_layers = n_layers\n\n    def create_decoder_layer(self) -&gt; nn.Module:\n        \"\"\"\u5358\u4e00\u306e\u30c7\u30b3\u30fc\u30c0\u30fc\u5c64\u3092\u4f5c\u6210\"\"\"\n        class DecoderLayer(nn.Module):\n            def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n                super().__init__()\n\n                # Masked Self-Attention\n                self.self_attn = nn.MultiheadAttention(\n                    d_model, n_heads, dropout=dropout, batch_first=True\n                )\n\n                # Cross-Attention\uff08\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u51fa\u529b\u3078\u306e\u6ce8\u610f\uff09\n                self.cross_attn = nn.MultiheadAttention(\n                    d_model, n_heads, dropout=dropout, batch_first=True\n                )\n\n                # Feed Forward Network\n                self.feed_forward = nn.Sequential(\n                    nn.Linear(d_model, d_ff),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(d_ff, d_model)\n                )\n\n                # Layer Normalization\n                self.norm1 = nn.LayerNorm(d_model)\n                self.norm2 = nn.LayerNorm(d_model)\n                self.norm3 = nn.LayerNorm(d_model)\n\n                # Dropout\n                self.dropout = nn.Dropout(dropout)\n\n            def forward(self, x, encoder_output, \n                       self_attn_mask=None, cross_attn_mask=None):\n                # Masked Self-Attention\n                self_attn_output, self_attn_weights = self.self_attn(\n                    x, x, x, attn_mask=self_attn_mask\n                )\n                x = x + self.dropout(self_attn_output)\n                x = self.norm1(x)\n\n                # Cross-Attention\n                cross_attn_output, cross_attn_weights = self.cross_attn(\n                    x, encoder_output, encoder_output, \n                    attn_mask=cross_attn_mask\n                )\n                x = x + self.dropout(cross_attn_output)\n                x = self.norm2(x)\n\n                # Feed Forward\n                ff_output = self.feed_forward(x)\n                x = x + self.dropout(ff_output)\n                x = self.norm3(x)\n\n                return x, self_attn_weights, cross_attn_weights\n\n        return DecoderLayer(self.d_model, self.n_heads, self.d_ff)\n\n    def demonstrate_masked_attention(self):\n        \"\"\"Masked Attention\u306e\u52d5\u4f5c\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== Masked Self-Attention ===\\n\")\n\n        seq_len = 6\n        d_model = 64\n\n        # \u30de\u30b9\u30af\u306e\u4f5c\u6210\n        mask = self._create_causal_mask(seq_len)\n\n        # \u53ef\u8996\u5316\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u30de\u30b9\u30af\u30d1\u30bf\u30fc\u30f3\n        ax1.imshow(mask, cmap='RdBu_r', aspect='auto')\n        ax1.set_title('Causal Mask (0: \u53ef\u8996, -\u221e: \u4e0d\u53ef\u8996)')\n        ax1.set_xlabel('Key\u4f4d\u7f6e')\n        ax1.set_ylabel('Query\u4f4d\u7f6e')\n\n        # \u6ce8\u610f\u306e\u6d41\u308c\n        ax2.set_xlim(0, seq_len)\n        ax2.set_ylim(0, seq_len)\n        ax2.set_aspect('equal')\n\n        # \u5404\u4f4d\u7f6e\u304b\u3089\u306e\u6ce8\u610f\u3092\u77e2\u5370\u3067\u8868\u793a\n        for i in range(seq_len):\n            for j in range(i + 1):  # i\u756a\u76ee\u306f0\u301ci\u756a\u76ee\u307e\u3067\u3092\u898b\u308b\n                ax2.arrow(j + 0.5, i + 0.5, \n                         (i - j) * 0.3, 0,\n                         head_width=0.15, head_length=0.1,\n                         fc='blue', ec='blue', alpha=0.5)\n\n        ax2.set_title('\u6ce8\u610f\u306e\u65b9\u5411\uff08\u904e\u53bb\u306e\u307f\u53c2\u7167\uff09')\n        ax2.set_xlabel('\u4f4d\u7f6e')\n        ax2.set_ylabel('\u73fe\u5728\u306e\u4f4d\u7f6e')\n        ax2.invert_yaxis()\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"\u7279\u5fb4:\")\n        print(\"\u2713 \u5404\u4f4d\u7f6e\u306f\u81ea\u5206\u3088\u308a\u524d\u306e\u4f4d\u7f6e\u306e\u307f\u53c2\u7167\u53ef\u80fd\")\n        print(\"\u2713 \u81ea\u5df1\u56de\u5e30\u7684\u306a\u751f\u6210\u3092\u53ef\u80fd\u306b\u3059\u308b\")\n        print(\"\u2713 \u5b66\u7fd2\u6642\u3068\u63a8\u8ad6\u6642\u306e\u4e00\u8cab\u6027\u3092\u4fdd\u3064\")\n\n    def _create_causal_mask(self, size: int) -&gt; torch.Tensor:\n        \"\"\"\u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u4f5c\u6210\"\"\"\n        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n        return mask\n\n    def visualize_cross_attention(self):\n        \"\"\"Cross-Attention\u3092\u53ef\u8996\u5316\"\"\"\n        print(\"\\n=== Cross-Attention ===\\n\")\n\n        # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n        src_tokens = [\"The\", \"cat\", \"is\", \"sleeping\"]\n        tgt_tokens = [\"\u732b\", \"\u304c\", \"\u5bdd\u3066\", \"\u3044\u308b\"]\n\n        # \u30e9\u30f3\u30c0\u30e0\u306a\u6ce8\u610f\u91cd\u307f\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3055\u308c\u308b\uff09\n        torch.manual_seed(42)\n        attention_weights = torch.softmax(\n            torch.randn(len(tgt_tokens), len(src_tokens)), dim=-1\n        )\n\n        # \u53ef\u8996\u5316\n        fig, ax = plt.subplots(figsize=(8, 6))\n\n        im = ax.imshow(attention_weights, cmap='Blues', aspect='auto')\n\n        # \u30e9\u30d9\u30eb\n        ax.set_xticks(range(len(src_tokens)))\n        ax.set_xticklabels(src_tokens)\n        ax.set_yticks(range(len(tgt_tokens)))\n        ax.set_yticklabels(tgt_tokens)\n\n        # \u5024\u3092\u8868\u793a\n        for i in range(len(tgt_tokens)):\n            for j in range(len(src_tokens)):\n                text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',\n                             ha=\"center\", va=\"center\", color=\"black\")\n\n        ax.set_xlabel('\u30bd\u30fc\u30b9\uff08\u82f1\u8a9e\uff09')\n        ax.set_ylabel('\u30bf\u30fc\u30b2\u30c3\u30c8\uff08\u65e5\u672c\u8a9e\uff09')\n        ax.set_title('Cross-Attention: \u30c7\u30b3\u30fc\u30c0\u30fc\u304c\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u51fa\u529b\u306b\u6ce8\u76ee')\n\n        plt.colorbar(im, ax=ax)\n        plt.tight_layout()\n        plt.show()\n\n        print(\"Cross-Attention\u306e\u5f79\u5272:\")\n        print(\"\u2713 \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u60c5\u5831\u3092\u9078\u629e\u7684\u306b\u5229\u7528\")\n        print(\"\u2713 \u30bd\u30fc\u30b9\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5bfe\u5fdc\u95a2\u4fc2\u3092\u5b66\u7fd2\")\n        print(\"\u2713 \u6587\u8108\u306b\u5fdc\u3058\u305f\u52d5\u7684\u306a\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\")\n\nclass EncoderDecoderArchitecture:\n    \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u5168\u4f53\u69cb\u9020\"\"\"\n\n    def __init__(self):\n        self.encoder = TransformerEncoder()\n        self.decoder = TransformerDecoder()\n\n    def explain_information_flow(self):\n        \"\"\"\u60c5\u5831\u306e\u6d41\u308c\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u60c5\u5831\u30d5\u30ed\u30fc ===\\n\")\n\n        fig, ax = plt.subplots(figsize=(14, 10))\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\n        encoder_x = 2\n        encoder_y = 7\n        encoder_width = 4\n        encoder_height = 6\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30d6\u30ed\u30c3\u30af\n        encoder_rect = FancyBboxPatch(\n            (encoder_x, encoder_y), encoder_width, encoder_height,\n            boxstyle=\"round,pad=0.1\", \n            facecolor='lightblue', edgecolor='darkblue', linewidth=2\n        )\n        ax.add_patch(encoder_rect)\n        ax.text(encoder_x + encoder_width/2, encoder_y + encoder_height + 0.5,\n                '\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc', ha='center', fontsize=14, weight='bold')\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\n        for i in range(3):\n            layer_y = encoder_y + encoder_height - (i + 1) * 1.8\n            layer_rect = Rectangle(\n                (encoder_x + 0.5, layer_y), encoder_width - 1, 1.2,\n                facecolor='white', edgecolor='darkblue'\n            )\n            ax.add_patch(layer_rect)\n            ax.text(encoder_x + encoder_width/2, layer_y + 0.6,\n                   f'\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64 {i+1}', ha='center', fontsize=10)\n\n        # \u30c7\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\n        decoder_x = 8\n        decoder_y = 7\n        decoder_width = 4\n        decoder_height = 6\n\n        # \u30c7\u30b3\u30fc\u30c0\u30fc\u30d6\u30ed\u30c3\u30af\n        decoder_rect = FancyBboxPatch(\n            (decoder_x, decoder_y), decoder_width, decoder_height,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightcoral', edgecolor='darkred', linewidth=2\n        )\n        ax.add_patch(decoder_rect)\n        ax.text(decoder_x + decoder_width/2, decoder_y + decoder_height + 0.5,\n                '\u30c7\u30b3\u30fc\u30c0\u30fc', ha='center', fontsize=14, weight='bold')\n\n        # \u30c7\u30b3\u30fc\u30c0\u30fc\u5c64\n        for i in range(3):\n            layer_y = decoder_y + decoder_height - (i + 1) * 1.8\n            layer_rect = Rectangle(\n                (decoder_x + 0.5, layer_y), decoder_width - 1, 1.2,\n                facecolor='white', edgecolor='darkred'\n            )\n            ax.add_patch(layer_rect)\n            ax.text(decoder_x + decoder_width/2, layer_y + 0.6,\n                   f'\u30c7\u30b3\u30fc\u30c0\u30fc\u5c64 {i+1}', ha='center', fontsize=10)\n\n        # Cross-Attention\u63a5\u7d9a\n        for i in range(3):\n            encoder_layer_y = encoder_y + encoder_height - (i + 1) * 1.8 + 0.6\n            decoder_layer_y = decoder_y + decoder_height - (i + 1) * 1.8 + 0.6\n\n            arrow = FancyArrowPatch(\n                (encoder_x + encoder_width, encoder_layer_y),\n                (decoder_x, decoder_layer_y),\n                connectionstyle=\"arc3,rad=0.2\",\n                arrowstyle='-&gt;', mutation_scale=20,\n                color='green', linewidth=2\n            )\n            ax.add_patch(arrow)\n\n        # \u5165\u529b\u3068\u51fa\u529b\n        ax.text(encoder_x + encoder_width/2, encoder_y - 1,\n                '\u5165\u529b\u30b7\u30fc\u30b1\u30f3\u30b9', ha='center', fontsize=12,\n                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow'))\n\n        ax.text(decoder_x + decoder_width/2, decoder_y - 1,\n                '\u51fa\u529b\u30b7\u30fc\u30b1\u30f3\u30b9', ha='center', fontsize=12,\n                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow'))\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u51fa\u529b\n        ax.text(encoder_x + encoder_width/2, encoder_y + encoder_height + 1.5,\n                '\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u8868\u73fe', ha='center', fontsize=11,\n                style='italic', color='darkblue')\n\n        # \u51e1\u4f8b\n        legend_elements = [\n            Line2D([0], [0], color='green', linewidth=2, \n                   label='Cross-Attention'),\n            mpatches.Patch(facecolor='lightblue', edgecolor='darkblue',\n                          label='\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc'),\n            mpatches.Patch(facecolor='lightcoral', edgecolor='darkred',\n                          label='\u30c7\u30b3\u30fc\u30c0\u30fc')\n        ]\n        ax.legend(handles=legend_elements, loc='upper right')\n\n        ax.set_xlim(0, 14)\n        ax.set_ylim(5, 15)\n        ax.axis('off')\n        ax.set_title('\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u306e\u60c5\u5831\u30d5\u30ed\u30fc', \n                    fontsize=16, weight='bold', pad=20)\n\n        plt.tight_layout()\n        plt.show()\n\n        print(\"\u60c5\u5831\u306e\u6d41\u308c:\")\n        print(\"1. \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u304c\u5165\u529b\u5168\u4f53\u3092\u51e6\u7406\")\n        print(\"2. \u5404\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5c64\u304c\u6587\u8108\u60c5\u5831\u3092\u62bd\u51fa\u30fb\u6d17\u7df4\")\n        print(\"3. \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u6700\u7d42\u51fa\u529b\u304c\u8c4a\u304b\u306a\u6587\u8108\u8868\u73fe\u306b\")\n        print(\"4. \u30c7\u30b3\u30fc\u30c0\u30fc\u304c\u51fa\u529b\u3092\u81ea\u5df1\u56de\u5e30\u7684\u306b\u751f\u6210\")\n        print(\"5. \u5404\u30c7\u30b3\u30fc\u30c0\u30fc\u5c64\u3067Cross-Attention\u3092\u901a\u3058\u3066\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u60c5\u5831\u3092\u53c2\u7167\")\n\nclass PracticalExample:\n    \"\"\"\u5b9f\u8df5\u7684\u306a\u4f8b\uff1a\u7ffb\u8a33\u30bf\u30b9\u30af\"\"\"\n\n    def __init__(self):\n        self.d_model = 256\n        self.n_heads = 8\n        self.n_layers = 3\n\n    def demonstrate_translation_process(self):\n        \"\"\"\u7ffb\u8a33\u30d7\u30ed\u30bb\u30b9\u306e\u5b9f\u6f14\"\"\"\n        print(\"=== \u7ffb\u8a33\u30bf\u30b9\u30af\u3067\u306e\u52d5\u4f5c\u4f8b ===\\n\")\n\n        # \u7c21\u5358\u306a\u8a9e\u5f59\n        src_vocab = {\n            '&lt;pad&gt;': 0, '&lt;sos&gt;': 1, '&lt;eos&gt;': 2,\n            'the': 3, 'cat': 4, 'is': 5, 'sleeping': 6,\n            'dog': 7, 'running': 8\n        }\n\n        tgt_vocab = {\n            '&lt;pad&gt;': 0, '&lt;sos&gt;': 1, '&lt;eos&gt;': 2,\n            '\u732b': 3, '\u304c': 4, '\u5bdd\u3066': 5, '\u3044\u308b': 6,\n            '\u72ac': 7, '\u8d70\u3063\u3066': 8\n        }\n\n        # \u4f8b\u6587\n        src_sentence = \"the cat is sleeping\"\n        src_tokens = ['&lt;sos&gt;'] + src_sentence.split() + ['&lt;eos&gt;']\n        src_ids = torch.tensor([[src_vocab.get(t, 0) for t in src_tokens]])\n\n        print(f\"\u5165\u529b\u6587: {src_sentence}\")\n        print(f\"\u30c8\u30fc\u30af\u30f3: {src_tokens}\")\n        print(f\"ID: {src_ids.tolist()[0]}\\n\")\n\n        # \u30e2\u30c7\u30eb\u306e\u4f5c\u6210\uff08\u7c21\u7565\u7248\uff09\n        class SimpleTranslator(nn.Module):\n            def __init__(self, src_vocab_size, tgt_vocab_size, \n                        d_model, n_heads, n_layers):\n                super().__init__()\n\n                # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\n                self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n                self.encoder_layers = nn.ModuleList([\n                    nn.TransformerEncoderLayer(\n                        d_model, n_heads, d_model * 4, batch_first=True\n                    ) for _ in range(n_layers)\n                ])\n\n                # \u30c7\u30b3\u30fc\u30c0\u30fc\n                self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n                self.decoder_layers = nn.ModuleList([\n                    nn.TransformerDecoderLayer(\n                        d_model, n_heads, d_model * 4, batch_first=True\n                    ) for _ in range(n_layers)\n                ])\n\n                # \u51fa\u529b\u5c64\n                self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n\n            def encode(self, src):\n                x = self.src_embedding(src)\n                for layer in self.encoder_layers:\n                    x = layer(x)\n                return x\n\n            def decode(self, tgt, memory):\n                x = self.tgt_embedding(tgt)\n                for layer in self.decoder_layers:\n                    x = layer(x, memory)\n                return self.output_projection(x)\n\n        # \u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        model = SimpleTranslator(\n            len(src_vocab), len(tgt_vocab),\n            self.d_model, self.n_heads, self.n_layers\n        )\n        model.eval()\n\n        # \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        with torch.no_grad():\n            encoder_output = model.encode(src_ids)\n            print(\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u51fa\u529b\u306e\u5f62\u72b6:\", encoder_output.shape)\n            print(\"(\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba, \u30b7\u30fc\u30b1\u30f3\u30b9\u9577, \u96a0\u308c\u6b21\u5143)\\n\")\n\n        # \u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff08\u8caa\u6b32\u6cd5\uff09\n        self._demonstrate_greedy_decoding(model, encoder_output, tgt_vocab)\n\n    def _demonstrate_greedy_decoding(self, model, encoder_output, tgt_vocab):\n        \"\"\"\u8caa\u6b32\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u5b9f\u6f14\"\"\"\n        print(\"=== \u8caa\u6b32\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 ===\\n\")\n\n        # \u9006\u5f15\u304d\u8f9e\u66f8\n        id_to_token = {v: k for k, v in tgt_vocab.items()}\n\n        # \u958b\u59cb\u30c8\u30fc\u30af\u30f3\n        decoder_input = torch.tensor([[tgt_vocab['&lt;sos&gt;']]])\n        generated_tokens = ['&lt;sos&gt;']\n\n        print(\"\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306e\u751f\u6210:\")\n\n        # \u6700\u592710\u30b9\u30c6\u30c3\u30d7\u307e\u3067\u751f\u6210\n        for step in range(10):\n            with torch.no_grad():\n                # \u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u4e88\u6e2c\n                output = model.decode(decoder_input, encoder_output)\n\n                # \u6700\u5f8c\u306e\u4f4d\u7f6e\u306e\u4e88\u6e2c\u3092\u53d6\u5f97\n                next_token_logits = output[0, -1, :]\n                next_token_probs = F.softmax(next_token_logits, dim=-1)\n                next_token_id = torch.argmax(next_token_probs).item()\n\n                # \u30c8\u30fc\u30af\u30f3\u306b\u5909\u63db\n                next_token = id_to_token.get(next_token_id, '&lt;unk&gt;')\n\n                print(f\"\u30b9\u30c6\u30c3\u30d7 {step + 1}: {next_token} \"\n                      f\"(\u78ba\u7387: {next_token_probs[next_token_id]:.3f})\")\n\n                # \u7d42\u4e86\u6761\u4ef6\n                if next_token == '&lt;eos&gt;':\n                    break\n\n                # \u6b21\u306e\u5165\u529b\u306b\u8ffd\u52a0\n                generated_tokens.append(next_token)\n                decoder_input = torch.cat([\n                    decoder_input,\n                    torch.tensor([[next_token_id]])\n                ], dim=1)\n\n        print(f\"\\n\u751f\u6210\u3055\u308c\u305f\u6587: {' '.join(generated_tokens[1:])}\")\n\nclass ComparisonWithOtherArchitectures:\n    \"\"\"\u4ed6\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3068\u306e\u6bd4\u8f03\"\"\"\n\n    def compare_architectures(self):\n        \"\"\"\u7570\u306a\u308b\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6bd4\u8f03\"\"\"\n        print(\"=== \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6bd4\u8f03 ===\\n\")\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n\n        # 1. Encoder-Decoder (Transformer)\n        ax1 = axes[0]\n        self._draw_encoder_decoder(ax1)\n        ax1.set_title('Encoder-Decoder\\n(\u7ffb\u8a33\u30fb\u8981\u7d04)', fontsize=12)\n\n        # 2. Decoder-only (GPT)\n        ax2 = axes[1]\n        self._draw_decoder_only(ax2)\n        ax2.set_title('Decoder-only\\n(\u8a00\u8a9e\u751f\u6210)', fontsize=12)\n\n        # 3. Encoder-only (BERT)\n        ax3 = axes[2]\n        self._draw_encoder_only(ax3)\n        ax3.set_title('Encoder-only\\n(\u5206\u985e\u30fb\u7406\u89e3)', fontsize=12)\n\n        for ax in axes:\n            ax.set_xlim(0, 10)\n            ax.set_ylim(0, 10)\n            ax.axis('off')\n\n        plt.suptitle('Transformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7a2e\u985e', fontsize=16, weight='bold')\n        plt.tight_layout()\n        plt.show()\n\n        self._print_architecture_comparison()\n\n    def _draw_encoder_decoder(self, ax):\n        \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u3092\u63cf\u753b\"\"\"\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\n        encoder = FancyBboxPatch(\n            (1, 4), 3, 4,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightblue', edgecolor='darkblue'\n        )\n        ax.add_patch(encoder)\n        ax.text(2.5, 6, 'Encoder', ha='center', fontsize=10)\n\n        # \u30c7\u30b3\u30fc\u30c0\u30fc\n        decoder = FancyBboxPatch(\n            (5, 4), 3, 4,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightcoral', edgecolor='darkred'\n        )\n        ax.add_patch(decoder)\n        ax.text(6.5, 6, 'Decoder', ha='center', fontsize=10)\n\n        # \u63a5\u7d9a\n        arrow = FancyArrowPatch(\n            (4, 6), (5, 6),\n            arrowstyle='-&gt;', mutation_scale=20,\n            color='green', linewidth=2\n        )\n        ax.add_patch(arrow)\n\n        # \u5165\u51fa\u529b\n        ax.text(2.5, 2.5, '\u5165\u529b', ha='center', fontsize=9)\n        ax.text(6.5, 2.5, '\u51fa\u529b', ha='center', fontsize=9)\n\n    def _draw_decoder_only(self, ax):\n        \"\"\"\u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u307f\u69cb\u9020\u3092\u63cf\u753b\"\"\"\n        # \u30c7\u30b3\u30fc\u30c0\u30fc\n        decoder = FancyBboxPatch(\n            (3, 4), 4, 4,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightcoral', edgecolor='darkred'\n        )\n        ax.add_patch(decoder)\n        ax.text(5, 6, 'Decoder', ha='center', fontsize=10)\n\n        # \u81ea\u5df1\u56de\u5e30\u7684\u306a\u77e2\u5370\n        arrow = FancyArrowPatch(\n            (7, 5), (7.5, 5), \n            arrowstyle='-&gt;', mutation_scale=15,\n            connectionstyle=\"arc3,rad=.5\",\n            color='darkred', linewidth=2\n        )\n        ax.add_patch(arrow)\n\n        # \u5165\u51fa\u529b\n        ax.text(5, 2.5, '\u5165\u529b\uff0b\u751f\u6210', ha='center', fontsize=9)\n\n    def _draw_encoder_only(self, ax):\n        \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u307f\u69cb\u9020\u3092\u63cf\u753b\"\"\"\n        # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\n        encoder = FancyBboxPatch(\n            (3, 4), 4, 4,\n            boxstyle=\"round,pad=0.1\",\n            facecolor='lightblue', edgecolor='darkblue'\n        )\n        ax.add_patch(encoder)\n        ax.text(5, 6, 'Encoder', ha='center', fontsize=10)\n\n        # \u53cc\u65b9\u5411\u77e2\u5370\n        arrow1 = FancyArrowPatch(\n            (4, 5), (4.5, 5),\n            arrowstyle='&lt;-&gt;', mutation_scale=15,\n            color='darkblue', linewidth=2\n        )\n        ax.add_patch(arrow1)\n\n        arrow2 = FancyArrowPatch(\n            (5.5, 5), (6, 5),\n            arrowstyle='&lt;-&gt;', mutation_scale=15,\n            color='darkblue', linewidth=2\n        )\n        ax.add_patch(arrow2)\n\n        # \u5165\u51fa\u529b\n        ax.text(5, 2.5, '\u5165\u529b\u2192\u8868\u73fe', ha='center', fontsize=9)\n\n    def _print_architecture_comparison(self):\n        \"\"\"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6bd4\u8f03\u8868\u3092\u51fa\u529b\"\"\"\n        print(\"\\n\u5404\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u7279\u5fb4:\\n\")\n\n        comparison = {\n            \"Encoder-Decoder\": {\n                \"\u5229\u70b9\": \"\u5165\u51fa\u529b\u304c\u7570\u306a\u308b\u5f62\u5f0f\u3001\u660e\u793a\u7684\u306a\u5909\u63db\",\n                \"\u6b20\u70b9\": \"\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u3001\u8907\u96d1\",\n                \"\u7528\u9014\": \"\u7ffb\u8a33\u3001\u8981\u7d04\u3001\u5bfe\u8a71\"\n            },\n            \"Decoder-only\": {\n                \"\u5229\u70b9\": \"\u30b7\u30f3\u30d7\u30eb\u3001\u5f37\u529b\u306a\u751f\u6210\u80fd\u529b\",\n                \"\u6b20\u70b9\": \"\u53cc\u65b9\u5411\u306e\u6587\u8108\u7406\u89e3\u304c\u56f0\u96e3\",\n                \"\u7528\u9014\": \"\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u3001\u30b3\u30fc\u30c9\u751f\u6210\"\n            },\n            \"Encoder-only\": {\n                \"\u5229\u70b9\": \"\u53cc\u65b9\u5411\u306e\u6587\u8108\u7406\u89e3\u3001\u9ad8\u901f\",\n                \"\u6b20\u70b9\": \"\u751f\u6210\u30bf\u30b9\u30af\u306b\u4e0d\u5411\u304d\",\n                \"\u7528\u9014\": \"\u5206\u985e\u3001\u56fa\u6709\u8868\u73fe\u8a8d\u8b58\u3001\u57cb\u3081\u8fbc\u307f\"\n            }\n        }\n\n        for arch, details in comparison.items():\n            print(f\"{arch}:\")\n            for key, value in details.items():\n                print(f\"  {key}: {value}\")\n            print()\n\n# \u5b9f\u884c\u4f8b\ndef main():\n    \"\"\"\u30e1\u30a4\u30f3\u5b9f\u884c\u95a2\u6570\"\"\"\n    print(\"=\" * 70)\n    print(\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u306e\u8a73\u7d30\")\n    print(\"=\" * 70 + \"\\n\")\n\n    # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u8aac\u660e\n    encoder_demo = TransformerEncoder()\n    encoder_demo.visualize_encoder_process()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u8aac\u660e\n    decoder_demo = TransformerDecoder()\n    decoder_demo.demonstrate_masked_attention()\n    decoder_demo.visualize_cross_attention()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u5168\u4f53\u69cb\u9020\n    arch_demo = EncoderDecoderArchitecture()\n    arch_demo.explain_information_flow()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u5b9f\u8df5\u4f8b\n    practical = PracticalExample()\n    practical.demonstrate_translation_process()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u6bd4\u8f03\n    comparison = ComparisonWithOtherArchitectures()\n    comparison.compare_architectures()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"part3/encoder-decoder/#122","title":"12.2 \u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u7279\u6b8a\u6027","text":""},{"location":"part3/encoder-decoder/#masked-self-attention","title":"Masked Self-Attention\u306e\u5fc5\u8981\u6027","text":"<p>```python class MaskedAttentionMechanics:     \"\"\"Masked Attention\u306e\u4ed5\u7d44\u307f\u3092\u8a73\u7d30\u306b\u89e3\u8aac\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 128\n    self.seq_len = 8\n\ndef explain_why_masking_needed(self):\n    \"\"\"\u306a\u305c\u30de\u30b9\u30ad\u30f3\u30b0\u304c\u5fc5\u8981\u304b\u3092\u8aac\u660e\"\"\"\n    print(\"=== Masked Self-Attention\u306e\u5fc5\u8981\u6027 ===\\n\")\n\n    print(\"\u5b66\u7fd2\u6642\u306e\u554f\u984c:\")\n    print(\"- Teacher Forcing: \u6b63\u89e3\u306e\u51fa\u529b\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u4e00\u5ea6\u306b\u5165\u529b\")\n    print(\"- \u3057\u304b\u3057\u3001\u672a\u6765\u306e\u60c5\u5831\u3092\u898b\u3066\u306f\u3044\u3051\u306a\u3044\")\n    print(\"- \u63a8\u8ad6\u6642\u3068\u540c\u3058\u6761\u4ef6\u3067\u5b66\u7fd2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\\n\")\n\n    # \u5177\u4f53\u4f8b\u3067\u8aac\u660e\n    self._demonstrate_information_leakage()\n\ndef _demonstrate_information_leakage(self):\n    \"\"\"\u60c5\u5831\u6f0f\u6d29\u306e\u554f\u984c\u3092\u5b9f\u8a3c\"\"\"\n    print(\"\u4f8b: 'I love cats' \u2192 '\u79c1\u306f\u732b\u304c\u597d\u304d'\\n\")\n\n    # \u6b63\u3057\u3044\u30de\u30b9\u30ad\u30f3\u30b0\n    correct_example = [\n        [\"\u79c1\", \"\uff1f\", \"\uff1f\", \"\uff1f\", \"\uff1f\"],\n        [\"\u79c1\", \"\u306f\", \"\uff1f\", \"\uff1f\", \"\uff1f\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\uff1f\", \"\uff1f\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\uff1f\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"]\n    ]\n\n    # \u9593\u9055\u3044\uff08\u30de\u30b9\u30ad\u30f3\u30b0\u306a\u3057\uff09\n    wrong_example = [\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"],  # \u5168\u90e8\u898b\u3048\u308b\uff01\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"],\n        [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\"]\n    ]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # \u6b63\u3057\u3044\u30de\u30b9\u30ad\u30f3\u30b0\n    ax1.set_title('\u6b63\u3057\u3044: Masked Self-Attention', fontsize=12)\n    for i, tokens in enumerate(correct_example):\n        for j, token in enumerate(tokens):\n            color = 'lightgreen' if token != \"\uff1f\" else 'lightgray'\n            rect = Rectangle((j, 4-i), 1, 1, \n                           facecolor=color, edgecolor='black')\n            ax1.add_patch(rect)\n            ax1.text(j+0.5, 4-i+0.5, token, \n                    ha='center', va='center')\n\n    ax1.set_xlim(0, 5)\n    ax1.set_ylim(0, 5)\n    ax1.set_xticks([])\n    ax1.set_yticks(range(5))\n    ax1.set_yticklabels([f'Step {i+1}' for i in range(5)])\n\n    # \u9593\u9055\u3063\u305f\u30b1\u30fc\u30b9\n    ax2.set_title('\u9593\u9055\u3044: \u30de\u30b9\u30ad\u30f3\u30b0\u306a\u3057', fontsize=12)\n    for i, tokens in enumerate(wrong_example):\n        for j, token in enumerate(tokens):\n            rect = Rectangle((j, 4-i), 1, 1,\n                           facecolor='lightcoral', edgecolor='black')\n            ax2.add_patch(rect)\n            ax2.text(j+0.5, 4-i+0.5, token,\n                    ha='center', va='center')\n\n    ax2.set_xlim(0, 5)\n    ax2.set_ylim(0, 5)\n    ax2.set_xticks([])\n    ax2.set_yticks(range(5))\n    ax2.set_yticklabels([f'Step {i+1}' for i in range(5)])\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8:\")\n    print(\"\u2713 \u5404\u30b9\u30c6\u30c3\u30d7\u3067\u898b\u3048\u308b\u60c5\u5831\u3092\u5236\u9650\")\n    print(\"\u2713 \u5b66\u7fd2\u6642\u3068\u63a8\u8ad6\u6642\u306e\u4e00\u8cab\u6027\u3092\u4fdd\u3064\")\n    print(\"\u2713 \u81ea\u5df1\u56de\u5e30\u7684\u306a\u751f\u6210\u3092\u53ef\u80fd\u306b\u3059\u308b\")\n</code></pre> <p>class CrossAttentionAnalysis:     \"\"\"Cross-Attention\u306e\u8a73\u7d30\u5206\u6790\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 256\n    self.n_heads = 8\n\ndef analyze_alignment_patterns(self):\n    \"\"\"\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n    print(\"=== Cross-Attention\u306e\u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u5206\u6790 ===\\n\")\n\n    # \u8907\u6570\u306e\u7ffb\u8a33\u4f8b\n    examples = [\n        {\n            \"src\": [\"The\", \"quick\", \"brown\", \"fox\"],\n            \"tgt\": [\"\u7d20\u65e9\u3044\", \"\u8336\u8272\u306e\", \"\u72d0\"],\n            \"alignment\": [\n                [0.1, 0.8, 0.05, 0.05],  # \u7d20\u65e9\u3044 \u2192 quick\n                [0.05, 0.1, 0.8, 0.05],  # \u8336\u8272\u306e \u2192 brown\n                [0.05, 0.05, 0.1, 0.8]   # \u72d0 \u2192 fox\n            ]\n        },\n        {\n            \"src\": [\"I\", \"love\", \"you\"],\n            \"tgt\": [\"\u79c1\u306f\", \"\u3042\u306a\u305f\u3092\", \"\u611b\u3057\u3066\", \"\u3044\u307e\u3059\"],\n            \"alignment\": [\n                [0.9, 0.05, 0.05],      # \u79c1\u306f \u2192 I\n                [0.1, 0.1, 0.8],        # \u3042\u306a\u305f\u3092 \u2192 you\n                [0.05, 0.9, 0.05],      # \u611b\u3057\u3066 \u2192 love\n                [0.3, 0.4, 0.3]         # \u3044\u307e\u3059 \u2192 (auxiliary)\n            ]\n        }\n    ]\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    for idx, example in enumerate(examples):\n        ax = axes[idx]\n        alignment = np.array(example[\"alignment\"])\n\n        im = ax.imshow(alignment, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n\n        # \u30e9\u30d9\u30eb\n        ax.set_xticks(range(len(example[\"src\"])))\n        ax.set_xticklabels(example[\"src\"])\n        ax.set_yticks(range(len(example[\"tgt\"])))\n        ax.set_yticklabels(example[\"tgt\"])\n\n        # \u30a2\u30e9\u30a4\u30e1\u30f3\u30c8\u5f37\u5ea6\u3092\u8868\u793a\n        for i in range(len(example[\"tgt\"])):\n            for j in range(len(example[\"src\"])):\n                ax.text(j, i, f'{alignment[i,j]:.2f}',\n                       ha='center', va='center')\n\n        ax.set_xlabel('\u30bd\u30fc\u30b9\u8a00\u8a9e')\n        ax.set_ylabel('\u30bf\u30fc\u30b2\u30c3\u30c8\u8a00\u8a9e')\n        ax.set_title(f'\u4f8b {idx+1}: Cross-Attention\u91cd\u307f')\n\n        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\u89b3\u5bdf:\")\n    print(\"\u2713 \u8a00\u8a9e\u9593\u306e\u5358\u8a9e\u5bfe\u5fdc\u304c\u5b66\u7fd2\u3055\u308c\u308b\")\n    print(\"\u2713 \u8a9e\u9806\u306e\u9055\u3044\u3082\u9069\u5207\u306b\u51e6\u7406\")\n    print(\"\u2713 \u6587\u6cd5\u7684\u8981\u7d20\uff08\u52a9\u8a5e\u306a\u3069\uff09\u306f\u5206\u6563\u7684\u306a\u6ce8\u610f\")\n</code></pre> <p>class ImplementationDetails:     \"\"\"\u5b9f\u88c5\u306e\u8a73\u7d30\"\"\"</p> <pre><code>def create_complete_transformer(self):\n    \"\"\"\u5b8c\u5168\u306aTransformer\u306e\u5b9f\u88c5\"\"\"\n    print(\"=== \u5b8c\u5168\u306aTransformer\u5b9f\u88c5 ===\\n\")\n\n    class CompleteTransformer(nn.Module):\n        def __init__(self, src_vocab_size, tgt_vocab_size,\n                    d_model=512, n_heads=8, n_layers=6,\n                    d_ff=2048, max_len=5000, dropout=0.1):\n            super().__init__()\n\n            self.d_model = d_model\n\n            # \u30bd\u30fc\u30b9\u5074\u57cb\u3081\u8fbc\u307f\n            self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n            self.src_pos_encoding = self._create_positional_encoding(\n                max_len, d_model\n            )\n\n            # \u30bf\u30fc\u30b2\u30c3\u30c8\u5074\u57cb\u3081\u8fbc\u307f\n            self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n            self.tgt_pos_encoding = self._create_positional_encoding(\n                max_len, d_model\n            )\n\n            # Transformer\u672c\u4f53\n            self.transformer = nn.Transformer(\n                d_model=d_model,\n                nhead=n_heads,\n                num_encoder_layers=n_layers,\n                num_decoder_layers=n_layers,\n                dim_feedforward=d_ff,\n                dropout=dropout,\n                batch_first=True\n            )\n\n            # \u51fa\u529b\u5c64\n            self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n\n            # \u305d\u306e\u4ed6\n            self.dropout = nn.Dropout(dropout)\n            self.scale = math.sqrt(d_model)\n\n        def _create_positional_encoding(self, max_len, d_model):\n            pe = torch.zeros(max_len, d_model)\n            position = torch.arange(0, max_len).unsqueeze(1).float()\n\n            div_term = torch.exp(\n                torch.arange(0, d_model, 2).float() *\n                -(math.log(10000.0) / d_model)\n            )\n\n            pe[:, 0::2] = torch.sin(position * div_term)\n            pe[:, 1::2] = torch.cos(position * div_term)\n\n            return nn.Parameter(pe.unsqueeze(0), requires_grad=False)\n\n        def create_masks(self, src, tgt):\n            \"\"\"\u30de\u30b9\u30af\u306e\u4f5c\u6210\"\"\"\n            # \u30d1\u30c7\u30a3\u30f3\u30b0\u30de\u30b9\u30af\n            src_pad_mask = (src == 0)  # 0\u306f\u30d1\u30c7\u30a3\u30f3\u30b0\u30c8\u30fc\u30af\u30f3\n            tgt_pad_mask = (tgt == 0)\n\n            # \u56e0\u679c\u7684\u30de\u30b9\u30af\uff08\u30c7\u30b3\u30fc\u30c0\u30fc\u7528\uff09\n            tgt_len = tgt.size(1)\n            tgt_mask = torch.triu(\n                torch.ones(tgt_len, tgt_len) * float('-inf'), \n                diagonal=1\n            ).to(tgt.device)\n\n            return src_pad_mask, tgt_pad_mask, tgt_mask\n\n        def forward(self, src, tgt):\n            # \u30de\u30b9\u30af\u306e\u4f5c\u6210\n            src_pad_mask, tgt_pad_mask, tgt_mask = self.create_masks(src, tgt)\n\n            # \u30bd\u30fc\u30b9\u5074\u306e\u57cb\u3081\u8fbc\u307f\n            src_seq_len = src.size(1)\n            src_emb = self.src_embedding(src) * self.scale\n            src_emb = src_emb + self.src_pos_encoding[:, :src_seq_len]\n            src_emb = self.dropout(src_emb)\n\n            # \u30bf\u30fc\u30b2\u30c3\u30c8\u5074\u306e\u57cb\u3081\u8fbc\u307f\n            tgt_seq_len = tgt.size(1)\n            tgt_emb = self.tgt_embedding(tgt) * self.scale\n            tgt_emb = tgt_emb + self.tgt_pos_encoding[:, :tgt_seq_len]\n            tgt_emb = self.dropout(tgt_emb)\n\n            # Transformer\u30d5\u30a9\u30ef\u30fc\u30c9\u30d1\u30b9\n            output = self.transformer(\n                src_emb, tgt_emb,\n                src_mask=None,\n                tgt_mask=tgt_mask,\n                src_key_padding_mask=src_pad_mask,\n                tgt_key_padding_mask=tgt_pad_mask\n            )\n\n            # \u51fa\u529b\u6295\u5f71\n            output = self.output_projection(output)\n\n            return output\n\n        def generate(self, src, max_len=50, temperature=1.0):\n            \"\"\"\u63a8\u8ad6\u6642\u306e\u751f\u6210\"\"\"\n            self.eval()\n            device = src.device\n            batch_size = src.size(0)\n\n            # \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u3092\u4e00\u5ea6\u3060\u3051\u5b9f\u884c\n            src_mask = (src == 0)\n            src_seq_len = src.size(1)\n            src_emb = self.src_embedding(src) * self.scale\n            src_emb = src_emb + self.src_pos_encoding[:, :src_seq_len]\n\n            memory = self.transformer.encoder(\n                src_emb,\n                src_key_padding_mask=src_mask\n            )\n\n            # \u958b\u59cb\u30c8\u30fc\u30af\u30f3\n            tgt = torch.ones(batch_size, 1, dtype=torch.long).to(device)\n\n            for _ in range(max_len):\n                # \u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u5b9f\u884c\n                tgt_seq_len = tgt.size(1)\n                tgt_mask = torch.triu(\n                    torch.ones(tgt_seq_len, tgt_seq_len) * float('-inf'),\n                    diagonal=1\n                ).to(device)\n\n                tgt_emb = self.tgt_embedding(tgt) * self.scale\n                tgt_emb = tgt_emb + self.tgt_pos_encoding[:, :tgt_seq_len]\n\n                output = self.transformer.decoder(\n                    tgt_emb, memory,\n                    tgt_mask=tgt_mask,\n                    memory_key_padding_mask=src_mask\n                )\n\n                # \u6700\u5f8c\u306e\u4f4d\u7f6e\u306e\u4e88\u6e2c\n                logits = self.output_projection(output[:, -1, :])\n                logits = logits / temperature\n\n                # \u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n                probs = F.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, 1)\n\n                # \u7d42\u4e86\u6761\u4ef6\uff08EOS\u30c8\u30fc\u30af\u30f3 = 2\uff09\n                if (next_token == 2).all():\n                    break\n\n                tgt = torch.cat([tgt, next_token], dim=1)\n\n            return tgt\n\n    # \u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3068\u60c5\u5831\u8868\u793a\n    model = CompleteTransformer(\n        src_vocab_size=10000,\n        tgt_vocab_size=10000\n    )\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u8a08\u7b97\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() \n                         if p.requires_grad)\n\n    print(f\"\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf:\")\n    print(f\"- \u7dcf\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {total_params:,}\")\n    print(f\"- \u5b66\u7fd2\u53ef\u80fd\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {trainable_params:,}\")\n    print(f\"- \u30e2\u30c7\u30eb\u30b5\u30a4\u30ba: {total_params * 4 / 1024**2:.1f} MB (float32)\")\n\n    return model\n</code></pre>"},{"location":"part3/encoder-decoder/#tips","title":"\u5b9f\u7528\u7684\u306aTips\u3068\u30c8\u30ea\u30c3\u30af","text":"<p>class PracticalTips:     \"\"\"\u5b9f\u88c5\u6642\u306e\u5b9f\u7528\u7684\u306a\u30d2\u30f3\u30c8\"\"\"</p> <pre><code>def share_best_practices(self):\n    \"\"\"\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u306e\u5171\u6709\"\"\"\n    print(\"=== \u5b9f\u88c5\u6642\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9 ===\\n\")\n\n    tips = {\n        \"1. \u521d\u671f\u5316\": [\n            \"Xavier/He\u521d\u671f\u5316\u3092\u4f7f\u7528\",\n            \"\u57cb\u3081\u8fbc\u307f\u5c64\u306f\u6b63\u898f\u5206\u5e03\u3067\u521d\u671f\u5316\",\n            \"\u5c64\u6b63\u898f\u5316\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u9069\u5207\u306b\u521d\u671f\u5316\"\n        ],\n\n        \"2. \u5b66\u7fd2\u306e\u5b89\u5b9a\u5316\": [\n            \"Learning rate warmup\u3092\u4f7f\u7528\",\n            \"Gradient clipping\u3092\u9069\u7528\",\n            \"Label smoothing\u3067\u904e\u5b66\u7fd2\u3092\u9632\u3050\"\n        ],\n\n        \"3. \u52b9\u7387\u7684\u306a\u5b9f\u88c5\": [\n            \"Attention\u8a08\u7b97\u3092\u30d0\u30c3\u30c1\u5316\",\n            \"Key-Value cache\u3092\u4f7f\u7528\uff08\u63a8\u8ad6\u6642\uff09\",\n            \"Mixed precision training\u3092\u6d3b\u7528\"\n        ],\n\n        \"4. \u30c7\u30d0\u30c3\u30b0\": [\n            \"\u5404\u5c64\u306e\u51fa\u529b\u5206\u5e03\u3092\u76e3\u8996\",\n            \"Attention\u91cd\u307f\u3092\u53ef\u8996\u5316\",\n            \"\u52fe\u914d\u306e\u6d41\u308c\u3092\u78ba\u8a8d\"\n        ]\n    }\n\n    for category, items in tips.items():\n        print(f\"{category}:\")\n        for item in items:\n            print(f\"  \u2022 {item}\")\n        print()\n\n    # \u30b3\u30fc\u30c9\u4f8b\n    print(\"=== \u5b9f\u88c5\u4f8b\uff1aLearning Rate Warmup ===\\n\")\n\n    print(\"\"\"\n</code></pre> <p>class WarmupScheduler:     def init(self, optimizer, d_model, warmup_steps=4000):         self.optimizer = optimizer         self.d_model = d_model         self.warmup_steps = warmup_steps         self.step_num = 0</p> <pre><code>def step(self):\n    self.step_num += 1\n    lr = self._get_lr()\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef _get_lr(self):\n    # Attention is All You Need \u306e\u5f0f\n    return self.d_model ** (-0.5) * min(\n        self.step_num ** (-0.5),\n        self.step_num * self.warmup_steps ** (-1.5)\n    )\n</code></pre> <p>\"\"\")</p> <p>def advanced_main():     \"\"\"\u30e1\u30a4\u30f3\u5b9f\u884c\u95a2\u6570\uff08\u8a73\u7d30\u7248\uff09\"\"\"     print(\"=\" * 70)     print(\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u306e\u6df1\u3044\u7406\u89e3\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># Masked Attention\u306e\u8aac\u660e\nmasked_demo = MaskedAttentionMechanics()\nmasked_demo.explain_why_masking_needed()\n\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# Cross-Attention\u306e\u5206\u6790\ncross_demo = CrossAttentionAnalysis()\ncross_demo.analyze_alignment_patterns()\n\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# \u5b8c\u5168\u306a\u5b9f\u88c5\nimpl_demo = ImplementationDetails()\nmodel = impl_demo.create_complete_transformer()\n\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# \u5b9f\u7528\u7684\u306a\u30d2\u30f3\u30c8\ntips = PracticalTips()\ntips.share_best_practices()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081\")\nprint(\"=\" * 70 + \"\\n\")\n\nprint(\"\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u306e\u8981\u70b9:\")\nprint(\"\u2022 \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc: \u5165\u529b\u5168\u4f53\u3092\u7406\u89e3\u3057\u3001\u8c4a\u304b\u306a\u8868\u73fe\u3092\u4f5c\u6210\")\nprint(\"\u2022 \u30c7\u30b3\u30fc\u30c0\u30fc: \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u60c5\u5831\u3092\u53c2\u7167\u3057\u306a\u304c\u3089\u51fa\u529b\u3092\u751f\u6210\")\nprint(\"\u2022 Cross-Attention: \u4e21\u8005\u3092\u7e4b\u3050\u91cd\u8981\u306a\u6a5f\u69cb\")\nprint(\"\u2022 Masked Attention: \u81ea\u5df1\u56de\u5e30\u7684\u751f\u6210\u3092\u53ef\u80fd\u306b\u3059\u308b\")\nprint(\"\\n\u3053\u306e\u69cb\u9020\u306b\u3088\u308a\u3001\u9ad8\u54c1\u8cea\u306a\u7cfb\u5217\u5909\u63db\u30bf\u30b9\u30af\u304c\u5b9f\u73fe\u3055\u308c\u307e\u3059\u3002\")\n</code></pre> <p>if name == \"main\":     # \u57fa\u672c\u7684\u306a\u8aac\u660e     main()</p> <pre><code>print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# \u8a73\u7d30\u306a\u8aac\u660e\nadvanced_main()\n</code></pre>"},{"location":"part3/feed-forward/","title":"Feed Forward Network","text":""},{"location":"part3/feed-forward/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u4f4d\u7f6e\u3054\u3068\u306e\u6df1\u3044\u601d\u8003","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u306b\u304a\u3044\u3066\u3001\u5404\u547d\u4ee4\u3084\u5f0f\u306b\u5bfe\u3057\u3066\u500b\u5225\u306b\u6700\u9069\u5316\u3092\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u4f8b\u3048\u3070\u3001\u5b9a\u6570\u7573\u307f\u8fbc\u307f\u3067\u306f\u5404\u5f0f\u3092\u72ec\u7acb\u306b\u8a55\u4fa1\u3057\u3001\u6700\u9069\u5316\u53ef\u80fd\u304b\u3092\u5224\u65ad\u3057\u307e\u3059\u3002\u3053\u306e\u300c\u4f4d\u7f6e\u3054\u3068\u306e\u72ec\u7acb\u3057\u305f\u51e6\u7406\u300d\u3068\u3044\u3046\u8003\u3048\u65b9\u304c\u3001Transformer\u306eFeed Forward Network\uff08FFN\uff09\u306e\u672c\u8cea\u3067\u3059\u3002</p> <p>Multi-Head Attention\u304c\u300c\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\u300d\u3092\u5b66\u7fd2\u3059\u308b\u306e\u306b\u5bfe\u3057\u3001FFN\u306f\u300c\u5404\u4f4d\u7f6e\u3067\u306e\u6df1\u3044\u7279\u5fb4\u5909\u63db\u300d\u3092\u62c5\u5f53\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u6587\u8108\u60c5\u5831\u3092\u7d71\u5408\u3057\u305f\u5f8c\u3001\u305d\u306e\u60c5\u5831\u3092\u3088\u308a\u8c4a\u304b\u306a\u8868\u73fe\u306b\u5909\u63db\u3059\u308b\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u4e00\u898b\u30b7\u30f3\u30d7\u30eb\u306b\u898b\u3048\u308bFFN\u304c\u3001\u306a\u305cTransformer\u306b\u4e0d\u53ef\u6b20\u306a\u306e\u304b\u3001\u305d\u3057\u3066\u3069\u306e\u3088\u3046\u306a\u8a08\u7b97\u3092\u884c\u3063\u3066\u3044\u308b\u306e\u304b\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/feed-forward/#101-ffn","title":"10.1 FFN\u306e\u5f79\u5272\u3068\u5fc5\u8981\u6027","text":""},{"location":"part3/feed-forward/#attention","title":"\u306a\u305cAttention\u3060\u3051\u3067\u306f\u4e0d\u5341\u5206\u306a\u306e\u304b","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Callable\nimport math\nfrom matplotlib.patches import Rectangle, FancyBboxPatch, Circle, FancyArrowPatch\nfrom matplotlib.patches import ConnectionPatch\nimport matplotlib.patches as mpatches\n\nclass FFNMotivation:\n    \"\"\"Feed Forward Network\u306e\u52d5\u6a5f\u3092\u8aac\u660e\"\"\"\n\n    def __init__(self):\n        self.d_model = 512\n        self.d_ff = 2048\n\n    def explain_limitations_of_attention_only(self):\n        \"\"\"Attention\u306e\u307f\u306e\u9650\u754c\u3092\u8aac\u660e\"\"\"\n        print(\"=== Attention \u306e\u307f\u306e\u9650\u754c ===\\n\")\n\n        print(\"Multi-Head Attention\u306e\u7279\u5fb4:\")\n        print(\"\u2713 \u5358\u8a9e\u9593\u306e\u95a2\u4fc2\u3092\u5b66\u7fd2\")\n        print(\"\u2713 \u6587\u8108\u60c5\u5831\u306e\u7d71\u5408\")\n        print(\"\u2713 \u4e26\u5217\u51e6\u7406\u53ef\u80fd\\n\")\n\n        print(\"\u3057\u304b\u3057\u3001\u4ee5\u4e0b\u306e\u9650\u754c\u304c\u3042\u308b:\")\n        print(\"\u2717 \u7dda\u5f62\u5909\u63db\u306e\u307f\uff08\u975e\u7dda\u5f62\u6027\u304c\u306a\u3044\uff09\")\n        print(\"\u2717 \u4f4d\u7f6e\u3054\u3068\u306e\u6df1\u3044\u7279\u5fb4\u5909\u63db\u304c\u3067\u304d\u306a\u3044\")\n        print(\"\u2717 \u8868\u73fe\u529b\u304c\u5236\u9650\u3055\u308c\u308b\\n\")\n\n        # \u5b9f\u9a13\u3067\u5b9f\u8a3c\n        self._demonstrate_linearity_limitation()\n\n    def _demonstrate_linearity_limitation(self):\n        \"\"\"\u7dda\u5f62\u6027\u306e\u9650\u754c\u3092\u5b9f\u8a3c\"\"\"\n        # \u7c21\u5358\u306a\u5206\u985e\u30bf\u30b9\u30af\n        print(\"\u5b9f\u9a13\uff1aXOR\u554f\u984c\uff08\u7dda\u5f62\u5206\u96e2\u4e0d\u53ef\u80fd\uff09\")\n\n        # \u30c7\u30fc\u30bf\n        X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n        y = torch.tensor([0, 1, 1, 0], dtype=torch.float32)\n\n        # \u7dda\u5f62\u30e2\u30c7\u30eb\uff08Attention\u306e\u307f\u306b\u76f8\u5f53\uff09\n        linear_model = nn.Linear(2, 1)\n\n        # \u975e\u7dda\u5f62\u30e2\u30c7\u30eb\uff08Attention + FFN\uff09\n        nonlinear_model = nn.Sequential(\n            nn.Linear(2, 4),\n            nn.ReLU(),\n            nn.Linear(4, 1)\n        )\n\n        # \u7c21\u6613\u7684\u306a\u5b66\u7fd2\n        for model, name in [(linear_model, \"\u7dda\u5f62\"), (nonlinear_model, \"\u975e\u7dda\u5f62\")]:\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n            for _ in range(1000):\n                optimizer.zero_grad()\n                pred = model(X).squeeze()\n                loss = F.binary_cross_entropy_with_logits(pred, y)\n                loss.backward()\n                optimizer.step()\n\n            # \u7d50\u679c\n            with torch.no_grad():\n                pred = torch.sigmoid(model(X).squeeze())\n                accuracy = ((pred &gt; 0.5).float() == y).float().mean()\n                print(f\"\\n{name}\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6: {accuracy:.1%}\")\n\n        # \u6c7a\u5b9a\u5883\u754c\u306e\u53ef\u8996\u5316\n        self._visualize_decision_boundaries(linear_model, nonlinear_model)\n\n    def _visualize_decision_boundaries(self, linear_model, nonlinear_model):\n        \"\"\"\u6c7a\u5b9a\u5883\u754c\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u30e1\u30c3\u30b7\u30e5\u30b0\u30ea\u30c3\u30c9\n        x_min, x_max = -0.5, 1.5\n        y_min, y_max = -0.5, 1.5\n        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                            np.linspace(y_min, y_max, 100))\n\n        # \u5404\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\n        for ax, model, title in [(ax1, linear_model, \"\u7dda\u5f62\u30e2\u30c7\u30eb\uff08Attention\u306e\u307f\uff09\"),\n                                 (ax2, nonlinear_model, \"\u975e\u7dda\u5f62\u30e2\u30c7\u30eb\uff08Attention + FFN\uff09\")]:\n\n            # \u4e88\u6e2c\n            with torch.no_grad():\n                Z = model(torch.tensor(np.c_[xx.ravel(), yy.ravel()], \n                                      dtype=torch.float32))\n                Z = torch.sigmoid(Z).numpy().reshape(xx.shape)\n\n            # \u30b3\u30f3\u30bf\u30fc\n            contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\n            ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n\n            # \u30c7\u30fc\u30bf\u70b9\n            colors = ['red', 'blue']\n            markers = ['o', 'x']\n            for i in range(2):\n                mask = (torch.tensor([0, 1, 1, 0]) == i).numpy()\n                ax.scatter(torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]])[mask, 0],\n                          torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]])[mask, 1],\n                          c=colors[i], marker=markers[i], s=200, edgecolors='black')\n\n            ax.set_xlim(x_min, x_max)\n            ax.set_ylim(y_min, y_max)\n            ax.set_title(title)\n            ax.grid(True, alpha=0.3)\n\n        plt.suptitle('XOR\u554f\u984c\uff1a\u975e\u7dda\u5f62\u6027\u306e\u5fc5\u8981\u6027', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/feed-forward/#ffn","title":"FFN\u306e\u69cb\u9020\u3068\u7279\u5fb4","text":"<pre><code>class FFNStructure:\n    \"\"\"Feed Forward Network\u306e\u69cb\u9020\"\"\"\n\n    def __init__(self, d_model: int = 512, d_ff: int = 2048):\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n    def explain_structure(self):\n        \"\"\"FFN\u306e\u69cb\u9020\u3092\u8aac\u660e\"\"\"\n        print(\"=== Feed Forward Network \u306e\u69cb\u9020 ===\\n\")\n\n        print(\"\u57fa\u672c\u69cb\u9020:\")\n        print(f\"1. \u7dda\u5f62\u5c64: {self.d_model} \u2192 {self.d_ff} (\u62e1\u5f35)\")\n        print(f\"2. \u6d3b\u6027\u5316\u95a2\u6570: ReLU \u307e\u305f\u306f GELU\")\n        print(f\"3. \u7dda\u5f62\u5c64: {self.d_ff} \u2192 {self.d_model} (\u5727\u7e2e)\")\n        print(f\"4. Dropout (\u30aa\u30d7\u30b7\u30e7\u30f3)\\n\")\n\n        print(\"\u7279\u5fb4:\")\n        print(f\"- \u62e1\u5f35\u7387: {self.d_ff / self.d_model:.0f}x (\u901a\u5e384x)\")\n        print(\"- \u4f4d\u7f6e\u3054\u3068\u306b\u72ec\u7acb\uff08position-wise\uff09\")\n        print(\"- \u30d1\u30e9\u30e1\u30fc\u30bf\u5171\u6709\uff08\u5168\u4f4d\u7f6e\u3067\u540c\u3058\u91cd\u307f\uff09\")\n        print(\"- \u975e\u7dda\u5f62\u5909\u63db\u306b\u3088\u308b\u8868\u73fe\u529b\")\n\n        # \u69cb\u9020\u306e\u53ef\u8996\u5316\n        self._visualize_ffn_structure()\n\n    def _visualize_ffn_structure(self):\n        \"\"\"FFN\u69cb\u9020\u306e\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n        # \u5404\u5c64\u306e\u30dc\u30c3\u30af\u30b9\n        layers = [\n            {\"name\": f\"Input\\n({self.d_model})\", \"x\": 0.1, \"width\": 0.15, \"color\": \"lightblue\"},\n            {\"name\": f\"Linear 1\\n({self.d_model}\u2192{self.d_ff})\", \"x\": 0.3, \"width\": 0.15, \"color\": \"lightgreen\"},\n            {\"name\": \"ReLU/GELU\", \"x\": 0.5, \"width\": 0.1, \"color\": \"yellow\"},\n            {\"name\": f\"Linear 2\\n({self.d_ff}\u2192{self.d_model})\", \"x\": 0.65, \"width\": 0.15, \"color\": \"lightcoral\"},\n            {\"name\": f\"Output\\n({self.d_model})\", \"x\": 0.85, \"width\": 0.15, \"color\": \"lightblue\"}\n        ]\n\n        for layer in layers:\n            rect = FancyBboxPatch(\n                (layer[\"x\"], 0.3), layer[\"width\"], 0.4,\n                boxstyle=\"round,pad=0.02\",\n                facecolor=layer[\"color\"],\n                edgecolor='black',\n                linewidth=2\n            )\n            ax.add_patch(rect)\n            ax.text(layer[\"x\"] + layer[\"width\"]/2, 0.5, layer[\"name\"],\n                   ha='center', va='center', fontsize=10, fontweight='bold')\n\n        # \u77e2\u5370\n        arrow_pairs = [(0, 1), (1, 2), (2, 3), (3, 4)]\n        for i, j in arrow_pairs:\n            start_x = layers[i][\"x\"] + layers[i][\"width\"]\n            end_x = layers[j][\"x\"]\n            ax.arrow(start_x, 0.5, end_x - start_x - 0.01, 0,\n                    head_width=0.05, head_length=0.01,\n                    fc='black', ec='black')\n\n        # \u6b21\u5143\u306e\u5909\u5316\u3092\u8868\u793a\n        ax.text(0.375, 0.8, f\"\u62e1\u5f35\\n{self.d_ff/self.d_model:.0f}x\",\n               ha='center', fontsize=10, color='green', fontweight='bold')\n        ax.text(0.725, 0.8, f\"\u5727\u7e2e\\n1/{self.d_ff/self.d_model:.0f}x\",\n               ha='center', fontsize=10, color='red', fontweight='bold')\n\n        ax.set_xlim(0, 1.1)\n        ax.set_ylim(0, 1)\n        ax.set_title('Feed Forward Network \u306e\u69cb\u9020', fontsize=16)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def compare_parameter_counts(self):\n        \"\"\"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u6bd4\u8f03\"\"\"\n        print(\"\\n=== \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u5206\u6790 ===\")\n\n        # FFN\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\n        ffn_params = 2 * self.d_model * self.d_ff\n\n        # Multi-Head Attention\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\uff088\u30d8\u30c3\u30c9\u3068\u4eee\u5b9a\uff09\n        n_heads = 8\n        mha_params = 4 * self.d_model * self.d_model  # Q, K, V, O\n\n        print(f\"\\nFFN:\")\n        print(f\"  Linear1: {self.d_model} \u00d7 {self.d_ff} = {self.d_model * self.d_ff:,}\")\n        print(f\"  Linear2: {self.d_ff} \u00d7 {self.d_model} = {self.d_ff * self.d_model:,}\")\n        print(f\"  \u5408\u8a08: {ffn_params:,}\")\n\n        print(f\"\\nMulti-Head Attention (8 heads):\")\n        print(f\"  Q, K, V, O: 4 \u00d7 {self.d_model} \u00d7 {self.d_model} = {mha_params:,}\")\n\n        print(f\"\\n\u6bd4\u7387: FFN / MHA = {ffn_params / mha_params:.1f}\")\n\n        # \u53ef\u8996\u5316\n        self._visualize_parameter_distribution()\n\n    def _visualize_parameter_distribution(self):\n        \"\"\"\u30d1\u30e9\u30e1\u30fc\u30bf\u5206\u5e03\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u6bd4\u8f03\n        ffn_params = 2 * self.d_model * self.d_ff\n        mha_params = 4 * self.d_model * self.d_model\n\n        # \u5186\u30b0\u30e9\u30d5\n        sizes = [mha_params, ffn_params]\n        labels = ['Multi-Head\\nAttention', 'Feed Forward\\nNetwork']\n        colors = ['lightblue', 'lightcoral']\n\n        ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n               startangle=90, textprops={'fontsize': 12})\n        ax1.set_title('Transformer\u30d6\u30ed\u30c3\u30af\u5185\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u5206\u5e03')\n\n        # \u68d2\u30b0\u30e9\u30d5\uff08\u8a73\u7d30\uff09\n        components = ['Q', 'K', 'V', 'O', 'FFN Linear1', 'FFN Linear2']\n        params = [\n            self.d_model * self.d_model,\n            self.d_model * self.d_model,\n            self.d_model * self.d_model,\n            self.d_model * self.d_model,\n            self.d_model * self.d_ff,\n            self.d_ff * self.d_model\n        ]\n\n        bars = ax2.bar(components, params, color=['lightblue']*4 + ['lightcoral']*2)\n        ax2.set_ylabel('\u30d1\u30e9\u30e1\u30fc\u30bf\u6570')\n        ax2.set_title('\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570')\n        ax2.tick_params(axis='x', rotation=45)\n\n        # \u5024\u3092\u8868\u793a\n        for bar, param in zip(bars, params):\n            height = bar.get_height()\n            ax2.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{param/1000:.0f}K',\n                    ha='center', va='bottom')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/feed-forward/#102-position-wise","title":"10.2 \u4f4d\u7f6e\u3054\u3068\u306e\u8a08\u7b97\uff08Position-wise\uff09","text":""},{"location":"part3/feed-forward/#position-wise","title":"Position-wise \u306e\u610f\u5473","text":"<pre><code>class PositionWiseComputation:\n    \"\"\"\u4f4d\u7f6e\u3054\u3068\u306e\u8a08\u7b97\u306e\u7406\u89e3\"\"\"\n\n    def explain_position_wise(self):\n        \"\"\"Position-wise\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== Position-wise Feed Forward ===\\n\")\n\n        print(\"\u300c\u4f4d\u7f6e\u3054\u3068\u300d\u306e\u610f\u5473:\")\n        print(\"- \u5404\u4f4d\u7f6e\uff08\u30c8\u30fc\u30af\u30f3\uff09\u306b\u5bfe\u3057\u3066\u72ec\u7acb\u306b\u9069\u7528\")\n        print(\"- \u4f4d\u7f6e\u9593\u306e\u76f8\u4e92\u4f5c\u7528\u306f\u306a\u3044\")\n        print(\"- \u3059\u3079\u3066\u306e\u4f4d\u7f6e\u3067\u540c\u3058\u91cd\u307f\u3092\u5171\u6709\")\n        print(\"- 1\u00d71 Convolution\u3068\u7b49\u4fa1\\n\")\n\n        # \u6bd4\u8f03\u3092\u53ef\u8996\u5316\n        self._visualize_position_wise_vs_fully_connected()\n\n    def _visualize_position_wise_vs_fully_connected(self):\n        \"\"\"Position-wise vs Fully Connected\u306e\u6bd4\u8f03\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        seq_len = 4\n        d_model = 3\n\n        # Position-wise\uff08\u5b9f\u969b\u306eFFN\uff09\n        ax1.set_title('Position-wise FFN\\n\uff08\u5404\u4f4d\u7f6e\u3067\u72ec\u7acb\uff09', fontsize=14)\n\n        # \u5404\u4f4d\u7f6e\u3092\u72ec\u7acb\u306b\u51e6\u7406\n        for pos in range(seq_len):\n            # \u5165\u529b\n            for dim in range(d_model):\n                circle = Circle((0, pos * 1.5 + dim * 0.4), 0.15,\n                              color='lightblue', ec='black')\n                ax1.add_patch(circle)\n                ax1.text(0, pos * 1.5 + dim * 0.4, f'x{pos},{dim}',\n                        ha='center', va='center', fontsize=8)\n\n            # FFN\uff08\u5404\u4f4d\u7f6e\u3067\u540c\u3058\uff09\n            rect = Rectangle((1, pos * 1.5 - 0.2), 1, 1,\n                           facecolor='lightgreen', edgecolor='black')\n            ax1.add_patch(rect)\n            ax1.text(1.5, pos * 1.5 + 0.3, 'FFN',\n                    ha='center', va='center', fontsize=10)\n\n            # \u51fa\u529b\n            for dim in range(d_model):\n                circle = Circle((3, pos * 1.5 + dim * 0.4), 0.15,\n                              color='lightcoral', ec='black')\n                ax1.add_patch(circle)\n                ax1.text(3, pos * 1.5 + dim * 0.4, f'y{pos},{dim}',\n                        ha='center', va='center', fontsize=8)\n\n            # \u77e2\u5370\n            ax1.arrow(0.2, pos * 1.5 + 0.3, 0.7, 0,\n                     head_width=0.1, head_length=0.05,\n                     fc='gray', ec='gray')\n            ax1.arrow(2.1, pos * 1.5 + 0.3, 0.7, 0,\n                     head_width=0.1, head_length=0.05,\n                     fc='gray', ec='gray')\n\n        ax1.set_xlim(-0.5, 3.5)\n        ax1.set_ylim(-0.5, seq_len * 1.5)\n        ax1.axis('off')\n\n        # Fully Connected\uff08\u4eee\u60f3\u7684\u306a\u4ee3\u66ff\u6848\uff09\n        ax2.set_title('Fully Connected\\n\uff08\u5168\u4f4d\u7f6e\u304c\u76f8\u4e92\u4f5c\u7528\uff09', fontsize=14)\n\n        # \u3059\u3079\u3066\u306e\u5165\u529b\n        all_inputs = []\n        for pos in range(seq_len):\n            for dim in range(d_model):\n                y = pos * 0.8 + dim * 0.25\n                circle = Circle((0, y), 0.1,\n                              color='lightblue', ec='black')\n                ax2.add_patch(circle)\n                all_inputs.append((0, y))\n\n        # \u4e2d\u592e\u306e\u51e6\u7406\n        rect = Rectangle((1.5, 0), 1, seq_len * 0.8 + 0.2,\n                       facecolor='yellow', edgecolor='black')\n        ax2.add_patch(rect)\n        ax2.text(2, seq_len * 0.4, 'FC',\n                ha='center', va='center', fontsize=12)\n\n        # \u3059\u3079\u3066\u306e\u51fa\u529b\n        all_outputs = []\n        for pos in range(seq_len):\n            for dim in range(d_model):\n                y = pos * 0.8 + dim * 0.25\n                circle = Circle((3.5, y), 0.1,\n                              color='lightcoral', ec='black')\n                ax2.add_patch(circle)\n                all_outputs.append((3.5, y))\n\n        # \u5168\u7d50\u5408\u3092\u8868\u3059\u7dda\uff08\u4e00\u90e8\u306e\u307f\uff09\n        for i in range(0, len(all_inputs), 3):\n            for j in range(0, len(all_outputs), 3):\n                ax2.plot([all_inputs[i][0] + 0.1, 1.5],\n                        [all_inputs[i][1], seq_len * 0.4],\n                        'gray', alpha=0.3, linewidth=0.5)\n                ax2.plot([2.5, all_outputs[j][0] - 0.1],\n                        [seq_len * 0.4, all_outputs[j][1]],\n                        'gray', alpha=0.3, linewidth=0.5)\n\n        ax2.set_xlim(-0.5, 4)\n        ax2.set_ylim(-0.5, seq_len * 0.8 + 0.5)\n        ax2.axis('off')\n\n        plt.suptitle('FFN\u306e\u300c\u4f4d\u7f6e\u3054\u3068\u300d\u51e6\u7406 vs \u5168\u7d50\u5408', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n    def demonstrate_weight_sharing(self):\n        \"\"\"\u91cd\u307f\u5171\u6709\u306e\u5b9f\u6f14\"\"\"\n        print(\"\\n=== \u91cd\u307f\u5171\u6709\u306e\u5b9f\u6f14 ===\")\n\n        # \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\n        batch_size = 2\n        seq_len = 3\n        d_model = 4\n        d_ff = 8\n\n        # FFN\u5c64\n        ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # \u5165\u529b\n        x = torch.randn(batch_size, seq_len, d_model)\n        print(f\"\u5165\u529b\u5f62\u72b6: {x.shape}\")\n\n        # Position-wise\u9069\u7528\uff08PyTorch\u306f\u81ea\u52d5\u7684\u306b\u51e6\u7406\uff09\n        output = ffn(x)\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\n\n        # \u624b\u52d5\u3067\u5404\u4f4d\u7f6e\u306b\u9069\u7528\u3057\u3066\u540c\u3058\u7d50\u679c\u3092\u78ba\u8a8d\n        manual_output = torch.zeros_like(x)\n        for pos in range(seq_len):\n            manual_output[:, pos, :] = ffn(x[:, pos, :])\n\n        # \u7d50\u679c\u304c\u540c\u3058\u3053\u3068\u3092\u78ba\u8a8d\n        difference = (output - manual_output).abs().max().item()\n        print(f\"\\n\u81ea\u52d5\u51e6\u7406\u3068\u624b\u52d5\u51e6\u7406\u306e\u5dee: {difference:.6f}\")\n        print(\"\u2192 \u540c\u3058\u91cd\u307f\u304c\u5168\u4f4d\u7f6e\u3067\u5171\u6709\u3055\u308c\u3066\u3044\u308b\")\n</code></pre>"},{"location":"part3/feed-forward/#11-convolution","title":"1\u00d71 Convolution \u3068\u306e\u7b49\u4fa1\u6027","text":"<pre><code>class ConvolutionEquivalence:\n    \"\"\"1\u00d71 Convolution\u3068\u306e\u7b49\u4fa1\u6027\"\"\"\n\n    def demonstrate_equivalence(self):\n        \"\"\"FFN\u30681\u00d71 Conv\u306e\u7b49\u4fa1\u6027\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== FFN \u3068 1\u00d71 Convolution \u306e\u7b49\u4fa1\u6027 ===\\n\")\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\n        batch_size = 2\n        seq_len = 10\n        d_model = 256\n        d_ff = 1024\n\n        # \u30c7\u30fc\u30bf\uff08\u540c\u3058\u521d\u671f\u5024\uff09\n        torch.manual_seed(42)\n        x = torch.randn(batch_size, seq_len, d_model)\n\n        # \u65b9\u6cd51: FFN\uff08Linear\u5c64\uff09\n        ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # \u65b9\u6cd52: 1\u00d71 Convolution\n        # Conv1d\u306f(batch, channels, length)\u306e\u9806\u5e8f\u3092\u671f\u5f85\n        conv = nn.Sequential(\n            nn.Conv1d(d_model, d_ff, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv1d(d_ff, d_model, kernel_size=1)\n        )\n\n        # \u91cd\u307f\u3092\u30b3\u30d4\u30fc\u3057\u3066\u540c\u3058\u306b\u3059\u308b\n        with torch.no_grad():\n            # Linear: (out_features, in_features)\n            # Conv1d: (out_channels, in_channels, kernel_size)\n            conv[0].weight.data = ffn[0].weight.data.unsqueeze(-1)\n            conv[0].bias.data = ffn[0].bias.data\n            conv[2].weight.data = ffn[2].weight.data.unsqueeze(-1)\n            conv[2].bias.data = ffn[2].bias.data\n\n        # \u8a08\u7b97\n        ffn_output = ffn(x)\n\n        # Conv1d\u306e\u305f\u3081\u306e\u8ee2\u7f6e\n        x_conv = x.transpose(1, 2)  # (batch, d_model, seq_len)\n        conv_output = conv(x_conv)\n        conv_output = conv_output.transpose(1, 2)  # (batch, seq_len, d_model)\n\n        # \u7d50\u679c\u306e\u6bd4\u8f03\n        difference = (ffn_output - conv_output).abs().max().item()\n        print(f\"FFN\u51fa\u529b\u5f62\u72b6: {ffn_output.shape}\")\n        print(f\"Conv\u51fa\u529b\u5f62\u72b6: {conv_output.shape}\")\n        print(f\"\u6700\u5927\u5dee: {difference:.6f}\")\n        print(\"\\n\u2192 FFN\u30681\u00d71 Convolution\u306f\u6570\u5b66\u7684\u306b\u7b49\u4fa1\uff01\")\n\n        # \u8a08\u7b97\u52b9\u7387\u306e\u6bd4\u8f03\n        self._compare_computation_efficiency()\n\n    def _compare_computation_efficiency(self):\n        \"\"\"\u8a08\u7b97\u52b9\u7387\u306e\u6bd4\u8f03\"\"\"\n        import time\n\n        print(\"\\n=== \u8a08\u7b97\u52b9\u7387\u306e\u6bd4\u8f03 ===\")\n\n        # \u5927\u304d\u3081\u306e\u30c7\u30fc\u30bf\n        batch_size = 32\n        seq_len = 512\n        d_model = 768\n        d_ff = 3072\n\n        x = torch.randn(batch_size, seq_len, d_model)\n\n        # FFN\n        ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # 1\u00d71 Conv\n        conv = nn.Sequential(\n            nn.Conv1d(d_model, d_ff, 1),\n            nn.ReLU(),\n            nn.Conv1d(d_ff, d_model, 1)\n        )\n\n        # FFN\u306e\u6642\u9593\u6e2c\u5b9a\n        start = time.time()\n        for _ in range(10):\n            _ = ffn(x)\n        ffn_time = time.time() - start\n\n        # Conv\u306e\u6642\u9593\u6e2c\u5b9a\n        x_conv = x.transpose(1, 2)\n        start = time.time()\n        for _ in range(10):\n            _ = conv(x_conv).transpose(1, 2)\n        conv_time = time.time() - start\n\n        print(f\"FFN\u6642\u9593: {ffn_time:.3f}\u79d2\")\n        print(f\"Conv\u6642\u9593: {conv_time:.3f}\u79d2\")\n        print(f\"\u6bd4\u7387: {conv_time/ffn_time:.2f}x\")\n</code></pre>"},{"location":"part3/feed-forward/#103","title":"10.3 \u6d3b\u6027\u5316\u95a2\u6570\u306e\u9078\u629e","text":""},{"location":"part3/feed-forward/#relu-vs-gelu","title":"ReLU vs GELU","text":"<pre><code>class ActivationFunctions:\n    \"\"\"\u6d3b\u6027\u5316\u95a2\u6570\u306e\u6bd4\u8f03\u3068\u5206\u6790\"\"\"\n\n    def compare_activation_functions(self):\n        \"\"\"\u4e3b\u8981\u306a\u6d3b\u6027\u5316\u95a2\u6570\u306e\u6bd4\u8f03\"\"\"\n        print(\"=== \u6d3b\u6027\u5316\u95a2\u6570\u306e\u6bd4\u8f03 ===\\n\")\n\n        # \u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9a\u7fa9\n        x = torch.linspace(-3, 3, 1000)\n\n        activations = {\n            'ReLU': F.relu(x),\n            'GELU': F.gelu(x),\n            'SiLU/Swish': F.silu(x),\n            'Mish': x * torch.tanh(F.softplus(x))\n        }\n\n        # \u5c0e\u95a2\u6570\uff08\u8fd1\u4f3c\uff09\n        x.requires_grad_(True)\n        derivatives = {}\n\n        for name, act_func in [\n            ('ReLU', lambda x: F.relu(x)),\n            ('GELU', lambda x: F.gelu(x)),\n            ('SiLU/Swish', lambda x: F.silu(x)),\n            ('Mish', lambda x: x * torch.tanh(F.softplus(x)))\n        ]:\n            y = act_func(x.clone())\n            y.sum().backward()\n            derivatives[name] = x.grad.clone()\n            x.grad.zero_()\n\n        # \u53ef\u8996\u5316\n        self._visualize_activations(x.detach(), activations, derivatives)\n\n    def _visualize_activations(self, x, activations, derivatives):\n        \"\"\"\u6d3b\u6027\u5316\u95a2\u6570\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n        # \u6d3b\u6027\u5316\u95a2\u6570\n        for name, y in activations.items():\n            ax1.plot(x, y, label=name, linewidth=2)\n\n        ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n        ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n        ax1.set_xlabel('x')\n        ax1.set_ylabel('f(x)')\n        ax1.set_title('\u6d3b\u6027\u5316\u95a2\u6570')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_xlim(-3, 3)\n\n        # \u5c0e\u95a2\u6570\n        for name, dy in derivatives.items():\n            ax2.plot(x, dy, label=name, linewidth=2)\n\n        ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n        ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n        ax2.set_xlabel('x')\n        ax2.set_ylabel(\"f'(x)\")\n        ax2.set_title('\u5c0e\u95a2\u6570')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        ax2.set_xlim(-3, 3)\n\n        plt.suptitle('Transformer\u3067\u4f7f\u7528\u3055\u308c\u308b\u6d3b\u6027\u5316\u95a2\u6570', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n    def explain_gelu_advantage(self):\n        \"\"\"GELU\u306e\u5229\u70b9\u3092\u8aac\u660e\"\"\"\n        print(\"\\n=== GELU (Gaussian Error Linear Unit) \u306e\u5229\u70b9 ===\\n\")\n\n        print(\"ReLU\u306e\u7279\u5fb4:\")\n        print(\"\u2713 \u30b7\u30f3\u30d7\u30eb\u3067\u9ad8\u901f\")\n        print(\"\u2713 \u52fe\u914d\u6d88\u5931\u3092\u9632\u3050\")\n        print(\"\u2717 x&lt;0\u3067\u52fe\u914d\u304c0\uff08Dead ReLU\u554f\u984c\uff09\")\n        print(\"\u2717 \u539f\u70b9\u3067\u5fae\u5206\u4e0d\u53ef\u80fd\\n\")\n\n        print(\"GELU\u306e\u7279\u5fb4:\")\n        print(\"\u2713 \u6ed1\u3089\u304b\u3067\u5fae\u5206\u53ef\u80fd\")\n        print(\"\u2713 \u78ba\u7387\u7684\u306a\u89e3\u91c8\u304c\u53ef\u80fd\")\n        print(\"\u2713 \u8ca0\u306e\u5024\u3082\u4e00\u90e8\u901a\u3059\")\n        print(\"\u2713 \u5b9f\u9a13\u7684\u306b\u512a\u308c\u305f\u6027\u80fd\")\n        print(\"\u2717 \u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u3084\u3084\u9ad8\u3044\\n\")\n\n        # \u5b9f\u9a13\uff1a\u52fe\u914d\u306e\u6d41\u308c\n        self._compare_gradient_flow()\n\n    def _compare_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u6bd4\u8f03\"\"\"\n        print(\"\u5b9f\u9a13\uff1a\u52fe\u914d\u306e\u6d41\u308c\u306e\u6bd4\u8f03\")\n\n        # \u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u69cb\u7bc9\n        depth = 20\n        d_model = 128\n\n        # ReLU\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n        relu_net = nn.Sequential(*[\n            nn.Sequential(\n                nn.Linear(d_model, d_model),\n                nn.ReLU()\n            ) for _ in range(depth)\n        ])\n\n        # GELU\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n        gelu_net = nn.Sequential(*[\n            nn.Sequential(\n                nn.Linear(d_model, d_model),\n                nn.GELU()\n            ) for _ in range(depth)\n        ])\n\n        # \u540c\u3058\u521d\u671f\u5316\n        for relu_layer, gelu_layer in zip(relu_net, gelu_net):\n            gelu_layer[0].weight.data = relu_layer[0].weight.data.clone()\n            gelu_layer[0].bias.data = relu_layer[0].bias.data.clone()\n\n        # \u5165\u529b\n        x = torch.randn(32, d_model, requires_grad=True)\n\n        # \u9806\u4f1d\u64ad\u3068\u9006\u4f1d\u64ad\n        for net, name in [(relu_net, \"ReLU\"), (gelu_net, \"GELU\")]:\n            x_copy = x.clone()\n            output = net(x_copy)\n            loss = output.mean()\n            loss.backward()\n\n            # \u52fe\u914d\u306e\u7d71\u8a08\n            grad_norms = []\n            for i, layer in enumerate(net):\n                if isinstance(layer, nn.Sequential):\n                    grad_norm = layer[0].weight.grad.norm().item()\n                    grad_norms.append(grad_norm)\n\n            print(f\"\\n{name}\u30cd\u30c3\u30c8\u30ef\u30fc\u30af:\")\n            print(f\"  \u6700\u521d\u306e\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0: {grad_norms[0]:.4f}\")\n            print(f\"  \u6700\u5f8c\u306e\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0: {grad_norms[-1]:.4f}\")\n            print(f\"  \u52fe\u914d\u306e\u6e1b\u8870\u7387: {grad_norms[-1]/grad_norms[0]:.6f}\")\n</code></pre>"},{"location":"part3/feed-forward/#_2","title":"\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5","text":"<pre><code>class CustomActivations:\n    \"\"\"\u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5\"\"\"\n\n    def implement_activations(self):\n        \"\"\"\u5404\u7a2e\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5\"\"\"\n        print(\"=== \u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5 ===\\n\")\n\n        class CustomGELU(nn.Module):\n            \"\"\"GELU\u306e\u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\"\"\"\n            def forward(self, x):\n                # GELU(x) = x * \u03a6(x)\n                # \u03a6(x) \u306f\u6a19\u6e96\u6b63\u898f\u5206\u5e03\u306e\u7d2f\u7a4d\u5206\u5e03\u95a2\u6570\n                return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n        class CustomSwish(nn.Module):\n            \"\"\"Swish/SiLU\u306e\u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\"\"\"\n            def __init__(self, beta=1.0):\n                super().__init__()\n                self.beta = beta\n\n            def forward(self, x):\n                return x * torch.sigmoid(self.beta * x)\n\n        class CustomMish(nn.Module):\n            \"\"\"Mish\u306e\u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\"\"\"\n            def forward(self, x):\n                return x * torch.tanh(F.softplus(x))\n\n        # \u30c6\u30b9\u30c8\n        x = torch.randn(10, 20)\n\n        # \u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\n        custom_gelu = CustomGELU()\n        custom_swish = CustomSwish()\n        custom_mish = CustomMish()\n\n        # PyTorch\u5b9f\u88c5\u3068\u306e\u6bd4\u8f03\n        print(\"\u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\u3068PyTorch\u5b9f\u88c5\u306e\u5dee:\")\n\n        # GELU\n        diff_gelu = (custom_gelu(x) - F.gelu(x)).abs().max().item()\n        print(f\"GELU: {diff_gelu:.6f}\")\n\n        # Swish/SiLU\n        diff_swish = (custom_swish(x) - F.silu(x)).abs().max().item()\n        print(f\"Swish: {diff_swish:.6f}\")\n\n        # \u8fd1\u4f3c\u7248GELU\uff08\u9ad8\u901f\u5316\uff09\n        self._implement_approximate_gelu()\n\n    def _implement_approximate_gelu(self):\n        \"\"\"\u8fd1\u4f3c\u7248GELU\u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== \u8fd1\u4f3c\u7248GELU ===\")\n\n        class ApproximateGELU(nn.Module):\n            \"\"\"\u9ad8\u901f\u306a\u8fd1\u4f3cGELU\"\"\"\n            def forward(self, x):\n                # tanh\u8fd1\u4f3c\n                # GELU(x) \u2248 0.5 * x * (1 + tanh(sqrt(2/\u03c0) * (x + 0.044715 * x^3)))\n                return 0.5 * x * (1 + torch.tanh(\n                    math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n                ))\n\n        # \u6bd4\u8f03\n        x = torch.linspace(-3, 3, 1000)\n\n        exact_gelu = F.gelu(x)\n        approx_gelu = ApproximateGELU()(x)\n\n        # \u8aa4\u5dee\u306e\u53ef\u8996\u5316\n        plt.figure(figsize=(10, 6))\n\n        plt.subplot(2, 1, 1)\n        plt.plot(x, exact_gelu, label='Exact GELU', linewidth=2)\n        plt.plot(x, approx_gelu, label='Approximate GELU', linestyle='--', linewidth=2)\n        plt.xlabel('x')\n        plt.ylabel('GELU(x)')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n\n        plt.subplot(2, 1, 2)\n        plt.plot(x, (approx_gelu - exact_gelu).abs(), 'r-', linewidth=2)\n        plt.xlabel('x')\n        plt.ylabel('|Approximate - Exact|')\n        plt.title('\u8fd1\u4f3c\u8aa4\u5dee')\n        plt.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.show()\n\n        max_error = (approx_gelu - exact_gelu).abs().max().item()\n        print(f\"\\n\u6700\u5927\u8fd1\u4f3c\u8aa4\u5dee: {max_error:.6f}\")\n</code></pre>"},{"location":"part3/feed-forward/#104-ffn","title":"10.4 FFN\u306e\u6700\u9069\u5316\u624b\u6cd5","text":""},{"location":"part3/feed-forward/#glu","title":"GLU\u5909\u7a2e\u306e\u5b9f\u88c5","text":"<pre><code>class GLUVariants:\n    \"\"\"Gated Linear Unit (GLU) \u306e\u5909\u7a2e\"\"\"\n\n    def explain_glu_family(self):\n        \"\"\"GLU\u30d5\u30a1\u30df\u30ea\u30fc\u306e\u8aac\u660e\"\"\"\n        print(\"=== GLU (Gated Linear Unit) \u30d5\u30a1\u30df\u30ea\u30fc ===\\n\")\n\n        print(\"\u57fa\u672c\u7684\u306aGLU:\")\n        print(\"GLU(x) = (xW + b) \u2297 \u03c3(xV + c)\")\n        print(\"  \u2297: \u8981\u7d20\u3054\u3068\u306e\u7a4d\")\n        print(\"  \u03c3: \u6d3b\u6027\u5316\u95a2\u6570\uff08sigmoid\uff09\\n\")\n\n        print(\"\u5909\u7a2e:\")\n        variants = {\n            \"GLU\": \"sigmoid gate\",\n            \"ReGLU\": \"ReLU gate\",\n            \"GEGLU\": \"GELU gate\",\n            \"SwiGLU\": \"Swish gate\",\n            \"Linear\": \"no gate (standard FFN)\"\n        }\n\n        for name, description in variants.items():\n            print(f\"  {name}: {description}\")\n\n    def implement_glu_variants(self):\n        \"\"\"GLU\u5909\u7a2e\u306e\u5b9f\u88c5\"\"\"\n\n        class GLU(nn.Module):\n            \"\"\"\u57fa\u672c\u7684\u306aGLU\"\"\"\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                self.linear = nn.Linear(d_model, d_ff * 2)\n\n            def forward(self, x):\n                x = self.linear(x)\n                x, gate = x.chunk(2, dim=-1)\n                return x * torch.sigmoid(gate)\n\n        class SwiGLU(nn.Module):\n            \"\"\"SwiGLU\uff08LLaMA\u3067\u4f7f\u7528\uff09\"\"\"\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                self.w1 = nn.Linear(d_model, d_ff)\n                self.w2 = nn.Linear(d_model, d_ff)\n                self.w3 = nn.Linear(d_ff, d_model)\n\n            def forward(self, x):\n                return self.w3(F.silu(self.w1(x)) * self.w2(x))\n\n        class GEGLU(nn.Module):\n            \"\"\"GEGLU\"\"\"\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                self.linear = nn.Linear(d_model, d_ff * 2)\n                self.output = nn.Linear(d_ff, d_model)\n\n            def forward(self, x):\n                x = self.linear(x)\n                x, gate = x.chunk(2, dim=-1)\n                x = x * F.gelu(gate)\n                return self.output(x)\n\n        # \u6027\u80fd\u6bd4\u8f03\n        self._compare_glu_performance()\n\n    def _compare_glu_performance(self):\n        \"\"\"GLU\u5909\u7a2e\u306e\u6027\u80fd\u6bd4\u8f03\"\"\"\n        print(\"\\n=== GLU\u5909\u7a2e\u306e\u6bd4\u8f03 ===\")\n\n        d_model = 512\n        d_ff = 2048\n        batch_size = 32\n        seq_len = 128\n\n        # \u6a19\u6e96FFN\n        standard_ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Linear(d_ff, d_model)\n        )\n\n        # GLU\u5909\u7a2e\n        class SwiGLU(nn.Module):\n            def __init__(self, d_model, d_ff):\n                super().__init__()\n                # d_ff\u3092\u8abf\u6574\u3057\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u5408\u308f\u305b\u308b\n                self.d_ff_adjusted = int(d_ff * 2 / 3)\n                self.w1 = nn.Linear(d_model, self.d_ff_adjusted)\n                self.w2 = nn.Linear(d_model, self.d_ff_adjusted)\n                self.w3 = nn.Linear(self.d_ff_adjusted, d_model)\n\n            def forward(self, x):\n                return self.w3(F.silu(self.w1(x)) * self.w2(x))\n\n        swiglu = SwiGLU(d_model, d_ff)\n\n        # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u6bd4\u8f03\n        standard_params = sum(p.numel() for p in standard_ffn.parameters())\n        swiglu_params = sum(p.numel() for p in swiglu.parameters())\n\n        print(f\"\u6a19\u6e96FFN \u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {standard_params:,}\")\n        print(f\"SwiGLU \u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {swiglu_params:,}\")\n        print(f\"\u6bd4\u7387: {swiglu_params / standard_params:.2f}\")\n\n        # \u5b9f\u969b\u306e\u8a08\u7b97\u3067\u8868\u73fe\u529b\u3092\u6bd4\u8f03\uff08\u7c21\u6613\u30c6\u30b9\u30c8\uff09\n        x = torch.randn(batch_size, seq_len, d_model)\n\n        with torch.no_grad():\n            standard_out = standard_ffn(x)\n            swiglu_out = swiglu(x)\n\n            # \u51fa\u529b\u306e\u7d71\u8a08\n            print(f\"\\n\u51fa\u529b\u306e\u7d71\u8a08:\")\n            print(f\"\u6a19\u6e96FFN - \u5e73\u5747: {standard_out.mean():.4f}, \u6a19\u6e96\u504f\u5dee: {standard_out.std():.4f}\")\n            print(f\"SwiGLU - \u5e73\u5747: {swiglu_out.mean():.4f}, \u6a19\u6e96\u504f\u5dee: {swiglu_out.std():.4f}\")\n</code></pre>"},{"location":"part3/feed-forward/#moemixture-of-expertsffn","title":"MoE\uff08Mixture of Experts\uff09FFN","text":"<pre><code>class MixtureOfExpertsFFN:\n    \"\"\"Mixture of Experts FFN\"\"\"\n\n    def explain_moe_concept(self):\n        \"\"\"MoE\u306e\u6982\u5ff5\u3092\u8aac\u660e\"\"\"\n        print(\"=== Mixture of Experts (MoE) FFN ===\\n\")\n\n        print(\"\u57fa\u672c\u30a2\u30a4\u30c7\u30a2:\")\n        print(\"- \u8907\u6570\u306e\u300c\u5c02\u9580\u5bb6\u300dFFN\u3092\u7528\u610f\")\n        print(\"- \u5404\u30c8\u30fc\u30af\u30f3\u306b\u5bfe\u3057\u3066\u9069\u5207\u306a\u5c02\u9580\u5bb6\u3092\u9078\u629e\")\n        print(\"- \u30b9\u30d1\u30fc\u30b9\u6027\u306b\u3088\u308a\u8a08\u7b97\u52b9\u7387\u3092\u7dad\u6301\\n\")\n\n        print(\"\u5229\u70b9:\")\n        print(\"\u2713 \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3092\u5897\u3084\u3057\u3066\u3082\u8a08\u7b97\u91cf\u306f\u4e00\u5b9a\")\n        print(\"\u2713 \u5404\u5c02\u9580\u5bb6\u304c\u7279\u5b9a\u306e\u30d1\u30bf\u30fc\u30f3\u306b\u7279\u5316\")\n        print(\"\u2713 \u30e2\u30c7\u30eb\u5bb9\u91cf\u306e\u52b9\u7387\u7684\u306a\u62e1\u5f35\\n\")\n\n        print(\"\u8ab2\u984c:\")\n        print(\"\u2717 \u8ca0\u8377\u5206\u6563\u306e\u554f\u984c\")\n        print(\"\u2717 \u5b66\u7fd2\u306e\u4e0d\u5b89\u5b9a\u6027\")\n        print(\"\u2717 \u5b9f\u88c5\u306e\u8907\u96d1\u3055\")\n\n        self._visualize_moe_concept()\n\n    def _visualize_moe_concept(self):\n        \"\"\"MoE\u306e\u6982\u5ff5\u3092\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        # \u5165\u529b\u30c8\u30fc\u30af\u30f3\n        tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n        for i, token in enumerate(tokens):\n            circle = Circle((0.1, 0.8 - i * 0.15), 0.05,\n                          color='lightblue', ec='black')\n            ax.add_patch(circle)\n            ax.text(0.1, 0.8 - i * 0.15, token,\n                   ha='center', va='center', fontsize=10)\n\n        # \u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n        gate_rect = FancyBboxPatch((0.25, 0.3), 0.15, 0.4,\n                                  boxstyle=\"round,pad=0.02\",\n                                  facecolor='yellow',\n                                  edgecolor='black')\n        ax.add_patch(gate_rect)\n        ax.text(0.325, 0.5, 'Gating\\nNetwork',\n               ha='center', va='center', fontsize=10)\n\n        # \u5c02\u9580\u5bb6FFN\n        experts = [\"Expert 1\\n(\u540d\u8a5e)\", \"Expert 2\\n(\u52d5\u8a5e)\", \n                  \"Expert 3\\n(\u524d\u7f6e\u8a5e)\", \"Expert 4\\n(\u4e00\u822c)\"]\n        colors = ['lightgreen', 'lightcoral', 'lightyellow', 'lightgray']\n\n        for i, (expert, color) in enumerate(zip(experts, colors)):\n            expert_rect = FancyBboxPatch((0.5, 0.7 - i * 0.15), 0.2, 0.1,\n                                       boxstyle=\"round,pad=0.02\",\n                                       facecolor=color,\n                                       edgecolor='black')\n            ax.add_patch(expert_rect)\n            ax.text(0.6, 0.75 - i * 0.15, expert,\n                   ha='center', va='center', fontsize=9)\n\n        # \u9078\u629e\u3055\u308c\u305f\u7d4c\u8def\u3092\u8868\u793a\n        token_to_expert = {\n            \"The\": 3, \"cat\": 0, \"sat\": 1, \"on\": 2, \"mat\": 0\n        }\n\n        for i, (token, expert_idx) in enumerate(token_to_expert.items()):\n            # \u30b2\u30fc\u30c8\u3078\u306e\u77e2\u5370\n            ax.arrow(0.15, 0.8 - i * 0.15, 0.08, 0,\n                    head_width=0.02, head_length=0.01,\n                    fc='gray', ec='gray', alpha=0.5)\n\n            # \u30b2\u30fc\u30c8\u304b\u3089\u5c02\u9580\u5bb6\u3078\u306e\u77e2\u5370\n            start_y = 0.5\n            end_y = 0.75 - expert_idx * 0.15\n            ax.annotate('', xy=(0.5, end_y), xytext=(0.4, start_y),\n                       arrowprops=dict(arrowstyle='-&gt;',\n                                     color='red' if i == 1 else 'blue',\n                                     linewidth=2,\n                                     alpha=0.7))\n\n            # \u30b2\u30fc\u30c8\u5024\u3092\u8868\u793a\n            ax.text(0.45, (start_y + end_y) / 2,\n                   f'{0.8:.1f}' if expert_idx == token_to_expert[tokens[i]] else '',\n                   fontsize=8, color='red' if i == 1 else 'blue')\n\n        # \u51fa\u529b\n        output_rect = FancyBboxPatch((0.8, 0.4), 0.1, 0.2,\n                                   boxstyle=\"round,pad=0.02\",\n                                   facecolor='lightsteelblue',\n                                   edgecolor='black')\n        ax.add_patch(output_rect)\n        ax.text(0.85, 0.5, 'Output',\n               ha='center', va='center', fontsize=10)\n\n        # \u5c02\u9580\u5bb6\u304b\u3089\u51fa\u529b\u3078\u306e\u77e2\u5370\n        for i in range(4):\n            ax.arrow(0.7, 0.75 - i * 0.15, 0.08, 0,\n                    head_width=0.02, head_length=0.01,\n                    fc='gray', ec='gray', alpha=0.3)\n\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0.2, 0.9)\n        ax.set_title('Mixture of Experts FFN', fontsize=16)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def implement_simple_moe(self):\n        \"\"\"\u30b7\u30f3\u30d7\u30eb\u306aMoE\u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== \u30b7\u30f3\u30d7\u30eb\u306aMoE\u5b9f\u88c5 ===\")\n\n        class SimpleMoE(nn.Module):\n            def __init__(self, d_model, d_ff, num_experts=4, top_k=2):\n                super().__init__()\n                self.num_experts = num_experts\n                self.top_k = top_k\n\n                # \u5c02\u9580\u5bb6FFN\n                self.experts = nn.ModuleList([\n                    nn.Sequential(\n                        nn.Linear(d_model, d_ff),\n                        nn.GELU(),\n                        nn.Linear(d_ff, d_model)\n                    ) for _ in range(num_experts)\n                ])\n\n                # \u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n                self.gate = nn.Linear(d_model, num_experts)\n\n            def forward(self, x):\n                batch_size, seq_len, d_model = x.shape\n\n                # \u30b2\u30fc\u30c8\u5024\u306e\u8a08\u7b97\n                gate_scores = self.gate(x)  # [batch, seq, num_experts]\n\n                # Top-k\u5c02\u9580\u5bb6\u306e\u9078\u629e\n                topk_scores, topk_indices = torch.topk(\n                    gate_scores, self.top_k, dim=-1\n                )\n\n                # Softmax\u3067\u6b63\u898f\u5316\n                topk_scores = F.softmax(topk_scores, dim=-1)\n\n                # \u51fa\u529b\u306e\u521d\u671f\u5316\n                output = torch.zeros_like(x)\n\n                # \u5404\u5c02\u9580\u5bb6\u306e\u51e6\u7406\n                for i in range(self.top_k):\n                    # \u9078\u629e\u3055\u308c\u305f\u5c02\u9580\u5bb6\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n                    expert_idx = topk_indices[:, :, i]  # [batch, seq]\n\n                    # \u5404\u5c02\u9580\u5bb6\u306b\u5bfe\u3057\u3066\u51e6\u7406\n                    for e in range(self.num_experts):\n                        # \u3053\u306e\u5c02\u9580\u5bb6\u304c\u9078\u3070\u308c\u305f\u30c8\u30fc\u30af\u30f3\n                        mask = (expert_idx == e)\n\n                        if mask.any():\n                            # \u5c02\u9580\u5bb6\u306e\u51fa\u529b\n                            expert_out = self.experts[e](x)\n\n                            # \u30b2\u30fc\u30c8\u5024\u3067\u91cd\u307f\u4ed8\u3051\n                            scores = topk_scores[:, :, i].unsqueeze(-1)\n                            output += torch.where(\n                                mask.unsqueeze(-1),\n                                expert_out * scores,\n                                torch.zeros_like(expert_out)\n                            )\n\n                return output, gate_scores\n\n        # \u30c6\u30b9\u30c8\n        moe = SimpleMoE(d_model=256, d_ff=1024, num_experts=4, top_k=2)\n        x = torch.randn(2, 10, 256)\n        output, gate_scores = moe(x)\n\n        print(f\"\u5165\u529b\u5f62\u72b6: {x.shape}\")\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\n        print(f\"\u30b2\u30fc\u30c8\u30b9\u30b3\u30a2\u5f62\u72b6: {gate_scores.shape}\")\n\n        # \u5c02\u9580\u5bb6\u306e\u9078\u629e\u30d1\u30bf\u30fc\u30f3\u3092\u5206\u6790\n        topk_scores, topk_indices = torch.topk(gate_scores, 2, dim=-1)\n        print(f\"\\n\u6700\u521d\u306e\u30d0\u30c3\u30c1\u3001\u6700\u521d\u306e5\u30c8\u30fc\u30af\u30f3\u306e\u5c02\u9580\u5bb6\u9078\u629e:\")\n        print(topk_indices[0, :5])\n</code></pre>"},{"location":"part3/feed-forward/#105-ffn","title":"10.5 FFN\u306e\u5b9f\u88c5\u3068\u7d71\u5408","text":""},{"location":"part3/feed-forward/#ffn_1","title":"\u5b8c\u5168\u306aFFN\u5b9f\u88c5","text":"<pre><code>class CompleteFeedForward:\n    \"\"\"\u5b8c\u5168\u306aFeed Forward\u5b9f\u88c5\"\"\"\n\n    def create_feed_forward(self, \n                           d_model: int = 512,\n                           d_ff: int = 2048,\n                           activation: str = 'gelu',\n                           dropout: float = 0.1,\n                           use_glu: bool = False) -&gt; nn.Module:\n        \"\"\"\u67d4\u8edf\u306aFFN\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f5c\u6210\"\"\"\n\n        class FeedForward(nn.Module):\n            def __init__(self):\n                super().__init__()\n\n                if use_glu:\n                    # GLU\u5909\u7a2e\uff08SwiGLU\uff09\n                    self.w1 = nn.Linear(d_model, d_ff)\n                    self.w2 = nn.Linear(d_model, d_ff)\n                    self.w3 = nn.Linear(d_ff, d_model)\n                    self.activation = self._get_activation(activation)\n                else:\n                    # \u6a19\u6e96FFN\n                    self.linear1 = nn.Linear(d_model, d_ff)\n                    self.activation = self._get_activation(activation)\n                    self.dropout1 = nn.Dropout(dropout)\n                    self.linear2 = nn.Linear(d_ff, d_model)\n                    self.dropout2 = nn.Dropout(dropout)\n\n                self.use_glu = use_glu\n                self._init_weights()\n\n            def _get_activation(self, name):\n                activations = {\n                    'relu': nn.ReLU(),\n                    'gelu': nn.GELU(),\n                    'silu': nn.SiLU(),\n                    'mish': nn.Mish()\n                }\n                return activations.get(name, nn.GELU())\n\n            def _init_weights(self):\n                # He\u306e\u521d\u671f\u5316\uff08ReLU\u7cfb\uff09\u307e\u305f\u306fXavier\u306e\u521d\u671f\u5316\n                for m in self.modules():\n                    if isinstance(m, nn.Linear):\n                        if isinstance(self.activation, (nn.ReLU, nn.GELU)):\n                            nn.init.kaiming_normal_(m.weight, mode='fan_in')\n                        else:\n                            nn.init.xavier_uniform_(m.weight)\n\n                        if m.bias is not None:\n                            nn.init.zeros_(m.bias)\n\n            def forward(self, x):\n                if self.use_glu:\n                    # SwiGLU: x = W3(SiLU(W1(x)) * W2(x))\n                    return self.w3(self.activation(self.w1(x)) * self.w2(x))\n                else:\n                    # \u6a19\u6e96FFN\n                    x = self.linear1(x)\n                    x = self.activation(x)\n                    x = self.dropout1(x)\n                    x = self.linear2(x)\n                    x = self.dropout2(x)\n                    return x\n\n        return FeedForward()\n\n    def test_implementations(self):\n        \"\"\"\u5b9f\u88c5\u306e\u30c6\u30b9\u30c8\"\"\"\n        print(\"=== FFN\u5b9f\u88c5\u306e\u30c6\u30b9\u30c8 ===\\n\")\n\n        # \u8a2d\u5b9a\n        batch_size = 2\n        seq_len = 10\n        d_model = 512\n        d_ff = 2048\n\n        # \u5165\u529b\n        x = torch.randn(batch_size, seq_len, d_model)\n\n        # \u5404\u7a2eFFN\n        implementations = [\n            (\"Standard ReLU\", self.create_feed_forward(activation='relu')),\n            (\"Standard GELU\", self.create_feed_forward(activation='gelu')),\n            (\"SwiGLU\", self.create_feed_forward(use_glu=True, activation='silu'))\n        ]\n\n        for name, ffn in implementations:\n            output = ffn(x)\n\n            # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\n            params = sum(p.numel() for p in ffn.parameters())\n\n            print(f\"{name}:\")\n            print(f\"  \u51fa\u529b\u5f62\u72b6: {output.shape}\")\n            print(f\"  \u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {params:,}\")\n            print(f\"  \u51fa\u529b\u7d71\u8a08 - \u5e73\u5747: {output.mean():.4f}, \u6a19\u6e96\u504f\u5dee: {output.std():.4f}\\n\")\n</code></pre>"},{"location":"part3/feed-forward/#ffn_2","title":"\u307e\u3068\u3081\uff1aFFN\u306e\u91cd\u8981\u6027","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060Feed Forward Network\u306e\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u5fc5\u8981\u6027\uff1a</li> <li>Attention\u306e\u7dda\u5f62\u6027\u3092\u88dc\u5b8c\u3059\u308b\u975e\u7dda\u5f62\u5909\u63db</li> <li>\u4f4d\u7f6e\u3054\u3068\u306e\u6df1\u3044\u7279\u5fb4\u62bd\u51fa</li> <li> <p>\u30e2\u30c7\u30eb\u306e\u8868\u73fe\u529b\u3092\u5927\u5e45\u306b\u5411\u4e0a</p> </li> <li> <p>\u69cb\u9020\uff1a</p> </li> <li>\u30b7\u30f3\u30d7\u30eb\u306a2\u5c64MLP</li> <li>\u62e1\u5f35\u2192\u6d3b\u6027\u5316\u2192\u5727\u7e2e\u306e\u30d1\u30bf\u30fc\u30f3</li> <li> <p>Position-wise\uff08\u4f4d\u7f6e\u3054\u3068\u306b\u72ec\u7acb\uff09</p> </li> <li> <p>\u6d3b\u6027\u5316\u95a2\u6570\uff1a</p> </li> <li>ReLU\uff1a\u30b7\u30f3\u30d7\u30eb\u3067\u9ad8\u901f</li> <li>GELU\uff1a\u6ed1\u3089\u304b\u3067\u9ad8\u6027\u80fd</li> <li> <p>GLU\u5909\u7a2e\uff1a\u3055\u3089\u306a\u308b\u8868\u73fe\u529b</p> </li> <li> <p>\u6700\u65b0\u306e\u624b\u6cd5\uff1a</p> </li> <li>SwiGLU\uff1aLLaMA\u3067\u63a1\u7528</li> <li>MoE\uff1a\u30b9\u30d1\u30fc\u30b9\u306a\u5c02\u9580\u5bb6\u30e2\u30c7\u30eb</li> <li>\u52b9\u7387\u7684\u306a\u5b9f\u88c5</li> </ol> <p>FFN\u306f\u3001Transformer\u30d6\u30ed\u30c3\u30af\u306e\u7d042/3\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5360\u3081\u308b\u91cd\u8981\u306a\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3067\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u8981\u7d20\u3092\u5b89\u5b9a\u3057\u3066\u6df1\u304f\u7a4d\u307f\u91cd\u306d\u308b\u305f\u3081\u306e\u6280\u8853\u3001\u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/feed-forward/#_3","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1aGeGLU\uff08GELU Gate\uff09\u3092\u5b9f\u88c5\u3057\u3001\u6a19\u6e96FFN\u3068\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1a\u7570\u306a\u308b\u62e1\u5f35\u7387\uff082x, 4x, 8x\uff09\u3067FFN\u3092\u4f5c\u6210\u3057\u3001\u6027\u80fd\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u30c8\u30ec\u30fc\u30c9\u30aa\u30d5\u3092\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u6700\u9069\u5316\u8ab2\u984c\uff1aSparse FFN\u3092\u5b9f\u88c5\u3057\u3001\u8a08\u7b97\u52b9\u7387\u3092\u6e2c\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1a\u306a\u305cFFN\u306e\u62e1\u5f35\u7387\u306f\u901a\u5e384x\u306a\u306e\u304b\u3001\u7406\u8ad6\u7684\u30fb\u5b9f\u9a13\u7684\u306b\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part3/multi-head-attention/","title":"Multi-Head Attention","text":""},{"location":"part3/multi-head-attention/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u306a\u305c\u300c\u30de\u30eb\u30c1\u30d8\u30c3\u30c9\u300d\u306a\u306e\u304b","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u5358\u4e00\u306e\u6700\u9069\u5316\u30d1\u30b9\u3067\u3059\u3079\u3066\u306e\u6700\u9069\u5316\u3092\u884c\u3046\u3088\u308a\u3001\u7279\u5b9a\u306e\u76ee\u7684\u306b\u7279\u5316\u3057\u305f\u8907\u6570\u306e\u30d1\u30b9\uff08\u5b9a\u6570\u7573\u307f\u8fbc\u307f\u3001\u30c7\u30c3\u30c9\u30b3\u30fc\u30c9\u524a\u9664\u3001\u30eb\u30fc\u30d7\u6700\u9069\u5316\u306a\u3069\uff09\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u65b9\u304c\u52b9\u679c\u7684\u3067\u3059\u3002\u5404\u30d1\u30b9\u306f\u7570\u306a\u308b\u89b3\u70b9\u304b\u3089\u30b3\u30fc\u30c9\u3092\u5206\u6790\u3057\u3001\u305d\u308c\u305e\u308c\u306e\u5f37\u307f\u3092\u6d3b\u304b\u3057\u307e\u3059\u3002</p> <p>Multi-Head Attention\u3082\u540c\u3058\u767a\u60f3\u3067\u3059\u3002\u5358\u4e00\u306e\u6ce8\u610f\u6a5f\u69cb\u3067\u306f\u306a\u304f\u3001\u8907\u6570\u306e\u300c\u30d8\u30c3\u30c9\u300d\u304c\u7570\u306a\u308b\u89b3\u70b9\u304b\u3089\u5165\u529b\u3092\u5206\u6790\u3057\u307e\u3059\u3002\u3042\u308b\u30d8\u30c3\u30c9\u306f\u6587\u6cd5\u7684\u306a\u95a2\u4fc2\u306b\u6ce8\u76ee\u3057\u3001\u5225\u306e\u30d8\u30c3\u30c9\u306f\u610f\u5473\u7684\u306a\u95a2\u9023\u6027\u3092\u3001\u3055\u3089\u306b\u5225\u306e\u30d8\u30c3\u30c9\u306f\u9577\u8ddd\u96e2\u306e\u4f9d\u5b58\u95a2\u4fc2\u3092\u6349\u3048\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u306a\u305cMulti-Head Attention\u304c\u5f37\u529b\u306a\u306e\u304b\u3001\u305d\u3057\u3066\u3069\u306e\u3088\u3046\u306b\u5b9f\u88c5\u3055\u308c\u308b\u306e\u304b\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/multi-head-attention/#91-single-head-multi-head","title":"9.1 Single-Head \u304b\u3089 Multi-Head \u3078","text":""},{"location":"part3/multi-head-attention/#single-head-attention","title":"Single-Head Attention \u306e\u9650\u754c","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Union\nimport math\nfrom matplotlib.patches import Rectangle, FancyBboxPatch, Circle\nfrom matplotlib.patches import ConnectionPatch\nimport matplotlib.patches as mpatches\n\nclass SingleHeadLimitations:\n    \"\"\"Single-Head Attention\u306e\u9650\u754c\u3092\u5b9f\u8a3c\"\"\"\n\n    def __init__(self, d_model: int = 512, seq_len: int = 10):\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.single_head_attention = self._create_single_head_attention()\n\n    def _create_single_head_attention(self):\n        \"\"\"Single-Head Attention\u306e\u5b9f\u88c5\"\"\"\n        class SingleHeadAttention(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.d_model = d_model\n                self.W_q = nn.Linear(d_model, d_model)\n                self.W_k = nn.Linear(d_model, d_model)\n                self.W_v = nn.Linear(d_model, d_model)\n\n            def forward(self, query, key, value, mask=None):\n                batch_size = query.size(0)\n\n                # Q, K, V \u306e\u8a08\u7b97\n                Q = self.W_q(query)\n                K = self.W_k(key)\n                V = self.W_v(value)\n\n                # \u6ce8\u610f\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                # \u6ce8\u610f\u306e\u91cd\u307f\n                attention_weights = F.softmax(scores, dim=-1)\n\n                # \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30d9\u30af\u30c8\u30eb\n                context = torch.matmul(attention_weights, V)\n\n                return context, attention_weights\n\n        return SingleHeadAttention(self.d_model)\n\n    def demonstrate_limitations(self):\n        \"\"\"Single-Head\u306e\u9650\u754c\u3092\u5b9f\u8a3c\"\"\"\n        print(\"=== Single-Head Attention \u306e\u9650\u754c ===\\n\")\n\n        # \u30c6\u30b9\u30c8\u6587\n        sentence = \"The cat sat on the mat while the dog played\"\n        words = sentence.split()\n\n        # \u30c0\u30df\u30fc\u306e\u57cb\u3081\u8fbc\u307f\n        torch.manual_seed(42)\n        embeddings = torch.randn(1, len(words), self.d_model)\n\n        # Single-Head Attention\u3092\u9069\u7528\n        with torch.no_grad():\n            output, attention_weights = self.single_head_attention(\n                embeddings, embeddings, embeddings\n            )\n\n        # \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\n        self._visualize_attention_pattern(words, attention_weights[0])\n\n        print(\"\\n\u554f\u984c\u70b9:\")\n        print(\"1. \u5358\u4e00\u306e\u8868\u73fe\u3057\u304b\u5b66\u7fd2\u3067\u304d\u306a\u3044\")\n        print(\"2. \u7570\u306a\u308b\u7a2e\u985e\u306e\u95a2\u4fc2\u3092\u540c\u6642\u306b\u6349\u3048\u3089\u308c\u306a\u3044\")\n        print(\"3. \u8868\u73fe\u529b\u304c\u9650\u5b9a\u7684\")\n\n    def _visualize_attention_pattern(self, words, attention_weights):\n        \"\"\"\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 8))\n\n        # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        im = ax.imshow(attention_weights.detach().numpy(), \n                      cmap='Blues', aspect='auto')\n\n        # \u8ef8\u306e\u8a2d\u5b9a\n        ax.set_xticks(range(len(words)))\n        ax.set_yticks(range(len(words)))\n        ax.set_xticklabels(words, rotation=45, ha='right')\n        ax.set_yticklabels(words)\n\n        # \u30ab\u30e9\u30fc\u30d0\u30fc\n        plt.colorbar(im, ax=ax)\n\n        ax.set_title('Single-Head Attention Pattern')\n        ax.set_xlabel('Keys')\n        ax.set_ylabel('Queries')\n\n        plt.tight_layout()\n        plt.show()\n\n    def compare_attention_types(self):\n        \"\"\"\u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u6ce8\u610f\u306e\u5fc5\u8981\u6027\"\"\"\n        print(\"\\n=== \u5fc5\u8981\u306a\u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u6ce8\u610f ===\\n\")\n\n        # \u4f8b\u6587\u3067\u306e\u7570\u306a\u308b\u95a2\u4fc2\n        sentence = \"The bank by the river has a beautiful view of the old bridge\"\n        words = sentence.split()\n\n        # \u7570\u306a\u308b\u30bf\u30a4\u30d7\u306e\u95a2\u4fc2\n        relationships = {\n            \"\u6587\u6cd5\u7684\u95a2\u4fc2\": [\n                (\"The\", \"bank\", \"\u51a0\u8a5e-\u540d\u8a5e\"),\n                (\"has\", \"view\", \"\u52d5\u8a5e-\u76ee\u7684\u8a9e\"),\n                (\"beautiful\", \"view\", \"\u5f62\u5bb9\u8a5e-\u540d\u8a5e\")\n            ],\n            \"\u610f\u5473\u7684\u95a2\u4fc2\": [\n                (\"bank\", \"river\", \"\u5834\u6240\u306e\u95a2\u9023\"),\n                (\"view\", \"bridge\", \"\u8996\u899a\u7684\u5bfe\u8c61\"),\n                (\"old\", \"bridge\", \"\u5c5e\u6027\")\n            ],\n            \"\u9577\u8ddd\u96e2\u4f9d\u5b58\": [\n                (\"bank\", \"view\", \"\u4e3b\u8a9e-\u76ee\u7684\u8a9e\"),\n                (\"The\", \"bridge\", \"\u51a0\u8a5e\u306e\u5bfe\u5fdc\")\n            ]\n        }\n\n        # \u53ef\u8996\u5316\n        self._visualize_relationship_types(words, relationships)\n\n    def _visualize_relationship_types(self, words, relationships):\n        \"\"\"\u7570\u306a\u308b\u95a2\u4fc2\u30bf\u30a4\u30d7\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n        for idx, (rel_type, relations) in enumerate(relationships.items()):\n            ax = axes[idx]\n\n            # \u5358\u8a9e\u3092\u914d\u7f6e\n            positions = {}\n            for i, word in enumerate(words):\n                x = i % 4\n                y = 3 - i // 4\n                positions[word] = (x, y)\n\n                # \u5358\u8a9e\u3092\u8868\u793a\n                circle = Circle((x, y), 0.3, color='lightblue', ec='black')\n                ax.add_patch(circle)\n                ax.text(x, y, word, ha='center', va='center', fontsize=8)\n\n            # \u95a2\u4fc2\u3092\u77e2\u5370\u3067\u8868\u793a\n            colors = plt.cm.tab10(np.arange(len(relations)))\n            for i, (w1, w2, label) in enumerate(relations):\n                if w1 in positions and w2 in positions:\n                    x1, y1 = positions[w1]\n                    x2, y2 = positions[w2]\n\n                    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n                               arrowprops=dict(arrowstyle='-&gt;', \n                                             color=colors[i],\n                                             linewidth=2,\n                                             connectionstyle=\"arc3,rad=0.3\"))\n\n                    # \u30e9\u30d9\u30eb\n                    mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n                    ax.text(mid_x, mid_y + 0.3, label, \n                           fontsize=7, ha='center',\n                           bbox=dict(boxstyle=\"round,pad=0.3\", \n                                   facecolor='white', alpha=0.8))\n\n            ax.set_xlim(-0.5, 3.5)\n            ax.set_ylim(-0.5, 3.5)\n            ax.set_title(rel_type, fontsize=14)\n            ax.axis('off')\n\n        plt.suptitle('\u5358\u4e00\u306e\u6ce8\u610f\u6a5f\u69cb\u3067\u306f\u6349\u3048\u304d\u308c\u306a\u3044\u591a\u69d8\u306a\u95a2\u4fc2', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/multi-head-attention/#multi-head","title":"Multi-Head \u306e\u57fa\u672c\u6982\u5ff5","text":"<pre><code>class MultiHeadConcept:\n    \"\"\"Multi-Head Attention\u306e\u6982\u5ff5\u8aac\u660e\"\"\"\n\n    def explain_multi_head_idea(self):\n        \"\"\"Multi-Head\u306e\u30a2\u30a4\u30c7\u30a2\u3092\u8aac\u660e\"\"\"\n        print(\"=== Multi-Head Attention \u306e\u57fa\u672c\u6982\u5ff5 ===\\n\")\n\n        print(\"\u30a2\u30ca\u30ed\u30b8\u30fc\uff1a\u4f1a\u8b70\u3067\u306e\u610f\u601d\u6c7a\u5b9a\")\n        print(\"- Single-Head: 1\u4eba\u306e\u5c02\u9580\u5bb6\u304c\u5168\u3066\u3092\u5224\u65ad\")\n        print(\"- Multi-Head: \u8907\u6570\u306e\u5c02\u9580\u5bb6\u304c\u7570\u306a\u308b\u89b3\u70b9\u304b\u3089\u5206\u6790\\n\")\n\n        print(\"\u5404\u30d8\u30c3\u30c9\u306e\u7279\u5316\u4f8b:\")\n        specialists = [\n            (\"Head 1\", \"\u6587\u6cd5\u5c02\u9580\u5bb6\", \"\u4e3b\u8a9e-\u52d5\u8a5e\u306e\u4e00\u81f4\u3001\u4fee\u98fe\u95a2\u4fc2\"),\n            (\"Head 2\", \"\u610f\u5473\u5c02\u9580\u5bb6\", \"\u5358\u8a9e\u306e\u610f\u5473\u7684\u95a2\u9023\u6027\"),\n            (\"Head 3\", \"\u6587\u8108\u5c02\u9580\u5bb6\", \"\u9577\u8ddd\u96e2\u306e\u6587\u8108\u4f9d\u5b58\"),\n            (\"Head 4\", \"\u4f4d\u7f6e\u5c02\u9580\u5bb6\", \"\u5358\u8a9e\u306e\u76f8\u5bfe\u4f4d\u7f6e\u95a2\u4fc2\")\n        ]\n\n        for head, role, focus in specialists:\n            print(f\"{head} ({role}): {focus}\")\n\n        # \u56f3\u89e3\n        self._visualize_multi_head_concept()\n\n    def _visualize_multi_head_concept(self):\n        \"\"\"Multi-Head\u306e\u6982\u5ff5\u3092\u56f3\u89e3\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        # \u5165\u529b\n        input_rect = FancyBboxPatch((0.1, 0.4), 0.15, 0.2,\n                                   boxstyle=\"round,pad=0.02\",\n                                   facecolor='lightblue',\n                                   edgecolor='black')\n        ax.add_patch(input_rect)\n        ax.text(0.175, 0.5, 'Input\\nSequence', ha='center', va='center')\n\n        # \u5404\u30d8\u30c3\u30c9\n        head_colors = ['lightgreen', 'lightcoral', 'lightyellow', 'lightpink']\n        head_names = ['Head 1\\n(Grammar)', 'Head 2\\n(Semantic)', \n                     'Head 3\\n(Context)', 'Head 4\\n(Position)']\n\n        for i, (color, name) in enumerate(zip(head_colors, head_names)):\n            y_pos = 0.7 - i * 0.15\n\n            # \u30d8\u30c3\u30c9\u306e\u77e9\u5f62\n            head_rect = FancyBboxPatch((0.4, y_pos), 0.2, 0.1,\n                                      boxstyle=\"round,pad=0.02\",\n                                      facecolor=color,\n                                      edgecolor='black')\n            ax.add_patch(head_rect)\n            ax.text(0.5, y_pos + 0.05, name, ha='center', va='center', fontsize=10)\n\n            # \u5165\u529b\u304b\u3089\u306e\u77e2\u5370\n            ax.arrow(0.25, 0.5, 0.14, y_pos + 0.05 - 0.5,\n                    head_width=0.02, head_length=0.01,\n                    fc='gray', ec='gray')\n\n        # \u7d50\u5408\n        concat_rect = FancyBboxPatch((0.7, 0.4), 0.15, 0.2,\n                                    boxstyle=\"round,pad=0.02\",\n                                    facecolor='lavender',\n                                    edgecolor='black')\n        ax.add_patch(concat_rect)\n        ax.text(0.775, 0.5, 'Concat', ha='center', va='center')\n\n        # \u5404\u30d8\u30c3\u30c9\u304b\u3089\u7d50\u5408\u3078\u306e\u77e2\u5370\n        for i in range(4):\n            y_pos = 0.75 - i * 0.15\n            ax.arrow(0.6, y_pos, 0.09, 0.5 - y_pos,\n                    head_width=0.02, head_length=0.01,\n                    fc='gray', ec='gray')\n\n        # \u51fa\u529b\n        output_rect = FancyBboxPatch((0.9, 0.4), 0.15, 0.2,\n                                    boxstyle=\"round,pad=0.02\",\n                                    facecolor='lightsteelblue',\n                                    edgecolor='black')\n        ax.add_patch(output_rect)\n        ax.text(0.975, 0.5, 'Output', ha='center', va='center')\n\n        # \u6700\u7d42\u77e2\u5370\n        ax.arrow(0.85, 0.5, 0.04, 0,\n                head_width=0.02, head_length=0.01,\n                fc='black', ec='black')\n\n        ax.set_xlim(0, 1.1)\n        ax.set_ylim(0.2, 0.8)\n        ax.set_title('Multi-Head Attention: \u8907\u6570\u306e\u5c02\u9580\u5bb6\u306b\u3088\u308b\u5206\u6790', fontsize=16)\n        ax.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/multi-head-attention/#92-multi-head-attention","title":"9.2 Multi-Head Attention \u306e\u6570\u5b66\u7684\u5b9a\u7fa9","text":""},{"location":"part3/multi-head-attention/#_2","title":"\u8a08\u7b97\u306e\u8a73\u7d30","text":"<pre><code>class MultiHeadMathematics:\n    \"\"\"Multi-Head Attention\u306e\u6570\u5b66\u7684\u8a73\u7d30\"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.d_v = d_model // n_heads\n\n    def explain_dimensions(self):\n        \"\"\"\u6b21\u5143\u306e\u5206\u5272\u3092\u8aac\u660e\"\"\"\n        print(\"=== Multi-Head Attention \u306e\u6b21\u5143 ===\\n\")\n\n        print(f\"\u30e2\u30c7\u30eb\u6b21\u5143 (d_model): {self.d_model}\")\n        print(f\"\u30d8\u30c3\u30c9\u6570 (n_heads): {self.n_heads}\")\n        print(f\"\u5404\u30d8\u30c3\u30c9\u306eKey/Query\u6b21\u5143 (d_k): {self.d_k}\")\n        print(f\"\u5404\u30d8\u30c3\u30c9\u306eValue\u6b21\u5143 (d_v): {self.d_v}\")\n\n        print(f\"\\n\u91cd\u8981\u306a\u95a2\u4fc2:\")\n        print(f\"d_model = n_heads \u00d7 d_k = {self.n_heads} \u00d7 {self.d_k} = {self.d_model}\")\n\n        # \u6b21\u5143\u5206\u5272\u306e\u53ef\u8996\u5316\n        self._visualize_dimension_split()\n\n    def _visualize_dimension_split(self):\n        \"\"\"\u6b21\u5143\u5206\u5272\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n        # Single-Head\n        ax1.set_title('Single-Head: \u5168\u6b21\u5143\u3092\u4f7f\u7528', fontsize=14)\n        single_rect = Rectangle((0, 0), self.d_model, 1,\n                              facecolor='lightblue', edgecolor='black')\n        ax1.add_patch(single_rect)\n        ax1.text(self.d_model/2, 0.5, f'd_model = {self.d_model}',\n                ha='center', va='center', fontsize=12)\n        ax1.set_xlim(-10, self.d_model + 10)\n        ax1.set_ylim(-0.5, 1.5)\n        ax1.set_xlabel('Dimension')\n        ax1.axis('off')\n\n        # Multi-Head\n        ax2.set_title(f'Multi-Head: {self.n_heads}\u500b\u306e\u30d8\u30c3\u30c9\u306b\u5206\u5272', fontsize=14)\n        colors = plt.cm.tab10(np.arange(self.n_heads))\n\n        for i in range(self.n_heads):\n            x_start = i * self.d_k\n            rect = Rectangle((x_start, 0), self.d_k, 1,\n                           facecolor=colors[i], edgecolor='black',\n                           alpha=0.7)\n            ax2.add_patch(rect)\n            ax2.text(x_start + self.d_k/2, 0.5, f'Head {i+1}\\n{self.d_k}',\n                    ha='center', va='center', fontsize=10)\n\n        ax2.set_xlim(-10, self.d_model + 10)\n        ax2.set_ylim(-0.5, 1.5)\n        ax2.set_xlabel('Dimension')\n        ax2.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def explain_projection_matrices(self):\n        \"\"\"\u6295\u5f71\u884c\u5217\u306e\u8aac\u660e\"\"\"\n        print(\"\\n=== \u6295\u5f71\u884c\u5217\u306e\u5f79\u5272 ===\\n\")\n\n        print(\"\u5404\u30d8\u30c3\u30c9\u3054\u3068\u306b\u72ec\u7acb\u3057\u305f\u6295\u5f71\u884c\u5217:\")\n        print(f\"- W_q^i: [d_model \u00d7 d_k] = [{self.d_model} \u00d7 {self.d_k}]\")\n        print(f\"- W_k^i: [d_model \u00d7 d_k] = [{self.d_model} \u00d7 {self.d_k}]\")\n        print(f\"- W_v^i: [d_model \u00d7 d_v] = [{self.d_model} \u00d7 {self.d_v}]\")\n\n        print(f\"\\n\u5168\u30d8\u30c3\u30c9\u5408\u8a08\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570:\")\n        total_params = self.n_heads * (3 * self.d_model * self.d_k)\n        print(f\"3 \u00d7 n_heads \u00d7 d_model \u00d7 d_k = 3 \u00d7 {self.n_heads} \u00d7 {self.d_model} \u00d7 {self.d_k}\")\n        print(f\"= {total_params:,} \u30d1\u30e9\u30e1\u30fc\u30bf\")\n\n        # \u5b9f\u88c5\u306e\u52b9\u7387\u5316\n        print(\"\\n\u5b9f\u88c5\u306e\u52b9\u7387\u5316:\")\n        print(\"\u500b\u5225\u306e\u884c\u5217\u3067\u306f\u306a\u304f\u3001\u5927\u304d\u306a\u884c\u5217\u3068\u3057\u3066\u5b9f\u88c5\")\n        print(f\"W_Q: [{self.d_model} \u00d7 {self.d_model}]\")\n        print(f\"W_K: [{self.d_model} \u00d7 {self.d_model}]\")\n        print(f\"W_V: [{self.d_model} \u00d7 {self.d_model}]\")\n</code></pre>"},{"location":"part3/multi-head-attention/#_3","title":"\u4e26\u5217\u8a08\u7b97\u306e\u4ed5\u7d44\u307f","text":"<pre><code>class ParallelComputation:\n    \"\"\"Multi-Head Attention\u306e\u4e26\u5217\u8a08\u7b97\"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n    def demonstrate_parallel_computation(self):\n        \"\"\"\u4e26\u5217\u8a08\u7b97\u306e\u5b9f\u6f14\"\"\"\n        print(\"=== Multi-Head \u306e\u4e26\u5217\u8a08\u7b97 ===\\n\")\n\n        # \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\n        batch_size = 2\n        seq_len = 4\n\n        # \u5165\u529b\n        X = torch.randn(batch_size, seq_len, self.d_model)\n\n        # \u6295\u5f71\u884c\u5217\n        W_Q = torch.randn(self.d_model, self.d_model)\n        W_K = torch.randn(self.d_model, self.d_model)\n        W_V = torch.randn(self.d_model, self.d_model)\n\n        # \u65b9\u6cd51: \u30eb\u30fc\u30d7\u306b\u3088\u308b\u8a08\u7b97\uff08\u975e\u52b9\u7387\uff09\n        print(\"\u65b9\u6cd51: \u30eb\u30fc\u30d7\u306b\u3088\u308b\u8a08\u7b97\")\n        heads_loop = []\n        for i in range(self.n_heads):\n            start_idx = i * self.d_k\n            end_idx = (i + 1) * self.d_k\n\n            # \u5404\u30d8\u30c3\u30c9\u306e\u6295\u5f71\n            Q_i = torch.matmul(X, W_Q[:, start_idx:end_idx])\n            K_i = torch.matmul(X, W_K[:, start_idx:end_idx])\n            V_i = torch.matmul(X, W_V[:, start_idx:end_idx])\n\n            # \u6ce8\u610f\u306e\u8a08\u7b97\n            scores_i = torch.matmul(Q_i, K_i.transpose(-2, -1)) / math.sqrt(self.d_k)\n            weights_i = F.softmax(scores_i, dim=-1)\n            head_i = torch.matmul(weights_i, V_i)\n\n            heads_loop.append(head_i)\n\n        output_loop = torch.cat(heads_loop, dim=-1)\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output_loop.shape}\")\n\n        # \u65b9\u6cd52: \u4e26\u5217\u8a08\u7b97\uff08\u52b9\u7387\u7684\uff09\n        print(\"\\n\u65b9\u6cd52: \u4e26\u5217\u8a08\u7b97\")\n\n        # \u4e00\u5ea6\u306b\u5168\u30d8\u30c3\u30c9\u306e\u6295\u5f71\u3092\u8a08\u7b97\n        Q = torch.matmul(X, W_Q)  # [batch, seq, d_model]\n        K = torch.matmul(X, W_K)\n        V = torch.matmul(X, W_V)\n\n        # \u5f62\u72b6\u3092\u5909\u66f4\u3057\u3066\u30d8\u30c3\u30c9\u306b\u5206\u5272\n        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n        # \u5f62\u72b6: [batch, n_heads, seq_len, d_k]\n\n        # \u5168\u30d8\u30c3\u30c9\u3067\u540c\u6642\u306b\u6ce8\u610f\u3092\u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        weights = F.softmax(scores, dim=-1)\n        heads = torch.matmul(weights, V)\n\n        # \u5f62\u72b6\u3092\u623b\u3057\u3066\u7d50\u5408\n        heads = heads.transpose(1, 2).contiguous()\n        output_parallel = heads.view(batch_size, seq_len, self.d_model)\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output_parallel.shape}\")\n\n        # \u8a08\u7b97\u6642\u9593\u306e\u6bd4\u8f03\u3092\u53ef\u8996\u5316\n        self._visualize_computation_efficiency()\n\n    def _visualize_computation_efficiency(self):\n        \"\"\"\u8a08\u7b97\u52b9\u7387\u306e\u53ef\u8996\u5316\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u30eb\u30fc\u30d7\u65b9\u5f0f\n        ax1.set_title('\u30eb\u30fc\u30d7\u306b\u3088\u308b\u9010\u6b21\u8a08\u7b97', fontsize=14)\n\n        for i in range(self.n_heads):\n            # \u5404\u30d8\u30c3\u30c9\u306e\u8a08\u7b97\u6642\u9593\n            rect = Rectangle((i * 1.2, 0), 1, 1,\n                           facecolor=plt.cm.Reds((i+1)/self.n_heads),\n                           edgecolor='black')\n            ax1.add_patch(rect)\n            ax1.text(i * 1.2 + 0.5, 0.5, f'Head {i+1}',\n                    ha='center', va='center', fontsize=10)\n\n            # \u77e2\u5370\u3067\u9806\u5e8f\u3092\u793a\u3059\n            if i &lt; self.n_heads - 1:\n                ax1.arrow(i * 1.2 + 1, 0.5, 0.15, 0,\n                         head_width=0.1, head_length=0.05,\n                         fc='black', ec='black')\n\n        ax1.set_xlim(-0.5, self.n_heads * 1.2)\n        ax1.set_ylim(-0.5, 1.5)\n        ax1.set_xlabel('Time \u2192')\n        ax1.axis('off')\n\n        # \u4e26\u5217\u65b9\u5f0f\n        ax2.set_title('\u4e26\u5217\u8a08\u7b97', fontsize=14)\n\n        # \u5168\u30d8\u30c3\u30c9\u3092\u540c\u6642\u306b\u8868\u793a\n        for i in range(self.n_heads):\n            rect = Rectangle((0, i * 0.15), 1, 0.12,\n                           facecolor=plt.cm.Greens((i+1)/self.n_heads),\n                           edgecolor='black')\n            ax2.add_patch(rect)\n            ax2.text(0.5, i * 0.15 + 0.06, f'Head {i+1}',\n                    ha='center', va='center', fontsize=10)\n\n        ax2.set_xlim(-0.5, 1.5)\n        ax2.set_ylim(-0.1, self.n_heads * 0.15 + 0.1)\n        ax2.set_xlabel('Time \u2192')\n        ax2.axis('off')\n\n        # \u6642\u9593\u306e\u6bd4\u8f03\n        ax1.text(self.n_heads * 0.6, -0.3, f'\u7dcf\u6642\u9593: {self.n_heads}T',\n                ha='center', fontsize=12, color='red')\n        ax2.text(0.5, -0.05, '\u7dcf\u6642\u9593: T',\n                ha='center', fontsize=12, color='green')\n\n        plt.suptitle('Multi-Head Attention \u306e\u8a08\u7b97\u52b9\u7387', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/multi-head-attention/#93-multi-head-attention","title":"9.3 Multi-Head Attention \u306e\u5b9f\u88c5","text":""},{"location":"part3/multi-head-attention/#_4","title":"\u5b8c\u5168\u306a\u5b9f\u88c5","text":"<pre><code>class MultiHeadAttentionImplementation:\n    \"\"\"Multi-Head Attention\u306e\u5b8c\u5168\u306a\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model: int = 512, n_heads: int = 8, dropout: float = 0.1):\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.d_v = d_model // n_heads\n        self.dropout = dropout\n\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n\n    def create_multi_head_attention(self) -&gt; nn.Module:\n        \"\"\"Multi-Head Attention\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f5c\u6210\"\"\"\n\n        class MultiHeadAttention(nn.Module):\n            def __init__(self, d_model, n_heads, dropout=0.1):\n                super().__init__()\n                self.d_model = d_model\n                self.n_heads = n_heads\n                self.d_k = d_model // n_heads\n                self.d_v = d_model // n_heads\n\n                # \u7dda\u5f62\u6295\u5f71\u5c64\n                self.W_q = nn.Linear(d_model, d_model, bias=False)\n                self.W_k = nn.Linear(d_model, d_model, bias=False)\n                self.W_v = nn.Linear(d_model, d_model, bias=False)\n                self.W_o = nn.Linear(d_model, d_model)\n\n                # Dropout\n                self.dropout = nn.Dropout(dropout)\n\n                # \u521d\u671f\u5316\n                self._init_weights()\n\n            def _init_weights(self):\n                # Xavier\u521d\u671f\u5316\n                for module in [self.W_q, self.W_k, self.W_v, self.W_o]:\n                    nn.init.xavier_uniform_(module.weight)\n\n                # \u51fa\u529b\u5c64\u306e\u30d0\u30a4\u30a2\u30b9\u306f0\u306b\n                if hasattr(self.W_o, 'bias') and self.W_o.bias is not None:\n                    nn.init.constant_(self.W_o.bias, 0.)\n\n            def forward(self, query, key, value, mask=None):\n                \"\"\"\n                Args:\n                    query: [batch_size, seq_len_q, d_model]\n                    key: [batch_size, seq_len_k, d_model]\n                    value: [batch_size, seq_len_v, d_model]\n                    mask: [batch_size, n_heads, seq_len_q, seq_len_k]\n                \"\"\"\n                batch_size = query.size(0)\n                seq_len_q = query.size(1)\n                seq_len_k = key.size(1)\n\n                # 1. \u7dda\u5f62\u6295\u5f71\n                Q = self.W_q(query)  # [batch_size, seq_len_q, d_model]\n                K = self.W_k(key)    # [batch_size, seq_len_k, d_model]\n                V = self.W_v(value)  # [batch_size, seq_len_v, d_model]\n\n                # 2. \u30d8\u30c3\u30c9\u306b\u5206\u5272\n                Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k)\n                K = K.view(batch_size, seq_len_k, self.n_heads, self.d_k)\n                V = V.view(batch_size, seq_len_k, self.n_heads, self.d_v)\n\n                # 3. \u8ee2\u7f6e\u3057\u3066\u30d8\u30c3\u30c9\u3092\u524d\u306b\n                Q = Q.transpose(1, 2)  # [batch_size, n_heads, seq_len_q, d_k]\n                K = K.transpose(1, 2)  # [batch_size, n_heads, seq_len_k, d_k]\n                V = V.transpose(1, 2)  # [batch_size, n_heads, seq_len_v, d_v]\n\n                # 4. \u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\n                attention_output, attention_weights = self.scaled_dot_product_attention(\n                    Q, K, V, mask\n                )\n\n                # 5. \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n                attention_output = attention_output.transpose(1, 2).contiguous()\n                attention_output = attention_output.view(\n                    batch_size, seq_len_q, self.d_model\n                )\n\n                # 6. \u6700\u7d42\u7dda\u5f62\u5c64\n                output = self.W_o(attention_output)\n\n                return output, attention_weights\n\n            def scaled_dot_product_attention(self, Q, K, V, mask=None):\n                \"\"\"\u30b9\u30b1\u30fc\u30eb\u30c9\u30c9\u30c3\u30c8\u7a4d\u6ce8\u610f\u306e\u8a08\u7b97\"\"\"\n                # \u30b9\u30b3\u30a2\u306e\u8a08\u7b97\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n                # \u30de\u30b9\u30af\u306e\u9069\u7528\n                if mask is not None:\n                    scores = scores.masked_fill(mask == 0, -1e9)\n\n                # Softmax\n                attention_weights = F.softmax(scores, dim=-1)\n                attention_weights = self.dropout(attention_weights)\n\n                # \u5024\u306e\u91cd\u307f\u4ed8\u304d\u548c\n                context = torch.matmul(attention_weights, V)\n\n                return context, attention_weights\n\n        return MultiHeadAttention(self.d_model, self.n_heads, self.dropout)\n\n    def test_implementation(self):\n        \"\"\"\u5b9f\u88c5\u306e\u30c6\u30b9\u30c8\"\"\"\n        print(\"=== Multi-Head Attention \u306e\u5b9f\u88c5\u30c6\u30b9\u30c8 ===\\n\")\n\n        # \u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u4f5c\u6210\n        mha = self.create_multi_head_attention()\n\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n        batch_size = 2\n        seq_len = 5\n        x = torch.randn(batch_size, seq_len, self.d_model)\n\n        # Forward pass\n        output, attention_weights = mha(x, x, x)\n\n        print(f\"\u5165\u529b\u5f62\u72b6: {x.shape}\")\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\n        print(f\"\u6ce8\u610f\u306e\u91cd\u307f\u5f62\u72b6: {attention_weights.shape}\")\n\n        # \u5404\u30d8\u30c3\u30c9\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\n        self._visualize_head_patterns(attention_weights[0])\n\n    def _visualize_head_patterns(self, attention_weights):\n        \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\"\"\"\n        n_heads = attention_weights.shape[0]\n        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n        axes = axes.ravel()\n\n        for head in range(n_heads):\n            ax = axes[head]\n            im = ax.imshow(attention_weights[head].detach().numpy(),\n                          cmap='Blues', aspect='auto')\n            ax.set_title(f'Head {head + 1}')\n            ax.set_xlabel('Key positions')\n            ax.set_ylabel('Query positions')\n\n            # \u30ab\u30e9\u30fc\u30d0\u30fc\u306f\u6700\u521d\u3068\u6700\u5f8c\u306e\u30d8\u30c3\u30c9\u306e\u307f\n            if head == 0 or head == n_heads - 1:\n                plt.colorbar(im, ax=ax, fraction=0.046)\n\n        plt.suptitle('\u5404\u30d8\u30c3\u30c9\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/multi-head-attention/#_5","title":"\u30de\u30b9\u30af\u306e\u5b9f\u88c5","text":"<pre><code>class AttentionMasking:\n    \"\"\"\u6ce8\u610f\u30de\u30b9\u30af\u306e\u5b9f\u88c5\u3068\u7406\u89e3\"\"\"\n\n    def explain_mask_types(self):\n        \"\"\"\u30de\u30b9\u30af\u306e\u7a2e\u985e\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u6ce8\u610f\u30de\u30b9\u30af\u306e\u7a2e\u985e ===\\n\")\n\n        mask_types = {\n            \"Padding Mask\": {\n                \"\u76ee\u7684\": \"\u30d1\u30c7\u30a3\u30f3\u30b0\u30c8\u30fc\u30af\u30f3\u3078\u306e\u6ce8\u610f\u3092\u9632\u3050\",\n                \"\u4f7f\u7528\u5834\u6240\": \"\u30a8\u30f3\u30b3\u30fc\u30c0\u3001\u30c7\u30b3\u30fc\u30c0\u4e21\u65b9\",\n                \"\u5f62\u72b6\": \"[batch_size, seq_len]\"\n            },\n            \"Look-ahead Mask\": {\n                \"\u76ee\u7684\": \"\u672a\u6765\u306e\u60c5\u5831\u3078\u306e\u6ce8\u610f\u3092\u9632\u3050\uff08\u81ea\u5df1\u56de\u5e30\uff09\",\n                \"\u4f7f\u7528\u5834\u6240\": \"\u30c7\u30b3\u30fc\u30c0\u306e\u81ea\u5df1\u6ce8\u610f\",\n                \"\u5f62\u72b6\": \"[seq_len, seq_len]\uff08\u4e0b\u4e09\u89d2\u884c\u5217\uff09\"\n            },\n            \"Cross-attention Mask\": {\n                \"\u76ee\u7684\": \"\u30a8\u30f3\u30b3\u30fc\u30c0\u51fa\u529b\u306e\u7279\u5b9a\u90e8\u5206\u3078\u306e\u6ce8\u610f\u3092\u5236\u5fa1\",\n                \"\u4f7f\u7528\u5834\u6240\": \"\u30c7\u30b3\u30fc\u30c0\u306e\u30af\u30ed\u30b9\u6ce8\u610f\",\n                \"\u5f62\u72b6\": \"[tgt_len, src_len]\"\n            }\n        }\n\n        for mask_name, properties in mask_types.items():\n            print(f\"{mask_name}:\")\n            for key, value in properties.items():\n                print(f\"  {key}: {value}\")\n            print()\n\n    def create_masks(self, seq_len: int = 6):\n        \"\"\"\u5404\u7a2e\u30de\u30b9\u30af\u3092\u4f5c\u6210\"\"\"\n\n        # 1. Padding Mask\n        # \u5b9f\u969b\u306e\u30c8\u30fc\u30af\u30f3\u9577\u3092\u4eee\u5b9a\n        actual_lengths = [4, 6]  # \u30d0\u30c3\u30c1\u5185\u306e\u5404\u7cfb\u5217\u306e\u5b9f\u969b\u306e\u9577\u3055\n        padding_mask = self._create_padding_mask(seq_len, actual_lengths)\n\n        # 2. Look-ahead Mask\n        look_ahead_mask = self._create_look_ahead_mask(seq_len)\n\n        # 3. Combined Mask\uff08\u30c7\u30b3\u30fc\u30c0\u7528\uff09\n        combined_mask = self._combine_masks(padding_mask[0], look_ahead_mask)\n\n        # \u53ef\u8996\u5316\n        self._visualize_masks(padding_mask[0], look_ahead_mask, combined_mask)\n\n        return padding_mask, look_ahead_mask, combined_mask\n\n    def _create_padding_mask(self, seq_len: int, actual_lengths: List[int]):\n        \"\"\"\u30d1\u30c7\u30a3\u30f3\u30b0\u30de\u30b9\u30af\u3092\u4f5c\u6210\"\"\"\n        batch_size = len(actual_lengths)\n        mask = torch.zeros(batch_size, seq_len)\n\n        for i, length in enumerate(actual_lengths):\n            if length &lt; seq_len:\n                mask[i, length:] = 1  # \u30d1\u30c7\u30a3\u30f3\u30b0\u4f4d\u7f6e\u30921\u306b\n\n        return mask\n\n    def _create_look_ahead_mask(self, seq_len: int):\n        \"\"\"Look-ahead\u30de\u30b9\u30af\u3092\u4f5c\u6210\"\"\"\n        # \u4e0b\u4e09\u89d2\u884c\u5217\uff08\u5bfe\u89d2\u7dda\u542b\u3080\uff09\u304c1\n        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n        return mask\n\n    def _combine_masks(self, padding_mask, look_ahead_mask):\n        \"\"\"\u30de\u30b9\u30af\u3092\u7d50\u5408\"\"\"\n        # padding_mask\u3092\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\n        padding_mask_expanded = padding_mask.unsqueeze(0).expand(\n            look_ahead_mask.shape[0], -1\n        )\n\n        # \u8ad6\u7406\u548c\uff08\u3069\u3061\u3089\u304b\u304c1\u306a\u30891\uff09\n        combined = torch.maximum(padding_mask_expanded, look_ahead_mask)\n        return combined\n\n    def _visualize_masks(self, padding_mask, look_ahead_mask, combined_mask):\n        \"\"\"\u30de\u30b9\u30af\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        masks = [\n            (padding_mask, \"Padding Mask\"),\n            (look_ahead_mask, \"Look-ahead Mask\"),\n            (combined_mask, \"Combined Mask\")\n        ]\n\n        for ax, (mask, title) in zip(axes, masks):\n            # \u30de\u30b9\u30af\u3092\u53ef\u8996\u5316\uff081\u304c\u9ed2\u30010\u304c\u767d\uff09\n            im = ax.imshow(mask, cmap='binary', aspect='auto')\n            ax.set_title(title, fontsize=14)\n            ax.set_xlabel('Position')\n            ax.set_ylabel('Position')\n\n            # \u30b0\u30ea\u30c3\u30c9\n            ax.set_xticks(np.arange(mask.shape[-1]))\n            ax.set_yticks(np.arange(mask.shape[0]))\n            ax.grid(True, alpha=0.3)\n\n            # \u5024\u3092\u8868\u793a\n            for i in range(mask.shape[0]):\n                for j in range(mask.shape[1]):\n                    ax.text(j, i, int(mask[i, j].item()),\n                           ha='center', va='center',\n                           color='white' if mask[i, j] &gt; 0.5 else 'black')\n\n        plt.suptitle('\u6ce8\u610f\u30de\u30b9\u30af\u306e\u7a2e\u985e\uff08\u9ed2=\u30de\u30b9\u30af\u3001\u767d=\u6ce8\u610f\u53ef\u80fd\uff09', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/multi-head-attention/#94","title":"9.4 \u5404\u30d8\u30c3\u30c9\u304c\u5b66\u7fd2\u3059\u308b\u8868\u73fe","text":""},{"location":"part3/multi-head-attention/#_6","title":"\u30d8\u30c3\u30c9\u306e\u7279\u5316\u3092\u5206\u6790","text":"<pre><code>class HeadSpecialization:\n    \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u7279\u5316\u3092\u5206\u6790\"\"\"\n\n    def __init__(self, model_path: Optional[str] = None):\n        self.model = None\n        if model_path:\n            # \u5b9f\u969b\u306e\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\n            pass\n        else:\n            # \u30c7\u30e2\u7528\u306e\u4eee\u60f3\u7684\u306a\u30d1\u30bf\u30fc\u30f3\n            self.create_demo_patterns()\n\n    def create_demo_patterns(self):\n        \"\"\"\u30c7\u30e2\u7528\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u4f5c\u6210\"\"\"\n        self.demo_patterns = {\n            \"positional\": self._create_positional_pattern,\n            \"syntactic\": self._create_syntactic_pattern,\n            \"semantic\": self._create_semantic_pattern,\n            \"broad\": self._create_broad_pattern\n        }\n\n    def _create_positional_pattern(self, seq_len: int = 10):\n        \"\"\"\u4f4d\u7f6e\u7684\u306a\u30d1\u30bf\u30fc\u30f3\uff08\u96a3\u63a5\u3059\u308b\u5358\u8a9e\u306b\u6ce8\u76ee\uff09\"\"\"\n        pattern = torch.zeros(seq_len, seq_len)\n        for i in range(seq_len):\n            for j in range(max(0, i-1), min(seq_len, i+2)):\n                pattern[i, j] = 1.0 if i != j else 0.5\n\n        # \u6b63\u898f\u5316\n        pattern = pattern / pattern.sum(dim=-1, keepdim=True)\n        return pattern\n\n    def _create_syntactic_pattern(self, seq_len: int = 10):\n        \"\"\"\u6587\u6cd5\u7684\u30d1\u30bf\u30fc\u30f3\uff08\u7279\u5b9a\u306e\u69cb\u9020\u306b\u6ce8\u76ee\uff09\"\"\"\n        pattern = torch.eye(seq_len) * 0.3\n\n        # \u52d5\u8a5e\u4f4d\u7f6e\uff08\u4eee\u5b9a\uff09\u304b\u3089\u4e3b\u8a9e\u30fb\u76ee\u7684\u8a9e\u3078\u306e\u6ce8\u76ee\n        verb_positions = [2, 6]\n        for verb_pos in verb_positions:\n            if verb_pos &lt; seq_len:\n                # \u4e3b\u8a9e\uff08\u524d\u65b9\uff09\u3078\u306e\u6ce8\u76ee\n                if verb_pos &gt; 0:\n                    pattern[verb_pos, verb_pos-1] = 0.4\n                # \u76ee\u7684\u8a9e\uff08\u5f8c\u65b9\uff09\u3078\u306e\u6ce8\u76ee\n                if verb_pos &lt; seq_len - 1:\n                    pattern[verb_pos, verb_pos+1] = 0.3\n\n        # \u6b63\u898f\u5316\n        pattern = pattern / pattern.sum(dim=-1, keepdim=True).clamp(min=1e-9)\n        return pattern\n\n    def _create_semantic_pattern(self, seq_len: int = 10):\n        \"\"\"\u610f\u5473\u7684\u30d1\u30bf\u30fc\u30f3\uff08\u95a2\u9023\u3059\u308b\u5358\u8a9e\u306b\u6ce8\u76ee\uff09\"\"\"\n        pattern = torch.rand(seq_len, seq_len) * 0.3\n        pattern = pattern + pattern.T  # \u5bfe\u79f0\u6027\n        pattern = pattern / pattern.sum(dim=-1, keepdim=True)\n        return pattern\n\n    def _create_broad_pattern(self, seq_len: int = 10):\n        \"\"\"\u5e83\u7bc4\u306a\u30d1\u30bf\u30fc\u30f3\uff08\u5168\u4f53\u7684\u306a\u6587\u8108\uff09\"\"\"\n        pattern = torch.ones(seq_len, seq_len) / seq_len\n        # \u81ea\u5df1\u3078\u306e\u6ce8\u76ee\u3092\u5c11\u3057\u5f37\u3081\u308b\n        pattern = pattern + torch.eye(seq_len) * 0.1\n        pattern = pattern / pattern.sum(dim=-1, keepdim=True)\n        return pattern\n\n    def analyze_head_patterns(self):\n        \"\"\"\u30d8\u30c3\u30c9\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n        print(\"=== \u5404\u30d8\u30c3\u30c9\u306e\u7279\u5316\u30d1\u30bf\u30fc\u30f3 ===\\n\")\n\n        seq_len = 10\n        words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"looked\", \"around\", \".\"]\n\n        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n        axes = axes.ravel()\n\n        patterns = [\n            (\"\u4f4d\u7f6e\u7684\u6ce8\u610f\", self._create_positional_pattern(seq_len)),\n            (\"\u6587\u6cd5\u7684\u6ce8\u610f\", self._create_syntactic_pattern(seq_len)),\n            (\"\u610f\u5473\u7684\u6ce8\u610f\", self._create_semantic_pattern(seq_len)),\n            (\"\u5e83\u7bc4\u306a\u6ce8\u610f\", self._create_broad_pattern(seq_len))\n        ]\n\n        for idx, (name, pattern) in enumerate(patterns):\n            ax = axes[idx]\n\n            im = ax.imshow(pattern.numpy(), cmap='Blues', aspect='auto')\n            ax.set_title(name, fontsize=14)\n            ax.set_xlabel('Attended to')\n            ax.set_ylabel('Attending from')\n\n            # \u5358\u8a9e\u3092\u8868\u793a\n            ax.set_xticks(range(seq_len))\n            ax.set_yticks(range(seq_len))\n            ax.set_xticklabels(words, rotation=45, ha='right')\n            ax.set_yticklabels(words)\n\n            plt.colorbar(im, ax=ax)\n\n        plt.suptitle('Multi-Head Attention: \u5404\u30d8\u30c3\u30c9\u306e\u7279\u5316\u3057\u305f\u30d1\u30bf\u30fc\u30f3', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n        # \u30d1\u30bf\u30fc\u30f3\u306e\u8aac\u660e\n        self._explain_patterns()\n\n    def _explain_patterns(self):\n        \"\"\"\u5404\u30d1\u30bf\u30fc\u30f3\u306e\u8aac\u660e\"\"\"\n        print(\"\\n\u5404\u30d1\u30bf\u30fc\u30f3\u306e\u7279\u5fb4:\")\n        print(\"\\n1. \u4f4d\u7f6e\u7684\u6ce8\u610f:\")\n        print(\"   - \u8fd1\u96a3\u306e\u5358\u8a9e\u306b\u6ce8\u76ee\")\n        print(\"   - \u5c40\u6240\u7684\u306a\u6587\u8108\u3092\u6349\u3048\u308b\")\n        print(\"   - n-gram\u306e\u3088\u3046\u306a\u7279\u5fb4\")\n\n        print(\"\\n2. \u6587\u6cd5\u7684\u6ce8\u610f:\")\n        print(\"   - \u6587\u6cd5\u7684\u306a\u4f9d\u5b58\u95a2\u4fc2\u306b\u6ce8\u76ee\")\n        print(\"   - \u4e3b\u8a9e-\u52d5\u8a5e\u3001\u52d5\u8a5e-\u76ee\u7684\u8a9e\u306a\u3069\")\n        print(\"   - \u69cb\u6587\u89e3\u6790\u7684\u306a\u60c5\u5831\")\n\n        print(\"\\n3. \u610f\u5473\u7684\u6ce8\u610f:\")\n        print(\"   - \u610f\u5473\u7684\u306b\u95a2\u9023\u3059\u308b\u5358\u8a9e\u306b\u6ce8\u76ee\")\n        print(\"   - \u96e2\u308c\u305f\u4f4d\u7f6e\u3067\u3082\u95a2\u9023\u304c\u3042\u308c\u3070\u6ce8\u76ee\")\n        print(\"   - \u6587\u8108\u7406\u89e3\u306b\u91cd\u8981\")\n\n        print(\"\\n4. \u5e83\u7bc4\u306a\u6ce8\u610f:\")\n        print(\"   - \u6587\u5168\u4f53\u3092\u5747\u7b49\u306b\u898b\u308b\")\n        print(\"   - \u30b0\u30ed\u30fc\u30d0\u30eb\u306a\u6587\u8108\")\n        print(\"   - \u6587\u306e\u8981\u7d04\u7684\u306a\u60c5\u5831\")\n</code></pre>"},{"location":"part3/multi-head-attention/#_7","title":"\u5b9f\u969b\u306e\u5b66\u7fd2\u30d1\u30bf\u30fc\u30f3","text":"<pre><code>class LearnedPatterns:\n    \"\"\"\u5b9f\u969b\u306b\u5b66\u7fd2\u3055\u308c\u308b\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n\n    def demonstrate_layer_wise_patterns(self):\n        \"\"\"\u5c64\u3054\u3068\u306e\u30d1\u30bf\u30fc\u30f3\u306e\u5909\u5316\"\"\"\n        print(\"=== \u5c64\u306b\u3088\u308b\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u5909\u5316 ===\\n\")\n\n        layers = [\"Layer 1\", \"Layer 6\", \"Layer 12\"]\n        layer_characteristics = [\n            \"\u8868\u5c64\u7684\u30fb\u4f4d\u7f6e\u7684\u30d1\u30bf\u30fc\u30f3\",\n            \"\u6587\u6cd5\u7684\u30fb\u69cb\u9020\u7684\u30d1\u30bf\u30fc\u30f3\",\n            \"\u610f\u5473\u7684\u30fb\u62bd\u8c61\u7684\u30d1\u30bf\u30fc\u30f3\"\n        ]\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, (layer, characteristic) in enumerate(zip(layers, layer_characteristics)):\n            ax = axes[idx]\n\n            # \u5c64\u304c\u6df1\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u30d1\u30bf\u30fc\u30f3\u304c\u5909\u5316\n            if idx == 0:  # \u6d45\u3044\u5c64\n                pattern = self._create_shallow_pattern()\n            elif idx == 1:  # \u4e2d\u9593\u5c64\n                pattern = self._create_middle_pattern()\n            else:  # \u6df1\u3044\u5c64\n                pattern = self._create_deep_pattern()\n\n            im = ax.imshow(pattern, cmap='Purples', aspect='auto')\n            ax.set_title(f'{layer}\\n{characteristic}', fontsize=12)\n            plt.colorbar(im, ax=ax)\n\n        plt.suptitle('Transformer\u306e\u5c64\u306b\u3088\u308b\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u9032\u5316', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n    def _create_shallow_pattern(self):\n        \"\"\"\u6d45\u3044\u5c64\u306e\u30d1\u30bf\u30fc\u30f3\uff08\u4f4d\u7f6e\u7684\uff09\"\"\"\n        size = 8\n        pattern = torch.zeros(size, size)\n\n        # \u5bfe\u89d2\u7dda\u4ed8\u8fd1\u306b\u5f37\u3044\u6ce8\u610f\n        for i in range(size):\n            for j in range(size):\n                distance = abs(i - j)\n                pattern[i, j] = np.exp(-distance * 0.5)\n\n        return pattern / pattern.sum(dim=-1, keepdim=True)\n\n    def _create_middle_pattern(self):\n        \"\"\"\u4e2d\u9593\u5c64\u306e\u30d1\u30bf\u30fc\u30f3\uff08\u69cb\u9020\u7684\uff09\"\"\"\n        size = 8\n        pattern = torch.zeros(size, size)\n\n        # \u30d6\u30ed\u30c3\u30af\u7684\u306a\u30d1\u30bf\u30fc\u30f3\n        pattern[0:3, 0:3] = 0.3  # \u53e5\u5358\u4f4d\n        pattern[3:6, 3:6] = 0.3\n        pattern[6:8, 6:8] = 0.3\n\n        # \u9577\u8ddd\u96e2\u306e\u63a5\u7d9a\n        pattern[1, 5] = 0.5  # \u6587\u6cd5\u7684\u4f9d\u5b58\n        pattern[5, 1] = 0.5\n\n        return pattern / pattern.sum(dim=-1, keepdim=True).clamp(min=1e-9)\n\n    def _create_deep_pattern(self):\n        \"\"\"\u6df1\u3044\u5c64\u306e\u30d1\u30bf\u30fc\u30f3\uff08\u610f\u5473\u7684\uff09\"\"\"\n        size = 8\n\n        # \u3088\u308a\u8907\u96d1\u3067\u62bd\u8c61\u7684\u306a\u30d1\u30bf\u30fc\u30f3\n        pattern = torch.rand(size, size) * 0.5 + 0.1\n\n        # \u7279\u5b9a\u306e\u610f\u5473\u7684\u95a2\u9023\u3092\u5f37\u8abf\n        semantic_pairs = [(0, 4), (1, 6), (2, 7)]\n        for i, j in semantic_pairs:\n            pattern[i, j] = 0.8\n            pattern[j, i] = 0.8\n\n        return pattern / pattern.sum(dim=-1, keepdim=True)\n</code></pre>"},{"location":"part3/multi-head-attention/#95-multi-head-attention","title":"9.5 Multi-Head Attention \u306e\u6700\u9069\u5316","text":""},{"location":"part3/multi-head-attention/#_8","title":"\u52b9\u7387\u7684\u306a\u5b9f\u88c5","text":"<pre><code>class EfficientMultiHeadAttention:\n    \"\"\"\u52b9\u7387\u7684\u306aMulti-Head Attention\u306e\u5b9f\u88c5\"\"\"\n\n    def explain_optimizations(self):\n        \"\"\"\u6700\u9069\u5316\u624b\u6cd5\u306e\u8aac\u660e\"\"\"\n        print(\"=== Multi-Head Attention \u306e\u6700\u9069\u5316 ===\\n\")\n\n        optimizations = {\n            \"Fused Operations\": {\n                \"\u8aac\u660e\": \"\u8907\u6570\u306e\u6f14\u7b97\u30921\u3064\u306e\u30ab\u30fc\u30cd\u30eb\u3067\u5b9f\u884c\",\n                \"\u52b9\u679c\": \"\u30e1\u30e2\u30ea\u30a2\u30af\u30bb\u30b9\u306e\u524a\u6e1b\",\n                \"\u4f8b\": \"QKV\u6295\u5f71\u30921\u3064\u306e\u884c\u5217\u6f14\u7b97\u306b\u7d71\u5408\"\n            },\n            \"Flash Attention\": {\n                \"\u8aac\u660e\": \"\u30bf\u30a4\u30eb\u5316\u3068\u30e1\u30e2\u30ea\u52b9\u7387\u7684\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\",\n                \"\u52b9\u679c\": \"O(n\u00b2)\u2192O(n)\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\",\n                \"\u4f8b\": \"\u9577\u3044\u7cfb\u5217\u3067\u3082\u52b9\u7387\u7684\"\n            },\n            \"Sparse Attention\": {\n                \"\u8aac\u660e\": \"\u6ce8\u610f\u884c\u5217\u3092\u30b9\u30d1\u30fc\u30b9\u306b\",\n                \"\u52b9\u679c\": \"\u8a08\u7b97\u91cf\u3092O(n\u00b2)\u2192O(n log n)\u306b\u524a\u6e1b\",\n                \"\u4f8b\": \"\u5c40\u6240\u7684+\u30b9\u30c8\u30e9\u30a4\u30c9\u30d1\u30bf\u30fc\u30f3\"\n            },\n            \"Low-rank Approximation\": {\n                \"\u8aac\u660e\": \"\u6ce8\u610f\u884c\u5217\u3092\u4f4e\u30e9\u30f3\u30af\u8fd1\u4f3c\",\n                \"\u52b9\u679c\": \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u3068\u8a08\u7b97\u91cf\u306e\u524a\u6e1b\",\n                \"\u4f8b\": \"Linformer, Performer\"\n            }\n        }\n\n        for name, details in optimizations.items():\n            print(f\"{name}:\")\n            for key, value in details.items():\n                print(f\"  {key}: {value}\")\n            print()\n\n    def implement_grouped_query_attention(self):\n        \"\"\"Grouped Query Attention (GQA) \u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== Grouped Query Attention ===\")\n        print(\"Key/Value\u306e\u30d8\u30c3\u30c9\u6570\u3092\u524a\u6e1b\u3057\u3066\u52b9\u7387\u5316\\n\")\n\n        class GroupedQueryAttention(nn.Module):\n            def __init__(self, d_model: int, n_heads: int, n_kv_heads: int):\n                super().__init__()\n                assert n_heads % n_kv_heads == 0\n\n                self.d_model = d_model\n                self.n_heads = n_heads\n                self.n_kv_heads = n_kv_heads\n                self.n_groups = n_heads // n_kv_heads\n                self.d_k = d_model // n_heads\n\n                # Q \u306f\u5168\u30d8\u30c3\u30c9\u5206\u3001K/V \u306f\u524a\u6e1b\u3055\u308c\u305f\u30d8\u30c3\u30c9\u6570\n                self.W_q = nn.Linear(d_model, d_model)\n                self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k)\n                self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k)\n                self.W_o = nn.Linear(d_model, d_model)\n\n                print(f\"\u901a\u5e38\u306eMHA: {3 * d_model * d_model} \u30d1\u30e9\u30e1\u30fc\u30bf\")\n                print(f\"GQA: {d_model * d_model + 2 * d_model * n_kv_heads * self.d_k} \u30d1\u30e9\u30e1\u30fc\u30bf\")\n                print(f\"\u524a\u6e1b\u7387: {1 - (1 + 2 * n_kv_heads / n_heads) / 3:.1%}\")\n\n            def forward(self, x):\n                batch_size, seq_len = x.shape[:2]\n\n                # Query \u306f\u5168\u30d8\u30c3\u30c9\n                Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)\n                Q = Q.transpose(1, 2)\n\n                # Key/Value \u306f\u524a\u6e1b\u3055\u308c\u305f\u30d8\u30c3\u30c9\u6570\n                K = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n                K = K.transpose(1, 2)\n                V = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n                V = V.transpose(1, 2)\n\n                # K/V \u3092\u7e70\u308a\u8fd4\u3057\u3066Q\u306e\u30d8\u30c3\u30c9\u6570\u306b\u5408\u308f\u305b\u308b\n                K = K.repeat_interleave(self.n_groups, dim=1)\n                V = V.repeat_interleave(self.n_groups, dim=1)\n\n                # \u901a\u5e38\u306e\u6ce8\u610f\u8a08\u7b97\n                scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n                weights = F.softmax(scores, dim=-1)\n                context = torch.matmul(weights, V)\n\n                # \u51fa\u529b\n                context = context.transpose(1, 2).contiguous()\n                context = context.view(batch_size, seq_len, self.d_model)\n                output = self.W_o(context)\n\n                return output\n\n        # \u4f8b\n        gqa = GroupedQueryAttention(d_model=512, n_heads=8, n_kv_heads=2)\n        return gqa\n</code></pre>"},{"location":"part3/multi-head-attention/#multi-head-attention_1","title":"\u307e\u3068\u3081\uff1aMulti-Head Attention \u306e\u5a01\u529b","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060Multi-Head Attention\u306e\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u8907\u6570\u8996\u70b9\u306e\u7d71\u5408\uff1a</li> <li>\u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u8868\u73fe\u3092\u5b66\u7fd2</li> <li>\u4f4d\u7f6e\u7684\u3001\u6587\u6cd5\u7684\u3001\u610f\u5473\u7684\u306a\u60c5\u5831\u3092\u4e26\u5217\u306b\u51e6\u7406</li> <li> <p>\u5358\u4e00\u30d8\u30c3\u30c9\u3067\u306f\u4e0d\u53ef\u80fd\u306a\u8c4a\u304b\u306a\u8868\u73fe</p> </li> <li> <p>\u52b9\u7387\u7684\u306a\u4e26\u5217\u8a08\u7b97\uff1a</p> </li> <li>\u3059\u3079\u3066\u306e\u30d8\u30c3\u30c9\u3092\u540c\u6642\u306b\u8a08\u7b97</li> <li>\u884c\u5217\u6f14\u7b97\u306b\u3088\u308b\u9ad8\u901f\u5316</li> <li> <p>GPU\u306b\u6700\u9069\u5316\u3055\u308c\u305f\u5b9f\u88c5</p> </li> <li> <p>\u67d4\u8edf\u306a\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\uff1a</p> </li> <li>\u5c64\u304c\u6df1\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u62bd\u8c61\u5ea6\u304c\u4e0a\u6607</li> <li>\u30bf\u30b9\u30af\u306b\u5fdc\u3058\u305f\u7279\u5316</li> <li> <p>\u30de\u30b9\u30af\u306b\u3088\u308b\u5236\u5fa1</p> </li> <li> <p>\u6700\u65b0\u306e\u6700\u9069\u5316\uff1a</p> </li> <li>Flash Attention \u306b\u3088\u308b\u7701\u30e1\u30e2\u30ea\u5316</li> <li>Grouped Query Attention \u306b\u3088\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u524a\u6e1b</li> <li>\u30b9\u30d1\u30fc\u30b9\u5316\u306b\u3088\u308b\u9ad8\u901f\u5316</li> </ol> <p>Multi-Head Attention\u306f\u3001Transformer\u306e\u8868\u73fe\u529b\u306e\u6e90\u6cc9\u3067\u3059\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u306e\u8c4a\u304b\u306a\u8868\u73fe\u3092\u3055\u3089\u306b\u5909\u63db\u3059\u308bFeed-Forward Network\u306b\u3064\u3044\u3066\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part3/multi-head-attention/#_9","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1a8\u30d8\u30c3\u30c9\u306eMulti-Head Attention\u3092\u5b9f\u88c5\u3057\u3001\u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1a\u4e8b\u524d\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u304b\u3089\u6ce8\u610f\u306e\u91cd\u307f\u3092\u62bd\u51fa\u3057\u3001\u5404\u5c64\u30fb\u5404\u30d8\u30c3\u30c9\u304c\u3069\u306e\u3088\u3046\u306a\u60c5\u5831\u306b\u6ce8\u76ee\u3057\u3066\u3044\u308b\u304b\u5206\u6790\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u6700\u9069\u5316\u8ab2\u984c\uff1aSparse Attention\u30d1\u30bf\u30fc\u30f3\u3092\u5b9f\u88c5\u3057\u3001\u901a\u5e38\u306eAttention\u3068\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1a\u306a\u305cd_model\u3092n_heads\u3067\u5272\u308a\u5207\u308c\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u304b\u3001\u6570\u5b66\u7684\u306b\u8aac\u660e\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300cFeed Forward Network\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part3/residual-normalization/","title":"\u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316","text":""},{"location":"part3/residual-normalization/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u6df1\u3055\u3078\u306e\u6311\u6226","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u8003\u3048\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u5404\u6700\u9069\u5316\u30d1\u30b9\u306f\u524d\u306e\u30d1\u30b9\u306e\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308a\u3001\u3055\u3089\u306a\u308b\u6539\u5584\u3092\u52a0\u3048\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u30d1\u30b9\u304c\u6df1\u304f\u306a\u308a\u3059\u304e\u308b\u3068\u3001\u521d\u671f\u306e\u60c5\u5831\u304c\u5931\u308f\u308c\u305f\u308a\u3001\u30a8\u30e9\u30fc\u304c\u7d2f\u7a4d\u3057\u305f\u308a\u3059\u308b\u554f\u984c\u304c\u751f\u3058\u307e\u3059\u3002\u305d\u3053\u3067\u3001\u5404\u30d1\u30b9\u304c\u300c\u30aa\u30d7\u30b7\u30e7\u30ca\u30eb\u300d\u306a\u6539\u5584\u3092\u884c\u3044\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5143\u306e\u72b6\u614b\u3092\u4fdd\u6301\u3067\u304d\u308b\u4ed5\u7d44\u307f\u304c\u91cd\u8981\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6df1\u5c64\u5b66\u7fd2\u3067\u3082\u540c\u3058\u8ab2\u984c\u304c\u3042\u308a\u307e\u3059\u3002\u5c64\u3092\u6df1\u304f\u3059\u308b\u3053\u3068\u3067\u8868\u73fe\u529b\u306f\u5411\u4e0a\u3057\u307e\u3059\u304c\u3001\u52fe\u914d\u6d88\u5931\u3084\u5b66\u7fd2\u306e\u4e0d\u5b89\u5b9a\u6027\u3068\u3044\u3046\u554f\u984c\u304c\u751f\u3058\u307e\u3059\u3002\u6b8b\u5dee\u63a5\u7d9a\uff08Residual Connection\uff09\u3068\u5c64\u6b63\u898f\u5316\uff08Layer Normalization\uff09\u306f\u3001\u3053\u308c\u3089\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3001100\u5c64\u3092\u8d85\u3048\u308b\u6df1\u3044Transformer\u3092\u53ef\u80fd\u306b\u3059\u308b\u91cd\u8981\u306a\u6280\u8853\u3067\u3059\u3002</p> <p>\u3053\u306e\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u306e\u6280\u8853\u304c\u3069\u306e\u3088\u3046\u306b\u6a5f\u80fd\u3057\u3001\u306a\u305cTransformer\u306e\u6210\u529f\u306b\u4e0d\u53ef\u6b20\u306a\u306e\u304b\u3092\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/residual-normalization/#111","title":"11.1 \u6df1\u5c64\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8ab2\u984c","text":""},{"location":"part3/residual-normalization/#_3","title":"\u52fe\u914d\u6d88\u5931\u30fb\u7206\u767a\u554f\u984c","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict, Optional, Callable\nimport math\nfrom matplotlib.patches import Rectangle, FancyBboxPatch, Circle, Arrow\nfrom matplotlib.patches import ConnectionPatch\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nclass DeepNetworkProblems:\n    \"\"\"\u6df1\u5c64\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u554f\u984c\u3092\u5b9f\u8a3c\"\"\"\n\n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def demonstrate_gradient_vanishing(self):\n        \"\"\"\u52fe\u914d\u6d88\u5931\u554f\u984c\u306e\u5b9f\u8a3c\"\"\"\n        print(\"=== \u52fe\u914d\u6d88\u5931\u554f\u984c ===\\n\")\n\n        # \u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f5c\u6210\n        class DeepNetwork(nn.Module):\n            def __init__(self, depth: int, use_residual: bool = False):\n                super().__init__()\n                self.depth = depth\n                self.use_residual = use_residual\n\n                # \u5c64\u3092\u4f5c\u6210\n                self.layers = nn.ModuleList([\n                    nn.Linear(64, 64) for _ in range(depth)\n                ])\n\n                # \u6d3b\u6027\u5316\u95a2\u6570\n                self.activation = nn.Tanh()  # Tanh\u306f\u52fe\u914d\u6d88\u5931\u3057\u3084\u3059\u3044\n\n                # \u521d\u671f\u5316\n                for layer in self.layers:\n                    nn.init.xavier_uniform_(layer.weight)\n                    nn.init.zeros_(layer.bias)\n\n            def forward(self, x):\n                activations = [x]\n\n                for i, layer in enumerate(self.layers):\n                    if self.use_residual and i &gt; 0:\n                        # \u6b8b\u5dee\u63a5\u7d9a\n                        out = layer(activations[-1])\n                        out = self.activation(out)\n                        out = out + activations[-1]  # \u6b8b\u5dee\u3092\u52a0\u7b97\n                    else:\n                        # \u901a\u5e38\u306e\u9806\u4f1d\u64ad\n                        out = layer(activations[-1])\n                        out = self.activation(out)\n\n                    activations.append(out)\n\n                return activations[-1], activations\n\n        # \u5b9f\u9a13\n        depths = [5, 10, 20, 50]\n        results = {'without_residual': {}, 'with_residual': {}}\n\n        for depth in depths:\n            for use_residual in [False, True]:\n                # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u4f5c\u6210\n                net = DeepNetwork(depth, use_residual)\n\n                # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n                x = torch.randn(32, 64, requires_grad=True)\n\n                # \u9806\u4f1d\u64ad\n                output, activations = net(x)\n                loss = output.mean()\n\n                # \u9006\u4f1d\u64ad\n                loss.backward()\n\n                # \u5404\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0\u3092\u8a18\u9332\n                grad_norms = []\n                for layer in net.layers:\n                    grad_norm = layer.weight.grad.norm().item()\n                    grad_norms.append(grad_norm)\n\n                key = 'with_residual' if use_residual else 'without_residual'\n                results[key][depth] = grad_norms\n\n        # \u7d50\u679c\u306e\u53ef\u8996\u5316\n        self._visualize_gradient_flow(results)\n\n    def _visualize_gradient_flow(self, results):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n        axes = axes.ravel()\n\n        depths = [5, 10, 20, 50]\n\n        for idx, depth in enumerate(depths):\n            ax = axes[idx]\n\n            # \u6b8b\u5dee\u306a\u3057\n            grad_norms_no_res = results['without_residual'][depth]\n            ax.semilogy(range(len(grad_norms_no_res)), grad_norms_no_res, \n                       'r-o', label='Without Residual', linewidth=2)\n\n            # \u6b8b\u5dee\u3042\u308a\n            if depth in results['with_residual']:\n                grad_norms_res = results['with_residual'][depth]\n                ax.semilogy(range(len(grad_norms_res)), grad_norms_res, \n                           'b-s', label='With Residual', linewidth=2)\n\n            ax.set_xlabel('Layer Index')\n            ax.set_ylabel('Gradient Norm (log scale)')\n            ax.set_title(f'Depth = {depth}')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n\n            # \u52fe\u914d\u6d88\u5931\u306e\u95be\u5024\u3092\u8868\u793a\n            ax.axhline(y=1e-5, color='gray', linestyle='--', alpha=0.5)\n            ax.text(depth * 0.7, 1e-5, 'Vanishing threshold', \n                   fontsize=8, color='gray')\n\n        plt.suptitle('\u52fe\u914d\u6d88\u5931\u554f\u984c\uff1a\u6b8b\u5dee\u63a5\u7d9a\u306e\u52b9\u679c', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n    def demonstrate_feature_degradation(self):\n        \"\"\"\u7279\u5fb4\u306e\u52a3\u5316\u3092\u5b9f\u8a3c\"\"\"\n        print(\"\\n=== \u7279\u5fb4\u306e\u52a3\u5316\u554f\u984c ===\")\n\n        # \u5165\u529b\u4fe1\u53f7\n        x = torch.randn(1, 100)\n\n        # \u5c64\u3092\u901a\u904e\u3059\u308b\u3054\u3068\u306e\u5909\u5316\u3092\u8ffd\u8de1\n        depths = [0, 5, 10, 20, 50]\n        features_no_res = {}\n        features_res = {}\n\n        # \u6b8b\u5dee\u306a\u3057\n        current = x.clone()\n        for depth in depths:\n            if depth &gt; 0:\n                for _ in range(depth - (depths[depths.index(depth)-1] if depths.index(depth) &gt; 0 else 0)):\n                    layer = nn.Linear(100, 100)\n                    nn.init.xavier_uniform_(layer.weight, gain=0.9)  # \u3084\u3084\u5c0f\u3055\u3081\u306e\u521d\u671f\u5316\n                    current = torch.tanh(layer(current))\n            features_no_res[depth] = current.clone()\n\n        # \u6b8b\u5dee\u3042\u308a\n        current = x.clone()\n        for depth in depths:\n            if depth &gt; 0:\n                for _ in range(depth - (depths[depths.index(depth)-1] if depths.index(depth) &gt; 0 else 0)):\n                    layer = nn.Linear(100, 100)\n                    nn.init.xavier_uniform_(layer.weight, gain=0.1)  # \u5c0f\u3055\u3044\u521d\u671f\u5316\n                    residual = current\n                    current = torch.tanh(layer(current)) + residual\n            features_res[depth] = current.clone()\n\n        # \u53ef\u8996\u5316\n        self._visualize_feature_degradation(x, features_no_res, features_res)\n\n    def _visualize_feature_degradation(self, original, features_no_res, features_res):\n        \"\"\"\u7279\u5fb4\u306e\u52a3\u5316\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 5, figsize=(16, 6))\n\n        depths = [0, 5, 10, 20, 50]\n\n        for idx, depth in enumerate(depths):\n            # \u6b8b\u5dee\u306a\u3057\n            ax = axes[0, idx]\n            feat = features_no_res[depth].detach().numpy().flatten()\n            ax.hist(feat, bins=30, alpha=0.7, color='red', density=True)\n            ax.set_title(f'Depth {depth}')\n            ax.set_xlim(-3, 3)\n            ax.set_ylim(0, 2)\n\n            if idx == 0:\n                ax.set_ylabel('Without Residual\\nDensity')\n\n            # \u7d71\u8a08\u60c5\u5831\n            ax.text(0.05, 0.95, f'\u03bc={feat.mean():.2f}\\n\u03c3={feat.std():.2f}',\n                   transform=ax.transAxes, verticalalignment='top',\n                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n            # \u6b8b\u5dee\u3042\u308a\n            ax = axes[1, idx]\n            feat = features_res[depth].detach().numpy().flatten()\n            ax.hist(feat, bins=30, alpha=0.7, color='blue', density=True)\n            ax.set_xlim(-3, 3)\n            ax.set_ylim(0, 2)\n\n            if idx == 0:\n                ax.set_ylabel('With Residual\\nDensity')\n\n            # \u7d71\u8a08\u60c5\u5831\n            ax.text(0.05, 0.95, f'\u03bc={feat.mean():.2f}\\n\u03c3={feat.std():.2f}',\n                   transform=ax.transAxes, verticalalignment='top',\n                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n        plt.suptitle('\u5c64\u3092\u901a\u904e\u3059\u308b\u3054\u3068\u306e\u7279\u5fb4\u5206\u5e03\u306e\u5909\u5316', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/residual-normalization/#_4","title":"\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8","text":"<pre><code>class InternalCovariateShift:\n    \"\"\"\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306e\u7406\u89e3\"\"\"\n\n    def explain_covariate_shift(self):\n        \"\"\"\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8 ===\\n\")\n\n        print(\"\u554f\u984c\uff1a\")\n        print(\"- \u5404\u5c64\u306e\u5165\u529b\u5206\u5e03\u304c\u5b66\u7fd2\u4e2d\u306b\u5909\u5316\")\n        print(\"- \u4e0b\u4f4d\u5c64\u306e\u66f4\u65b0\u304c\u4e0a\u4f4d\u5c64\u306e\u5165\u529b\u3092\u5909\u3048\u308b\")\n        print(\"- \u5404\u5c64\u304c\u52d5\u304f\u6a19\u7684\u3092\u8ffd\u3044\u304b\u3051\u308b\u72b6\u614b\\n\")\n\n        print(\"\u5f71\u97ff\uff1a\")\n        print(\"- \u5b66\u7fd2\u901f\u5ea6\u306e\u4f4e\u4e0b\")\n        print(\"- \u5b66\u7fd2\u7387\u3092\u5c0f\u3055\u304f\u3059\u308b\u5fc5\u8981\")\n        print(\"- \u53ce\u675f\u304c\u56f0\u96e3\")\n\n        # \u5b9f\u9a13\u3067\u5b9f\u8a3c\n        self._demonstrate_covariate_shift()\n\n    def _demonstrate_covariate_shift(self):\n        \"\"\"\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306e\u5b9f\u8a3c\"\"\"\n        # \u30b7\u30f3\u30d7\u30eb\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n        class SimpleNetwork(nn.Module):\n            def __init__(self, use_norm: bool = False):\n                super().__init__()\n                self.use_norm = use_norm\n\n                self.layers = nn.ModuleList([\n                    nn.Linear(50, 50) for _ in range(5)\n                ])\n\n                if use_norm:\n                    self.norms = nn.ModuleList([\n                        nn.LayerNorm(50) for _ in range(5)\n                    ])\n\n                self.activation = nn.ReLU()\n\n            def forward(self, x):\n                stats = []\n\n                for i, layer in enumerate(self.layers):\n                    x = layer(x)\n\n                    # \u6b63\u898f\u5316\u524d\u306e\u7d71\u8a08\n                    mean_before = x.mean(dim=-1, keepdim=True)\n                    std_before = x.std(dim=-1, keepdim=True)\n\n                    if self.use_norm:\n                        x = self.norms[i](x)\n\n                    x = self.activation(x)\n\n                    # \u7d71\u8a08\u3092\u8a18\u9332\n                    stats.append({\n                        'mean': mean_before.mean().item(),\n                        'std': std_before.mean().item()\n                    })\n\n                return x, stats\n\n        # \u5b66\u7fd2\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\n        net_no_norm = SimpleNetwork(use_norm=False)\n        net_with_norm = SimpleNetwork(use_norm=True)\n\n        # \u8907\u6570\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u7d71\u8a08\u3092\u8ffd\u8de1\n        steps = 10\n        stats_history = {'no_norm': [], 'with_norm': []}\n\n        for step in range(steps):\n            # \u30e9\u30f3\u30c0\u30e0\u306a\u5165\u529b\n            x = torch.randn(32, 50)\n\n            # \u5404\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u901a\u3059\n            _, stats_no_norm = net_no_norm(x)\n            _, stats_with_norm = net_with_norm(x)\n\n            stats_history['no_norm'].append(stats_no_norm)\n            stats_history['with_norm'].append(stats_with_norm)\n\n            # \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\uff08\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\uff09\n            with torch.no_grad():\n                for param in net_no_norm.parameters():\n                    param.add_(torch.randn_like(param) * 0.01)\n                for param in net_with_norm.parameters():\n                    if param.dim() &gt; 1:  # \u91cd\u307f\u884c\u5217\u306e\u307f\n                        param.add_(torch.randn_like(param) * 0.01)\n\n        # \u53ef\u8996\u5316\n        self._visualize_covariate_shift(stats_history)\n\n    def _visualize_covariate_shift(self, stats_history):\n        \"\"\"\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # \u5404\u5c64\u306e\u5e73\u5747\u306e\u5909\u5316\n        for layer_idx in range(5):\n            # \u6b63\u898f\u5316\u306a\u3057\n            means_no_norm = [step[layer_idx]['mean'] \n                            for step in stats_history['no_norm']]\n            axes[0, 0].plot(means_no_norm, label=f'Layer {layer_idx+1}',\n                          linewidth=2)\n\n            # \u6b63\u898f\u5316\u3042\u308a\n            means_with_norm = [step[layer_idx]['mean'] \n                              for step in stats_history['with_norm']]\n            axes[0, 1].plot(means_with_norm, label=f'Layer {layer_idx+1}',\n                          linewidth=2)\n\n        axes[0, 0].set_title('Without Normalization')\n        axes[0, 0].set_xlabel('Step')\n        axes[0, 0].set_ylabel('Mean Activation')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n\n        axes[0, 1].set_title('With Layer Normalization')\n        axes[0, 1].set_xlabel('Step')\n        axes[0, 1].set_ylabel('Mean Activation')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n\n        # \u5404\u5c64\u306e\u6a19\u6e96\u504f\u5dee\u306e\u5909\u5316\n        for layer_idx in range(5):\n            # \u6b63\u898f\u5316\u306a\u3057\n            stds_no_norm = [step[layer_idx]['std'] \n                           for step in stats_history['no_norm']]\n            axes[1, 0].plot(stds_no_norm, label=f'Layer {layer_idx+1}',\n                          linewidth=2)\n\n            # \u6b63\u898f\u5316\u3042\u308a\n            stds_with_norm = [step[layer_idx]['std'] \n                             for step in stats_history['with_norm']]\n            axes[1, 1].plot(stds_with_norm, label=f'Layer {layer_idx+1}',\n                          linewidth=2)\n\n        axes[1, 0].set_xlabel('Step')\n        axes[1, 0].set_ylabel('Std Activation')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n\n        axes[1, 1].set_xlabel('Step')\n        axes[1, 1].set_ylabel('Std Activation')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n\n        plt.suptitle('\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\uff1a\u5c64\u6b63\u898f\u5316\u306e\u52b9\u679c', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/residual-normalization/#112-residual-connection","title":"11.2 \u6b8b\u5dee\u63a5\u7d9a\uff08Residual Connection\uff09","text":""},{"location":"part3/residual-normalization/#_5","title":"\u6b8b\u5dee\u63a5\u7d9a\u306e\u539f\u7406","text":"<pre><code>class ResidualConnectionPrinciple:\n    \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u539f\u7406\u3068\u5b9f\u88c5\"\"\"\n\n    def explain_residual_connection(self):\n        \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u8aac\u660e\"\"\"\n        print(\"=== \u6b8b\u5dee\u63a5\u7d9a\u306e\u539f\u7406 ===\\n\")\n\n        print(\"\u57fa\u672c\u7684\u306a\u30a2\u30a4\u30c7\u30a2\uff1a\")\n        print(\"y = F(x) + x\")\n        print(\"- F(x): \u5c64\u306e\u5909\u63db\")\n        print(\"- x: \u5165\u529b\uff08\u6052\u7b49\u5199\u50cf\uff09\\n\")\n\n        print(\"\u5229\u70b9\uff1a\")\n        print(\"1. \u52fe\u914d\u306e\u9ad8\u901f\u9053\u8def\uff1a\u52fe\u914d\u304c\u76f4\u63a5\u4f1d\u64ad\")\n        print(\"2. \u6052\u7b49\u5199\u50cf\u306e\u5b66\u7fd2\uff1aF(x)=0\u3067\u6052\u7b49\u95a2\u6570\")\n        print(\"3. \u7279\u5fb4\u306e\u4fdd\u5b58\uff1a\u60c5\u5831\u304c\u5931\u308f\u308c\u306a\u3044\")\n\n        # \u56f3\u89e3\n        self._visualize_residual_connection()\n\n    def _visualize_residual_connection(self):\n        \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u56f3\u89e3\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n        # \u901a\u5e38\u306e\u63a5\u7d9a\n        ax1.set_title('\u901a\u5e38\u306e\u63a5\u7d9a', fontsize=14)\n\n        # \u30d6\u30ed\u30c3\u30af\n        blocks = [\n            {'pos': (0.2, 0.5), 'label': 'x'},\n            {'pos': (0.5, 0.5), 'label': 'F(x)'},\n            {'pos': (0.8, 0.5), 'label': 'y = F(x)'}\n        ]\n\n        for block in blocks:\n            if block['label'] == 'F(x)':\n                rect = FancyBboxPatch(\n                    (block['pos'][0] - 0.1, block['pos'][1] - 0.1),\n                    0.2, 0.2,\n                    boxstyle=\"round,pad=0.02\",\n                    facecolor='lightgreen',\n                    edgecolor='black',\n                    linewidth=2\n                )\n                ax1.add_patch(rect)\n            else:\n                circle = Circle(block['pos'], 0.05,\n                              color='lightblue', ec='black')\n                ax1.add_patch(circle)\n\n            ax1.text(block['pos'][0], block['pos'][1], block['label'],\n                    ha='center', va='center', fontsize=12, fontweight='bold')\n\n        # \u77e2\u5370\n        ax1.arrow(0.25, 0.5, 0.15, 0, head_width=0.03, head_length=0.02,\n                 fc='black', ec='black')\n        ax1.arrow(0.6, 0.5, 0.15, 0, head_width=0.03, head_length=0.02,\n                 fc='black', ec='black')\n\n        ax1.set_xlim(0, 1)\n        ax1.set_ylim(0.2, 0.8)\n        ax1.axis('off')\n\n        # \u6b8b\u5dee\u63a5\u7d9a\n        ax2.set_title('\u6b8b\u5dee\u63a5\u7d9a', fontsize=14)\n\n        # \u30d6\u30ed\u30c3\u30af\uff08\u540c\u3058\u4f4d\u7f6e\uff09\n        for block in blocks:\n            if block['label'] == 'F(x)':\n                rect = FancyBboxPatch(\n                    (block['pos'][0] - 0.1, block['pos'][1] - 0.1),\n                    0.2, 0.2,\n                    boxstyle=\"round,pad=0.02\",\n                    facecolor='lightgreen',\n                    edgecolor='black',\n                    linewidth=2\n                )\n                ax2.add_patch(rect)\n            else:\n                circle = Circle(block['pos'], 0.05,\n                              color='lightblue', ec='black')\n                ax2.add_patch(circle)\n\n            label = block['label']\n            if label == 'y = F(x)':\n                label = 'y = F(x) + x'\n            ax2.text(block['pos'][0], block['pos'][1], label,\n                    ha='center', va='center', fontsize=12, fontweight='bold')\n\n        # \u901a\u5e38\u306e\u77e2\u5370\n        ax2.arrow(0.25, 0.5, 0.15, 0, head_width=0.03, head_length=0.02,\n                 fc='black', ec='black')\n        ax2.arrow(0.6, 0.5, 0.15, 0, head_width=0.03, head_length=0.02,\n                 fc='black', ec='black')\n\n        # \u30b9\u30ad\u30c3\u30d7\u63a5\u7d9a\uff08\u66f2\u7dda\uff09\n        ax2.annotate('', xy=(0.75, 0.5), xytext=(0.25, 0.5),\n                    arrowprops=dict(arrowstyle='-&gt;',\n                                  connectionstyle=\"arc3,rad=-.5\",\n                                  color='red', linewidth=2))\n        ax2.text(0.5, 0.3, 'Skip Connection', ha='center', \n                color='red', fontsize=10)\n\n        # \u52a0\u7b97\u8a18\u53f7\n        ax2.text(0.75, 0.4, '+', fontsize=20, ha='center', va='center',\n                color='red', fontweight='bold')\n\n        ax2.set_xlim(0, 1)\n        ax2.set_ylim(0.2, 0.8)\n        ax2.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    def implement_residual_block(self):\n        \"\"\"\u6b8b\u5dee\u30d6\u30ed\u30c3\u30af\u306e\u5b9f\u88c5\"\"\"\n        print(\"\\n=== \u6b8b\u5dee\u30d6\u30ed\u30c3\u30af\u306e\u5b9f\u88c5 ===\")\n\n        class ResidualBlock(nn.Module):\n            def __init__(self, d_model: int, dropout: float = 0.1):\n                super().__init__()\n                self.d_model = d_model\n\n                # \u30b5\u30d6\u30ec\u30a4\u30e4\u30fc\uff08\u4f8b\uff1aFFN\uff09\n                self.sublayer = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(d_model * 4, d_model),\n                    nn.Dropout(dropout)\n                )\n\n                # \u521d\u671f\u5316\uff1a\u6700\u521d\u306f\u6052\u7b49\u5199\u50cf\u306b\u8fd1\u304f\n                for m in self.sublayer:\n                    if isinstance(m, nn.Linear):\n                        nn.init.xavier_uniform_(m.weight, gain=0.1)\n                        nn.init.zeros_(m.bias)\n\n            def forward(self, x):\n                # \u6b8b\u5dee\u63a5\u7d9a\n                residual = x\n                output = self.sublayer(x)\n                output = output + residual\n\n                return output\n\n        # \u30c6\u30b9\u30c8\n        block = ResidualBlock(d_model=256)\n        x = torch.randn(2, 10, 256)\n\n        with torch.no_grad():\n            y = block(x)\n\n            # \u521d\u671f\u72b6\u614b\u3067\u306f\u5165\u529b\u306b\u8fd1\u3044\n            diff = (y - x).abs().mean()\n            print(f\"\u5165\u529b\u3068\u51fa\u529b\u306e\u5e73\u5747\u7d76\u5bfe\u5dee: {diff:.6f}\")\n            print(\"\u2192 \u521d\u671f\u5316\u306b\u3088\u308a\u6052\u7b49\u5199\u50cf\u306b\u8fd1\u3044\u72b6\u614b\u304b\u3089\u30b9\u30bf\u30fc\u30c8\")\n\n        # \u52fe\u914d\u306e\u6d41\u308c\u3092\u78ba\u8a8d\n        self._check_gradient_flow(block)\n\n    def _check_gradient_flow(self, block):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u78ba\u8a8d\"\"\"\n        x = torch.randn(2, 10, 256, requires_grad=True)\n        y = block(x)\n        loss = y.sum()\n        loss.backward()\n\n        print(f\"\\n\u5165\u529b\u306e\u52fe\u914d\u30ce\u30eb\u30e0: {x.grad.norm().item():.4f}\")\n        print(\"\u2192 \u6b8b\u5dee\u63a5\u7d9a\u306b\u3088\u308a\u52fe\u914d\u304c\u76f4\u63a5\u4f1d\u64ad\")\n</code></pre>"},{"location":"part3/residual-normalization/#_6","title":"\u6b8b\u5dee\u63a5\u7d9a\u306e\u6570\u5b66\u7684\u89e3\u6790","text":"<pre><code>class ResidualMathematicalAnalysis:\n    \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u6570\u5b66\u7684\u89e3\u6790\"\"\"\n\n    def analyze_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u6570\u5b66\u7684\u306b\u89e3\u6790\"\"\"\n        print(\"=== \u6b8b\u5dee\u63a5\u7d9a\u306e\u52fe\u914d\u89e3\u6790 ===\\n\")\n\n        print(\"\u901a\u5e38\u306e\u5c64:\")\n        print(\"\u2202L/\u2202x = \u2202L/\u2202y \u00b7 \u2202y/\u2202x = \u2202L/\u2202y \u00b7 \u2202F(x)/\u2202x\")\n        print(\"\u2192 \u6df1\u3044\u5c64\u3067\u306f \u2202F/\u2202x \u306e\u7a4d\u304c\u52fe\u914d\u6d88\u5931/\u7206\u767a\u3092\u5f15\u304d\u8d77\u3053\u3059\\n\")\n\n        print(\"\u6b8b\u5dee\u63a5\u7d9a:\")\n        print(\"y = F(x) + x\")\n        print(\"\u2202L/\u2202x = \u2202L/\u2202y \u00b7 \u2202y/\u2202x = \u2202L/\u2202y \u00b7 (\u2202F(x)/\u2202x + I)\")\n        print(\"\u2192 \u6052\u7b49\u884c\u5217 I \u306b\u3088\u308a\u52fe\u914d\u304c\u76f4\u63a5\u4f1d\u64ad\uff01\")\n\n        # \u5b9f\u9a13\u3067\u78ba\u8a8d\n        self._experiment_gradient_flow()\n\n    def _experiment_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u5b9f\u9a13\"\"\"\n        depths = [10, 20, 50, 100]\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n\n        for depth in depths:\n            # \u7406\u8ad6\u5024\uff1a\u901a\u5e38\u306e\u5c64\n            x = np.linspace(0, depth, 100)\n            grad_normal = 0.9 ** x  # \u5404\u5c64\u30670.9\u500d\u306b\u6e1b\u8870\u3068\u4eee\u5b9a\n            ax.semilogy(x, grad_normal, '--', label=f'Normal (depth={depth})')\n\n            # \u7406\u8ad6\u5024\uff1a\u6b8b\u5dee\u63a5\u7d9a\n            # \u6b8b\u5dee\u63a5\u7d9a\u3067\u306f\u52fe\u914d\u304c\u307b\u307c\u4fdd\u305f\u308c\u308b\n            grad_residual = np.ones_like(x) * 0.95\n            ax.semilogy(x, grad_residual, '-', label=f'Residual (depth={depth})')\n\n        ax.set_xlabel('Layer Depth')\n        ax.set_ylabel('Gradient Magnitude (log scale)')\n        ax.set_title('\u7406\u8ad6\u7684\u306a\u52fe\u914d\u306e\u6e1b\u8870')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # \u52fe\u914d\u6d88\u5931\u306e\u95be\u5024\n        ax.axhline(y=1e-5, color='red', linestyle=':', linewidth=2)\n        ax.text(50, 1e-5, 'Vanishing Gradient Threshold', \n               color='red', fontsize=10)\n\n        plt.tight_layout()\n        plt.show()\n\n    def explain_identity_mapping(self):\n        \"\"\"\u6052\u7b49\u5199\u50cf\u306e\u91cd\u8981\u6027\u3092\u8aac\u660e\"\"\"\n        print(\"\\n=== \u6052\u7b49\u5199\u50cf\u306e\u5b66\u7fd2 ===\\n\")\n\n        print(\"\u554f\u984c\uff1a\u6df1\u3044\u5c64\u3067\u6052\u7b49\u95a2\u6570\u3092\u5b66\u7fd2\u3059\u308b\u306e\u306f\u56f0\u96e3\")\n        print(\"H(x) = x \u3092\u5b66\u7fd2\u3057\u305f\u3044\u5834\u5408:\")\n        print(\"- \u901a\u5e38\u306e\u5c64\uff1aH(x) \u3092\u76f4\u63a5\u5b66\u7fd2\")\n        print(\"- \u6b8b\u5dee\u63a5\u7d9a\uff1aF(x) = H(x) - x = 0 \u3092\u5b66\u7fd2\\n\")\n\n        print(\"F(x) = 0 \u306e\u5b66\u7fd2\u306f\u7c21\u5358\uff01\")\n        print(\"\u2192 \u521d\u671f\u5316\u3067\u91cd\u307f\u3092\u5c0f\u3055\u304f\u3059\u308c\u3070\u81ea\u7136\u306b\u5b9f\u73fe\")\n\n        # \u5b9f\u9a13\n        self._demonstrate_identity_learning()\n\n    def _demonstrate_identity_learning(self):\n        \"\"\"\u6052\u7b49\u5199\u50cf\u5b66\u7fd2\u306e\u5b9f\u9a13\"\"\"\n        # \u30bf\u30b9\u30af\uff1a\u5165\u529b\u3092\u305d\u306e\u307e\u307e\u51fa\u529b\u3059\u308b\uff08\u6052\u7b49\u5199\u50cf\uff09\n\n        class NormalNetwork(nn.Module):\n            def __init__(self, depth: int):\n                super().__init__()\n                layers = []\n                for _ in range(depth):\n                    layers.extend([\n                        nn.Linear(100, 100),\n                        nn.ReLU()\n                    ])\n                self.net = nn.Sequential(*layers[:-1])  # \u6700\u5f8c\u306eReLU\u3092\u9664\u304f\n\n            def forward(self, x):\n                return self.net(x)\n\n        class ResidualNetwork(nn.Module):\n            def __init__(self, depth: int):\n                super().__init__()\n                self.blocks = nn.ModuleList([\n                    nn.Sequential(\n                        nn.Linear(100, 100),\n                        nn.ReLU(),\n                        nn.Linear(100, 100)\n                    ) for _ in range(depth)\n                ])\n\n                # \u5c0f\u3055\u3044\u521d\u671f\u5316\n                for block in self.blocks:\n                    for m in block:\n                        if isinstance(m, nn.Linear):\n                            nn.init.normal_(m.weight, 0, 0.01)\n                            nn.init.zeros_(m.bias)\n\n            def forward(self, x):\n                for block in self.blocks:\n                    x = x + block(x)\n                return x\n\n        # \u5b66\u7fd2\n        depth = 10\n        normal_net = NormalNetwork(depth)\n        residual_net = ResidualNetwork(depth)\n\n        # \u540c\u3058\u5165\u529b\n        x = torch.randn(100, 100)\n        target = x.clone()  # \u6052\u7b49\u5199\u50cf\n\n        # \u521d\u671f\u306e\u51fa\u529b\u3092\u6bd4\u8f03\n        with torch.no_grad():\n            normal_out = normal_net(x)\n            residual_out = residual_net(x)\n\n            normal_error = (normal_out - target).pow(2).mean()\n            residual_error = (residual_out - target).pow(2).mean()\n\n            print(f\"\\n\u521d\u671f\u30a8\u30e9\u30fc:\")\n            print(f\"\u901a\u5e38\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af: {normal_error:.4f}\")\n            print(f\"\u6b8b\u5dee\u30cd\u30c3\u30c8\u30ef\u30fc\u30af: {residual_error:.4f}\")\n            print(\"\u2192 \u6b8b\u5dee\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u6700\u521d\u304b\u3089\u6052\u7b49\u5199\u50cf\u306b\u8fd1\u3044\uff01\")\n</code></pre>"},{"location":"part3/residual-normalization/#113-layer-normalization","title":"11.3 \u5c64\u6b63\u898f\u5316\uff08Layer Normalization\uff09","text":""},{"location":"part3/residual-normalization/#layernorm-vs-batchnorm","title":"LayerNorm vs BatchNorm","text":"<pre><code>class NormalizationComparison:\n    \"\"\"\u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n\n    def explain_normalization_methods(self):\n        \"\"\"\u6b63\u898f\u5316\u624b\u6cd5\u3092\u8aac\u660e\"\"\"\n        print(\"=== \u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03 ===\\n\")\n\n        methods = {\n            \"Batch Normalization\": {\n                \"\u6b63\u898f\u5316\u8ef8\": \"\u30d0\u30c3\u30c1\u6b21\u5143\",\n                \"\u5229\u70b9\": [\"\u30d0\u30c3\u30c1\u7d71\u8a08\u306b\u3088\u308b\u5f37\u529b\u306a\u6b63\u898f\u5316\", \"CNN\u3067\u52b9\u679c\u7684\"],\n                \"\u6b20\u70b9\": [\"\u53ef\u5909\u9577\u7cfb\u5217\u3067\u554f\u984c\", \"\u63a8\u8ad6\u6642\u306b\u30d0\u30c3\u30c1\u7d71\u8a08\u304c\u5fc5\u8981\", \"\u5c0f\u30d0\u30c3\u30c1\u3067\u4e0d\u5b89\u5b9a\"]\n            },\n            \"Layer Normalization\": {\n                \"\u6b63\u898f\u5316\u8ef8\": \"\u7279\u5fb4\u6b21\u5143\",\n                \"\u5229\u70b9\": [\"\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044\", \"\u7cfb\u5217\u9577\u306b\u67d4\u8edf\", \"\u63a8\u8ad6\u6642\u3082\u540c\u3058\u52d5\u4f5c\"],\n                \"\u6b20\u70b9\": [\"\u30d0\u30c3\u30c1\u60c5\u5831\u3092\u6d3b\u7528\u3067\u304d\u306a\u3044\"]\n            },\n            \"Group Normalization\": {\n                \"\u6b63\u898f\u5316\u8ef8\": \"\u30c1\u30e3\u30cd\u30eb\u306e\u30b0\u30eb\u30fc\u30d7\",\n                \"\u5229\u70b9\": [\"\u5c0f\u30d0\u30c3\u30c1\u3067\u3082\u5b89\u5b9a\", \"CNN\u3067\u52b9\u679c\u7684\"],\n                \"\u6b20\u70b9\": [\"\u30b0\u30eb\u30fc\u30d7\u6570\u306e\u8abf\u6574\u304c\u5fc5\u8981\"]\n            },\n            \"Instance Normalization\": {\n                \"\u6b63\u898f\u5316\u8ef8\": \"\u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u5fb4\",\n                \"\u5229\u70b9\": [\"\u30b9\u30bf\u30a4\u30eb\u8ee2\u9001\u3067\u52b9\u679c\u7684\"],\n                \"\u6b20\u70b9\": [\"\u4e00\u822c\u7684\u306a\u30bf\u30b9\u30af\u3067\u306f\u52b9\u679c\u9650\u5b9a\u7684\"]\n            }\n        }\n\n        for name, props in methods.items():\n            print(f\"{name}:\")\n            print(f\"  \u6b63\u898f\u5316\u8ef8: {props['\u6b63\u898f\u5316\u8ef8']}\")\n            print(f\"  \u5229\u70b9: {', '.join(props['\u5229\u70b9'])}\")\n            print(f\"  \u6b20\u70b9: {', '.join(props['\u6b20\u70b9'])}\")\n            print()\n\n    def visualize_normalization_axes(self):\n        \"\"\"\u6b63\u898f\u5316\u8ef8\u306e\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n        # \u30c7\u30fc\u30bf\u306e\u5f62\u72b6\n        batch_size, seq_len, d_model = 4, 6, 8\n\n        # BatchNorm\n        ax = axes[0, 0]\n        ax.set_title('Batch Normalization', fontsize=14)\n\n        # 3D\u30c7\u30fc\u30bf\u30922D\u3067\u8868\u73fe\n        for b in range(batch_size):\n            for s in range(seq_len):\n                rect = Rectangle((s, b), 1, 1,\n                               facecolor=plt.cm.Blues((b*seq_len + s) % 10 / 10),\n                               edgecolor='black', linewidth=0.5)\n                ax.add_patch(rect)\n\n        # \u6b63\u898f\u5316\u306e\u65b9\u5411\u3092\u793a\u3059\n        for s in range(seq_len):\n            ax.arrow(s + 0.5, -0.5, 0, batch_size + 0.3,\n                    head_width=0.2, head_length=0.1,\n                    fc='red', ec='red', linewidth=2)\n\n        ax.text(seq_len/2, -1, 'Normalize across batch',\n               ha='center', color='red', fontsize=12)\n        ax.set_xlim(-0.5, seq_len)\n        ax.set_ylim(-1.5, batch_size)\n        ax.set_xlabel('Sequence Position')\n        ax.set_ylabel('Batch')\n        ax.invert_yaxis()\n\n        # LayerNorm\n        ax = axes[0, 1]\n        ax.set_title('Layer Normalization', fontsize=14)\n\n        for b in range(batch_size):\n            for s in range(seq_len):\n                rect = Rectangle((s, b), 1, 1,\n                               facecolor=plt.cm.Greens((b*seq_len + s) % 10 / 10),\n                               edgecolor='black', linewidth=0.5)\n                ax.add_patch(rect)\n\n        # \u6b63\u898f\u5316\u306e\u65b9\u5411\u3092\u793a\u3059\n        for b in range(batch_size):\n            ax.arrow(-0.5, b + 0.5, seq_len + 0.3, 0,\n                    head_width=0.2, head_length=0.1,\n                    fc='blue', ec='blue', linewidth=2)\n\n        ax.text(-1, batch_size/2, 'Normalize\\nacross features',\n               ha='right', va='center', color='blue', fontsize=12)\n        ax.set_xlim(-1.5, seq_len)\n        ax.set_ylim(-0.5, batch_size)\n        ax.set_xlabel('Sequence Position (Features)')\n        ax.set_ylabel('Batch')\n        ax.invert_yaxis()\n\n        # \u7d71\u8a08\u306e\u8a08\u7b97\u65b9\u6cd5\n        ax = axes[1, 0]\n        ax.text(0.5, 0.8, 'BatchNorm Statistics:', ha='center', fontsize=14, \n                fontweight='bold', transform=ax.transAxes)\n        ax.text(0.1, 0.6, \n                '\u2022 Mean/Var computed across batch dimension\\n'\n                '\u2022 Different statistics for each position\\n'\n                '\u2022 Requires batch statistics at inference',\n                transform=ax.transAxes, fontsize=12)\n        ax.axis('off')\n\n        ax = axes[1, 1]\n        ax.text(0.5, 0.8, 'LayerNorm Statistics:', ha='center', fontsize=14,\n                fontweight='bold', transform=ax.transAxes)\n        ax.text(0.1, 0.6,\n                '\u2022 Mean/Var computed across feature dimension\\n'\n                '\u2022 Different statistics for each sample\\n'\n                '\u2022 No batch dependency',\n                transform=ax.transAxes, fontsize=12)\n        ax.axis('off')\n\n        plt.suptitle('\u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/residual-normalization/#layernorm","title":"LayerNorm\u306e\u5b9f\u88c5\u3068\u7406\u89e3","text":"<pre><code>class LayerNormImplementation:\n    \"\"\"Layer Normalization\u306e\u5b9f\u88c5\"\"\"\n\n    def implement_layer_norm(self):\n        \"\"\"LayerNorm\u306e\u5b9f\u88c5\"\"\"\n        print(\"=== Layer Normalization \u306e\u5b9f\u88c5 ===\\n\")\n\n        class LayerNorm(nn.Module):\n            def __init__(self, d_model: int, eps: float = 1e-6):\n                super().__init__()\n                self.d_model = d_model\n                self.eps = eps\n\n                # \u5b66\u7fd2\u53ef\u80fd\u306a\u30d1\u30e9\u30e1\u30fc\u30bf\n                self.gamma = nn.Parameter(torch.ones(d_model))\n                self.beta = nn.Parameter(torch.zeros(d_model))\n\n            def forward(self, x):\n                # x: [batch_size, seq_len, d_model]\n\n                # \u7d71\u8a08\u91cf\u306e\u8a08\u7b97\uff08\u6700\u5f8c\u306e\u6b21\u5143\u3067\uff09\n                mean = x.mean(dim=-1, keepdim=True)\n                var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n                # \u6b63\u898f\u5316\n                x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n\n                # \u30a2\u30d5\u30a3\u30f3\u5909\u63db\n                output = self.gamma * x_normalized + self.beta\n\n                return output\n\n        # PyTorch\u306e\u5b9f\u88c5\u3068\u6bd4\u8f03\n        d_model = 256\n        x = torch.randn(2, 10, d_model)\n\n        custom_ln = LayerNorm(d_model)\n        pytorch_ln = nn.LayerNorm(d_model)\n\n        # \u540c\u3058\u30d1\u30e9\u30e1\u30fc\u30bf\u306b\u8a2d\u5b9a\n        with torch.no_grad():\n            pytorch_ln.weight.copy_(custom_ln.gamma)\n            pytorch_ln.bias.copy_(custom_ln.beta)\n\n        # \u51fa\u529b\u3092\u6bd4\u8f03\n        custom_out = custom_ln(x)\n        pytorch_out = pytorch_ln(x)\n\n        diff = (custom_out - pytorch_out).abs().max()\n        print(f\"\u30ab\u30b9\u30bf\u30e0\u5b9f\u88c5\u3068PyTorch\u5b9f\u88c5\u306e\u6700\u5927\u5dee: {diff:.6f}\")\n\n        # \u52b9\u679c\u3092\u53ef\u8996\u5316\n        self._visualize_normalization_effect(x, custom_out)\n\n    def _visualize_normalization_effect(self, input_tensor, output_tensor):\n        \"\"\"\u6b63\u898f\u5316\u306e\u52b9\u679c\u3092\u53ef\u8996\u5316\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n        # \u5165\u529b\u306e\u5206\u5e03\n        ax = axes[0, 0]\n        input_flat = input_tensor.flatten().detach().numpy()\n        ax.hist(input_flat, bins=50, alpha=0.7, color='blue', density=True)\n        ax.set_title('Input Distribution')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Density')\n        ax.axvline(input_flat.mean(), color='red', linestyle='--', \n                  label=f'Mean: {input_flat.mean():.2f}')\n        ax.axvline(input_flat.mean() + input_flat.std(), color='orange', \n                  linestyle='--', label=f'Std: {input_flat.std():.2f}')\n        ax.axvline(input_flat.mean() - input_flat.std(), color='orange', \n                  linestyle='--')\n        ax.legend()\n\n        # \u51fa\u529b\u306e\u5206\u5e03\n        ax = axes[0, 1]\n        output_flat = output_tensor.flatten().detach().numpy()\n        ax.hist(output_flat, bins=50, alpha=0.7, color='green', density=True)\n        ax.set_title('Output Distribution (After LayerNorm)')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Density')\n        ax.axvline(output_flat.mean(), color='red', linestyle='--',\n                  label=f'Mean: {output_flat.mean():.2f}')\n        ax.axvline(output_flat.mean() + output_flat.std(), color='orange',\n                  linestyle='--', label=f'Std: {output_flat.std():.2f}')\n        ax.axvline(output_flat.mean() - output_flat.std(), color='orange',\n                  linestyle='--')\n        ax.legend()\n\n        # \u5404\u4f4d\u7f6e\u3067\u306e\u7d71\u8a08\n        ax = axes[1, 0]\n        means = input_tensor.mean(dim=-1).flatten().detach().numpy()\n        stds = input_tensor.std(dim=-1).flatten().detach().numpy()\n        positions = range(len(means))\n\n        ax.scatter(positions, means, alpha=0.6, label='Mean')\n        ax.scatter(positions, stds, alpha=0.6, label='Std')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Value')\n        ax.set_title('Statistics per Position (Before Norm)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n        # \u6b63\u898f\u5316\u5f8c\u306e\u7d71\u8a08\n        ax = axes[1, 1]\n        means_norm = output_tensor.mean(dim=-1).flatten().detach().numpy()\n        stds_norm = output_tensor.std(dim=-1).flatten().detach().numpy()\n\n        ax.scatter(positions, means_norm, alpha=0.6, label='Mean')\n        ax.scatter(positions, stds_norm, alpha=0.6, label='Std')\n        ax.set_xlabel('Position')\n        ax.set_ylabel('Value')\n        ax.set_title('Statistics per Position (After Norm)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(-2, 2)\n\n        plt.suptitle('Layer Normalization \u306e\u52b9\u679c', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/residual-normalization/#114-pre-ln-vs-post-ln","title":"11.4 Pre-LN vs Post-LN","text":""},{"location":"part3/residual-normalization/#_7","title":"\u6b63\u898f\u5316\u306e\u914d\u7f6e","text":"<pre><code>class NormalizationPlacement:\n    \"\"\"\u6b63\u898f\u5316\u306e\u914d\u7f6e\u306b\u95a2\u3059\u308b\u5206\u6790\"\"\"\n\n    def explain_pre_post_ln(self):\n        \"\"\"Pre-LN vs Post-LN\u306e\u8aac\u660e\"\"\"\n        print(\"=== Pre-LN vs Post-LN ===\\n\")\n\n        print(\"Post-LN\uff08\u30aa\u30ea\u30b8\u30ca\u30eb\uff09:\")\n        print(\"  x \u2192 Sublayer \u2192 Add \u2192 LayerNorm \u2192 output\")\n        print(\"  LN(x + Sublayer(x))\\n\")\n\n        print(\"Pre-LN\uff08\u6539\u826f\u7248\uff09:\")\n        print(\"  x \u2192 LayerNorm \u2192 Sublayer \u2192 Add \u2192 output\")\n        print(\"  x + Sublayer(LN(x))\\n\")\n\n        print(\"Pre-LN\u306e\u5229\u70b9:\")\n        print(\"\u2713 \u5b66\u7fd2\u304c\u3088\u308a\u5b89\u5b9a\")\n        print(\"\u2713 Warmup\u304c\u4e0d\u8981\u307e\u305f\u306f\u77ed\u7e2e\u53ef\u80fd\")\n        print(\"\u2713 \u3088\u308a\u6df1\u3044\u30e2\u30c7\u30eb\u304c\u53ef\u80fd\")\n        print(\"\u2713 \u52fe\u914d\u306e\u6d41\u308c\u304c\u6539\u5584\")\n\n        # \u5b9f\u88c5\u3092\u6bd4\u8f03\n        self._implement_both_variants()\n\n    def _implement_both_variants(self):\n        \"\"\"\u4e21\u65b9\u306e\u5909\u7a2e\u3092\u5b9f\u88c5\"\"\"\n\n        class PostLNBlock(nn.Module):\n            \"\"\"Post-LN: \u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u69cb\u6210\"\"\"\n            def __init__(self, d_model: int):\n                super().__init__()\n                self.attention = nn.MultiheadAttention(d_model, 8, batch_first=True)\n                self.norm1 = nn.LayerNorm(d_model)\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Linear(d_model * 4, d_model)\n                )\n                self.norm2 = nn.LayerNorm(d_model)\n                self.dropout = nn.Dropout(0.1)\n\n            def forward(self, x):\n                # Attention \u2192 Add \u2192 Norm\n                attn_out, _ = self.attention(x, x, x)\n                x = self.norm1(x + self.dropout(attn_out))\n\n                # FFN \u2192 Add \u2192 Norm\n                ffn_out = self.ffn(x)\n                x = self.norm2(x + self.dropout(ffn_out))\n\n                return x\n\n        class PreLNBlock(nn.Module):\n            \"\"\"Pre-LN: \u6539\u826f\u3055\u308c\u305f\u69cb\u6210\"\"\"\n            def __init__(self, d_model: int):\n                super().__init__()\n                self.norm1 = nn.LayerNorm(d_model)\n                self.attention = nn.MultiheadAttention(d_model, 8, batch_first=True)\n                self.norm2 = nn.LayerNorm(d_model)\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Linear(d_model * 4, d_model)\n                )\n                self.dropout = nn.Dropout(0.1)\n\n            def forward(self, x):\n                # Norm \u2192 Attention \u2192 Add\n                attn_out, _ = self.attention(self.norm1(x), self.norm1(x), self.norm1(x))\n                x = x + self.dropout(attn_out)\n\n                # Norm \u2192 FFN \u2192 Add\n                ffn_out = self.ffn(self.norm2(x))\n                x = x + self.dropout(ffn_out)\n\n                return x\n\n        # \u5b89\u5b9a\u6027\u306e\u6bd4\u8f03\n        self._compare_training_stability()\n\n    def _compare_training_stability(self):\n        \"\"\"\u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\u3092\u6bd4\u8f03\"\"\"\n        print(\"\\n\u5b66\u7fd2\u5b89\u5b9a\u6027\u306e\u6bd4\u8f03\u5b9f\u9a13...\")\n\n        # \u6df1\u3044\u30e2\u30c7\u30eb\u3067\u306e\u52fe\u914d\u306e\u632f\u308b\u821e\u3044\u3092\u30b7\u30df\u30e5\u30ec\u30fc\u30c8\n        depths = [6, 12, 24]\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, depth in enumerate(depths):\n            ax = axes[idx]\n\n            # \u4eee\u60f3\u7684\u306a\u5b66\u7fd2\u66f2\u7dda\n            epochs = np.arange(100)\n\n            # Post-LN: \u6df1\u3044\u30e2\u30c7\u30eb\u3067\u4e0d\u5b89\u5b9a\n            post_ln_loss = np.exp(-epochs / 30) * (1 + 0.3 * np.sin(epochs / 5))\n            if depth &gt; 12:\n                # \u6df1\u3044\u30e2\u30c7\u30eb\u3067\u306f\u521d\u671f\u306b\u767a\u6563\n                post_ln_loss[:20] = post_ln_loss[:20] * (1 + np.random.randn(20) * 0.5)\n                post_ln_loss[:10] = np.clip(post_ln_loss[:10] * 2, 0, 5)\n\n            # Pre-LN: \u5b89\u5b9a\n            pre_ln_loss = np.exp(-epochs / 25) * (1 + 0.1 * np.sin(epochs / 5))\n\n            ax.plot(epochs, post_ln_loss, 'r-', label='Post-LN', linewidth=2)\n            ax.plot(epochs, pre_ln_loss, 'b-', label='Pre-LN', linewidth=2)\n\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel('Loss')\n            ax.set_title(f'Depth = {depth} layers')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n            ax.set_ylim(0, 3)\n\n            # Warmup\u671f\u9593\u3092\u793a\u3059\n            if depth &gt; 12:\n                ax.axvspan(0, 20, alpha=0.2, color='gray')\n                ax.text(10, 2.5, 'Warmup\\nRequired', ha='center', fontsize=10)\n\n        plt.suptitle('Pre-LN vs Post-LN: \u5b66\u7fd2\u306e\u5b89\u5b9a\u6027', fontsize=16)\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"part3/residual-normalization/#115","title":"11.5 \u6700\u65b0\u306e\u6b63\u898f\u5316\u6280\u8853","text":""},{"location":"part3/residual-normalization/#rmsnorm","title":"RMSNorm","text":"<pre><code>class ModernNormalizationTechniques:\n    \"\"\"\u6700\u65b0\u306e\u6b63\u898f\u5316\u6280\u8853\"\"\"\n\n    def implement_rmsnorm(self):\n        \"\"\"RMSNorm\u306e\u5b9f\u88c5\"\"\"\n        print(\"=== RMSNorm (Root Mean Square Normalization) ===\\n\")\n\n        print(\"LayerNorm\u306e\u7c21\u7565\u7248:\")\n        print(\"- \u5e73\u5747\u3092\u5f15\u304b\u306a\u3044\uff08\u30bb\u30f3\u30bf\u30ea\u30f3\u30b0\u306a\u3057\uff09\")\n        print(\"- RMS\u3067\u6b63\u898f\u5316\")\n        print(\"- \u8a08\u7b97\u52b9\u7387\u304c\u826f\u3044\")\n        print(\"- LLaMA\u3067\u63a1\u7528\\n\")\n\n        class RMSNorm(nn.Module):\n            def __init__(self, d_model: int, eps: float = 1e-6):\n                super().__init__()\n                self.d_model = d_model\n                self.eps = eps\n                self.weight = nn.Parameter(torch.ones(d_model))\n\n            def forward(self, x):\n                # RMS\u306e\u8a08\u7b97\n                rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n\n                # \u6b63\u898f\u5316\n                x_normalized = x / (rms + self.eps)\n\n                # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n                return self.weight * x_normalized\n\n        # LayerNorm\u3068\u306e\u6bd4\u8f03\n        d_model = 256\n        x = torch.randn(2, 10, d_model) * 3 + 1  # \u5e73\u57471\u3001\u6a19\u6e96\u504f\u5dee3\n\n        rmsnorm = RMSNorm(d_model)\n        layernorm = nn.LayerNorm(d_model)\n\n        rms_out = rmsnorm(x)\n        ln_out = layernorm(x)\n\n        print(\"\u5165\u529b\u306e\u7d71\u8a08:\")\n        print(f\"  \u5e73\u5747: {x.mean():.3f}, \u6a19\u6e96\u504f\u5dee: {x.std():.3f}\")\n\n        print(\"\\nRMSNorm\u51fa\u529b:\")\n        print(f\"  \u5e73\u5747: {rms_out.mean():.3f}, \u6a19\u6e96\u504f\u5dee: {rms_out.std():.3f}\")\n\n        print(\"\\nLayerNorm\u51fa\u529b:\")\n        print(f\"  \u5e73\u5747: {ln_out.mean():.3f}, \u6a19\u6e96\u504f\u5dee: {ln_out.std():.3f}\")\n\n        # \u8a08\u7b97\u901f\u5ea6\u306e\u6bd4\u8f03\n        self._compare_computation_speed()\n\n    def _compare_computation_speed(self):\n        \"\"\"\u8a08\u7b97\u901f\u5ea6\u306e\u6bd4\u8f03\"\"\"\n        import time\n\n        print(\"\\n=== \u8a08\u7b97\u901f\u5ea6\u306e\u6bd4\u8f03 ===\")\n\n        # \u5927\u304d\u3081\u306e\u30c6\u30f3\u30bd\u30eb\n        batch_size = 128\n        seq_len = 512\n        d_model = 1024\n\n        x = torch.randn(batch_size, seq_len, d_model).cuda()\n\n        # RMSNorm\n        class RMSNorm(nn.Module):\n            def __init__(self, d_model):\n                super().__init__()\n                self.weight = nn.Parameter(torch.ones(d_model))\n                self.eps = 1e-6\n\n            def forward(self, x):\n                rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n                return self.weight * x / (rms + self.eps)\n\n        rmsnorm = RMSNorm(d_model).cuda()\n        layernorm = nn.LayerNorm(d_model).cuda()\n\n        # \u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\n        for _ in range(10):\n            _ = rmsnorm(x)\n            _ = layernorm(x)\n\n        # \u8a08\u6e2c\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(100):\n            _ = rmsnorm(x)\n        torch.cuda.synchronize()\n        rms_time = time.time() - start\n\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(100):\n            _ = layernorm(x)\n        torch.cuda.synchronize()\n        ln_time = time.time() - start\n\n        print(f\"RMSNorm: {rms_time:.3f}\u79d2\")\n        print(f\"LayerNorm: {ln_time:.3f}\u79d2\")\n        print(f\"\u901f\u5ea6\u5411\u4e0a: {ln_time/rms_time:.2f}x\")\n</code></pre>"},{"location":"part3/residual-normalization/#deepnorm","title":"DeepNorm","text":"<pre><code>class DeepNormTechnique:\n    \"\"\"DeepNorm: 1000\u5c64\u306eTransformer\u3092\u53ef\u80fd\u306b\"\"\"\n\n    def explain_deepnorm(self):\n        \"\"\"DeepNorm\u306e\u8aac\u660e\"\"\"\n        print(\"=== DeepNorm ===\\n\")\n\n        print(\"\u8d85\u6df1\u5c64Transformer\u306e\u305f\u3081\u306e\u6280\u8853:\")\n        print(\"1. \u7279\u5225\u306a\u521d\u671f\u5316\")\n        print(\"2. \u6b8b\u5dee\u63a5\u7d9a\u306e\u91cd\u307f\u4ed8\u3051\")\n        print(\"3. 1000\u5c64\u4ee5\u4e0a\u306e\u30e2\u30c7\u30eb\u304c\u53ef\u80fd\\n\")\n\n        class DeepNormBlock(nn.Module):\n            def __init__(self, d_model: int, depth: int):\n                super().__init__()\n                self.d_model = d_model\n                self.depth = depth\n\n                # \u30b5\u30d6\u30ec\u30a4\u30e4\u30fc\n                self.attention = nn.MultiheadAttention(d_model, 8, batch_first=True)\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, d_model * 4),\n                    nn.ReLU(),\n                    nn.Linear(d_model * 4, d_model)\n                )\n\n                # LayerNorm\n                self.norm1 = nn.LayerNorm(d_model)\n                self.norm2 = nn.LayerNorm(d_model)\n\n                # DeepNorm\u306e\u5b9a\u6570\n                self.alpha = self._compute_alpha(depth)\n                self.beta = self._compute_beta(depth)\n\n                # \u7279\u5225\u306a\u521d\u671f\u5316\n                self._deepnorm_init()\n\n            def _compute_alpha(self, N):\n                \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u91cd\u307f\"\"\"\n                return (2 * N) ** 0.25\n\n            def _compute_beta(self, N):\n                \"\"\"\u521d\u671f\u5316\u306e\u30b9\u30b1\u30fc\u30eb\"\"\"\n                return (8 * N) ** -0.25\n\n            def _deepnorm_init(self):\n                \"\"\"DeepNorm\u521d\u671f\u5316\"\"\"\n                # Xavier\u306e\u5909\u7a2e\n                for name, param in self.named_parameters():\n                    if 'weight' in name and param.dim() &gt; 1:\n                        nn.init.xavier_normal_(param, gain=self.beta)\n\n            def forward(self, x):\n                # DeepNorm\u6b8b\u5dee\u63a5\u7d9a\n                # x_l+1 = LN(\u03b1 * x_l + sublayer(x_l))\n\n                # Attention\n                residual = x\n                x = self.norm1(x)\n                attn_out, _ = self.attention(x, x, x)\n                x = self.alpha * residual + attn_out\n\n                # FFN\n                residual = x\n                x = self.norm2(x)\n                ffn_out = self.ffn(x)\n                x = self.alpha * residual + ffn_out\n\n                return x\n\n        print(f\"\u4f8b: 100\u5c64\u306e\u30e2\u30c7\u30eb\")\n        print(f\"  \u03b1 = {(2 * 100) ** 0.25:.3f}\")\n        print(f\"  \u03b2 = {(8 * 100) ** -0.25:.3f}\")\n</code></pre>"},{"location":"part3/residual-normalization/#116-transformer","title":"11.6 \u7d71\u5408\uff1a\u5b8c\u5168\u306aTransformer\u30d6\u30ed\u30c3\u30af","text":""},{"location":"part3/residual-normalization/#_8","title":"\u3059\u3079\u3066\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u308b","text":"<pre><code>class CompleteTransformerBlock:\n    \"\"\"\u5b8c\u5168\u306aTransformer\u30d6\u30ed\u30c3\u30af\u306e\u5b9f\u88c5\"\"\"\n\n    def create_transformer_block(self,\n                                d_model: int = 512,\n                                n_heads: int = 8,\n                                d_ff: int = 2048,\n                                dropout: float = 0.1,\n                                norm_type: str = 'layer',\n                                norm_position: str = 'pre') -&gt; nn.Module:\n        \"\"\"\u67d4\u8edf\u306aTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n\n        class TransformerBlock(nn.Module):\n            def __init__(self):\n                super().__init__()\n\n                # Attention\n                self.attention = nn.MultiheadAttention(\n                    d_model, n_heads, dropout=dropout, batch_first=True\n                )\n\n                # FFN\n                self.ffn = nn.Sequential(\n                    nn.Linear(d_model, d_ff),\n                    nn.GELU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(d_ff, d_model),\n                    nn.Dropout(dropout)\n                )\n\n                # Normalization\n                if norm_type == 'layer':\n                    self.norm1 = nn.LayerNorm(d_model)\n                    self.norm2 = nn.LayerNorm(d_model)\n                elif norm_type == 'rms':\n                    self.norm1 = self._create_rmsnorm(d_model)\n                    self.norm2 = self._create_rmsnorm(d_model)\n\n                self.dropout = nn.Dropout(dropout)\n                self.norm_position = norm_position\n\n            def _create_rmsnorm(self, d_model):\n                class RMSNorm(nn.Module):\n                    def __init__(self, d_model):\n                        super().__init__()\n                        self.weight = nn.Parameter(torch.ones(d_model))\n                        self.eps = 1e-6\n\n                    def forward(self, x):\n                        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n                        return self.weight * x / (rms + self.eps)\n\n                return RMSNorm(d_model)\n\n            def forward(self, x, mask=None):\n                if self.norm_position == 'pre':\n                    # Pre-LN\n                    # Attention\n                    residual = x\n                    x_norm = self.norm1(x)\n                    attn_out, _ = self.attention(x_norm, x_norm, x_norm, attn_mask=mask)\n                    x = residual + self.dropout(attn_out)\n\n                    # FFN\n                    residual = x\n                    x_norm = self.norm2(x)\n                    ffn_out = self.ffn(x_norm)\n                    x = residual + ffn_out\n\n                else:  # post\n                    # Post-LN\n                    # Attention\n                    residual = x\n                    attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n                    x = self.norm1(residual + self.dropout(attn_out))\n\n                    # FFN\n                    residual = x\n                    ffn_out = self.ffn(x)\n                    x = self.norm2(residual + ffn_out)\n\n                return x\n\n        return TransformerBlock()\n\n    def test_configurations(self):\n        \"\"\"\u7570\u306a\u308b\u69cb\u6210\u306e\u30c6\u30b9\u30c8\"\"\"\n        print(\"=== \u7570\u306a\u308bTransformer\u69cb\u6210\u306e\u30c6\u30b9\u30c8 ===\\n\")\n\n        configs = [\n            (\"Post-LN + LayerNorm\", {'norm_position': 'post', 'norm_type': 'layer'}),\n            (\"Pre-LN + LayerNorm\", {'norm_position': 'pre', 'norm_type': 'layer'}),\n            (\"Pre-LN + RMSNorm\", {'norm_position': 'pre', 'norm_type': 'rms'})\n        ]\n\n        # \u5165\u529b\n        batch_size = 2\n        seq_len = 10\n        d_model = 512\n        x = torch.randn(batch_size, seq_len, d_model)\n\n        for name, config in configs:\n            print(f\"\\n{name}:\")\n            block = self.create_transformer_block(**config)\n\n            # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\n            params = sum(p.numel() for p in block.parameters())\n            print(f\"  \u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {params:,}\")\n\n            # \u51fa\u529b\n            with torch.no_grad():\n                output = block(x)\n                print(f\"  \u51fa\u529b\u5f62\u72b6: {output.shape}\")\n                print(f\"  \u51fa\u529b\u7d71\u8a08 - \u5e73\u5747: {output.mean():.4f}, \u6a19\u6e96\u504f\u5dee: {output.std():.4f}\")\n\n            # \u52fe\u914d\u306e\u6d41\u308c\n            x_grad = x.clone().requires_grad_(True)\n            y = block(x_grad)\n            loss = y.sum()\n            loss.backward()\n\n            print(f\"  \u5165\u529b\u52fe\u914d\u30ce\u30eb\u30e0: {x_grad.grad.norm():.4f}\")\n</code></pre>"},{"location":"part3/residual-normalization/#_9","title":"\u307e\u3068\u3081\uff1a\u6df1\u3055\u3092\u53ef\u80fd\u306b\u3059\u308b\u6280\u8853","text":"<p>\u3053\u306e\u7ae0\u3067\u5b66\u3093\u3060\u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316\u306e\u91cd\u8981\u306a\u30dd\u30a4\u30f3\u30c8\uff1a</p> <ol> <li>\u6b8b\u5dee\u63a5\u7d9a\uff1a</li> <li>\u52fe\u914d\u306e\u9ad8\u901f\u9053\u8def\u3092\u63d0\u4f9b</li> <li>\u6052\u7b49\u5199\u50cf\u306e\u5b66\u7fd2\u3092\u5bb9\u6613\u306b</li> <li>100\u5c64\u4ee5\u4e0a\u306e\u6df1\u3055\u3092\u53ef\u80fd\u306b</li> <li> <p>\u30b7\u30f3\u30d7\u30eb\u3060\u304c\u9769\u547d\u7684</p> </li> <li> <p>\u5c64\u6b63\u898f\u5316\uff1a</p> </li> <li>\u5185\u90e8\u5171\u5909\u91cf\u30b7\u30d5\u30c8\u3092\u6291\u5236</li> <li>\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044</li> <li>\u7cfb\u5217\u51e6\u7406\u306b\u6700\u9069</li> <li> <p>\u5b66\u7fd2\u306e\u5b89\u5b9a\u5316</p> </li> <li> <p>\u914d\u7f6e\u306e\u91cd\u8981\u6027\uff1a</p> </li> <li>Pre-LN\uff1a\u3088\u308a\u5b89\u5b9a\u3057\u305f\u5b66\u7fd2</li> <li>Post-LN\uff1a\u30aa\u30ea\u30b8\u30ca\u30eb\u3060\u304c\u4e0d\u5b89\u5b9a</li> <li> <p>\u6df1\u3044\u30e2\u30c7\u30eb\u3067\u306fPre-LN\u304c\u6a19\u6e96</p> </li> <li> <p>\u6700\u65b0\u306e\u6280\u8853\uff1a</p> </li> <li>RMSNorm\uff1a\u8a08\u7b97\u52b9\u7387\u306e\u6539\u5584</li> <li>DeepNorm\uff1a1000\u5c64\u3092\u53ef\u80fd\u306b</li> <li>\u7d99\u7d9a\u7684\u306a\u6539\u5584</li> </ol> <p>\u3053\u308c\u3089\u306e\u6280\u8853\u306b\u3088\u308a\u3001Transformer\u306f\u9a5a\u7570\u7684\u306a\u6df1\u3055\u3092\u5b9f\u73fe\u3057\u3001\u8907\u96d1\u306a\u30bf\u30b9\u30af\u3092\u89e3\u6c7a\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\u3002\u6b21\u7ae0\u3067\u306f\u3001\u3053\u308c\u3089\u3059\u3079\u3066\u306e\u8981\u7d20\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u5b8c\u5168\u306a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fb\u30c7\u30b3\u30fc\u30c0\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part3/residual-normalization/#_10","title":"\u6f14\u7fd2\u554f\u984c","text":"<ol> <li> <p>\u5b9f\u88c5\u8ab2\u984c\uff1aDeepNorm\u3092\u5b9f\u88c5\u3057\u3001\u901a\u5e38\u306e\u6b8b\u5dee\u63a5\u7d9a\u3068\u6bd4\u8f03\u3057\u3066\u3001\u3069\u308c\u3060\u3051\u6df1\u3044\u30e2\u30c7\u30eb\u304c\u5b89\u5b9a\u3057\u3066\u5b66\u7fd2\u3067\u304d\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u5206\u6790\u8ab2\u984c\uff1aPre-LN\u3068Post-LN\u3067\u3001\u5c64\u3092\u91cd\u306d\u305f\u3068\u304d\u306e\u6d3b\u6027\u5316\u306e\u5206\u5e03\u304c\u3069\u3046\u5909\u5316\u3059\u308b\u304b\u8abf\u67fb\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u6700\u9069\u5316\u8ab2\u984c\uff1aGroupNorm\u3092Transformer\u306b\u9069\u7528\u3057\u3001LayerNorm\u3068\u6027\u80fd\u3092\u6bd4\u8f03\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> <li> <p>\u7406\u8ad6\u8ab2\u984c\uff1a\u306a\u305c\u6b8b\u5dee\u63a5\u7d9a\u3067\u306f\u52a0\u7b97\u3092\u4f7f\u3044\u3001\u4e57\u7b97\u3084\u9023\u7d50\u3067\u306f\u306a\u3044\u306e\u304b\u3001\u6570\u5b66\u7684\u306b\u8003\u5bdf\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> </li> </ol> <p>\u6b21\u7ae0\u300c\u30a8\u30f3\u30b3\u30fc\u30c0\u3068\u30c7\u30b3\u30fc\u30c0\u300d\u3078\u7d9a\u304f\u3002</p>"},{"location":"part4/component-implementation/","title":"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5b9f\u88c5","text":""},{"location":"part4/component-implementation/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u90e8\u54c1\u304b\u3089\u5168\u4f53\u3078","text":"<p>\u512a\u308c\u305f\u30b3\u30f3\u30d1\u30a4\u30e9\u306f\u3001\u3088\u304f\u8a2d\u8a08\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u5b57\u53e5\u89e3\u6790\u5668\u3001\u69cb\u6587\u89e3\u6790\u5668\u3001\u610f\u5473\u89e3\u6790\u5668\u3001\u30b3\u30fc\u30c9\u751f\u6210\u5668\u2014\u305d\u308c\u305e\u308c\u304c\u72ec\u7acb\u3057\u3066\u52d5\u4f5c\u3057\u3001\u660e\u78ba\u306a\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067\u63a5\u7d9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>Transformer\u3082\u540c\u3058\u8a2d\u8a08\u54f2\u5b66\u306b\u5f93\u3044\u307e\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u8a73\u7d30\u306b\u5b9f\u88c5\u3057\u3001\u305d\u308c\u3089\u304c\u3069\u306e\u3088\u3046\u306b\u7d44\u307f\u5408\u308f\u3055\u3063\u3066\u5f37\u529b\u306a\u30e2\u30c7\u30eb\u3092\u5f62\u6210\u3059\u308b\u304b\u3092\u898b\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"part4/component-implementation/#141-multi-head-attention","title":"14.1 Multi-Head Attention\u306e\u5b8c\u5168\u5b9f\u88c5","text":""},{"location":"part4/component-implementation/#multi-head","title":"\u306a\u305cMulti-Head\u304c\u5fc5\u8981\u304b","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Optional, Tuple, List, Dict, Any import math from matplotlib.patches import Rectangle, FancyBboxPatch, Circle import matplotlib.patches as mpatches</p> <p>class MultiHeadAttentionImplementation:     \"\"\"Multi-Head Attention\u306e\u8a73\u7d30\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 512\n    self.n_heads = 8\n    self.d_k = self.d_model // self.n_heads  # 64\n\ndef explain_multi_head_benefits(self):\n    \"\"\"Multi-Head\u306e\u5229\u70b9\u3092\u8aac\u660e\"\"\"\n    print(\"=== Multi-Head Attention\u306e\u5229\u70b9 ===\\n\")\n\n    print(\"1. \u4e26\u5217\u7684\u306a\u8868\u73fe\u5b66\u7fd2:\")\n    print(\"   - \u5404\u30d8\u30c3\u30c9\u304c\u7570\u306a\u308b\u7279\u5fb4\u306b\u6ce8\u76ee\")\n    print(\"   - \u6587\u6cd5\u3001\u610f\u5473\u3001\u6587\u8108\u306a\u3069\u591a\u69d8\u306a\u95a2\u4fc2\u3092\u6349\u3048\u308b\\n\")\n\n    print(\"2. \u8a08\u7b97\u52b9\u7387:\")\n    print(\"   - \u30d8\u30c3\u30c9\u3054\u3068\u306e\u6b21\u5143\u524a\u6e1b\u3067\u7dcf\u8a08\u7b97\u91cf\u3092\u6291\u5236\")\n    print(\"   - \u4e26\u5217\u8a08\u7b97\u304c\u53ef\u80fd\\n\")\n\n    print(\"3. \u8868\u73fe\u529b\u306e\u5411\u4e0a:\")\n    print(\"   - \u5358\u4e00\u306e\u6ce8\u610f\u6a5f\u69cb\u3088\u308a\u8c4a\u304b\u306a\u8868\u73fe\")\n    print(\"   - \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u52b9\u679c\u306b\u3088\u308b\u9811\u5065\u6027\\n\")\n\n    # \u8996\u899a\u7684\u8aac\u660e\n    self._visualize_multi_head_concept()\n\ndef _visualize_multi_head_concept(self):\n    \"\"\"Multi-Head\u306e\u6982\u5ff5\u3092\u53ef\u8996\u5316\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # Single Head\n    ax1.set_title('Single-Head Attention', fontsize=14, weight='bold')\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(0, 10)\n\n    # \u5165\u529b\n    input_rect = Rectangle((1, 2), 2, 6, facecolor='lightblue', \n                          edgecolor='darkblue', linewidth=2)\n    ax1.add_patch(input_rect)\n    ax1.text(2, 5, 'Input\\n(d_model)', ha='center', va='center')\n\n    # Attention\n    attn_rect = Rectangle((4, 3), 3, 4, facecolor='lightcoral',\n                         edgecolor='darkred', linewidth=2)\n    ax1.add_patch(attn_rect)\n    ax1.text(5.5, 5, 'Attention', ha='center', va='center')\n\n    # \u51fa\u529b\n    output_rect = Rectangle((8, 2), 2, 6, facecolor='lightgreen',\n                           edgecolor='darkgreen', linewidth=2)\n    ax1.add_patch(output_rect)\n    ax1.text(9, 5, 'Output\\n(d_model)', ha='center', va='center')\n\n    # \u77e2\u5370\n    ax1.arrow(3, 5, 0.8, 0, head_width=0.3, head_length=0.2, \n             fc='black', ec='black')\n    ax1.arrow(7, 5, 0.8, 0, head_width=0.3, head_length=0.2,\n             fc='black', ec='black')\n\n    ax1.axis('off')\n\n    # Multi-Head\n    ax2.set_title('Multi-Head Attention (8 heads)', fontsize=14, weight='bold')\n    ax2.set_xlim(0, 12)\n    ax2.set_ylim(0, 10)\n\n    # \u5165\u529b\n    input_rect2 = Rectangle((1, 2), 2, 6, facecolor='lightblue',\n                           edgecolor='darkblue', linewidth=2)\n    ax2.add_patch(input_rect2)\n    ax2.text(2, 5, 'Input\\n(d_model)', ha='center', va='center')\n\n    # \u8907\u6570\u306e\u30d8\u30c3\u30c9\n    colors = plt.cm.Set3(np.linspace(0, 1, 8))\n    for i in range(8):\n        y_pos = 1 + i * 0.9\n        head_rect = Rectangle((4, y_pos), 2, 0.7, \n                             facecolor=colors[i], alpha=0.7,\n                             edgecolor='black', linewidth=1)\n        ax2.add_patch(head_rect)\n        ax2.text(5, y_pos + 0.35, f'H{i+1}', ha='center', \n                va='center', fontsize=8)\n\n        # \u77e2\u5370\n        ax2.arrow(3, 5, 0.8, y_pos + 0.35 - 5, \n                 head_width=0.15, head_length=0.1,\n                 fc='gray', ec='gray', alpha=0.5)\n\n    # Concat\n    concat_rect = Rectangle((7, 2), 2, 6, facecolor='lightyellow',\n                           edgecolor='orange', linewidth=2)\n    ax2.add_patch(concat_rect)\n    ax2.text(8, 5, 'Concat', ha='center', va='center')\n\n    # \u51fa\u529b\n    output_rect2 = Rectangle((10, 2), 2, 6, facecolor='lightgreen',\n                            edgecolor='darkgreen', linewidth=2)\n    ax2.add_patch(output_rect2)\n    ax2.text(11, 5, 'Output\\n(d_model)', ha='center', va='center')\n\n    # \u77e2\u5370\n    for i in range(8):\n        y_pos = 1 + i * 0.9\n        ax2.arrow(6, y_pos + 0.35, 0.8, 5 - (y_pos + 0.35),\n                 head_width=0.15, head_length=0.1,\n                 fc='gray', ec='gray', alpha=0.5)\n\n    ax2.arrow(9, 5, 0.8, 0, head_width=0.3, head_length=0.2,\n             fc='black', ec='black')\n\n    ax2.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class OptimizedMultiHeadAttention(nn.Module):     \"\"\"\u6700\u9069\u5316\u3055\u308c\u305fMulti-Head Attention\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1,\n             use_bias: bool = True):\n    super().__init__()\n    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n\n    self.d_model = d_model\n    self.n_heads = n_heads\n    self.d_k = d_model // n_heads\n    self.scale = 1.0 / math.sqrt(self.d_k)\n\n    # \u52b9\u7387\u7684\u306a\u5b9f\u88c5\uff1aQ, K, V\u3092\u4e00\u3064\u306e\u884c\u5217\u3067\u8a08\u7b97\n    self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=use_bias)\n\n    # \u51fa\u529b\u6295\u5f71\n    self.out_proj = nn.Linear(d_model, d_model, bias=use_bias)\n\n    # Dropout\n    self.attn_dropout = nn.Dropout(dropout)\n    self.proj_dropout = nn.Dropout(dropout)\n\n    # \u521d\u671f\u5316\n    self._init_weights()\n\ndef _init_weights(self):\n    \"\"\"\u91cd\u307f\u306e\u521d\u671f\u5316\"\"\"\n    # Xavier\u521d\u671f\u5316\n    nn.init.xavier_uniform_(self.qkv_proj.weight)\n    nn.init.xavier_uniform_(self.out_proj.weight)\n\n    if self.qkv_proj.bias is not None:\n        nn.init.zeros_(self.qkv_proj.bias)\n    if self.out_proj.bias is not None:\n        nn.init.zeros_(self.out_proj.bias)\n\ndef forward(self, query: torch.Tensor, \n            key: Optional[torch.Tensor] = None,\n            value: Optional[torch.Tensor] = None,\n            mask: Optional[torch.Tensor] = None,\n            need_weights: bool = False) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Args:\n        query: [batch_size, seq_len, d_model]\n        key: [batch_size, seq_len, d_model] (None for self-attention)\n        value: [batch_size, seq_len, d_model] (None for self-attention)\n        mask: [batch_size, seq_len, seq_len] or [seq_len, seq_len]\n        need_weights: \u6ce8\u610f\u91cd\u307f\u3092\u8fd4\u3059\u304b\u3069\u3046\u304b\n\n    Returns:\n        output: [batch_size, seq_len, d_model]\n        attn_weights: [batch_size, n_heads, seq_len, seq_len] (if need_weights)\n    \"\"\"\n    batch_size, seq_len, _ = query.shape\n\n    # Self-attention\u306e\u5834\u5408\n    if key is None:\n        key = query\n    if value is None:\n        value = query\n\n    # Q, K, V\u3092\u4e00\u5ea6\u306b\u8a08\u7b97\uff08\u52b9\u7387\u7684\uff09\n    if key is query:  # self-attention\n        qkv = self.qkv_proj(query)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L, D]\n        q, k, v = qkv[0], qkv[1], qkv[2]\n    else:  # cross-attention\n        q = self.qkv_proj(query)[:, :, :self.d_model]\n        k = self.qkv_proj(key)[:, :, self.d_model:2*self.d_model]\n        v = self.qkv_proj(value)[:, :, 2*self.d_model:]\n\n        q = q.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        k = k.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        v = v.reshape(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n    # Scaled Dot-Product Attention\n    attn_output, attn_weights = self._scaled_dot_product_attention(\n        q, k, v, mask, self.attn_dropout if self.training else None\n    )\n\n    # \u30d8\u30c3\u30c9\u3092\u7d50\u5408\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n\n    # \u51fa\u529b\u6295\u5f71\n    output = self.out_proj(attn_output)\n    output = self.proj_dropout(output)\n\n    if need_weights:\n        return output, attn_weights\n    else:\n        return output, None\n\ndef _scaled_dot_product_attention(self, q: torch.Tensor, k: torch.Tensor,\n                                 v: torch.Tensor, mask: Optional[torch.Tensor],\n                                 dropout: Optional[nn.Dropout]) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Scaled Dot-Product Attention\u306e\u8a08\u7b97\"\"\"\n    # \u6ce8\u610f\u30b9\u30b3\u30a2\u306e\u8a08\u7b97\n    scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n\n    # \u30de\u30b9\u30af\u306e\u9069\u7528\n    if mask is not None:\n        if mask.dim() == 2:\n            mask = mask.unsqueeze(0).unsqueeze(0)\n        elif mask.dim() == 3:\n            mask = mask.unsqueeze(1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    # Softmax\n    attn_weights = F.softmax(scores, dim=-1)\n\n    # Dropout\n    if dropout is not None:\n        attn_weights = dropout(attn_weights)\n\n    # \u91cd\u307f\u4ed8\u304d\u548c\n    attn_output = torch.matmul(attn_weights, v)\n\n    return attn_output, attn_weights\n\ndef get_attention_maps(self, query: torch.Tensor,\n                      key: Optional[torch.Tensor] = None,\n                      mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"\u6ce8\u610f\u30de\u30c3\u30d7\u306e\u53d6\u5f97\uff08\u53ef\u8996\u5316\u7528\uff09\"\"\"\n    with torch.no_grad():\n        _, attn_weights = self.forward(query, key, need_weights=True)\n    return attn_weights\n</code></pre> <p>class FlashAttentionDemo:     \"\"\"Flash Attention\u306e\u6982\u5ff5\u8aac\u660e\"\"\"</p> <pre><code>def explain_flash_attention(self):\n    \"\"\"Flash Attention\u306e\u8aac\u660e\"\"\"\n    print(\"=== Flash Attention ===\\n\")\n\n    print(\"\u554f\u984c: \u6a19\u6e96\u7684\u306aAttention\u306e\u30e1\u30e2\u30ea\u30dc\u30c8\u30eb\u30cd\u30c3\u30af\")\n    print(\"- O(N\u00b2)\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\")\n    print(\"- GPU\u30e1\u30e2\u30ea\u5e2f\u57df\u5e45\u306e\u5236\u7d04\\n\")\n\n    print(\"Flash Attention\u306e\u89e3\u6c7a\u7b56:\")\n    print(\"1. \u30bf\u30a4\u30ea\u30f3\u30b0: \u5c0f\u3055\u306a\u30d6\u30ed\u30c3\u30af\u3067\u8a08\u7b97\")\n    print(\"2. \u518d\u8a08\u7b97: \u4e2d\u9593\u7d50\u679c\u3092\u4fdd\u5b58\u305b\u305a\u518d\u8a08\u7b97\")\n    print(\"3. \u30ab\u30fc\u30cd\u30eb\u878d\u5408: \u8907\u6570\u306e\u64cd\u4f5c\u30921\u3064\u306e\u30ab\u30fc\u30cd\u30eb\u306b\\n\")\n\n    # \u56f3\u89e3\n    self._visualize_flash_attention()\n\ndef _visualize_flash_attention(self):\n    \"\"\"Flash Attention\u306e\u52d5\u4f5c\u3092\u53ef\u8996\u5316\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # \u6a19\u6e96\u7684\u306aAttention\n    ax1.set_title('\u6a19\u6e96\u7684\u306aAttention', fontsize=12)\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(0, 10)\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\n    memory_blocks = [\n        {\"name\": \"Q\", \"pos\": (1, 7), \"size\": (2, 2), \"color\": \"lightblue\"},\n        {\"name\": \"K\", \"pos\": (1, 4), \"size\": (2, 2), \"color\": \"lightgreen\"},\n        {\"name\": \"QK\u1d40\", \"pos\": (4, 5.5), \"size\": (3, 3), \"color\": \"yellow\"},\n        {\"name\": \"Softmax\", \"pos\": (8, 5.5), \"size\": (3, 3), \"color\": \"orange\"}\n    ]\n\n    for block in memory_blocks:\n        rect = Rectangle(block[\"pos\"], block[\"size\"][0], block[\"size\"][1],\n                       facecolor=block[\"color\"], edgecolor='black', linewidth=2)\n        ax1.add_patch(rect)\n        ax1.text(block[\"pos\"][0] + block[\"size\"][0]/2,\n                block[\"pos\"][1] + block[\"size\"][1]/2,\n                block[\"name\"], ha='center', va='center')\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u8868\u793a\n    ax1.text(5, 1, '\u30e1\u30e2\u30ea: O(N\u00b2)', fontsize=14, weight='bold',\n            ha='center', color='red')\n\n    ax1.axis('off')\n\n    # Flash Attention\n    ax2.set_title('Flash Attention', fontsize=12)\n    ax2.set_xlim(0, 10)\n    ax2.set_ylim(0, 10)\n\n    # \u30bf\u30a4\u30ea\u30f3\u30b0\n    tile_size = 1.5\n    for i in range(2):\n        for j in range(2):\n            x = 2 + j * (tile_size + 0.5)\n            y = 4 + i * (tile_size + 0.5)\n\n            tile = Rectangle((x, y), tile_size, tile_size,\n                           facecolor='lightcyan', edgecolor='darkblue',\n                           linewidth=2, linestyle='--')\n            ax2.add_patch(tile)\n            ax2.text(x + tile_size/2, y + tile_size/2,\n                    f'Tile\\n{i},{j}', ha='center', va='center', fontsize=8)\n\n    # On-chip SRAM\n    sram = Rectangle((6, 4), 3, 3, facecolor='lightpink',\n                    edgecolor='darkred', linewidth=2)\n    ax2.add_patch(sram)\n    ax2.text(7.5, 5.5, 'On-chip\\nSRAM', ha='center', va='center')\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u8868\u793a\n    ax2.text(5, 1, '\u30e1\u30e2\u30ea: O(N)', fontsize=14, weight='bold',\n            ha='center', color='green')\n\n    ax2.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\u5229\u70b9:\")\n    print(\"\u2713 \u30e1\u30e2\u30ea\u52b9\u7387: O(N\u00b2) \u2192 O(N)\")\n    print(\"\u2713 \u901f\u5ea6\u5411\u4e0a: \u30e1\u30e2\u30ea\u5e2f\u57df\u5e45\u306e\u6709\u52b9\u6d3b\u7528\")\n    print(\"\u2713 \u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u51e6\u7406\u304c\u53ef\u80fd\")\n</code></pre>"},{"location":"part4/component-implementation/#142","title":"14.2 \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u767a\u5c55","text":"<p>class AdvancedPositionalEncoding:     \"\"\"\u9ad8\u5ea6\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 128\n    self.max_len = 100\n\ndef compare_encoding_methods(self):\n    \"\"\"\u7570\u306a\u308b\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n    print(\"=== \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u624b\u6cd5\u306e\u6bd4\u8f03 ===\\n\")\n\n    methods = {\n        \"Sinusoidal\": self._sinusoidal_encoding,\n        \"Learned\": self._learned_encoding,\n        \"RoPE\": self._rope_encoding,\n        \"ALiBi\": self._alibi_encoding\n    }\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n\n    for idx, (name, method) in enumerate(methods.items()):\n        ax = axes[idx]\n        encoding = method()\n\n        # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n        im = ax.imshow(encoding[:20, :64], cmap='RdBu_r', \n                      aspect='auto', vmin=-1, vmax=1)\n\n        ax.set_title(f'{name} Encoding', fontsize=12)\n        ax.set_xlabel('Dimension')\n        ax.set_ylabel('Position')\n\n        # \u30ab\u30e9\u30fc\u30d0\u30fc\n        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n\n    self._explain_each_method()\n\ndef _sinusoidal_encoding(self):\n    \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n    pe = torch.zeros(self.max_len, self.d_model)\n    position = torch.arange(0, self.max_len).unsqueeze(1).float()\n\n    div_term = torch.exp(\n        torch.arange(0, self.d_model, 2).float() *\n        -(math.log(10000.0) / self.d_model)\n    )\n\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n\n    return pe.numpy()\n\ndef _learned_encoding(self):\n    \"\"\"\u5b66\u7fd2\u53ef\u80fd\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n    # \u30e9\u30f3\u30c0\u30e0\u521d\u671f\u5316\uff08\u5b9f\u969b\u306f\u5b66\u7fd2\u3055\u308c\u308b\uff09\n    pe = torch.randn(self.max_len, self.d_model) * 0.1\n    return pe.numpy()\n\ndef _rope_encoding(self):\n    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n    # \u7c21\u7565\u5316\u3057\u305f\u5b9f\u88c5\n    freqs = 1.0 / (10000 ** (torch.arange(0, self.d_model, 2).float() / self.d_model))\n    positions = torch.arange(self.max_len).float()\n\n    # \u56de\u8ee2\u884c\u5217\u306e\u8981\u7d20\n    angles = positions.unsqueeze(1) * freqs.unsqueeze(0)\n    pe = torch.zeros(self.max_len, self.d_model)\n    pe[:, 0::2] = torch.cos(angles)\n    pe[:, 1::2] = torch.sin(angles)\n\n    return pe.numpy()\n\ndef _alibi_encoding(self):\n    \"\"\"Attention with Linear Biases (ALiBi)\"\"\"\n    # \u76f8\u5bfe\u4f4d\u7f6e\u30d0\u30a4\u30a2\u30b9\n    positions = torch.arange(self.max_len)\n    relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)\n\n    # \u7dda\u5f62\u30d0\u30a4\u30a2\u30b9\uff08\u30d8\u30c3\u30c9\u3054\u3068\u306b\u7570\u306a\u308b\u30b9\u30ed\u30fc\u30d7\uff09\n    slopes = torch.tensor([2**(-i/4) for i in range(8)])  # 8\u30d8\u30c3\u30c9\u306e\u4f8b\n    biases = relative_positions.unsqueeze(0) * slopes.unsqueeze(1).unsqueeze(2)\n\n    # \u53ef\u8996\u5316\u7528\u306b\u6700\u521d\u306e\u30d8\u30c3\u30c9\u306e\u30d0\u30a4\u30a2\u30b9\u3092\u8fd4\u3059\n    return biases[0].numpy()[:self.max_len, :self.d_model]\n\ndef _explain_each_method(self):\n    \"\"\"\u5404\u624b\u6cd5\u306e\u8aac\u660e\"\"\"\n    print(\"\\n\u624b\u6cd5\u306e\u7279\u5fb4:\\n\")\n\n    print(\"1. Sinusoidal (\u6b63\u5f26\u6ce2):\")\n    print(\"   \u2713 \u5b66\u7fd2\u4e0d\u8981\")\n    print(\"   \u2713 \u4efb\u610f\u306e\u9577\u3055\u306b\u5916\u633f\u53ef\u80fd\")\n    print(\"   \u2713 \u76f8\u5bfe\u4f4d\u7f6e\u306e\u8a08\u7b97\u304c\u53ef\u80fd\\n\")\n\n    print(\"2. Learned (\u5b66\u7fd2\u578b):\")\n    print(\"   \u2713 \u30bf\u30b9\u30af\u306b\u6700\u9069\u5316\")\n    print(\"   \u2717 \u56fa\u5b9a\u9577\")\n    print(\"   \u2717 \u5916\u633f\u6027\u80fd\u304c\u4f4e\u3044\\n\")\n\n    print(\"3. RoPE (\u56de\u8ee2\u4f4d\u7f6e\u57cb\u3081\u8fbc\u307f):\")\n    print(\"   \u2713 \u76f8\u5bfe\u4f4d\u7f6e\u3092\u81ea\u7136\u306b\u8868\u73fe\")\n    print(\"   \u2713 \u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306b\u5f37\u3044\")\n    print(\"   \u2713 \u8a08\u7b97\u52b9\u7387\u304c\u826f\u3044\\n\")\n\n    print(\"4. ALiBi (\u7dda\u5f62\u30d0\u30a4\u30a2\u30b9):\")\n    print(\"   \u2713 \u975e\u5e38\u306b\u30b7\u30f3\u30d7\u30eb\")\n    print(\"   \u2713 \u5916\u633f\u6027\u80fd\u304c\u9ad8\u3044\")\n    print(\"   \u2713 \u57cb\u3081\u8fbc\u307f\u3067\u306f\u306a\u304f\u30d0\u30a4\u30a2\u30b9\u3068\u3057\u3066\u4f5c\u7528\")\n</code></pre> <p>class RoPEImplementation(nn.Module):     \"\"\"Rotary Position Embedding (RoPE) \u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, d_model: int, max_seq_len: int = 5000, base: float = 10000):\n    super().__init__()\n    self.d_model = d_model\n    self.max_seq_len = max_seq_len\n    self.base = base\n\n    # \u4e8b\u524d\u8a08\u7b97\n    self._precompute_freqs()\n\ndef _precompute_freqs(self):\n    \"\"\"\u5468\u6ce2\u6570\u306e\u4e8b\u524d\u8a08\u7b97\"\"\"\n    # \u5468\u6ce2\u6570\n    theta = torch.arange(0, self.d_model, 2).float()\n    freqs = 1.0 / (self.base ** (theta / self.d_model))\n\n    # \u4f4d\u7f6e\n    positions = torch.arange(self.max_seq_len).float()\n\n    # \u5468\u6ce2\u6570\u3068\u4f4d\u7f6e\u306e\u7a4d\n    freqs = torch.outer(positions, freqs)\n\n    # cos\u3068sin\u3092\u4e8b\u524d\u8a08\u7b97\n    self.register_buffer('cos_cached', torch.cos(freqs))\n    self.register_buffer('sin_cached', torch.sin(freqs))\n\ndef forward(self, x: torch.Tensor, seq_len: Optional[int] = None) -&gt; torch.Tensor:\n    \"\"\"\n    RoPE\u3092\u9069\u7528\n    Args:\n        x: [batch_size, seq_len, n_heads, d_head]\n        seq_len: \u30b7\u30fc\u30b1\u30f3\u30b9\u9577\uff08None\u306e\u5834\u5408\u306fx\u304b\u3089\u63a8\u5b9a\uff09\n    \"\"\"\n    if seq_len is None:\n        seq_len = x.shape[1]\n\n    # \u9069\u5207\u306a\u30b5\u30a4\u30ba\u306b\u30b9\u30e9\u30a4\u30b9\n    cos = self.cos_cached[:seq_len].unsqueeze(1)  # [seq_len, 1, d_model/2]\n    sin = self.sin_cached[:seq_len].unsqueeze(1)\n\n    # x\u3092\u5076\u6570\u30fb\u5947\u6570\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5206\u5272\n    x_even = x[..., 0::2]\n    x_odd = x[..., 1::2]\n\n    # \u56de\u8ee2\u3092\u9069\u7528\n    x_rotated = torch.stack([\n        x_even * cos - x_odd * sin,\n        x_even * sin + x_odd * cos\n    ], dim=-1)\n\n    # \u5143\u306e\u5f62\u72b6\u306b\u623b\u3059\n    x_rotated = x_rotated.flatten(-2)\n\n    return x_rotated\n\ndef rotate_queries_keys(self, q: torch.Tensor, k: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\u30af\u30a8\u30ea\u3068\u30ad\u30fc\u306bRoPE\u3092\u9069\u7528\"\"\"\n    q_rotated = self.forward(q)\n    k_rotated = self.forward(k)\n    return q_rotated, k_rotated\n</code></pre>"},{"location":"part4/component-implementation/#143","title":"14.3 \u5c64\u6b63\u898f\u5316\u3068\u305d\u306e\u5909\u7a2e","text":"<p>class NormalizationTechniques:     \"\"\"\u6b63\u898f\u5316\u6280\u8853\u306e\u5b9f\u88c5\u3068\u6bd4\u8f03\"\"\"</p> <pre><code>def compare_normalization_methods(self):\n    \"\"\"\u7570\u306a\u308b\u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n    print(\"=== \u6b63\u898f\u5316\u624b\u6cd5\u306e\u6bd4\u8f03 ===\\n\")\n\n    batch_size = 32\n    seq_len = 100\n    d_model = 512\n\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n    x = torch.randn(batch_size, seq_len, d_model) * 3 + 1\n\n    # \u5404\u6b63\u898f\u5316\u624b\u6cd5\n    methods = {\n        \"LayerNorm\": nn.LayerNorm(d_model),\n        \"RMSNorm\": self._create_rmsnorm(d_model),\n        \"GroupNorm\": nn.GroupNorm(8, d_model)  # 8\u30b0\u30eb\u30fc\u30d7\n    }\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    for idx, (name, norm) in enumerate(methods.items()):\n        ax = axes[idx]\n\n        # \u6b63\u898f\u5316\u524d\u5f8c\u306e\u5206\u5e03\n        with torch.no_grad():\n            if name == \"GroupNorm\":\n                # GroupNorm\u306f\u7570\u306a\u308b\u5f62\u72b6\u3092\u671f\u5f85\n                x_reshaped = x.transpose(1, 2)  # [B, D, L]\n                normalized = norm(x_reshaped)\n                normalized = normalized.transpose(1, 2)  # [B, L, D]\u306b\u623b\u3059\n            else:\n                normalized = norm(x)\n\n        # \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\n        ax.hist(x.flatten().numpy(), bins=50, alpha=0.5, \n               label='Before', density=True, color='red')\n        ax.hist(normalized.flatten().numpy(), bins=50, alpha=0.5,\n               label='After', density=True, color='blue')\n\n        ax.set_title(f'{name}')\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Density')\n        ax.legend()\n\n        # \u7d71\u8a08\u60c5\u5831\n        mean_before = x.mean().item()\n        std_before = x.std().item()\n        mean_after = normalized.mean().item()\n        std_after = normalized.std().item()\n\n        ax.text(0.02, 0.98, \n               f'Before: \u03bc={mean_before:.2f}, \u03c3={std_before:.2f}\\n'\n               f'After: \u03bc={mean_after:.2f}, \u03c3={std_after:.2f}',\n               transform=ax.transAxes, verticalalignment='top',\n               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    plt.tight_layout()\n    plt.show()\n\n    self._explain_normalization_differences()\n\ndef _create_rmsnorm(self, d_model: int) -&gt; nn.Module:\n    \"\"\"RMSNorm\u306e\u5b9f\u88c5\"\"\"\n    class RMSNorm(nn.Module):\n        def __init__(self, d_model: int, eps: float = 1e-6):\n            super().__init__()\n            self.eps = eps\n            self.weight = nn.Parameter(torch.ones(d_model))\n\n        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            # RMS\u8a08\u7b97\n            rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n            x = x / rms\n            return x * self.weight\n\n    return RMSNorm(d_model)\n\ndef _explain_normalization_differences(self):\n    \"\"\"\u6b63\u898f\u5316\u624b\u6cd5\u306e\u9055\u3044\u3092\u8aac\u660e\"\"\"\n    print(\"\\n\u5404\u624b\u6cd5\u306e\u7279\u5fb4:\\n\")\n\n    print(\"1. Layer Normalization:\")\n    print(\"   - \u5404\u30b5\u30f3\u30d7\u30eb\u306e\u7279\u5fb4\u6b21\u5143\u3067\u6b63\u898f\u5316\")\n    print(\"   - \u5e73\u5747\u3068\u5206\u6563\u3092\u4f7f\u7528\")\n    print(\"   - \u5b66\u7fd2\u53ef\u80fd\u306a\u30b9\u30b1\u30fc\u30eb\u30fb\u30b7\u30d5\u30c8\u30d1\u30e9\u30e1\u30fc\u30bf\\n\")\n\n    print(\"2. RMS Normalization:\")\n    print(\"   - \u5e73\u5747\u3092\u8a08\u7b97\u3057\u306a\u3044\uff08\u8a08\u7b97\u52b9\u7387\u304c\u826f\u3044\uff09\")\n    print(\"   - RMS\u306e\u307f\u3067\u6b63\u898f\u5316\")\n    print(\"   - LLaMA\u306a\u3069\u6700\u65b0\u30e2\u30c7\u30eb\u3067\u63a1\u7528\\n\")\n\n    print(\"3. Group Normalization:\")\n    print(\"   - \u30c1\u30e3\u30cd\u30eb\u3092\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u3051\u3066\u6b63\u898f\u5316\")\n    print(\"   - \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\u3057\u306a\u3044\")\n    print(\"   - Vision Transformer\u3067\u3088\u304f\u4f7f\u7528\")\n</code></pre> <p>class PrePostNormComparison:     \"\"\"Pre-Norm vs Post-Norm\u306e\u6bd4\u8f03\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 256\n    self.n_heads = 8\n\ndef create_pre_norm_block(self) -&gt; nn.Module:\n    \"\"\"Pre-Norm\u30d6\u30ed\u30c3\u30af\"\"\"\n    class PreNormBlock(nn.Module):\n        def __init__(self, d_model, n_heads):\n            super().__init__()\n            self.norm1 = nn.LayerNorm(d_model)\n            self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n            self.norm2 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, 4 * d_model),\n                nn.ReLU(),\n                nn.Linear(4 * d_model, d_model)\n            )\n\n        def forward(self, x):\n            # Pre-Norm: \u6b63\u898f\u5316\u3057\u3066\u304b\u3089\u51e6\u7406\n            attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n            x = x + attn_out\n\n            ffn_out = self.ffn(self.norm2(x))\n            x = x + ffn_out\n\n            return x\n\n    return PreNormBlock(self.d_model, self.n_heads)\n\ndef create_post_norm_block(self) -&gt; nn.Module:\n    \"\"\"Post-Norm\u30d6\u30ed\u30c3\u30af\"\"\"\n    class PostNormBlock(nn.Module):\n        def __init__(self, d_model, n_heads):\n            super().__init__()\n            self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n            self.norm1 = nn.LayerNorm(d_model)\n            self.ffn = nn.Sequential(\n                nn.Linear(d_model, 4 * d_model),\n                nn.ReLU(),\n                nn.Linear(4 * d_model, d_model)\n            )\n            self.norm2 = nn.LayerNorm(d_model)\n\n        def forward(self, x):\n            # Post-Norm: \u51e6\u7406\u3057\u3066\u304b\u3089\u6b63\u898f\u5316\n            attn_out, _ = self.attn(x, x, x)\n            x = self.norm1(x + attn_out)\n\n            ffn_out = self.ffn(x)\n            x = self.norm2(x + ffn_out)\n\n            return x\n\n    return PostNormBlock(self.d_model, self.n_heads)\n\ndef compare_gradient_flow(self):\n    \"\"\"\u52fe\u914d\u30d5\u30ed\u30fc\u306e\u6bd4\u8f03\"\"\"\n    print(\"=== Pre-Norm vs Post-Norm \u52fe\u914d\u30d5\u30ed\u30fc ===\\n\")\n\n    # \u30e2\u30c7\u30eb\u4f5c\u6210\n    pre_norm_model = nn.Sequential(*[\n        self.create_pre_norm_block() for _ in range(12)\n    ])\n    post_norm_model = nn.Sequential(*[\n        self.create_post_norm_block() for _ in range(12)\n    ])\n\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n    batch_size = 8\n    seq_len = 50\n    x = torch.randn(batch_size, seq_len, self.d_model)\n\n    # \u52fe\u914d\u3092\u8a08\u7b97\n    models = {\"Pre-Norm\": pre_norm_model, \"Post-Norm\": post_norm_model}\n    gradients = {}\n\n    for name, model in models.items():\n        model.zero_grad()\n        output = model(x)\n        loss = output.mean()\n        loss.backward()\n\n        # \u5404\u5c64\u306e\u52fe\u914d\u30ce\u30eb\u30e0\u3092\u8a18\u9332\n        grad_norms = []\n        for i, layer in enumerate(model):\n            # \u6700\u521d\u306eLinear\u5c64\u306e\u52fe\u914d\u3092\u53d6\u5f97\n            if hasattr(layer, 'attn'):\n                grad_norm = layer.attn.in_proj_weight.grad.norm().item()\n                grad_norms.append(grad_norm)\n\n        gradients[name] = grad_norms\n\n    # \u53ef\u8996\u5316\n    self._plot_gradient_comparison(gradients)\n\ndef _plot_gradient_comparison(self, gradients: Dict[str, List[float]]):\n    \"\"\"\u52fe\u914d\u306e\u6bd4\u8f03\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    plt.figure(figsize=(10, 6))\n\n    for name, grad_norms in gradients.items():\n        layers = range(1, len(grad_norms) + 1)\n        plt.plot(layers, grad_norms, marker='o', label=name, linewidth=2)\n\n    plt.xlabel('Layer')\n    plt.ylabel('Gradient Norm')\n    plt.title('\u52fe\u914d\u30ce\u30eb\u30e0\u306e\u5c64\u3054\u3068\u306e\u5909\u5316')\n    plt.legend()\n    plt.yscale('log')\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\u89b3\u5bdf:\")\n    print(\"\u2022 Pre-Norm: \u3088\u308a\u5b89\u5b9a\u3057\u305f\u52fe\u914d\u30d5\u30ed\u30fc\")\n    print(\"\u2022 Post-Norm: \u6df1\u3044\u5c64\u3067\u52fe\u914d\u304c\u6e1b\u8870\u3057\u3084\u3059\u3044\")\n    print(\"\u2022 \u6df1\u3044\u30e2\u30c7\u30eb\u3067\u306fPre-Norm\u304c\u63a8\u5968\u3055\u308c\u308b\")\n</code></pre>"},{"location":"part4/component-implementation/#144-ffn","title":"14.4 \u9ad8\u5ea6\u306aFFN\u5b9f\u88c5","text":"<p>class AdvancedFFNImplementations:     \"\"\"\u9ad8\u5ea6\u306aFeed Forward Network\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    self.d_model = 512\n    self.d_ff = 2048\n\ndef implement_glu_variants(self):\n    \"\"\"GLU\u7cfb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5\"\"\"\n    print(\"=== GLU\u7cfb\u6d3b\u6027\u5316\u95a2\u6570 ===\\n\")\n\n    class GLU(nn.Module):\n        \"\"\"Gated Linear Unit\"\"\"\n        def __init__(self, d_model: int, d_ff: int):\n            super().__init__()\n            self.linear1 = nn.Linear(d_model, d_ff * 2)\n            self.linear2 = nn.Linear(d_ff, d_model)\n\n        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            x = self.linear1(x)\n            x, gate = x.chunk(2, dim=-1)\n            x = x * torch.sigmoid(gate)\n            x = self.linear2(x)\n            return x\n\n    class SwiGLU(nn.Module):\n        \"\"\"SwiGLU (Swish-Gated Linear Unit)\"\"\"\n        def __init__(self, d_model: int, d_ff: int):\n            super().__init__()\n            self.linear1 = nn.Linear(d_model, d_ff * 2)\n            self.linear2 = nn.Linear(d_ff, d_model)\n\n        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            x = self.linear1(x)\n            x, gate = x.chunk(2, dim=-1)\n            x = x * F.silu(gate)  # SiLU = x * sigmoid(x)\n            x = self.linear2(x)\n            return x\n\n    class GeGLU(nn.Module):\n        \"\"\"GeGLU (GELU-Gated Linear Unit)\"\"\"\n        def __init__(self, d_model: int, d_ff: int):\n            super().__init__()\n            self.linear1 = nn.Linear(d_model, d_ff * 2)\n            self.linear2 = nn.Linear(d_ff, d_model)\n\n        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            x = self.linear1(x)\n            x, gate = x.chunk(2, dim=-1)\n            x = x * F.gelu(gate)\n            x = self.linear2(x)\n            return x\n\n    # \u6bd4\u8f03\n    self._compare_glu_variants(GLU, SwiGLU, GeGLU)\n\ndef _compare_glu_variants(self, *glu_classes):\n    \"\"\"GLU\u5909\u7a2e\u306e\u6bd4\u8f03\"\"\"\n    x = torch.randn(1, 100, self.d_model)\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    for idx, glu_class in enumerate(glu_classes):\n        ax = axes[idx]\n\n        # \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        model = glu_class(self.d_model, self.d_ff)\n\n        # \u9806\u4f1d\u64ad\n        with torch.no_grad():\n            output = model(x)\n\n        # \u6d3b\u6027\u5316\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\n        # \u4e2d\u9593\u5c64\u306e\u6d3b\u6027\u5316\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30d5\u30c3\u30af\n        activations = []\n        def hook(module, input, output):\n            if hasattr(output, 'chunk'):\n                x, gate = output.chunk(2, dim=-1)\n                activations.append(gate)\n\n        handle = model.linear1.register_forward_hook(hook)\n        with torch.no_grad():\n            _ = model(x)\n        handle.remove()\n\n        if activations:\n            gate_values = activations[0][0, :, :100].numpy()\n            im = ax.imshow(gate_values.T, cmap='RdBu_r', aspect='auto',\n                          vmin=-2, vmax=2)\n            ax.set_title(f'{glu_class.__name__}')\n            ax.set_xlabel('Position')\n            ax.set_ylabel('Hidden Dim (first 100)')\n            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"GLU\u7cfb\u306e\u5229\u70b9:\")\n    print(\"\u2713 \u3088\u308a\u8868\u73fe\u529b\u306e\u9ad8\u3044\u975e\u7dda\u5f62\u5909\u63db\")\n    print(\"\u2713 \u52fe\u914d\u30d5\u30ed\u30fc\u306e\u6539\u5584\")\n    print(\"\u2713 \u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\u5411\u4e0a\")\n</code></pre> <p>class MixtureOfExperts(nn.Module):     \"\"\"Mixture of Experts (MoE) \u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, d_model: int, d_ff: int, n_experts: int = 8, \n             top_k: int = 2):\n    super().__init__()\n    self.d_model = d_model\n    self.n_experts = n_experts\n    self.top_k = top_k\n\n    # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\uff08\u5404\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306fFFN\uff09\n    self.experts = nn.ModuleList([\n        nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        ) for _ in range(n_experts)\n    ])\n\n    # \u30b2\u30fc\u30c6\u30a3\u30f3\u30b0\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n    self.gate = nn.Linear(d_model, n_experts)\n\n    # \u30ce\u30a4\u30ba\uff08\u5b66\u7fd2\u6642\uff09\n    self.noise_std = 0.1\n\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n        x: [batch_size, seq_len, d_model]\n    Returns:\n        output: [batch_size, seq_len, d_model]\n    \"\"\"\n    batch_size, seq_len, d_model = x.shape\n\n    # \u30b2\u30fc\u30c8\u5024\u306e\u8a08\u7b97\n    gate_logits = self.gate(x)  # [B, L, n_experts]\n\n    # \u30ce\u30a4\u30ba\u3092\u8ffd\u52a0\uff08\u5b66\u7fd2\u6642\uff09\n    if self.training:\n        noise = torch.randn_like(gate_logits) * self.noise_std\n        gate_logits = gate_logits + noise\n\n    # Top-k\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3092\u9078\u629e\n    topk_gate_values, topk_indices = torch.topk(\n        gate_logits, self.top_k, dim=-1\n    )  # [B, L, top_k]\n\n    # Softmax\u3067\u6b63\u898f\u5316\n    topk_gate_values = F.softmax(topk_gate_values, dim=-1)\n\n    # \u51fa\u529b\u306e\u521d\u671f\u5316\n    output = torch.zeros_like(x)\n\n    # \u5404\u30c8\u30fc\u30af\u30f3\u306b\u5bfe\u3057\u3066Top-k\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3092\u9069\u7528\n    for i in range(self.top_k):\n        # \u9078\u629e\u3055\u308c\u305f\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n        expert_idx = topk_indices[..., i]  # [B, L]\n        gate_value = topk_gate_values[..., i:i+1]  # [B, L, 1]\n\n        # \u30d0\u30c3\u30c1\u51e6\u7406\u306e\u305f\u3081\u3001\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5316\n        for e in range(self.n_experts):\n            # \u3053\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u304c\u9078\u629e\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u306e\u30de\u30b9\u30af\n            mask = (expert_idx == e)\n            if mask.any():\n                # \u30de\u30b9\u30af\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u3092\u62bd\u51fa\n                masked_x = x[mask]\n                # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3092\u9069\u7528\n                expert_out = self.experts[e](masked_x)\n                # \u30b2\u30fc\u30c8\u5024\u3067\u91cd\u307f\u4ed8\u3051\u3057\u3066\u51fa\u529b\u306b\u52a0\u7b97\n                output[mask] += expert_out * gate_value[mask]\n\n    return output\n\ndef analyze_expert_usage(self, x: torch.Tensor):\n    \"\"\"\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u306e\u4f7f\u7528\u72b6\u6cc1\u3092\u5206\u6790\"\"\"\n    with torch.no_grad():\n        gate_logits = self.gate(x)\n        _, topk_indices = torch.topk(gate_logits, self.top_k, dim=-1)\n\n        # \u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3054\u3068\u306e\u9078\u629e\u56de\u6570\n        expert_counts = torch.zeros(self.n_experts)\n        for i in range(self.n_experts):\n            expert_counts[i] = (topk_indices == i).sum()\n\n        # \u53ef\u8996\u5316\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(self.n_experts), expert_counts.numpy())\n        plt.xlabel('Expert ID')\n        plt.ylabel('Selection Count')\n        plt.title('Expert Usage Distribution')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n        return expert_counts\n</code></pre>"},{"location":"part4/component-implementation/#_3","title":"\u5b9f\u884c\u4f8b","text":"<p>def main():     \"\"\"\u30e1\u30a4\u30f3\u5b9f\u884c\u95a2\u6570\"\"\"     print(\"=\" * 70)     print(\"\u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u8a73\u7d30\u5b9f\u88c5\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># Multi-Head Attention\nmha_demo = MultiHeadAttentionImplementation()\nmha_demo.explain_multi_head_benefits()\n\n# \u6700\u9069\u5316\u3055\u308c\u305fMulti-Head Attention\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nprint(\"=== \u6700\u9069\u5316\u3055\u308c\u305fMulti-Head Attention ===\\n\")\n\nmodel = OptimizedMultiHeadAttention(d_model=512, n_heads=8)\nx = torch.randn(2, 100, 512)\noutput, _ = model(x)\nprint(f\"\u5165\u529b\u5f62\u72b6: {x.shape}\")\nprint(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\n\n# Flash Attention\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nflash_demo = FlashAttentionDemo()\nflash_demo.explain_flash_attention()\n\n# \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\npos_demo = AdvancedPositionalEncoding()\npos_demo.compare_encoding_methods()\n\n# RoPE\u5b9f\u88c5\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nprint(\"=== RoPE\u5b9f\u88c5\u4f8b ===\\n\")\nrope = RoPEImplementation(d_model=128)\nx = torch.randn(2, 50, 8, 16)  # [batch, seq_len, n_heads, d_head]\nx_rotated = rope(x)\nprint(f\"RoPE\u9069\u7528\u524d: {x.shape}\")\nprint(f\"RoPE\u9069\u7528\u5f8c: {x_rotated.shape}\")\n\n# \u6b63\u898f\u5316\u624b\u6cd5\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nnorm_demo = NormalizationTechniques()\nnorm_demo.compare_normalization_methods()\n\n# Pre-Norm vs Post-Norm\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nnorm_comparison = PrePostNormComparison()\nnorm_comparison.compare_gradient_flow()\n\n# \u9ad8\u5ea6\u306aFFN\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nffn_demo = AdvancedFFNImplementations()\nffn_demo.implement_glu_variants()\n\n# MoE\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\nprint(\"=== Mixture of Experts ===\\n\")\nmoe = MixtureOfExperts(d_model=256, d_ff=1024, n_experts=8, top_k=2)\nx = torch.randn(4, 50, 256)\noutput = moe(x)\nprint(f\"MoE\u5165\u529b: {x.shape}\")\nprint(f\"MoE\u51fa\u529b: {output.shape}\")\nmoe.analyze_expert_usage(x)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081:\")\nprint(\"\u2022 \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306b\u306f\u591a\u69d8\u306a\u5b9f\u88c5\u65b9\u6cd5\u304c\u5b58\u5728\")\nprint(\"\u2022 \u30bf\u30b9\u30af\u3084\u30ea\u30bd\u30fc\u30b9\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u624b\u6cd5\u3092\u9078\u629e\")\nprint(\"\u2022 \u6700\u65b0\u306e\u7814\u7a76\u6210\u679c\u3092\u53d6\u308a\u5165\u308c\u308b\u3053\u3068\u3067\u6027\u80fd\u5411\u4e0a\")\nprint(\"\u2022 \u5b9f\u88c5\u306e\u8a73\u7d30\u304c\u6027\u80fd\u306b\u5927\u304d\u304f\u5f71\u97ff\")\n</code></pre> <p>if name == \"main\":     main()</p>"},{"location":"part4/debugging-visualization/","title":"\u30c7\u30d0\u30c3\u30b0\u3068\u53ef\u8996\u5316","text":""},{"location":"part4/debugging-visualization/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u898b\u3048\u306a\u3044\u3082\u306e\u3092\u898b\u308b","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u30c7\u30d0\u30c3\u30b0\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u69cb\u6587\u6728\u3092\u53ef\u8996\u5316\u3057\u3001\u4e2d\u9593\u8868\u73fe\u3092\u30c0\u30f3\u30d7\u3057\u3001\u6700\u9069\u5316\u306e\u5404\u6bb5\u968e\u3092\u8ffd\u8de1\u3059\u308b\u3053\u3068\u3067\u3001\u8907\u96d1\u306a\u5909\u63db\u904e\u7a0b\u3092\u7406\u89e3\u3067\u304d\u307e\u3059\u3002print\u6587\u30c7\u30d0\u30c3\u30b0\u304b\u3089\u59cb\u3081\u3066\u3001\u6700\u7d42\u7684\u306b\u306f\u6d17\u7df4\u3055\u308c\u305f\u30c7\u30d0\u30c3\u30ac\u30fc\u3084\u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\u3092\u4f7f\u3046\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002</p> <p>\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u3001\u7279\u306bTransformer\u306e\u3088\u3046\u306a\u8907\u96d1\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3067\u306f\u3001\u5185\u90e8\u3067\u4f55\u304c\u8d77\u304d\u3066\u3044\u308b\u304b\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u304c\u6210\u529f\u306e\u9375\u3067\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001Transformer\u3092\u30c7\u30d0\u30c3\u30b0\u3057\u3001\u305d\u306e\u52d5\u4f5c\u3092\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306e\u5b9f\u8df5\u7684\u306a\u6280\u8853\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part4/debugging-visualization/#151","title":"15.1 \u6ce8\u610f\u6a5f\u69cb\u306e\u53ef\u8996\u5316","text":""},{"location":"part4/debugging-visualization/#attention-weight","title":"Attention Weight\u306e\u8a73\u7d30\u5206\u6790","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Dict, List, Tuple, Optional, Any import matplotlib.patches as mpatches from matplotlib.patches import Rectangle, Circle, FancyBboxPatch from matplotlib.collections import LineCollection import matplotlib.cm as cm from IPython.display import HTML, display import ipywidgets as widgets import warnings warnings.filterwarnings('ignore')</p> <p>class AttentionVisualizer:     \"\"\"\u6ce8\u610f\u6a5f\u69cb\u306e\u5305\u62ec\u7684\u306a\u53ef\u8996\u5316\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: nn.Module):\n    self.model = model\n    self.attention_weights = {}\n    self.hooks = []\n\ndef register_hooks(self):\n    \"\"\"\u6ce8\u610f\u91cd\u307f\u3092\u8a18\u9332\u3059\u308b\u30d5\u30c3\u30af\u3092\u767b\u9332\"\"\"\n    def create_hook(name):\n        def hook_fn(module, input, output):\n            # output\u306f(output, attention_weights)\u306e\u30bf\u30d7\u30eb\n            if isinstance(output, tuple) and len(output) == 2:\n                _, attn_weights = output\n                if attn_weights is not None:\n                    self.attention_weights[name] = attn_weights.detach()\n            return output\n        return hook_fn\n\n    # \u3059\u3079\u3066\u306eMultiHeadAttention\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u30d5\u30c3\u30af\u3092\u767b\u9332\n    for name, module in self.model.named_modules():\n        if isinstance(module, nn.MultiheadAttention):\n            hook = module.register_forward_hook(create_hook(name))\n            self.hooks.append(hook)\n\ndef remove_hooks(self):\n    \"\"\"\u30d5\u30c3\u30af\u3092\u524a\u9664\"\"\"\n    for hook in self.hooks:\n        hook.remove()\n    self.hooks = []\n\ndef visualize_attention_pattern(self, tokens: List[str], \n                              layer_name: str = None):\n    \"\"\"\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u53ef\u8996\u5316\"\"\"\n    if layer_name is None:\n        # \u6700\u521d\u306e\u5c64\u3092\u4f7f\u7528\n        layer_name = list(self.attention_weights.keys())[0]\n\n    attn_weights = self.attention_weights[layer_name]\n\n    # \u30d0\u30c3\u30c1\u306e\u6700\u521d\u306e\u30b5\u30f3\u30d7\u30eb\u3001\u3059\u3079\u3066\u306e\u30d8\u30c3\u30c9\u306e\u5e73\u5747\n    if attn_weights.dim() == 4:  # [batch, heads, seq, seq]\n        attn_weights = attn_weights[0].mean(dim=0)\n    elif attn_weights.dim() == 3:  # [batch, seq, seq]\n        attn_weights = attn_weights[0]\n\n    attn_weights = attn_weights.cpu().numpy()\n\n    # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(attn_weights, xticklabels=tokens, yticklabels=tokens,\n               cmap='Blues', cbar_kws={'label': 'Attention Weight'})\n    plt.title(f'Attention Pattern - {layer_name}')\n    plt.xlabel('Keys (Attended to)')\n    plt.ylabel('Queries (Attending from)')\n    plt.tight_layout()\n    plt.show()\n\ndef visualize_head_diversity(self, tokens: List[str], \n                           layer_name: str = None):\n    \"\"\"\u5404\u30d8\u30c3\u30c9\u306e\u591a\u69d8\u6027\u3092\u53ef\u8996\u5316\"\"\"\n    if layer_name is None:\n        layer_name = list(self.attention_weights.keys())[0]\n\n    attn_weights = self.attention_weights[layer_name]\n\n    if attn_weights.dim() == 4:\n        attn_weights = attn_weights[0]  # \u6700\u521d\u306e\u30d0\u30c3\u30c1\n    else:\n        print(\"Multi-head\u60c5\u5831\u304c\u3042\u308a\u307e\u305b\u3093\")\n        return\n\n    n_heads = attn_weights.shape[0]\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.flatten()\n\n    for head_idx in range(min(n_heads, 8)):\n        ax = axes[head_idx]\n        head_weights = attn_weights[head_idx].cpu().numpy()\n\n        im = ax.imshow(head_weights, cmap='Blues', aspect='auto')\n        ax.set_title(f'Head {head_idx + 1}')\n\n        # \u7c21\u7565\u5316\u306e\u305f\u3081\u3001\u30e9\u30d9\u30eb\u306f\u6700\u521d\u3068\u6700\u5f8c\u306e\u30d8\u30c3\u30c9\u306e\u307f\n        if head_idx == 0:\n            ax.set_yticks(range(len(tokens)))\n            ax.set_yticklabels(tokens, fontsize=8)\n        else:\n            ax.set_yticks([])\n\n        if head_idx &gt;= 4:\n            ax.set_xticks(range(len(tokens)))\n            ax.set_xticklabels(tokens, rotation=45, fontsize=8)\n        else:\n            ax.set_xticks([])\n\n    plt.suptitle(f'Head Diversity - {layer_name}', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    # \u30d8\u30c3\u30c9\u9593\u306e\u985e\u4f3c\u6027\u3092\u8a08\u7b97\n    self._compute_head_similarity(attn_weights)\n\ndef _compute_head_similarity(self, attn_weights: torch.Tensor):\n    \"\"\"\u30d8\u30c3\u30c9\u9593\u306e\u985e\u4f3c\u6027\u3092\u8a08\u7b97\"\"\"\n    n_heads = attn_weights.shape[0]\n    similarity_matrix = torch.zeros(n_heads, n_heads)\n\n    for i in range(n_heads):\n        for j in range(n_heads):\n            # \u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\n            similarity = F.cosine_similarity(\n                attn_weights[i].flatten(),\n                attn_weights[j].flatten(),\n                dim=0\n            )\n            similarity_matrix[i, j] = similarity\n\n    # \u985e\u4f3c\u6027\u30de\u30c8\u30ea\u30c3\u30af\u30b9\u3092\u8868\u793a\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(similarity_matrix.numpy(), annot=True, fmt='.2f',\n               cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n               xticklabels=[f'H{i+1}' for i in range(n_heads)],\n               yticklabels=[f'H{i+1}' for i in range(n_heads)])\n    plt.title('Head Similarity Matrix')\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class AttentionFlowVisualizer:     \"\"\"\u6ce8\u610f\u306e\u6d41\u308c\u3092\u53ef\u8996\u5316\"\"\"</p> <pre><code>def create_attention_flow_diagram(self, tokens: List[str], \n                                attention_weights: np.ndarray):\n    \"\"\"\u6ce8\u610f\u306e\u6d41\u308c\u56f3\u3092\u4f5c\u6210\"\"\"\n    seq_len = len(tokens)\n    fig, ax = plt.subplots(figsize=(12, 8))\n\n    # \u30c8\u30fc\u30af\u30f3\u306e\u914d\u7f6e\n    y_positions = np.linspace(0.1, 0.9, seq_len)\n    x_left = 0.2\n    x_right = 0.8\n\n    # \u5de6\u5074\uff08Query\uff09\u3068\u53f3\u5074\uff08Key\uff09\u306b\u30c8\u30fc\u30af\u30f3\u3092\u914d\u7f6e\n    for i, (token, y) in enumerate(zip(tokens, y_positions)):\n        # Query\u5074\n        ax.text(x_left, y, token, ha='right', va='center',\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue'),\n               fontsize=12)\n        # Key\u5074\n        ax.text(x_right, y, token, ha='left', va='center',\n               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen'),\n               fontsize=12)\n\n    # \u6ce8\u610f\u306e\u77e2\u5370\u3092\u63cf\u753b\n    for i in range(seq_len):\n        for j in range(seq_len):\n            weight = attention_weights[i, j]\n            if weight &gt; 0.1:  # \u95be\u5024\n                # \u77e2\u5370\u306e\u592a\u3055\u3068\u900f\u660e\u5ea6\u3092\u91cd\u307f\u306b\u5fdc\u3058\u3066\u8abf\u6574\n                arrow = mpatches.FancyArrowPatch(\n                    (x_left + 0.05, y_positions[i]),\n                    (x_right - 0.05, y_positions[j]),\n                    connectionstyle=\"arc3,rad=0.2\",\n                    arrowstyle='-&gt;', \n                    mutation_scale=20,\n                    linewidth=weight * 5,\n                    alpha=weight,\n                    color='purple'\n                )\n                ax.add_patch(arrow)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.set_title('Attention Flow Visualization', fontsize=16, pad=20)\n\n    # \u51e1\u4f8b\n    ax.text(0.5, 0.02, 'Query \u2192 Key (arrow thickness = attention weight)',\n           ha='center', fontsize=10, style='italic')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class InteractiveAttentionExplorer:     \"\"\"\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u6ce8\u610f\u63a2\u7d22\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: nn.Module, tokenizer):\n    self.model = model\n    self.tokenizer = tokenizer\n    self.visualizer = AttentionVisualizer(model)\n\ndef create_interactive_widget(self):\n    \"\"\"\u5bfe\u8a71\u578b\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u4f5c\u6210\"\"\"\n    # \u30c6\u30ad\u30b9\u30c8\u5165\u529b\n    text_input = widgets.Textarea(\n        value='The cat sat on the mat.',\n        description='Text:',\n        layout=widgets.Layout(width='500px', height='80px')\n    )\n\n    # \u5c64\u9078\u629e\n    layer_dropdown = widgets.Dropdown(\n        options=['Layer 1', 'Layer 2', 'Layer 3', 'Layer 4'],\n        value='Layer 1',\n        description='Layer:'\n    )\n\n    # \u30d8\u30c3\u30c9\u9078\u629e\n    head_slider = widgets.IntSlider(\n        value=1,\n        min=1,\n        max=8,\n        step=1,\n        description='Head:',\n        continuous_update=False\n    )\n\n    # \u53ef\u8996\u5316\u30bf\u30a4\u30d7\n    viz_type = widgets.RadioButtons(\n        options=['Heatmap', 'Flow Diagram', 'Head Comparison'],\n        value='Heatmap',\n        description='Viz Type:'\n    )\n\n    # \u51fa\u529b\u30a8\u30ea\u30a2\n    output = widgets.Output()\n\n    def update_visualization(change):\n        with output:\n            output.clear_output(wait=True)\n\n            # \u30c8\u30fc\u30af\u30f3\u5316\n            tokens = text_input.value.split()  # \u7c21\u6613\u7248\n\n            # \u30c0\u30df\u30fc\u306e\u6ce8\u610f\u91cd\u307f\uff08\u5b9f\u969b\u306f\u30e2\u30c7\u30eb\u304b\u3089\u53d6\u5f97\uff09\n            seq_len = len(tokens)\n            attention_weights = np.random.rand(seq_len, seq_len)\n            attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n\n            if viz_type.value == 'Heatmap':\n                self._plot_heatmap(tokens, attention_weights)\n            elif viz_type.value == 'Flow Diagram':\n                flow_viz = AttentionFlowVisualizer()\n                flow_viz.create_attention_flow_diagram(tokens, attention_weights)\n            else:\n                self._plot_head_comparison(tokens)\n\n    # \u30a4\u30d9\u30f3\u30c8\u30cf\u30f3\u30c9\u30e9\u3092\u767b\u9332\n    text_input.observe(update_visualization, names='value')\n    layer_dropdown.observe(update_visualization, names='value')\n    head_slider.observe(update_visualization, names='value')\n    viz_type.observe(update_visualization, names='value')\n\n    # \u521d\u671f\u8868\u793a\n    update_visualization(None)\n\n    # \u30ec\u30a4\u30a2\u30a6\u30c8\n    controls = widgets.VBox([text_input, layer_dropdown, head_slider, viz_type])\n    return widgets.HBox([controls, output])\n\ndef _plot_heatmap(self, tokens: List[str], weights: np.ndarray):\n    \"\"\"\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(weights, xticklabels=tokens, yticklabels=tokens,\n               cmap='Blues', cbar=True)\n    plt.title('Attention Weights Heatmap')\n    plt.tight_layout()\n    plt.show()\n\ndef _plot_head_comparison(self, tokens: List[str]):\n    \"\"\"\u30d8\u30c3\u30c9\u6bd4\u8f03\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u30678\u3064\u306e\u30d8\u30c3\u30c9\u3092\u8868\u793a\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.flatten()\n\n    for i in range(8):\n        ax = axes[i]\n        # \u5404\u30d8\u30c3\u30c9\u3067\u7570\u306a\u308b\u30d1\u30bf\u30fc\u30f3\u3092\u751f\u6210\n        if i &lt; 2:  # \u5c40\u6240\u7684\u306a\u6ce8\u610f\n            weights = np.eye(len(tokens))\n            for j in range(1, 2):\n                weights += np.eye(len(tokens), k=j) * 0.5\n                weights += np.eye(len(tokens), k=-j) * 0.5\n        elif i &lt; 4:  # \u9577\u8ddd\u96e2\u306e\u6ce8\u610f\n            weights = np.random.rand(len(tokens), len(tokens))\n            weights = weights / weights.sum(axis=1, keepdims=True)\n        else:  # \u7279\u5b9a\u30d1\u30bf\u30fc\u30f3\n            weights = np.zeros((len(tokens), len(tokens)))\n            weights[:, 0] = 0.5  # \u6700\u521d\u306e\u30c8\u30fc\u30af\u30f3\u306b\u6ce8\u76ee\n            weights[:, -1] = 0.5  # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306b\u6ce8\u76ee\n\n        im = ax.imshow(weights, cmap='Blues')\n        ax.set_title(f'Head {i+1}')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.suptitle('Attention Head Patterns')\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part4/debugging-visualization/#152","title":"15.2 \u52fe\u914d\u30d5\u30ed\u30fc\u306e\u8ffd\u8de1","text":"<p>class GradientFlowAnalyzer:     \"\"\"\u52fe\u914d\u30d5\u30ed\u30fc\u306e\u5206\u6790\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: nn.Module):\n    self.model = model\n    self.gradient_data = {}\n    self.activation_data = {}\n\ndef register_gradient_hooks(self):\n    \"\"\"\u52fe\u914d\u3092\u8a18\u9332\u3059\u308b\u30d5\u30c3\u30af\u3092\u767b\u9332\"\"\"\n    def create_grad_hook(name):\n        def hook_fn(grad):\n            self.gradient_data[name] = {\n                'mean': grad.mean().item(),\n                'std': grad.std().item(),\n                'max': grad.max().item(),\n                'min': grad.min().item(),\n                'norm': grad.norm().item()\n            }\n        return hook_fn\n\n    for name, param in self.model.named_parameters():\n        if param.requires_grad:\n            param.register_hook(create_grad_hook(name))\n\ndef analyze_gradient_flow(self, loss: torch.Tensor):\n    \"\"\"\u52fe\u914d\u30d5\u30ed\u30fc\u3092\u5206\u6790\"\"\"\n    # \u9006\u4f1d\u64ad\n    loss.backward()\n\n    # \u52fe\u914d\u7d71\u8a08\u3092\u53ef\u8996\u5316\n    self._plot_gradient_statistics()\n\n    # \u52fe\u914d\u6d88\u5931\u30fb\u7206\u767a\u306e\u691c\u51fa\n    self._detect_gradient_issues()\n\ndef _plot_gradient_statistics(self):\n    \"\"\"\u52fe\u914d\u7d71\u8a08\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # \u30ec\u30a4\u30e4\u30fc\u540d\u3092\u6574\u7406\n    layer_names = list(self.gradient_data.keys())\n    layer_indices = range(len(layer_names))\n\n    # \u5e73\u5747\u52fe\u914d\n    ax = axes[0, 0]\n    means = [self.gradient_data[name]['mean'] for name in layer_names]\n    ax.bar(layer_indices, means)\n    ax.set_title('Mean Gradient per Layer')\n    ax.set_xlabel('Layer Index')\n    ax.set_ylabel('Mean Gradient')\n    ax.set_yscale('symlog')\n\n    # \u52fe\u914d\u30ce\u30eb\u30e0\n    ax = axes[0, 1]\n    norms = [self.gradient_data[name]['norm'] for name in layer_names]\n    ax.plot(layer_indices, norms, 'o-')\n    ax.set_title('Gradient Norm per Layer')\n    ax.set_xlabel('Layer Index')\n    ax.set_ylabel('Gradient Norm')\n    ax.set_yscale('log')\n\n    # \u52fe\u914d\u306e\u5206\u6563\n    ax = axes[1, 0]\n    stds = [self.gradient_data[name]['std'] for name in layer_names]\n    ax.bar(layer_indices, stds, color='orange')\n    ax.set_title('Gradient Std per Layer')\n    ax.set_xlabel('Layer Index')\n    ax.set_ylabel('Gradient Std')\n\n    # \u52fe\u914d\u306e\u6700\u5927\u5024\u30fb\u6700\u5c0f\u5024\n    ax = axes[1, 1]\n    maxs = [self.gradient_data[name]['max'] for name in layer_names]\n    mins = [self.gradient_data[name]['min'] for name in layer_names]\n    ax.plot(layer_indices, maxs, 'g-', label='Max')\n    ax.plot(layer_indices, mins, 'r-', label='Min')\n    ax.set_title('Gradient Range per Layer')\n    ax.set_xlabel('Layer Index')\n    ax.set_ylabel('Gradient Value')\n    ax.legend()\n    ax.set_yscale('symlog')\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u8a73\u7d30\u306a\u30ec\u30dd\u30fc\u30c8\n    self._print_gradient_report()\n\ndef _detect_gradient_issues(self):\n    \"\"\"\u52fe\u914d\u306e\u554f\u984c\u3092\u691c\u51fa\"\"\"\n    issues = []\n\n    for name, stats in self.gradient_data.items():\n        # \u52fe\u914d\u6d88\u5931\n        if stats['norm'] &lt; 1e-6:\n            issues.append(f\"\u52fe\u914d\u6d88\u5931\u306e\u53ef\u80fd\u6027: {name} (norm={stats['norm']:.2e})\")\n\n        # \u52fe\u914d\u7206\u767a\n        if stats['norm'] &gt; 1e3:\n            issues.append(f\"\u52fe\u914d\u7206\u767a\u306e\u53ef\u80fd\u6027: {name} (norm={stats['norm']:.2e})\")\n\n        # \u4e0d\u5b89\u5b9a\u306a\u52fe\u914d\n        if stats['std'] / (abs(stats['mean']) + 1e-8) &gt; 10:\n            issues.append(f\"\u4e0d\u5b89\u5b9a\u306a\u52fe\u914d: {name} (\u5909\u52d5\u4fc2\u6570\u304c\u5927\u304d\u3044)\")\n\n    if issues:\n        print(\"=== \u691c\u51fa\u3055\u308c\u305f\u554f\u984c ===\")\n        for issue in issues:\n            print(f\"\u26a0\ufe0f  {issue}\")\n    else:\n        print(\"\u2705 \u52fe\u914d\u30d5\u30ed\u30fc\u306f\u6b63\u5e38\u3067\u3059\")\n\ndef _print_gradient_report(self):\n    \"\"\"\u52fe\u914d\u30ec\u30dd\u30fc\u30c8\u3092\u51fa\u529b\"\"\"\n    print(\"\\n=== \u52fe\u914d\u30d5\u30ed\u30fc\u8a73\u7d30\u30ec\u30dd\u30fc\u30c8 ===\\n\")\n\n    # \u6700\u3082\u5927\u304d\u3044/\u5c0f\u3055\u3044\u52fe\u914d\u3092\u6301\u3064\u5c64\n    sorted_by_norm = sorted(self.gradient_data.items(), \n                           key=lambda x: x[1]['norm'])\n\n    print(\"\u52fe\u914d\u30ce\u30eb\u30e0\u304c\u6700\u3082\u5c0f\u3055\u3044\u5c64 (Top 5):\")\n    for name, stats in sorted_by_norm[:5]:\n        print(f\"  {name}: {stats['norm']:.2e}\")\n\n    print(\"\\n\u52fe\u914d\u30ce\u30eb\u30e0\u304c\u6700\u3082\u5927\u304d\u3044\u5c64 (Top 5):\")\n    for name, stats in sorted_by_norm[-5:]:\n        print(f\"  {name}: {stats['norm']:.2e}\")\n</code></pre> <p>class ActivationAnalyzer:     \"\"\"\u6d3b\u6027\u5316\u306e\u5206\u6790\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: nn.Module):\n    self.model = model\n    self.activation_data = {}\n    self.hooks = []\n\ndef register_activation_hooks(self):\n    \"\"\"\u6d3b\u6027\u5316\u3092\u8a18\u9332\u3059\u308b\u30d5\u30c3\u30af\u3092\u767b\u9332\"\"\"\n    def create_hook(name):\n        def hook_fn(module, input, output):\n            if isinstance(output, torch.Tensor):\n                self.activation_data[name] = {\n                    'mean': output.mean().item(),\n                    'std': output.std().item(),\n                    'zeros': (output == 0).float().mean().item(),\n                    'shape': list(output.shape),\n                    'histogram': output.detach().cpu().numpy().flatten()\n                }\n        return hook_fn\n\n    for name, module in self.model.named_modules():\n        if len(list(module.children())) == 0:  # \u30ea\u30fc\u30d5\u30e2\u30b8\u30e5\u30fc\u30eb\n            hook = module.register_forward_hook(create_hook(name))\n            self.hooks.append(hook)\n\ndef analyze_activations(self, input_data: torch.Tensor):\n    \"\"\"\u6d3b\u6027\u5316\u3092\u5206\u6790\"\"\"\n    # \u9806\u4f1d\u64ad\n    with torch.no_grad():\n        _ = self.model(input_data)\n\n    # \u6d3b\u6027\u5316\u306e\u5206\u5e03\u3092\u53ef\u8996\u5316\n    self._plot_activation_distributions()\n\n    # \u30c7\u30c3\u30c9\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u691c\u51fa\n    self._detect_dead_neurons()\n\ndef _plot_activation_distributions(self):\n    \"\"\"\u6d3b\u6027\u5316\u5206\u5e03\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    n_layers = len(self.activation_data)\n    n_cols = 4\n    n_rows = (n_layers + n_cols - 1) // n_cols\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n    axes = axes.flatten() if n_rows &gt; 1 else [axes]\n\n    for idx, (name, data) in enumerate(self.activation_data.items()):\n        if idx &gt;= len(axes):\n            break\n\n        ax = axes[idx]\n\n        # \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\n        hist_data = data['histogram']\n        if len(hist_data) &gt; 10000:\n            # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n            indices = np.random.choice(len(hist_data), 10000, replace=False)\n            hist_data = hist_data[indices]\n\n        ax.hist(hist_data, bins=50, alpha=0.7, density=True)\n        ax.axvline(data['mean'], color='red', linestyle='--', \n                  label=f'Mean: {data[\"mean\"]:.3f}')\n        ax.set_title(f'{name}\\n(zeros: {data[\"zeros\"]*100:.1f}%)')\n        ax.set_xlabel('Activation Value')\n        ax.set_ylabel('Density')\n        ax.legend()\n\n    # \u4f59\u3063\u305faxes\u3092\u975e\u8868\u793a\n    for idx in range(len(self.activation_data), len(axes)):\n        axes[idx].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef _detect_dead_neurons(self):\n    \"\"\"\u30c7\u30c3\u30c9\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u691c\u51fa\"\"\"\n    print(\"\\n=== \u30c7\u30c3\u30c9\u30cb\u30e5\u30fc\u30ed\u30f3\u691c\u51fa ===\\n\")\n\n    dead_threshold = 0.9  # 90%\u4ee5\u4e0a\u304c\u30bc\u30ed\n\n    for name, data in self.activation_data.items():\n        if data['zeros'] &gt; dead_threshold:\n            print(f\"\u26a0\ufe0f  {name}: {data['zeros']*100:.1f}% \u304c\u30bc\u30ed (\u30c7\u30c3\u30c9\u30cb\u30e5\u30fc\u30ed\u30f3\u306e\u53ef\u80fd\u6027)\")\n</code></pre>"},{"location":"part4/debugging-visualization/#153","title":"15.3 \u5b66\u7fd2\u904e\u7a0b\u306e\u76e3\u8996","text":"<p>class TrainingMonitor:     \"\"\"\u5b66\u7fd2\u904e\u7a0b\u306e\u5305\u62ec\u7684\u306a\u76e3\u8996\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self):\n    self.metrics = {\n        'loss': [],\n        'learning_rate': [],\n        'gradient_norm': [],\n        'weight_update_ratio': [],\n        'val_loss': [],\n        'val_accuracy': []\n    }\n    self.batch_metrics = {\n        'loss': [],\n        'gradient_norm': []\n    }\n\ndef log_batch(self, loss: float, model: nn.Module, \n              optimizer: torch.optim.Optimizer):\n    \"\"\"\u30d0\u30c3\u30c1\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30af\u30b9\u3092\u8a18\u9332\"\"\"\n    # \u640d\u5931\n    self.batch_metrics['loss'].append(loss)\n\n    # \u52fe\u914d\u30ce\u30eb\u30e0\n    total_norm = 0\n    for p in model.parameters():\n        if p.grad is not None:\n            total_norm += p.grad.norm().item() ** 2\n    total_norm = total_norm ** 0.5\n    self.batch_metrics['gradient_norm'].append(total_norm)\n\n    # \u91cd\u307f\u66f4\u65b0\u6bd4\u7387\n    if len(self.batch_metrics['loss']) % 100 == 0:\n        self._compute_weight_update_ratio(model, optimizer)\n\ndef log_epoch(self, epoch: int, train_loss: float, val_loss: float,\n              val_accuracy: float, learning_rate: float):\n    \"\"\"\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306e\u30e1\u30c8\u30ea\u30af\u30b9\u3092\u8a18\u9332\"\"\"\n    self.metrics['loss'].append(train_loss)\n    self.metrics['val_loss'].append(val_loss)\n    self.metrics['val_accuracy'].append(val_accuracy)\n    self.metrics['learning_rate'].append(learning_rate)\n\n    # \u30d0\u30c3\u30c1\u30e1\u30c8\u30ea\u30af\u30b9\u306e\u5e73\u5747\n    if self.batch_metrics['gradient_norm']:\n        avg_grad_norm = np.mean(self.batch_metrics['gradient_norm'])\n        self.metrics['gradient_norm'].append(avg_grad_norm)\n\ndef _compute_weight_update_ratio(self, model: nn.Module,\n                               optimizer: torch.optim.Optimizer):\n    \"\"\"\u91cd\u307f\u66f4\u65b0\u6bd4\u7387\u3092\u8a08\u7b97\"\"\"\n    ratios = []\n\n    for group in optimizer.param_groups:\n        for p in group['params']:\n            if p.grad is not None:\n                # \u66f4\u65b0\u91cf / \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30ce\u30eb\u30e0\n                update = group['lr'] * p.grad\n                ratio = update.norm().item() / (p.norm().item() + 1e-8)\n                ratios.append(ratio)\n\n    if ratios:\n        self.metrics['weight_update_ratio'].append(np.mean(ratios))\n\ndef plot_training_curves(self):\n    \"\"\"\u5b66\u7fd2\u66f2\u7dda\u3092\u30d7\u30ed\u30c3\u30c8\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n    # \u640d\u5931\n    ax = axes[0, 0]\n    epochs = range(1, len(self.metrics['loss']) + 1)\n    ax.plot(epochs, self.metrics['loss'], 'b-', label='Train Loss')\n    ax.plot(epochs, self.metrics['val_loss'], 'r-', label='Val Loss')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.set_title('Training and Validation Loss')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # \u7cbe\u5ea6\n    ax = axes[0, 1]\n    ax.plot(epochs, self.metrics['val_accuracy'], 'g-')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Accuracy')\n    ax.set_title('Validation Accuracy')\n    ax.grid(True, alpha=0.3)\n\n    # \u5b66\u7fd2\u7387\n    ax = axes[0, 2]\n    ax.plot(epochs, self.metrics['learning_rate'], 'orange')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Learning Rate')\n    ax.set_title('Learning Rate Schedule')\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n\n    # \u52fe\u914d\u30ce\u30eb\u30e0\n    ax = axes[1, 0]\n    ax.plot(epochs, self.metrics['gradient_norm'], 'purple')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Gradient Norm')\n    ax.set_title('Average Gradient Norm')\n    ax.set_yscale('log')\n    ax.grid(True, alpha=0.3)\n\n    # \u91cd\u307f\u66f4\u65b0\u6bd4\u7387\n    ax = axes[1, 1]\n    if self.metrics['weight_update_ratio']:\n        ax.plot(self.metrics['weight_update_ratio'], 'brown')\n        ax.set_xlabel('Update Step')\n        ax.set_ylabel('Update/Weight Ratio')\n        ax.set_title('Weight Update Ratio')\n        ax.axhline(y=1e-3, color='r', linestyle='--', \n                  label='Typical Good Range')\n        ax.legend()\n        ax.set_yscale('log')\n        ax.grid(True, alpha=0.3)\n\n    # \u30d0\u30c3\u30c1\u3054\u3068\u306e\u640d\u5931\n    ax = axes[1, 2]\n    ax.plot(self.batch_metrics['loss'][:1000], alpha=0.5)  # \u6700\u521d\u306e1000\u30d0\u30c3\u30c1\n    ax.set_xlabel('Batch')\n    ax.set_ylabel('Loss')\n    ax.set_title('Batch Loss (First 1000 batches)')\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\ndef generate_training_report(self):\n    \"\"\"\u5b66\u7fd2\u30ec\u30dd\u30fc\u30c8\u3092\u751f\u6210\"\"\"\n    print(\"=== \u5b66\u7fd2\u30b5\u30de\u30ea\u30fc\u30ec\u30dd\u30fc\u30c8 ===\\n\")\n\n    # \u6700\u7d42\u7684\u306a\u30e1\u30c8\u30ea\u30af\u30b9\n    print(f\"\u6700\u7d42\u30a8\u30dd\u30c3\u30af:\")\n    print(f\"  \u8a13\u7df4\u640d\u5931: {self.metrics['loss'][-1]:.4f}\")\n    print(f\"  \u691c\u8a3c\u640d\u5931: {self.metrics['val_loss'][-1]:.4f}\")\n    print(f\"  \u691c\u8a3c\u7cbe\u5ea6: {self.metrics['val_accuracy'][-1]:.4f}\")\n\n    # \u6700\u826f\u306e\u30a8\u30dd\u30c3\u30af\n    best_val_loss_epoch = np.argmin(self.metrics['val_loss']) + 1\n    best_val_acc_epoch = np.argmax(self.metrics['val_accuracy']) + 1\n\n    print(f\"\\n\u6700\u826f\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9:\")\n    print(f\"  \u6700\u5c0f\u691c\u8a3c\u640d\u5931: {min(self.metrics['val_loss']):.4f} (Epoch {best_val_loss_epoch})\")\n    print(f\"  \u6700\u9ad8\u691c\u8a3c\u7cbe\u5ea6: {max(self.metrics['val_accuracy']):.4f} (Epoch {best_val_acc_epoch})\")\n\n    # \u904e\u5b66\u7fd2\u306e\u691c\u51fa\n    if len(self.metrics['loss']) &gt; 5:\n        recent_train = np.mean(self.metrics['loss'][-5:])\n        recent_val = np.mean(self.metrics['val_loss'][-5:])\n\n        if recent_val &gt; recent_train * 1.5:\n            print(\"\\n\u26a0\ufe0f  \u904e\u5b66\u7fd2\u306e\u5146\u5019\u304c\u898b\u3089\u308c\u307e\u3059\")\n\n    # \u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\n    if self.batch_metrics['gradient_norm']:\n        grad_std = np.std(self.batch_metrics['gradient_norm'])\n        grad_mean = np.mean(self.batch_metrics['gradient_norm'])\n\n        if grad_std / grad_mean &gt; 2:\n            print(\"\\n\u26a0\ufe0f  \u52fe\u914d\u304c\u4e0d\u5b89\u5b9a\u3067\u3059\")\n</code></pre>"},{"location":"part4/debugging-visualization/#154","title":"15.4 \u30e2\u30c7\u30eb\u8a3a\u65ad\u30c4\u30fc\u30eb","text":"<p>class ModelDiagnostics:     \"\"\"\u30e2\u30c7\u30eb\u306e\u5305\u62ec\u7684\u306a\u8a3a\u65ad\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: nn.Module):\n    self.model = model\n\ndef run_diagnostics(self, sample_input: torch.Tensor):\n    \"\"\"\u5b8c\u5168\u306a\u8a3a\u65ad\u3092\u5b9f\u884c\"\"\"\n    print(\"=== \u30e2\u30c7\u30eb\u8a3a\u65ad\u958b\u59cb ===\\n\")\n\n    # 1. \u30e2\u30c7\u30eb\u69cb\u9020\u306e\u5206\u6790\n    self._analyze_model_structure()\n\n    # 2. \u30d1\u30e9\u30e1\u30fc\u30bf\u5206\u6790\n    self._analyze_parameters()\n\n    # 3. \u8a08\u7b97\u91cf\u5206\u6790\n    self._analyze_computation(sample_input)\n\n    # 4. \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u5206\u6790\n    self._analyze_memory(sample_input)\n\n    # 5. \u63a8\u8ad6\u901f\u5ea6\u6e2c\u5b9a\n    self._measure_inference_speed(sample_input)\n\n    print(\"\\n=== \u8a3a\u65ad\u5b8c\u4e86 ===\")\n\ndef _analyze_model_structure(self):\n    \"\"\"\u30e2\u30c7\u30eb\u69cb\u9020\u3092\u5206\u6790\"\"\"\n    print(\"1. \u30e2\u30c7\u30eb\u69cb\u9020\u5206\u6790\")\n\n    total_params = sum(p.numel() for p in self.model.parameters())\n    trainable_params = sum(p.numel() for p in self.model.parameters() \n                         if p.requires_grad)\n\n    print(f\"  \u7dcf\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {total_params:,}\")\n    print(f\"  \u5b66\u7fd2\u53ef\u80fd\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {trainable_params:,}\")\n    print(f\"  \u56fa\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {total_params - trainable_params:,}\")\n\n    # \u5c64\u3054\u3068\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\n    print(\"\\n  \u5c64\u3054\u3068\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570:\")\n    for name, module in self.model.named_modules():\n        if len(list(module.children())) == 0:  # \u30ea\u30fc\u30d5\u30e2\u30b8\u30e5\u30fc\u30eb\n            params = sum(p.numel() for p in module.parameters())\n            if params &gt; 0:\n                print(f\"    {name}: {params:,}\")\n\ndef _analyze_parameters(self):\n    \"\"\"\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5206\u6790\"\"\"\n    print(\"\\n2. \u30d1\u30e9\u30e1\u30fc\u30bf\u5206\u6790\")\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u7d71\u8a08\n    all_params = []\n    for p in self.model.parameters():\n        all_params.extend(p.detach().cpu().numpy().flatten())\n\n    all_params = np.array(all_params)\n\n    print(f\"  \u5e73\u5747: {np.mean(all_params):.4f}\")\n    print(f\"  \u6a19\u6e96\u504f\u5dee: {np.std(all_params):.4f}\")\n    print(f\"  \u6700\u5c0f\u5024: {np.min(all_params):.4f}\")\n    print(f\"  \u6700\u5927\u5024: {np.max(all_params):.4f}\")\n\n    # \u30b9\u30d1\u30fc\u30b9\u6027\n    sparsity = (np.abs(all_params) &lt; 1e-6).mean()\n    print(f\"  \u30b9\u30d1\u30fc\u30b9\u6027: {sparsity*100:.2f}%\")\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u5206\u5e03\u306e\u30d7\u30ed\u30c3\u30c8\n    plt.figure(figsize=(10, 4))\n    plt.hist(all_params, bins=100, alpha=0.7, density=True)\n    plt.xlabel('Parameter Value')\n    plt.ylabel('Density')\n    plt.title('Parameter Distribution')\n    plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\ndef _analyze_computation(self, sample_input: torch.Tensor):\n    \"\"\"\u8a08\u7b97\u91cf\u3092\u5206\u6790\"\"\"\n    print(\"\\n3. \u8a08\u7b97\u91cf\u5206\u6790\")\n\n    # FLOPs\u3092\u6982\u7b97\uff08\u7c21\u6613\u7248\uff09\n    total_mult_adds = 0\n\n    def count_operations(module, input, output):\n        nonlocal total_mult_adds\n\n        if isinstance(module, nn.Linear):\n            # Linear\u5c64: input_features * output_features\n            total_mult_adds += input[0].numel() * module.out_features\n        elif isinstance(module, nn.MultiheadAttention):\n            # Attention: O(n^2 * d)\u306e\u8a08\u7b97\u91cf\n            seq_len = input[0].shape[1]\n            d_model = input[0].shape[2]\n            total_mult_adds += seq_len * seq_len * d_model\n\n    # \u30d5\u30c3\u30af\u3092\u767b\u9332\n    hooks = []\n    for module in self.model.modules():\n        if isinstance(module, (nn.Linear, nn.MultiheadAttention)):\n            hook = module.register_forward_hook(count_operations)\n            hooks.append(hook)\n\n    # \u9806\u4f1d\u64ad\n    with torch.no_grad():\n        _ = self.model(sample_input)\n\n    # \u30d5\u30c3\u30af\u3092\u524a\u9664\n    for hook in hooks:\n        hook.remove()\n\n    print(f\"  \u63a8\u5b9aFLOP\u6570: {total_mult_adds:,}\")\n    print(f\"  GFLOP: {total_mult_adds / 1e9:.2f}\")\n\ndef _analyze_memory(self, sample_input: torch.Tensor):\n    \"\"\"\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u5206\u6790\"\"\"\n    print(\"\\n4. \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u5206\u6790\")\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30e1\u30e2\u30ea\n    param_memory = sum(p.numel() * p.element_size() \n                      for p in self.model.parameters())\n    print(f\"  \u30d1\u30e9\u30e1\u30fc\u30bf\u30e1\u30e2\u30ea: {param_memory / 1024**2:.2f} MB\")\n\n    # \u52fe\u914d\u306e\u30e1\u30e2\u30ea\uff08\u5b66\u7fd2\u6642\uff09\n    grad_memory = sum(p.numel() * p.element_size() \n                     for p in self.model.parameters() \n                     if p.requires_grad)\n    print(f\"  \u52fe\u914d\u30e1\u30e2\u30ea: {grad_memory / 1024**2:.2f} MB\")\n\n    # \u6d3b\u6027\u5316\u306e\u30e1\u30e2\u30ea\uff08\u6982\u7b97\uff09\n    # \u5b9f\u969b\u306e\u6e2c\u5b9a\u306b\u306f\u30e1\u30e2\u30ea\u30d7\u30ed\u30d5\u30a1\u30a4\u30e9\u30fc\u304c\u5fc5\u8981\n    print(f\"  \u6d3b\u6027\u5316\u30e1\u30e2\u30ea: \u5165\u529b\u30b5\u30a4\u30ba\u306b\u4f9d\u5b58\")\n\ndef _measure_inference_speed(self, sample_input: torch.Tensor):\n    \"\"\"\u63a8\u8ad6\u901f\u5ea6\u3092\u6e2c\u5b9a\"\"\"\n    print(\"\\n5. \u63a8\u8ad6\u901f\u5ea6\u6e2c\u5b9a\")\n\n    # \u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\n    for _ in range(10):\n        with torch.no_grad():\n            _ = self.model(sample_input)\n\n    # \u6e2c\u5b9a\n    import time\n    n_runs = 100\n\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    start_time = time.time()\n\n    for _ in range(n_runs):\n        with torch.no_grad():\n            _ = self.model(sample_input)\n\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    end_time = time.time()\n\n    avg_time = (end_time - start_time) / n_runs * 1000  # ms\n\n    print(f\"  \u5e73\u5747\u63a8\u8ad6\u6642\u9593: {avg_time:.2f} ms\")\n    print(f\"  \u30b9\u30eb\u30fc\u30d7\u30c3\u30c8: {1000/avg_time:.2f} samples/sec\")\n</code></pre>"},{"location":"part4/debugging-visualization/#_3","title":"\u5b9f\u884c\u4f8b\u3068\u30c7\u30e2","text":"<p>def run_comprehensive_demo():     \"\"\"\u5305\u62ec\u7684\u306a\u30c7\u30e2\u3092\u5b9f\u884c\"\"\"     print(\"=\" * 70)     print(\"Transformer\u30c7\u30d0\u30c3\u30b0\u30fb\u53ef\u8996\u5316\u30c4\u30fc\u30eb\u306e\u30c7\u30e2\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># \u30c0\u30df\u30fc\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\nclass DummyTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(1000, 256)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=256, nhead=8, dim_feedforward=1024,\n                batch_first=True\n            ),\n            num_layers=4\n        )\n        self.output = nn.Linear(256, 1000)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.transformer(x)\n        x = self.output(x)\n        return x\n\nmodel = DummyTransformer()\n\n# 1. \u6ce8\u610f\u306e\u53ef\u8996\u5316\nprint(\"=== 1. \u6ce8\u610f\u6a5f\u69cb\u306e\u53ef\u8996\u5316 ===\\n\")\nvisualizer = AttentionVisualizer(model)\n\n# \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\u3067\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u3092\u751f\u6210\ntokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\ndummy_attention = torch.rand(1, 8, len(tokens), len(tokens))\ndummy_attention = F.softmax(dummy_attention, dim=-1)\n\n# \u6ce8\u610f\u30d5\u30ed\u30fc\u56f3\nflow_viz = AttentionFlowVisualizer()\nflow_viz.create_attention_flow_diagram(tokens, dummy_attention[0, 0].numpy())\n\n# 2. \u52fe\u914d\u30d5\u30ed\u30fc\u5206\u6790\nprint(\"\\n=== 2. \u52fe\u914d\u30d5\u30ed\u30fc\u5206\u6790 ===\\n\")\ngrad_analyzer = GradientFlowAnalyzer(model)\ngrad_analyzer.register_gradient_hooks()\n\n# \u30c0\u30df\u30fc\u306e\u9806\u4f1d\u64ad\u3068\u9006\u4f1d\u64ad\ninput_ids = torch.randint(0, 1000, (2, 10))\noutput = model(input_ids)\nloss = output.mean()\ngrad_analyzer.analyze_gradient_flow(loss)\n\n# 3. \u6d3b\u6027\u5316\u5206\u6790\nprint(\"\\n=== 3. \u6d3b\u6027\u5316\u5206\u6790 ===\\n\")\nact_analyzer = ActivationAnalyzer(model)\nact_analyzer.register_activation_hooks()\nact_analyzer.analyze_activations(input_ids)\n\n# 4. \u5b66\u7fd2\u76e3\u8996\nprint(\"\\n=== 4. \u5b66\u7fd2\u904e\u7a0b\u306e\u76e3\u8996 ===\\n\")\nmonitor = TrainingMonitor()\n\n# \u30c0\u30df\u30fc\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\nfor epoch in range(10):\n    # \u30a8\u30dd\u30c3\u30af\u3054\u3068\u306e\u30c0\u30df\u30fc\u30e1\u30c8\u30ea\u30af\u30b9\n    train_loss = 2.5 * np.exp(-0.3 * epoch) + np.random.normal(0, 0.1)\n    val_loss = 2.5 * np.exp(-0.25 * epoch) + np.random.normal(0, 0.15)\n    val_acc = 1 - np.exp(-0.5 * epoch) + np.random.normal(0, 0.05)\n    lr = 0.001 * (0.1 ** (epoch // 3))\n\n    monitor.log_epoch(epoch, train_loss, val_loss, val_acc, lr)\n\n    # \u30d0\u30c3\u30c1\u3054\u3068\u306e\u30c0\u30df\u30fc\u30e1\u30c8\u30ea\u30af\u30b9\n    for batch in range(50):\n        batch_loss = train_loss + np.random.normal(0, 0.2)\n        monitor.log_batch(batch_loss, model, \n                        torch.optim.Adam(model.parameters()))\n\nmonitor.plot_training_curves()\nmonitor.generate_training_report()\n\n# 5. \u30e2\u30c7\u30eb\u8a3a\u65ad\nprint(\"\\n=== 5. \u30e2\u30c7\u30eb\u8a3a\u65ad ===\\n\")\ndiagnostics = ModelDiagnostics(model)\ndiagnostics.run_diagnostics(input_ids)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u30c7\u30e2\u5b8c\u4e86\")\nprint(\"=\" * 70)\n</code></pre> <p>if name == \"main\":     run_comprehensive_demo()</p>"},{"location":"part4/minimal-transformer/","title":"\u6700\u5c0f\u9650\u306eTransformer\u5b9f\u88c5","text":""},{"location":"part4/minimal-transformer/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u672c\u8cea\u3092\u63b4\u3080","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u3092\u521d\u3081\u3066\u5b9f\u88c5\u3059\u308b\u3068\u304d\u3001\u3044\u304d\u306a\u308aGCC\u3084LLVM\u306e\u3088\u3046\u306a\u5de8\u5927\u306a\u30b7\u30b9\u30c6\u30e0\u3092\u4f5c\u308d\u3046\u3068\u306f\u3057\u307e\u305b\u3093\u3002\u307e\u305a\u3001\u5b57\u53e5\u89e3\u6790\u5668\u3068\u7c21\u5358\u306a\u69cb\u6587\u89e3\u6790\u5668\u3092\u4f5c\u308a\u3001\u56db\u5247\u6f14\u7b97\u304c\u3067\u304d\u308b\u96fb\u5353\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u52d5\u304f\u3082\u306e\u3092\u4f5c\u3063\u3066\u304b\u3089\u3001\u5f90\u3005\u306b\u6a5f\u80fd\u3092\u8ffd\u52a0\u3057\u3066\u3044\u304f\u306e\u3067\u3059\u3002</p> <p>Transformer\u3082\u540c\u3058\u30a2\u30d7\u30ed\u30fc\u30c1\u3067\u5b66\u3073\u307e\u3057\u3087\u3046\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u6700\u5c0f\u9650\u306e\u6a5f\u80fd\u306b\u7d5e\u3063\u305f\u300c\u52d5\u304fTransformer\u300d\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u4f59\u8a08\u306a\u6700\u9069\u5316\u3084\u8907\u96d1\u306a\u6a5f\u80fd\u306f\u5f8c\u56de\u3057\u306b\u3057\u3066\u3001\u30b3\u30a2\u3068\u306a\u308b\u4ed5\u7d44\u307f\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u306b\u96c6\u4e2d\u3057\u307e\u3059\u3002</p>"},{"location":"part4/minimal-transformer/#131","title":"13.1 \u8a2d\u8a08\u65b9\u91dd","text":""},{"location":"part4/minimal-transformer/#_2","title":"\u4f55\u3092\u4f5c\u308b\u304b","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom typing import Optional, Tuple, List, Dict\nimport time\n\nclass MinimalTransformerDesign:\n    \"\"\"\u6700\u5c0f\u9650\u306eTransformer\u8a2d\u8a08\"\"\"\n\n    def explain_design_principles(self):\n        \"\"\"\u8a2d\u8a08\u65b9\u91dd\u306e\u8aac\u660e\"\"\"\n        print(\"=== \u6700\u5c0f\u9650\u306eTransformer\u306e\u8a2d\u8a08\u65b9\u91dd ===\\n\")\n\n        print(\"\u76ee\u6a19:\")\n        print(\"\u2022 \u30b7\u30f3\u30d7\u30eb\u3067\u7406\u89e3\u3057\u3084\u3059\u3044\u5b9f\u88c5\")\n        print(\"\u2022 \u52d5\u4f5c\u306e\u691c\u8a3c\u304c\u5bb9\u6613\")\n        print(\"\u2022 \u30b3\u30a2\u6982\u5ff5\u306e\u660e\u78ba\u306a\u8868\u73fe\")\n        print(\"\u2022 \u62e1\u5f35\u53ef\u80fd\u306a\u69cb\u9020\\n\")\n\n        print(\"\u542b\u3081\u308b\u3082\u306e:\")\n        print(\"\u2713 Self-Attention\u6a5f\u69cb\")\n        print(\"\u2713 \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\") \n        print(\"\u2713 Feed Forward Network\")\n        print(\"\u2713 \u6b8b\u5dee\u63a5\u7d9a\u3068\u5c64\u6b63\u898f\u5316\")\n        print(\"\u2713 \u57fa\u672c\u7684\u306a\u30c8\u30fc\u30af\u30f3\u5316\\n\")\n\n        print(\"\u7701\u7565\u3059\u308b\u3082\u306e:\")\n        print(\"\u2717 Multi-Head\uff08\u6700\u521d\u306f Single-Head\uff09\")\n        print(\"\u2717 \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\uff08\u30c7\u30b3\u30fc\u30c0\u30fc\u306e\u307f\uff09\")\n        print(\"\u2717 \u8907\u96d1\u306a\u6700\u9069\u5316\")\n        print(\"\u2717 \u9ad8\u5ea6\u306a\u30c8\u30fc\u30af\u30f3\u5316\uff08BPE\u306a\u3069\uff09\\n\")\n\n        # \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u56f3\n        self._visualize_minimal_architecture()\n\n    def _visualize_minimal_architecture(self):\n        \"\"\"\u6700\u5c0f\u9650\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u53ef\u8996\u5316\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 12))\n\n        # \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u4f4d\u7f6e\n        components = [\n            {\"name\": \"\u5165\u529b\u30c8\u30fc\u30af\u30f3\", \"y\": 1, \"color\": \"lightgreen\"},\n            {\"name\": \"\u57cb\u3081\u8fbc\u307f\u5c64\", \"y\": 2.5, \"color\": \"lightblue\"},\n            {\"name\": \"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\", \"y\": 4, \"color\": \"lightyellow\"},\n            {\"name\": \"Self-Attention\", \"y\": 6, \"color\": \"lightcoral\"},\n            {\"name\": \"\u6b8b\u5dee\u63a5\u7d9a + \u5c64\u6b63\u898f\u5316\", \"y\": 7.5, \"color\": \"lightgray\"},\n            {\"name\": \"Feed Forward\", \"y\": 9, \"color\": \"lightcoral\"},\n            {\"name\": \"\u6b8b\u5dee\u63a5\u7d9a + \u5c64\u6b63\u898f\u5316\", \"y\": 10.5, \"color\": \"lightgray\"},\n            {\"name\": \"\u51fa\u529b\u5c64\", \"y\": 12, \"color\": \"lightgreen\"}\n        ]\n\n        # \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u63cf\u753b\n        for comp in components:\n            rect = plt.Rectangle((2, comp[\"y\"]-0.4), 6, 0.8,\n                               facecolor=comp[\"color\"], \n                               edgecolor='black', linewidth=2)\n            ax.add_patch(rect)\n            ax.text(5, comp[\"y\"], comp[\"name\"], \n                   ha='center', va='center', fontsize=12, weight='bold')\n\n        # \u77e2\u5370\u3067\u63a5\u7d9a\n        for i in range(len(components)-1):\n            ax.arrow(5, components[i][\"y\"]+0.5, 0, \n                    components[i+1][\"y\"]-components[i][\"y\"]-1,\n                    head_width=0.3, head_length=0.2, \n                    fc='black', ec='black')\n\n        # \u6b8b\u5dee\u63a5\u7d9a\u306e\u77e2\u5370\n        # Attention\u5468\u308a\u306e\u6b8b\u5dee\n        ax.arrow(1.5, 5.5, 0, 2.5, head_width=0.2, head_length=0.1,\n                fc='blue', ec='blue', linestyle='--', linewidth=2)\n        # FFN\u5468\u308a\u306e\u6b8b\u5dee\n        ax.arrow(1.5, 8.5, 0, 2.5, head_width=0.2, head_length=0.1,\n                fc='blue', ec='blue', linestyle='--', linewidth=2)\n\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 13)\n        ax.axis('off')\n        ax.set_title('\u6700\u5c0f\u9650\u306eTransformer\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3', \n                    fontsize=16, weight='bold', pad=20)\n\n        plt.tight_layout()\n        plt.show()\n\n# \u5b9f\u88c5\u958b\u59cb\nclass MinimalSelfAttention(nn.Module):\n    \"\"\"\u6700\u5c0f\u9650\u306eSelf-Attention\u5b9f\u88c5\"\"\"\n\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.d_model = d_model\n\n        # Q, K, V \u306e\u7dda\u5f62\u5909\u63db\n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n\n        # \u51fa\u529b\u6295\u5f71\n        self.w_o = nn.Linear(d_model, d_model, bias=False)\n\n        # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u4fc2\u6570\n        self.scale = 1.0 / math.sqrt(d_model)\n\n    def forward(self, x: torch.Tensor, \n                mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            mask: [batch_size, seq_len, seq_len] or None\n        Returns:\n            output: [batch_size, seq_len, d_model]\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n\n        # Q, K, V \u3092\u8a08\u7b97\n        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n        V = self.w_v(x)  # [batch_size, seq_len, d_model]\n\n        # \u6ce8\u610f\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\n        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        # scores: [batch_size, seq_len, seq_len]\n\n        # \u30de\u30b9\u30af\u3092\u9069\u7528\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\u3067\u6ce8\u610f\u91cd\u307f\u3092\u8a08\u7b97\n        attn_weights = F.softmax(scores, dim=-1)\n\n        # \u91cd\u307f\u4ed8\u304d\u5e73\u5747\u3092\u8a08\u7b97\n        attn_output = torch.matmul(attn_weights, V)\n        # attn_output: [batch_size, seq_len, d_model]\n\n        # \u51fa\u529b\u6295\u5f71\n        output = self.w_o(attn_output)\n\n        return output\n\nclass MinimalFeedForward(nn.Module):\n    \"\"\"\u6700\u5c0f\u9650\u306eFeed Forward Network\"\"\"\n\n    def __init__(self, d_model: int, d_ff: Optional[int] = None):\n        super().__init__()\n        if d_ff is None:\n            d_ff = 4 * d_model  # \u4e00\u822c\u7684\u306a\u8a2d\u5b9a\n\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.activation = nn.ReLU()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n        Returns:\n            output: [batch_size, seq_len, d_model]\n        \"\"\"\n        # \u7b2c\u4e00\u5c64\uff1a\u62e1\u5f35\n        x = self.linear1(x)\n        x = self.activation(x)\n\n        # \u7b2c\u4e8c\u5c64\uff1a\u5727\u7e2e\n        x = self.linear2(x)\n\n        return x\n\nclass MinimalTransformerBlock(nn.Module):\n    \"\"\"\u6700\u5c0f\u9650\u306eTransformer\u30d6\u30ed\u30c3\u30af\"\"\"\n\n    def __init__(self, d_model: int, d_ff: Optional[int] = None):\n        super().__init__()\n\n        # Self-Attention\n        self.attention = MinimalSelfAttention(d_model)\n\n        # Feed Forward\n        self.feed_forward = MinimalFeedForward(d_model, d_ff)\n\n        # \u5c64\u6b63\u898f\u5316\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x: torch.Tensor, \n                mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            mask: [batch_size, seq_len, seq_len] or None\n        Returns:\n            output: [batch_size, seq_len, d_model]\n        \"\"\"\n        # Self-Attention with residual connection\n        attn_output = self.attention(x, mask)\n        x = self.norm1(x + attn_output)\n\n        # Feed Forward with residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + ff_output)\n\n        return x\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4e8b\u524d\u8a08\u7b97\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n\n        # \u5468\u6ce2\u6570\u9805\u3092\u8a08\u7b97\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * \n            -(math.log(10000.0) / d_model)\n        )\n\n        # sin/cos \u3092\u9069\u7528\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # \u30d0\u30c3\u30d5\u30a1\u3068\u3057\u3066\u767b\u9332\uff08\u5b66\u7fd2\u3055\u308c\u306a\u3044\uff09\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n        Returns:\n            output: [batch_size, seq_len, d_model]\n        \"\"\"\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n\nclass MinimalTransformer(nn.Module):\n    \"\"\"\u6700\u5c0f\u9650\u306e\u5b8c\u5168\u306aTransformer\"\"\"\n\n    def __init__(self, vocab_size: int, d_model: int = 256, \n                 n_layers: int = 4, d_ff: Optional[int] = None,\n                 max_len: int = 1024):\n        super().__init__()\n\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        # Transformer\u30d6\u30ed\u30c3\u30af\u306e\u30b9\u30bf\u30c3\u30af\n        self.layers = nn.ModuleList([\n            MinimalTransformerBlock(d_model, d_ff)\n            for _ in range(n_layers)\n        ])\n\n        # \u51fa\u529b\u5c64\n        self.output_projection = nn.Linear(d_model, vocab_size)\n\n        # \u91cd\u307f\u306e\u521d\u671f\u5316\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"\u91cd\u307f\u306e\u521d\u671f\u5316\"\"\"\n        for p in self.parameters():\n            if p.dim() &gt; 1:\n                nn.init.xavier_uniform_(p)\n\n    def create_causal_mask(self, seq_len: int, device: torch.device) -&gt; torch.Tensor:\n        \"\"\"\u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u4f5c\u6210\"\"\"\n        mask = torch.triu(\n            torch.ones(seq_len, seq_len, device=device), \n            diagonal=1\n        )\n        return mask == 0  # 0\u304c\u30de\u30b9\u30af\u3055\u308c\u308b\u4f4d\u7f6e\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len] - \u30c8\u30fc\u30af\u30f3ID\n        Returns:\n            output: [batch_size, seq_len, vocab_size] - \u30ed\u30b8\u30c3\u30c8\n        \"\"\"\n        batch_size, seq_len = x.shape\n        device = x.device\n\n        # \u57cb\u3081\u8fbc\u307f\n        x = self.embedding(x) * math.sqrt(self.d_model)  # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n\n        # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n        x = self.pos_encoding(x)\n\n        # \u56e0\u679c\u7684\u30de\u30b9\u30af\u3092\u4f5c\u6210\n        mask = self.create_causal_mask(seq_len, device)\n        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n\n        # \u5404\u5c64\u3092\u901a\u904e\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        # \u51fa\u529b\u6295\u5f71\n        output = self.output_projection(x)\n\n        return output\n\n    @torch.no_grad()\n    def generate(self, prompt: torch.Tensor, max_new_tokens: int = 50,\n                temperature: float = 1.0, top_k: Optional[int] = None) -&gt; torch.Tensor:\n        \"\"\"\n        \u30c6\u30ad\u30b9\u30c8\u751f\u6210\n        Args:\n            prompt: [batch_size, seq_len] - \u958b\u59cb\u30c8\u30fc\u30af\u30f3\n            max_new_tokens: \u751f\u6210\u3059\u308b\u6700\u5927\u30c8\u30fc\u30af\u30f3\u6570\n            temperature: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\n            top_k: Top-k\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\uff08None\u306e\u5834\u5408\u306f\u4f7f\u7528\u3057\u306a\u3044\uff09\n        Returns:\n            generated: [batch_size, seq_len + max_new_tokens]\n        \"\"\"\n        self.eval()\n        generated = prompt\n\n        for _ in range(max_new_tokens):\n            # \u73fe\u5728\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3067\u4e88\u6e2c\n            logits = self(generated)\n\n            # \u6700\u5f8c\u306e\u30c8\u30fc\u30af\u30f3\u306e\u4e88\u6e2c\u3092\u53d6\u5f97\n            next_token_logits = logits[:, -1, :] / temperature\n\n            # Top-k\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n            if top_k is not None:\n                values, indices = next_token_logits.topk(top_k)\n                next_token_logits = torch.full_like(\n                    next_token_logits, float('-inf')\n                )\n                next_token_logits.scatter_(1, indices, values)\n\n            # \u78ba\u7387\u5206\u5e03\u306b\u5909\u63db\u3057\u3066\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n\n            # \u751f\u6210\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u3092\u8ffd\u52a0\n            generated = torch.cat([generated, next_token], dim=1)\n\n        return generated\n\n# \u52d5\u4f5c\u78ba\u8a8d\nclass MinimalTransformerDemo:\n    \"\"\"\u6700\u5c0f\u9650\u306eTransformer\u306e\u30c7\u30e2\"\"\"\n\n    def __init__(self):\n        # \u7c21\u5358\u306a\u8a9e\u5f59\n        self.vocab = ['&lt;pad&gt;', '&lt;unk&gt;', 'the', 'cat', 'sat', 'on', \n                     'mat', 'dog', 'runs', 'fast', '.']\n        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n\n        # \u30e2\u30c7\u30eb\u306e\u4f5c\u6210\n        self.model = MinimalTransformer(\n            vocab_size=len(self.vocab),\n            d_model=64,\n            n_layers=2,\n            d_ff=256,\n            max_len=32\n        )\n\n    def tokenize(self, text: str) -&gt; List[int]:\n        \"\"\"\u7c21\u5358\u306a\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n        tokens = text.lower().split()\n        return [self.token_to_id.get(token, 1) for token in tokens]  # 1 = &lt;unk&gt;\n\n    def decode(self, token_ids: List[int]) -&gt; str:\n        \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u6587\u5b57\u5217\u306b\u5909\u63db\"\"\"\n        tokens = [self.id_to_token.get(id, '&lt;unk&gt;') for id in token_ids]\n        return ' '.join(tokens)\n\n    def demonstrate_forward_pass(self):\n        \"\"\"\u9806\u4f1d\u64ad\u306e\u30c7\u30e2\"\"\"\n        print(\"=== \u9806\u4f1d\u64ad\u306e\u30c7\u30e2 ===\\n\")\n\n        # \u30b5\u30f3\u30d7\u30eb\u5165\u529b\n        text = \"the cat sat on the mat\"\n        token_ids = self.tokenize(text)\n        print(f\"\u5165\u529b\u30c6\u30ad\u30b9\u30c8: {text}\")\n        print(f\"\u30c8\u30fc\u30af\u30f3ID: {token_ids}\\n\")\n\n        # \u30d0\u30c3\u30c1\u5316\u3057\u3066\u30c6\u30f3\u30bd\u30eb\u306b\u5909\u63db\n        input_tensor = torch.tensor([token_ids])\n\n        # \u9806\u4f1d\u64ad\n        with torch.no_grad():\n            output = self.model(input_tensor)\n\n        print(f\"\u5165\u529b\u5f62\u72b6: {input_tensor.shape}\")\n        print(f\"\u51fa\u529b\u5f62\u72b6: {output.shape}\")\n        print(f\"  (batch_size={output.shape[0]}, \"\n              f\"seq_len={output.shape[1]}, \"\n              f\"vocab_size={output.shape[2]})\\n\")\n\n        # \u5404\u4f4d\u7f6e\u3067\u306e\u4e88\u6e2c\u3092\u8868\u793a\n        print(\"\u5404\u4f4d\u7f6e\u3067\u306e\u4e0a\u4f4d\u4e88\u6e2c:\")\n        for pos in range(len(token_ids)):\n            probs = F.softmax(output[0, pos], dim=0)\n            top_probs, top_indices = probs.topk(3)\n\n            print(f\"\\n\u4f4d\u7f6e {pos} ('{self.vocab[token_ids[pos]]}' \u306e\u5f8c):\")\n            for prob, idx in zip(top_probs, top_indices):\n                token = self.vocab[idx]\n                print(f\"  {token}: {prob:.3f}\")\n\n    def visualize_attention_patterns(self):\n        \"\"\"\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316\"\"\"\n        print(\"\\n=== \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u53ef\u8996\u5316 ===\\n\")\n\n        # \u5165\u529b\u3092\u6e96\u5099\n        text = \"the cat sat on the mat\"\n        token_ids = self.tokenize(text)\n        input_tensor = torch.tensor([token_ids])\n\n        # \u6700\u521d\u306e\u30d6\u30ed\u30c3\u30af\u306e\u6ce8\u610f\u91cd\u307f\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30d5\u30c3\u30af\n        attention_weights = None\n\n        def hook_fn(module, input, output):\n            nonlocal attention_weights\n            # MinimalSelfAttention\u306e\u51fa\u529b\u3092\u53d6\u5f97\n            Q = module.w_q(input[0])\n            K = module.w_k(input[0])\n            scores = torch.matmul(Q, K.transpose(-2, -1)) * module.scale\n\n            # \u30de\u30b9\u30af\u304c\u3042\u308b\u5834\u5408\u306f\u9069\u7528\n            if input[1] is not None:\n                scores = scores.masked_fill(input[1] == 0, -1e9)\n\n            attention_weights = F.softmax(scores, dim=-1)\n\n        # \u30d5\u30c3\u30af\u3092\u767b\u9332\n        handle = self.model.layers[0].attention.register_forward_hook(hook_fn)\n\n        # \u9806\u4f1d\u64ad\n        with torch.no_grad():\n            _ = self.model(input_tensor)\n\n        # \u30d5\u30c3\u30af\u3092\u524a\u9664\n        handle.remove()\n\n        # \u53ef\u8996\u5316\n        if attention_weights is not None:\n            plt.figure(figsize=(8, 6))\n\n            # \u6ce8\u610f\u91cd\u307f\u3092\u30d7\u30ed\u30c3\u30c8\n            attn = attention_weights[0].cpu().numpy()\n            im = plt.imshow(attn, cmap='Blues', aspect='auto')\n\n            # \u30e9\u30d9\u30eb\n            tokens = [self.vocab[id] for id in token_ids]\n            plt.xticks(range(len(tokens)), tokens, rotation=45)\n            plt.yticks(range(len(tokens)), tokens)\n\n            plt.xlabel('Key (\u53c2\u7167\u3055\u308c\u308b\u4f4d\u7f6e)')\n            plt.ylabel('Query (\u73fe\u5728\u306e\u4f4d\u7f6e)')\n            plt.title('Self-Attention\u91cd\u307f\uff08\u6700\u521d\u306e\u5c64\uff09')\n\n            # \u30ab\u30e9\u30fc\u30d0\u30fc\n            plt.colorbar(im)\n\n            # \u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u5883\u754c\u3092\u8868\u793a\n            for i in range(len(tokens)):\n                for j in range(i+1, len(tokens)):\n                    plt.gca().add_patch(plt.Rectangle(\n                        (j-0.5, i-0.5), 1, 1, \n                        fill=True, color='gray', alpha=0.3\n                    ))\n\n            plt.tight_layout()\n            plt.show()\n\n            print(\"\u30b0\u30ec\u30fc\u306e\u9818\u57df\uff1a\u56e0\u679c\u7684\u30de\u30b9\u30af\u306b\u3088\u308a\u30a2\u30af\u30bb\u30b9\u4e0d\u53ef\")\n            print(\"\u9752\u306e\u6fc3\u3055\uff1a\u6ce8\u610f\u306e\u5f37\u3055\")\n\nclass DebugTools:\n    \"\"\"\u30c7\u30d0\u30c3\u30b0\u7528\u30c4\u30fc\u30eb\"\"\"\n\n    def __init__(self, model: MinimalTransformer):\n        self.model = model\n\n    def check_gradient_flow(self):\n        \"\"\"\u52fe\u914d\u306e\u6d41\u308c\u3092\u30c1\u30a7\u30c3\u30af\"\"\"\n        print(\"=== \u52fe\u914d\u30d5\u30ed\u30fc\u306e\u30c1\u30a7\u30c3\u30af ===\\n\")\n\n        # \u30c0\u30df\u30fc\u30c7\u30fc\u30bf\n        batch_size = 2\n        seq_len = 10\n        input_data = torch.randint(0, self.model.vocab_size, \n                                  (batch_size, seq_len))\n        target = torch.randint(0, self.model.vocab_size, \n                              (batch_size, seq_len))\n\n        # \u9806\u4f1d\u64ad\n        output = self.model(input_data)\n        loss = F.cross_entropy(\n            output.reshape(-1, self.model.vocab_size),\n            target.reshape(-1)\n        )\n\n        # \u9006\u4f1d\u64ad\n        loss.backward()\n\n        # \u5404\u5c64\u306e\u52fe\u914d\u3092\u30c1\u30a7\u30c3\u30af\n        print(\"\u5c64\u3054\u3068\u306e\u52fe\u914d\u30ce\u30eb\u30e0:\")\n        for i, layer in enumerate(self.model.layers):\n            grad_norms = {}\n\n            # Attention\u5c64\u306e\u52fe\u914d\n            for name, param in layer.attention.named_parameters():\n                if param.grad is not None:\n                    grad_norms[f\"attention.{name}\"] = param.grad.norm().item()\n\n            # FFN\u5c64\u306e\u52fe\u914d\n            for name, param in layer.feed_forward.named_parameters():\n                if param.grad is not None:\n                    grad_norms[f\"ffn.{name}\"] = param.grad.norm().item()\n\n            print(f\"\\nLayer {i}:\")\n            for name, norm in grad_norms.items():\n                print(f\"  {name}: {norm:.4f}\")\n\n        # \u57cb\u3081\u8fbc\u307f\u5c64\u306e\u52fe\u914d\n        if self.model.embedding.weight.grad is not None:\n            print(f\"\\nEmbedding: {self.model.embedding.weight.grad.norm().item():.4f}\")\n\n        # \u51fa\u529b\u5c64\u306e\u52fe\u914d\n        if self.model.output_projection.weight.grad is not None:\n            print(f\"Output: {self.model.output_projection.weight.grad.norm().item():.4f}\")\n\n    def profile_inference(self):\n        \"\"\"\u63a8\u8ad6\u6642\u9593\u306e\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\"\"\"\n        print(\"\\n=== \u63a8\u8ad6\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9 ===\\n\")\n\n        # \u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\n        dummy_input = torch.randint(0, self.model.vocab_size, (1, 50))\n        for _ in range(10):\n            with torch.no_grad():\n                _ = self.model(dummy_input)\n\n        # \u7570\u306a\u308b\u30b7\u30fc\u30b1\u30f3\u30b9\u9577\u3067\u30c6\u30b9\u30c8\n        seq_lengths = [10, 50, 100, 200]\n        times = []\n\n        for seq_len in seq_lengths:\n            input_data = torch.randint(0, self.model.vocab_size, (1, seq_len))\n\n            # \u6642\u9593\u8a08\u6e2c\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n            start_time = time.time()\n\n            with torch.no_grad():\n                for _ in range(100):\n                    _ = self.model(input_data)\n\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n            end_time = time.time()\n\n            avg_time = (end_time - start_time) / 100 * 1000  # \u30df\u30ea\u79d2\n            times.append(avg_time)\n\n            print(f\"\u30b7\u30fc\u30b1\u30f3\u30b9\u9577 {seq_len}: {avg_time:.2f} ms\")\n\n        # \u8907\u96d1\u5ea6\u306e\u78ba\u8a8d\n        print(f\"\\n\u6642\u9593\u8907\u96d1\u5ea6: O(n\u00b2) - \u30b7\u30fc\u30b1\u30f3\u30b9\u9577\u306e2\u4e57\u306b\u6bd4\u4f8b\")\n        print(f\"\u6bd4\u7387\u78ba\u8a8d: {times[1]/times[0]:.1f}x @ 5x\u9577, \"\n              f\"{times[2]/times[0]:.1f}x @ 10x\u9577\")\n\n# \u5b9f\u884c\u4f8b\ndef main():\n    \"\"\"\u30e1\u30a4\u30f3\u5b9f\u884c\u95a2\u6570\"\"\"\n    print(\"=\" * 70)\n    print(\"\u6700\u5c0f\u9650\u306eTransformer\u5b9f\u88c5\")\n    print(\"=\" * 70 + \"\\n\")\n\n    # \u8a2d\u8a08\u8aac\u660e\n    design = MinimalTransformerDesign()\n    design.explain_design_principles()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u30c7\u30e2\u306e\u5b9f\u884c\n    demo = MinimalTransformerDemo()\n    demo.demonstrate_forward_pass()\n    demo.visualize_attention_patterns()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n    # \u30c7\u30d0\u30c3\u30b0\u30c4\u30fc\u30eb\n    debug = DebugTools(demo.model)\n    debug.check_gradient_flow()\n    debug.profile_inference()\n\n    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n    print(\"\u307e\u3068\u3081:\")\n    print(\"\u2022 \u6700\u5c0f\u9650\u306e\u5b9f\u88c5\u3067Transformer\u306e\u52d5\u4f5c\u3092\u78ba\u8a8d\")\n    print(\"\u2022 \u5404\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u306e\u5f79\u5272\u304c\u660e\u78ba\")\n    print(\"\u2022 \u30c7\u30d0\u30c3\u30b0\u3068\u30d7\u30ed\u30d5\u30a1\u30a4\u30ea\u30f3\u30b0\u304c\u5bb9\u6613\")\n    print(\"\u2022 \u3053\u306e\u57fa\u76e4\u306e\u4e0a\u306b\u3001\u3088\u308a\u9ad8\u5ea6\u306a\u6a5f\u80fd\u3092\u8ffd\u52a0\u53ef\u80fd\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"part4/minimal-transformer/#132","title":"13.2 \u30b9\u30c6\u30c3\u30d7\u30d0\u30a4\u30b9\u30c6\u30c3\u30d7\u5b9f\u88c5","text":""},{"location":"part4/minimal-transformer/#1","title":"1. \u30c7\u30fc\u30bf\u306e\u6e96\u5099","text":"<p>```python import torch from torch.utils.data import Dataset, DataLoader import random from typing import List, Tuple, Dict</p> <p>class SimpleTextDataset(Dataset):     \"\"\"\u30b7\u30f3\u30d7\u30eb\u306a\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\"\"\"</p> <pre><code>def __init__(self, texts: List[str], vocab: Dict[str, int], \n             seq_len: int = 32):\n    self.vocab = vocab\n    self.seq_len = seq_len\n    self.data = []\n\n    # \u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u5316\n    for text in texts:\n        tokens = self._tokenize(text)\n        # \u56fa\u5b9a\u9577\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u306b\u5206\u5272\n        for i in range(0, len(tokens) - seq_len, seq_len // 2):\n            self.data.append(tokens[i:i + seq_len + 1])\n\ndef _tokenize(self, text: str) -&gt; List[int]:\n    \"\"\"\u7c21\u5358\u306a\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n    words = text.lower().split()\n    return [self.vocab.get(word, self.vocab['&lt;unk&gt;']) for word in words]\n\ndef __len__(self):\n    return len(self.data)\n\ndef __getitem__(self, idx):\n    tokens = self.data[idx]\n    # \u5165\u529b\u3068\u76ee\u6a19\u3092\u4f5c\u6210\uff08\u76ee\u6a19\u306f1\u3064\u305a\u3064\u305a\u308c\u3066\u3044\u308b\uff09\n    input_tokens = tokens[:-1]\n    target_tokens = tokens[1:]\n\n    # \u30d1\u30c7\u30a3\u30f3\u30b0\n    if len(input_tokens) &lt; self.seq_len:\n        pad_len = self.seq_len - len(input_tokens)\n        input_tokens += [self.vocab['&lt;pad&gt;']] * pad_len\n        target_tokens += [self.vocab['&lt;pad&gt;']] * pad_len\n\n    return (torch.tensor(input_tokens), torch.tensor(target_tokens))\n</code></pre> <p>class TrainingPipeline:     \"\"\"\u5b66\u7fd2\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\"\"\"</p> <pre><code>def __init__(self, model: MinimalTransformer, vocab: Dict[str, int]):\n    self.model = model\n    self.vocab = vocab\n    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    self.model.to(self.device)\n\ndef train_step(self, batch: Tuple[torch.Tensor, torch.Tensor], \n               optimizer: torch.optim.Optimizer) -&gt; float:\n    \"\"\"1\u30b9\u30c6\u30c3\u30d7\u306e\u5b66\u7fd2\"\"\"\n    inputs, targets = batch\n    inputs = inputs.to(self.device)\n    targets = targets.to(self.device)\n\n    # \u9806\u4f1d\u64ad\n    outputs = self.model(inputs)\n\n    # \u640d\u5931\u8a08\u7b97\uff08\u30d1\u30c7\u30a3\u30f3\u30b0\u30c8\u30fc\u30af\u30f3\u306f\u7121\u8996\uff09\n    loss = F.cross_entropy(\n        outputs.reshape(-1, len(self.vocab)),\n        targets.reshape(-1),\n        ignore_index=self.vocab['&lt;pad&gt;']\n    )\n\n    # \u9006\u4f1d\u64ad\n    optimizer.zero_grad()\n    loss.backward()\n\n    # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n    optimizer.step()\n\n    return loss.item()\n\ndef evaluate(self, dataloader: DataLoader) -&gt; float:\n    \"\"\"\u8a55\u4fa1\"\"\"\n    self.model.eval()\n    total_loss = 0\n    n_batches = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs, targets = batch\n            inputs = inputs.to(self.device)\n            targets = targets.to(self.device)\n\n            outputs = self.model(inputs)\n            loss = F.cross_entropy(\n                outputs.reshape(-1, len(self.vocab)),\n                targets.reshape(-1),\n                ignore_index=self.vocab['&lt;pad&gt;']\n            )\n\n            total_loss += loss.item()\n            n_batches += 1\n\n    self.model.train()\n    return total_loss / n_batches\n\ndef train(self, train_dataloader: DataLoader, \n          val_dataloader: DataLoader,\n          n_epochs: int = 10,\n          learning_rate: float = 1e-3):\n    \"\"\"\u5b66\u7fd2\u30eb\u30fc\u30d7\"\"\"\n    print(\"=== \u5b66\u7fd2\u958b\u59cb ===\\n\")\n\n    optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(n_epochs):\n        # \u5b66\u7fd2\n        epoch_loss = 0\n        n_batches = 0\n\n        for batch in train_dataloader:\n            loss = self.train_step(batch, optimizer)\n            epoch_loss += loss\n            n_batches += 1\n\n        avg_train_loss = epoch_loss / n_batches\n        train_losses.append(avg_train_loss)\n\n        # \u8a55\u4fa1\n        val_loss = self.evaluate(val_dataloader)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch+1}/{n_epochs}\")\n        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n\n        # \u751f\u6210\u30b5\u30f3\u30d7\u30eb\n        if (epoch + 1) % 5 == 0:\n            self._generate_sample()\n            print()\n\n    # \u5b66\u7fd2\u66f2\u7dda\u3092\u30d7\u30ed\u30c3\u30c8\n    self._plot_learning_curve(train_losses, val_losses)\n\ndef _generate_sample(self):\n    \"\"\"\u30b5\u30f3\u30d7\u30eb\u751f\u6210\"\"\"\n    print(\"\\n  \u751f\u6210\u30b5\u30f3\u30d7\u30eb:\")\n\n    # \u30d7\u30ed\u30f3\u30d7\u30c8\n    prompt = \"the cat\"\n    prompt_tokens = [self.vocab.get(w, self.vocab['&lt;unk&gt;']) \n                    for w in prompt.split()]\n    prompt_tensor = torch.tensor([prompt_tokens]).to(self.device)\n\n    # \u751f\u6210\n    generated = self.model.generate(\n        prompt_tensor, \n        max_new_tokens=10,\n        temperature=0.8,\n        top_k=5\n    )\n\n    # \u30c7\u30b3\u30fc\u30c9\n    generated_tokens = generated[0].cpu().tolist()\n    id_to_token = {v: k for k, v in self.vocab.items()}\n    generated_text = ' '.join([id_to_token.get(id, '&lt;unk&gt;') \n                              for id in generated_tokens])\n\n    print(f\"  '{generated_text}'\")\n\ndef _plot_learning_curve(self, train_losses: List[float], \n                        val_losses: List[float]):\n    \"\"\"\u5b66\u7fd2\u66f2\u7dda\u306e\u30d7\u30ed\u30c3\u30c8\"\"\"\n    plt.figure(figsize=(10, 6))\n\n    epochs = range(1, len(train_losses) + 1)\n    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n    plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('\u5b66\u7fd2\u66f2\u7dda')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part4/minimal-transformer/#_3","title":"\u5b9f\u969b\u306e\u5b66\u7fd2\u4f8b","text":"<p>def training_example():     \"\"\"\u5b66\u7fd2\u306e\u5b9f\u4f8b\"\"\"     print(\"=== \u6700\u5c0f\u9650\u306eTransformer\u306e\u5b66\u7fd2\u4f8b ===\\n\")</p> <pre><code># \u7c21\u5358\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\ntexts = [\n    \"the cat sat on the mat\",\n    \"the dog runs fast\",\n    \"the cat runs on the mat\",\n    \"the dog sat on the floor\",\n    \"cats and dogs are animals\",\n    \"the mat is on the floor\",\n    \"animals run and play\",\n    \"the fast cat runs\",\n    \"dogs play on the mat\",\n    \"the floor is clean\"\n] * 10  # \u30c7\u30fc\u30bf\u3092\u5897\u3084\u3059\n\n# \u8a9e\u5f59\u306e\u69cb\u7bc9\nvocab = {'&lt;pad&gt;': 0, '&lt;unk&gt;': 1}\nfor text in texts:\n    for word in text.lower().split():\n        if word not in vocab:\n            vocab[word] = len(vocab)\n\nprint(f\"\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(vocab)}\")\nprint(f\"\u8a9e\u5f59: {list(vocab.keys())[:10]}...\\n\")\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\ndataset = SimpleTextDataset(texts, vocab, seq_len=8)\n\n# \u8a13\u7df4\u30fb\u691c\u8a3c\u306b\u5206\u5272\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size]\n)\n\n# \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# \u30e2\u30c7\u30eb\u306e\u4f5c\u6210\nmodel = MinimalTransformer(\n    vocab_size=len(vocab),\n    d_model=64,\n    n_layers=2,\n    d_ff=256,\n    max_len=32\n)\n\nprint(f\"\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {sum(p.numel() for p in model.parameters()):,}\\n\")\n\n# \u5b66\u7fd2\npipeline = TrainingPipeline(model, vocab)\npipeline.train(train_dataloader, val_dataloader, n_epochs=20)\n</code></pre>"},{"location":"part4/minimal-transformer/#_4","title":"\u8a73\u7d30\u306a\u5206\u6790","text":"<p>class DetailedAnalysis:     \"\"\"\u8a73\u7d30\u306a\u5206\u6790\u30c4\u30fc\u30eb\"\"\"</p> <pre><code>def __init__(self, model: MinimalTransformer):\n    self.model = model\n\ndef analyze_attention_heads(self):\n    \"\"\"\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u8a73\u7d30\u5206\u6790\"\"\"\n    print(\"=== \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u8a73\u7d30\u5206\u6790 ===\\n\")\n\n    # \u30c6\u30b9\u30c8\u5165\u529b\n    test_sentences = [\n        \"the cat sat\",\n        \"cat sat on\", \n        \"on the mat\"\n    ]\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    for idx, sentence in enumerate(test_sentences):\n        tokens = sentence.split()\n        token_ids = [2 + i for i in range(len(tokens))]  # \u4eee\u306eID\n        input_tensor = torch.tensor([token_ids])\n\n        # \u6ce8\u610f\u91cd\u307f\u3092\u53d6\u5f97\n        with torch.no_grad():\n            # \u57cb\u3081\u8fbc\u307f\n            x = self.model.embedding(input_tensor) * math.sqrt(self.model.d_model)\n            x = self.model.pos_encoding(x)\n\n            # \u6700\u521d\u306e\u5c64\u306eAttention\u3092\u8a08\u7b97\n            layer = self.model.layers[0]\n            Q = layer.attention.w_q(x)\n            K = layer.attention.w_k(x)\n\n            scores = torch.matmul(Q, K.transpose(-2, -1)) * layer.attention.scale\n            seq_len = len(tokens)\n            mask = self.model.create_causal_mask(seq_len, x.device)\n            scores = scores.masked_fill(~mask.unsqueeze(0), -1e9)\n\n            attn_weights = F.softmax(scores, dim=-1)[0]\n\n        # \u53ef\u8996\u5316\n        ax = axes[idx]\n        im = ax.imshow(attn_weights.cpu().numpy(), cmap='Blues', \n                      vmin=0, vmax=1)\n\n        ax.set_xticks(range(len(tokens)))\n        ax.set_xticklabels(tokens)\n        ax.set_yticks(range(len(tokens)))\n        ax.set_yticklabels(tokens)\n        ax.set_title(f'\"{sentence}\"')\n\n        # \u5024\u3092\u8868\u793a\n        for i in range(len(tokens)):\n            for j in range(len(tokens)):\n                if j &lt;= i:  # \u56e0\u679c\u7684\u30de\u30b9\u30af\n                    value = attn_weights[i, j].item()\n                    ax.text(j, i, f'{value:.2f}', \n                           ha='center', va='center',\n                           color='white' if value &gt; 0.5 else 'black')\n\n    plt.suptitle('\u7570\u306a\u308b\u5165\u529b\u3067\u306e\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\u89b3\u5bdf:\")\n    print(\"\u2022 \u76f4\u524d\u306e\u30c8\u30fc\u30af\u30f3\u3078\u306e\u5f37\u3044\u6ce8\u610f\")\n    print(\"\u2022 \u4f4d\u7f6e\u304c\u96e2\u308c\u308b\u307b\u3069\u6ce8\u610f\u304c\u5f31\u307e\u308b\u50be\u5411\")\n    print(\"\u2022 \u6587\u8108\u306b\u3088\u3063\u3066\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u304c\u5909\u5316\")\n\ndef parameter_statistics(self):\n    \"\"\"\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u7d71\u8a08\u60c5\u5831\"\"\"\n    print(\"\\n=== \u30d1\u30e9\u30e1\u30fc\u30bf\u7d71\u8a08 ===\\n\")\n\n    stats = {}\n\n    for name, param in self.model.named_parameters():\n        stats[name] = {\n            'shape': list(param.shape),\n            'mean': param.mean().item(),\n            'std': param.std().item(),\n            'min': param.min().item(),\n            'max': param.max().item()\n        }\n\n    # \u8868\u5f62\u5f0f\u3067\u51fa\u529b\n    print(f\"{'Layer':&lt;40} {'Shape':&lt;20} {'Mean':&lt;10} {'Std':&lt;10}\")\n    print(\"-\" * 80)\n\n    for name, stat in stats.items():\n        shape_str = str(stat['shape'])\n        print(f\"{name:&lt;40} {shape_str:&lt;20} \"\n              f\"{stat['mean']:&lt;10.4f} {stat['std']:&lt;10.4f}\")\n</code></pre>"},{"location":"part4/minimal-transformer/#_5","title":"\u6700\u9069\u5316\u306e\u30d2\u30f3\u30c8","text":"<p>def optimization_tips():     \"\"\"\u6700\u9069\u5316\u306e\u30d2\u30f3\u30c8\"\"\"     print(\"\\n=== \u6700\u9069\u5316\u306e\u30d2\u30f3\u30c8 ===\\n\")</p> <pre><code>tips = {\n    \"\u30e1\u30e2\u30ea\u52b9\u7387\": [\n        \"\u52fe\u914d\u7d2f\u7a4d\u3067\u5b9f\u52b9\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u5897\u3084\u3059\",\n        \"Mixed Precision Training (AMP) \u306e\u4f7f\u7528\",\n        \"\u52fe\u914d\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u3067\u4e2d\u9593\u6d3b\u6027\u5316\u3092\u524a\u6e1b\"\n    ],\n\n    \"\u8a08\u7b97\u52b9\u7387\": [\n        \"Flash Attention\u306a\u3069\u306e\u6700\u9069\u5316\u3055\u308c\u305fAttention\u5b9f\u88c5\",\n        \"Key-Value \u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u6d3b\u7528\uff08\u63a8\u8ad6\u6642\uff09\",\n        \"\u91cf\u5b50\u5316\u306b\u3088\u308b\u9ad8\u901f\u5316\"\n    ],\n\n    \"\u5b66\u7fd2\u306e\u5b89\u5b9a\u6027\": [\n        \"Learning rate scheduler\u306e\u4f7f\u7528\",\n        \"Gradient clipping\u306e\u9069\u5207\u306a\u8a2d\u5b9a\", \n        \"Weight decay\u306e\u8abf\u6574\"\n    ]\n}\n\nfor category, items in tips.items():\n    print(f\"{category}:\")\n    for item in items:\n        print(f\"  \u2022 {item}\")\n    print()\n\n# \u30b3\u30fc\u30c9\u4f8b\nprint(\"=== \u30b3\u30fc\u30c9\u4f8b\uff1a\u52fe\u914d\u7d2f\u7a4d ===\")\nprint(\"\"\"\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(dataloader):\n    loss = compute_loss(batch)\n    loss = loss / accumulation_steps  # \u6b63\u898f\u5316\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n\"\"\")\n</code></pre>"},{"location":"part4/minimal-transformer/#_6","title":"\u30e1\u30a4\u30f3\u5b9f\u884c","text":"<p>if name == \"main\":     # \u57fa\u672c\u7684\u306a\u30c7\u30e2     main()</p> <pre><code>print(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# \u5b66\u7fd2\u4f8b\ntraining_example()\n\nprint(\"\\n\" + \"=\" * 70 + \"\\n\")\n\n# \u8a73\u7d30\u5206\u6790\nmodel = MinimalTransformer(vocab_size=100, d_model=64, n_layers=2)\nanalysis = DetailedAnalysis(model)\nanalysis.analyze_attention_heads()\nanalysis.parameter_statistics()\n\n# \u6700\u9069\u5316\u306e\u30d2\u30f3\u30c8\noptimization_tips()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7:\")\nprint(\"\u2022 Multi-Head Attention\u306e\u8ffd\u52a0\")\nprint(\"\u2022 \u3088\u308a\u9ad8\u5ea6\u306a\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\")\nprint(\"\u2022 \u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u30fb\u30c7\u30b3\u30fc\u30c0\u30fc\u69cb\u9020\u3078\u306e\u62e1\u5f35\")\nprint(\"\u2022 \u5b9f\u7528\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u7d71\u5408\")\n</code></pre>"},{"location":"part4/validation/","title":"\u52d5\u4f5c\u691c\u8a3c","text":""},{"location":"part4/validation/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u6b63\u3057\u3055\u306e\u8a3c\u660e","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u3092\u4f5c\u3063\u305f\u5f8c\u3001\u6700\u3082\u91cd\u8981\u306a\u306e\u306f\u305d\u306e\u6b63\u3057\u3055\u3092\u691c\u8a3c\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u30c6\u30b9\u30c8\u30b9\u30a4\u30fc\u30c8\u3092\u5b9f\u884c\u3057\u3001\u65e2\u77e5\u306e\u5165\u529b\u306b\u5bfe\u3057\u3066\u671f\u5f85\u3055\u308c\u308b\u51fa\u529b\u304c\u5f97\u3089\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002\u30a8\u30c3\u30b8\u30b1\u30fc\u30b9\u3092\u63a2\u3057\u3001\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u6e2c\u5b9a\u3057\u3001\u4ed6\u306e\u5b9f\u88c5\u3068\u6bd4\u8f03\u3057\u307e\u3059\u3002</p> <p>Transformer\u306e\u5b9f\u88c5\u3067\u3082\u540c\u3058\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u5fc5\u8981\u3067\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u5b9f\u88c5\u3057\u305fTransformer\u304c\u6b63\u3057\u304f\u52d5\u4f5c\u3059\u308b\u3053\u3068\u3092\u4f53\u7cfb\u7684\u306b\u691c\u8a3c\u3059\u308b\u65b9\u6cd5\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part4/validation/#161","title":"16.1 \u5358\u4f53\u30c6\u30b9\u30c8\u306e\u5b9f\u88c5","text":""},{"location":"part4/validation/#_3","title":"\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u30ec\u30d9\u30eb\u306e\u30c6\u30b9\u30c8","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import unittest from typing import Tuple, Optional, List import math import matplotlib.pyplot as plt import seaborn as sns from torch.testing import assert_close</p> <p>class TestMultiHeadAttention(unittest.TestCase):     \"\"\"Multi-Head Attention\u306e\u5358\u4f53\u30c6\u30b9\u30c8\"\"\"</p> <pre><code>def setUp(self):\n    \"\"\"\u30c6\u30b9\u30c8\u306e\u521d\u671f\u8a2d\u5b9a\"\"\"\n    self.d_model = 512\n    self.n_heads = 8\n    self.batch_size = 2\n    self.seq_len = 10\n\n    # \u30c6\u30b9\u30c8\u5bfe\u8c61\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\n    self.attention = nn.MultiheadAttention(\n        self.d_model, self.n_heads, batch_first=True\n    )\n\ndef test_output_shape(self):\n    \"\"\"\u51fa\u529b\u5f62\u72b6\u306e\u30c6\u30b9\u30c8\"\"\"\n    # \u5165\u529b\u30c7\u30fc\u30bf\n    x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n    # \u9806\u4f1d\u64ad\n    output, weights = self.attention(x, x, x)\n\n    # \u5f62\u72b6\u306e\u78ba\u8a8d\n    self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\n    self.assertEqual(weights.shape, (self.batch_size, self.seq_len, self.seq_len))\n\n    print(\"\u2705 \u51fa\u529b\u5f62\u72b6\u30c6\u30b9\u30c8: PASS\")\n\ndef test_attention_mask(self):\n    \"\"\"\u6ce8\u610f\u30de\u30b9\u30af\u306e\u30c6\u30b9\u30c8\"\"\"\n    x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n\n    # \u56e0\u679c\u7684\u30de\u30b9\u30af\u306e\u4f5c\u6210\n    mask = torch.triu(torch.ones(self.seq_len, self.seq_len) * float('-inf'), diagonal=1)\n\n    # \u30de\u30b9\u30af\u4ed8\u304d\u9806\u4f1d\u64ad\n    output, weights = self.attention(x, x, x, attn_mask=mask)\n\n    # \u672a\u6765\u306e\u4f4d\u7f6e\u3078\u306e\u6ce8\u610f\u304c0\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n    for i in range(self.seq_len):\n        for j in range(i + 1, self.seq_len):\n            self.assertAlmostEqual(\n                weights[0, i, j].item(), 0.0, places=5,\n                msg=f\"Position {i} should not attend to future position {j}\"\n            )\n\n    print(\"\u2705 \u6ce8\u610f\u30de\u30b9\u30af\u30c6\u30b9\u30c8: PASS\")\n\ndef test_key_value_different(self):\n    \"\"\"\u7570\u306a\u308bKey/Value\u306e\u30c6\u30b9\u30c8\"\"\"\n    # Query, Key, Value\u304c\u7570\u306a\u308b\u5834\u5408\n    q = torch.randn(self.batch_size, self.seq_len, self.d_model)\n    k = torch.randn(self.batch_size, self.seq_len * 2, self.d_model)\n    v = torch.randn(self.batch_size, self.seq_len * 2, self.d_model)\n\n    output, weights = self.attention(q, k, v)\n\n    # \u51fa\u529b\u5f62\u72b6\u306e\u78ba\u8a8d\n    self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\n    self.assertEqual(weights.shape, (self.batch_size, self.seq_len, self.seq_len * 2))\n\n    print(\"\u2705 \u7570\u306a\u308bKey/Value\u30c6\u30b9\u30c8: PASS\")\n\ndef test_attention_weights_sum(self):\n    \"\"\"\u6ce8\u610f\u91cd\u307f\u306e\u548c\u304c1\u3067\u3042\u308b\u3053\u3068\u3092\u30c6\u30b9\u30c8\"\"\"\n    x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n    _, weights = self.attention(x, x, x)\n\n    # \u5404\u884c\u306e\u548c\u304c1\u306b\u8fd1\u3044\u3053\u3068\u3092\u78ba\u8a8d\n    row_sums = weights.sum(dim=-1)\n    expected = torch.ones_like(row_sums)\n\n    assert_close(row_sums, expected, rtol=1e-5, atol=1e-5)\n\n    print(\"\u2705 \u6ce8\u610f\u91cd\u307f\u306e\u548c\u30c6\u30b9\u30c8: PASS\")\n\ndef test_gradient_flow(self):\n    \"\"\"\u52fe\u914d\u304c\u6b63\u3057\u304f\u6d41\u308c\u308b\u3053\u3068\u3092\u30c6\u30b9\u30c8\"\"\"\n    x = torch.randn(self.batch_size, self.seq_len, self.d_model, requires_grad=True)\n\n    output, _ = self.attention(x, x, x)\n    loss = output.mean()\n    loss.backward()\n\n    # \u52fe\u914d\u304c\u8a08\u7b97\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\n    self.assertIsNotNone(x.grad)\n    self.assertFalse(torch.isnan(x.grad).any())\n    self.assertFalse(torch.isinf(x.grad).any())\n\n    print(\"\u2705 \u52fe\u914d\u30d5\u30ed\u30fc\u30c6\u30b9\u30c8: PASS\")\n</code></pre> <p>class TestPositionalEncoding(unittest.TestCase):     \"\"\"\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u30c6\u30b9\u30c8\"\"\"</p> <pre><code>def setUp(self):\n    self.d_model = 512\n    self.max_len = 1000\n\ndef test_sinusoidal_encoding_properties(self):\n    \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u6027\u8cea\u3092\u30c6\u30b9\u30c8\"\"\"\n    # \u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u751f\u6210\n    pe = self._create_sinusoidal_encoding(self.max_len, self.d_model)\n\n    # 1. \u5024\u306e\u7bc4\u56f2\u304c[-1, 1]\u3067\u3042\u308b\u3053\u3068\n    self.assertLessEqual(pe.max().item(), 1.0)\n    self.assertGreaterEqual(pe.min().item(), -1.0)\n\n    # 2. \u5076\u6570\u6b21\u5143\u304csin\u3001\u5947\u6570\u6b21\u5143\u304ccos\u3067\u3042\u308b\u3053\u3068\n    pos = 10  # \u30c6\u30b9\u30c8\u4f4d\u7f6e\n    for i in range(0, self.d_model, 2):\n        div_term = 10000 ** (i / self.d_model)\n        expected_sin = math.sin(pos / div_term)\n        expected_cos = math.cos(pos / div_term)\n\n        self.assertAlmostEqual(pe[pos, i].item(), expected_sin, places=5)\n        if i + 1 &lt; self.d_model:\n            self.assertAlmostEqual(pe[pos, i + 1].item(), expected_cos, places=5)\n\n    print(\"\u2705 \u6b63\u5f26\u6ce2\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30c6\u30b9\u30c8: PASS\")\n\ndef test_relative_position_encoding(self):\n    \"\"\"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u6027\u8cea\u3092\u30c6\u30b9\u30c8\"\"\"\n    pe = self._create_sinusoidal_encoding(self.max_len, self.d_model)\n\n    # \u56fa\u5b9a\u3055\u308c\u305f\u76f8\u5bfe\u8ddd\u96e2\u3067\u306e\u5185\u7a4d\u304c\u4e00\u5b9a\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n    distance = 5\n    products = []\n\n    for pos in range(10, 20):\n        dot_product = torch.dot(pe[pos], pe[pos + distance])\n        products.append(dot_product.item())\n\n    # \u6a19\u6e96\u504f\u5dee\u304c\u5c0f\u3055\u3044\u3053\u3068\u3092\u78ba\u8a8d\n    std = np.std(products)\n    self.assertLess(std, 0.01, \"\u76f8\u5bfe\u4f4d\u7f6e\u306e\u5185\u7a4d\u306f\u4e00\u5b9a\u3067\u3042\u308b\u3079\u304d\")\n\n    print(\"\u2705 \u76f8\u5bfe\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30c6\u30b9\u30c8: PASS\")\n\ndef _create_sinusoidal_encoding(self, max_len: int, d_model: int) -&gt; torch.Tensor:\n    \"\"\"\u6b63\u5f26\u6ce2\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f5c\u6210\"\"\"\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len).unsqueeze(1).float()\n\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n    )\n\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n\n    return pe\n</code></pre> <p>class TestTransformerBlock(unittest.TestCase):     \"\"\"Transformer\u30d6\u30ed\u30c3\u30af\u306e\u30c6\u30b9\u30c8\"\"\"</p> <pre><code>def setUp(self):\n    self.d_model = 256\n    self.n_heads = 8\n    self.d_ff = 1024\n    self.dropout = 0.1\n\n    # Transformer\u30d6\u30ed\u30c3\u30af\u306e\u4f5c\u6210\n    self.encoder_layer = nn.TransformerEncoderLayer(\n        d_model=self.d_model,\n        nhead=self.n_heads,\n        dim_feedforward=self.d_ff,\n        dropout=self.dropout,\n        batch_first=True\n    )\n\ndef test_residual_connections(self):\n    \"\"\"\u6b8b\u5dee\u63a5\u7d9a\u306e\u30c6\u30b9\u30c8\"\"\"\n    batch_size = 2\n    seq_len = 10\n\n    # \u5165\u529b\n    x = torch.randn(batch_size, seq_len, self.d_model)\n\n    # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u3092\u7121\u52b9\u5316\uff08\u30c6\u30b9\u30c8\u306e\u305f\u3081\uff09\n    self.encoder_layer.eval()\n\n    # \u5c0f\u3055\u306a\u91cd\u307f\u3067\u521d\u671f\u5316\uff08\u6b8b\u5dee\u63a5\u7d9a\u306e\u52b9\u679c\u3092\u898b\u3084\u3059\u304f\u3059\u308b\uff09\n    for param in self.encoder_layer.parameters():\n        param.data.mul_(0.01)\n\n    output = self.encoder_layer(x)\n\n    # \u51fa\u529b\u304c\u5165\u529b\u306b\u8fd1\u3044\u3053\u3068\u3092\u78ba\u8a8d\uff08\u6b8b\u5dee\u63a5\u7d9a\u306e\u52b9\u679c\uff09\n    diff = (output - x).abs().mean()\n    self.assertLess(diff.item(), 0.5, \"\u6b8b\u5dee\u63a5\u7d9a\u306b\u3088\u308a\u51fa\u529b\u306f\u5165\u529b\u306b\u8fd1\u3044\u306f\u305a\")\n\n    print(\"\u2705 \u6b8b\u5dee\u63a5\u7d9a\u30c6\u30b9\u30c8: PASS\")\n\ndef test_layer_norm_effect(self):\n    \"\"\"\u5c64\u6b63\u898f\u5316\u306e\u52b9\u679c\u3092\u30c6\u30b9\u30c8\"\"\"\n    batch_size = 2\n    seq_len = 10\n\n    # \u5927\u304d\u306a\u5024\u3092\u6301\u3064\u5165\u529b\n    x = torch.randn(batch_size, seq_len, self.d_model) * 100\n\n    output = self.encoder_layer(x)\n\n    # \u51fa\u529b\u306e\u5404\u4f4d\u7f6e\u3067\u306e\u5e73\u5747\u3068\u5206\u6563\u3092\u8a08\u7b97\n    mean = output.mean(dim=-1)\n    var = output.var(dim=-1)\n\n    # \u5c64\u6b63\u898f\u5316\u306b\u3088\u308a\u3001\u5e73\u5747\u304c0\u306b\u8fd1\u304f\u3001\u5206\u6563\u304c1\u306b\u8fd1\u3044\u3053\u3068\u3092\u78ba\u8a8d\n    self.assertLess(mean.abs().mean().item(), 0.1)\n    self.assertLess((var - 1).abs().mean().item(), 0.5)\n\n    print(\"\u2705 \u5c64\u6b63\u898f\u5316\u30c6\u30b9\u30c8: PASS\")\n</code></pre>"},{"location":"part4/validation/#162","title":"16.2 \u7d71\u5408\u30c6\u30b9\u30c8","text":"<p>class IntegrationTests:     \"\"\"\u30e2\u30c7\u30eb\u5168\u4f53\u306e\u7d71\u5408\u30c6\u30b9\u30c8\"\"\"</p> <pre><code>def __init__(self, model_class):\n    self.model_class = model_class\n\ndef test_full_forward_pass(self):\n    \"\"\"\u5b8c\u5168\u306a\u9806\u4f1d\u64ad\u306e\u30c6\u30b9\u30c8\"\"\"\n    print(\"\\n=== \u7d71\u5408\u30c6\u30b9\u30c8: \u5b8c\u5168\u306a\u9806\u4f1d\u64ad ===\")\n\n    # \u30e2\u30c7\u30eb\u306e\u4f5c\u6210\n    vocab_size = 1000\n    d_model = 256\n    model = self.model_class(vocab_size=vocab_size, d_model=d_model)\n    model.eval()\n\n    # \u30c6\u30b9\u30c8\u30b1\u30fc\u30b9\n    test_cases = [\n        {\"batch_size\": 1, \"seq_len\": 10},\n        {\"batch_size\": 4, \"seq_len\": 50},\n        {\"batch_size\": 8, \"seq_len\": 100},\n    ]\n\n    for case in test_cases:\n        batch_size = case[\"batch_size\"]\n        seq_len = case[\"seq_len\"]\n\n        # \u5165\u529b\u30c7\u30fc\u30bf\n        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n\n        # \u9806\u4f1d\u64ad\n        with torch.no_grad():\n            output = model(input_ids)\n\n        # \u51fa\u529b\u5f62\u72b6\u306e\u78ba\u8a8d\n        expected_shape = (batch_size, seq_len, vocab_size)\n        assert output.shape == expected_shape, \\\n            f\"Expected shape {expected_shape}, got {output.shape}\"\n\n        # NaN\u3084Inf\u304c\u306a\u3044\u3053\u3068\u3092\u78ba\u8a8d\n        assert not torch.isnan(output).any(), \"Output contains NaN\"\n        assert not torch.isinf(output).any(), \"Output contains Inf\"\n\n        print(f\"\u2705 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba={batch_size}, \u30b7\u30fc\u30b1\u30f3\u30b9\u9577={seq_len}: PASS\")\n\ndef test_generation_consistency(self):\n    \"\"\"\u751f\u6210\u306e\u4e00\u8cab\u6027\u30c6\u30b9\u30c8\"\"\"\n    print(\"\\n=== \u7d71\u5408\u30c6\u30b9\u30c8: \u751f\u6210\u306e\u4e00\u8cab\u6027 ===\")\n\n    vocab_size = 100\n    model = self.model_class(vocab_size=vocab_size, d_model=128)\n    model.eval()\n\n    # \u30b7\u30fc\u30c9\u56fa\u5b9a\n    torch.manual_seed(42)\n\n    # \u540c\u3058\u30d7\u30ed\u30f3\u30d7\u30c8\u304b\u3089\u751f\u6210\n    prompt = torch.tensor([[1, 2, 3]])\n\n    # \u8907\u6570\u56de\u751f\u6210\n    outputs = []\n    for _ in range(3):\n        torch.manual_seed(42)  # \u540c\u3058\u30b7\u30fc\u30c9\n        output = model.generate(prompt, max_new_tokens=10, temperature=1.0)\n        outputs.append(output)\n\n    # \u3059\u3079\u3066\u306e\u51fa\u529b\u304c\u540c\u3058\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n    for i in range(1, len(outputs)):\n        assert torch.equal(outputs[0], outputs[i]), \\\n            f\"\u751f\u6210\u7d50\u679c\u304c\u4e00\u8cab\u3057\u3066\u3044\u307e\u305b\u3093: {i}\u56de\u76ee\"\n\n    print(\"\u2705 \u751f\u6210\u306e\u4e00\u8cab\u6027: PASS\")\n\ndef test_attention_pattern_analysis(self):\n    \"\"\"\u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\"\"\"\n    print(\"\\n=== \u7d71\u5408\u30c6\u30b9\u30c8: \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u5206\u6790 ===\")\n\n    # \u7279\u5225\u306a\u30c6\u30b9\u30c8\u30b1\u30fc\u30b9\uff1a\u7e70\u308a\u8fd4\u3057\u30d1\u30bf\u30fc\u30f3\n    vocab_size = 50\n    model = self.model_class(vocab_size=vocab_size, d_model=128, n_layers=2)\n    model.eval()\n\n    # \u7e70\u308a\u8fd4\u3057\u306e\u3042\u308b\u5165\u529b\n    # \"A B C A B C A B C\"\u306e\u3088\u3046\u306a\u30d1\u30bf\u30fc\u30f3\n    pattern = [10, 20, 30]\n    input_ids = torch.tensor([pattern * 3]).to(torch.long)\n\n    # \u6ce8\u610f\u91cd\u307f\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30d5\u30c3\u30af\n    attention_weights = []\n\n    def hook_fn(module, input, output):\n        if isinstance(output, tuple) and len(output) == 2:\n            _, attn = output\n            if attn is not None:\n                attention_weights.append(attn.detach())\n\n    # \u30d5\u30c3\u30af\u3092\u767b\u9332\n    hooks = []\n    for module in model.modules():\n        if isinstance(module, nn.MultiheadAttention):\n            hook = module.register_forward_hook(hook_fn)\n            hooks.append(hook)\n\n    # \u9806\u4f1d\u64ad\n    with torch.no_grad():\n        _ = model(input_ids)\n\n    # \u30d5\u30c3\u30af\u3092\u524a\u9664\n    for hook in hooks:\n        hook.remove()\n\n    # \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u306e\u5206\u6790\n    if attention_weights:\n        # \u6700\u521d\u306e\u5c64\u306e\u6ce8\u610f\u91cd\u307f\u3092\u5206\u6790\n        attn = attention_weights[0][0].mean(dim=0)  # \u30d8\u30c3\u30c9\u306e\u5e73\u5747\n\n        # \u540c\u3058\u30c8\u30fc\u30af\u30f3\u3078\u306e\u6ce8\u610f\u304c\u9ad8\u3044\u3053\u3068\u3092\u78ba\u8a8d\n        for i in range(3):\n            for j in range(3):\n                if i != j:\n                    pos1 = i * 3\n                    pos2 = j * 3\n                    # \u540c\u3058\u30c8\u30fc\u30af\u30f3\uff08\u4f4d\u7f6e\u306f\u9055\u3046\uff09\u3078\u306e\u6ce8\u610f\n                    similarity = attn[pos1, pos2].item()\n                    print(f\"  \u4f4d\u7f6e{pos1} \u2192 \u4f4d\u7f6e{pos2}\u306e\u6ce8\u610f: {similarity:.3f}\")\n\n    print(\"\u2705 \u6ce8\u610f\u30d1\u30bf\u30fc\u30f3\u5206\u6790: \u5b8c\u4e86\")\n</code></pre>"},{"location":"part4/validation/#163","title":"16.3 \u6027\u80fd\u30d9\u30f3\u30c1\u30de\u30fc\u30af","text":"<p>class PerformanceBenchmark:     \"\"\"\u6027\u80fd\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u30c6\u30b9\u30c8\"\"\"</p> <pre><code>def __init__(self, model):\n    self.model = model\n    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    self.model.to(self.device)\n\ndef benchmark_inference_speed(self):\n    \"\"\"\u63a8\u8ad6\u901f\u5ea6\u306e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\"\"\"\n    print(\"\\n=== \u6027\u80fd\u30d9\u30f3\u30c1\u30de\u30fc\u30af: \u63a8\u8ad6\u901f\u5ea6 ===\")\n\n    # \u30c6\u30b9\u30c8\u8a2d\u5b9a\n    batch_sizes = [1, 4, 16, 32]\n    seq_lengths = [10, 50, 100, 200]\n    vocab_size = 1000\n\n    results = {}\n\n    for batch_size in batch_sizes:\n        results[batch_size] = {}\n\n        for seq_len in seq_lengths:\n            # \u5165\u529b\u30c7\u30fc\u30bf\n            input_ids = torch.randint(0, vocab_size, \n                                    (batch_size, seq_len)).to(self.device)\n\n            # \u30a6\u30a9\u30fc\u30e0\u30a2\u30c3\u30d7\n            for _ in range(10):\n                with torch.no_grad():\n                    _ = self.model(input_ids)\n\n            # \u6642\u9593\u6e2c\u5b9a\n            import time\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n\n            start_time = time.time()\n            n_iterations = 100\n\n            for _ in range(n_iterations):\n                with torch.no_grad():\n                    _ = self.model(input_ids)\n\n            torch.cuda.synchronize() if torch.cuda.is_available() else None\n            end_time = time.time()\n\n            # \u5e73\u5747\u6642\u9593\uff08\u30df\u30ea\u79d2\uff09\n            avg_time = (end_time - start_time) / n_iterations * 1000\n            throughput = batch_size / (avg_time / 1000)  # samples/sec\n\n            results[batch_size][seq_len] = {\n                'time_ms': avg_time,\n                'throughput': throughput\n            }\n\n            print(f\"  Batch={batch_size}, Seq={seq_len}: \"\n                  f\"{avg_time:.2f}ms, {throughput:.1f} samples/sec\")\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    self._visualize_benchmark_results(results)\n\n    return results\n\ndef benchmark_memory_usage(self):\n    \"\"\"\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u30d9\u30f3\u30c1\u30de\u30fc\u30af\"\"\"\n    print(\"\\n=== \u6027\u80fd\u30d9\u30f3\u30c1\u30de\u30fc\u30af: \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf ===\")\n\n    if not torch.cuda.is_available():\n        print(\"  GPU\u304c\u5229\u7528\u3067\u304d\u306a\u3044\u305f\u3081\u3001\u30e1\u30e2\u30ea\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u3092\u30b9\u30ad\u30c3\u30d7\")\n        return\n\n    seq_lengths = [10, 50, 100, 200, 500]\n    batch_size = 4\n    vocab_size = 1000\n\n    memory_usage = []\n\n    for seq_len in seq_lengths:\n        # \u30e1\u30e2\u30ea\u3092\u30af\u30ea\u30a2\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n        # \u5165\u529b\u30c7\u30fc\u30bf\n        input_ids = torch.randint(0, vocab_size, \n                                (batch_size, seq_len)).to(self.device)\n\n        # \u9806\u4f1d\u64ad\n        with torch.no_grad():\n            _ = self.model(input_ids)\n\n        # \u30d4\u30fc\u30af\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n        peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n        memory_usage.append(peak_memory)\n\n        print(f\"  Seq={seq_len}: {peak_memory:.1f} MB\")\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6210\u9577\u7387\u3092\u5206\u6790\n    self._analyze_memory_scaling(seq_lengths, memory_usage)\n\ndef _visualize_benchmark_results(self, results):\n    \"\"\"\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u7d50\u679c\u3092\u53ef\u8996\u5316\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # \u63a8\u8ad6\u6642\u9593\n    for batch_size, seq_results in results.items():\n        seq_lengths = list(seq_results.keys())\n        times = [seq_results[seq]['time_ms'] for seq in seq_lengths]\n        ax1.plot(seq_lengths, times, marker='o', label=f'Batch={batch_size}')\n\n    ax1.set_xlabel('Sequence Length')\n    ax1.set_ylabel('Inference Time (ms)')\n    ax1.set_title('Inference Time vs Sequence Length')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_yscale('log')\n\n    # \u30b9\u30eb\u30fc\u30d7\u30c3\u30c8\n    for batch_size, seq_results in results.items():\n        seq_lengths = list(seq_results.keys())\n        throughputs = [seq_results[seq]['throughput'] for seq in seq_lengths]\n        ax2.plot(seq_lengths, throughputs, marker='o', label=f'Batch={batch_size}')\n\n    ax2.set_xlabel('Sequence Length')\n    ax2.set_ylabel('Throughput (samples/sec)')\n    ax2.set_title('Throughput vs Sequence Length')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    ax2.set_yscale('log')\n\n    plt.tight_layout()\n    plt.show()\n\ndef _analyze_memory_scaling(self, seq_lengths, memory_usage):\n    \"\"\"\u30e1\u30e2\u30ea\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u306e\u5206\u6790\"\"\"\n    # O(n^2)\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\n    coeffs = np.polyfit(seq_lengths, memory_usage, 2)\n    poly = np.poly1d(coeffs)\n\n    plt.figure(figsize=(8, 6))\n    plt.scatter(seq_lengths, memory_usage, label='Actual', s=100)\n\n    # \u30d5\u30a3\u30c3\u30c8\u66f2\u7dda\n    x_fit = np.linspace(min(seq_lengths), max(seq_lengths), 100)\n    y_fit = poly(x_fit)\n    plt.plot(x_fit, y_fit, 'r--', label=f'Quadratic Fit', alpha=0.7)\n\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Memory Usage (MB)')\n    plt.title('Memory Usage Scaling')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    print(f\"\\n  \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306f O(n^{2:.1f}) \u3067\u30b9\u30b1\u30fc\u30eb\")\n</code></pre>"},{"location":"part4/validation/#164","title":"16.4 \u6bd4\u8f03\u691c\u8a3c","text":"<p>class ComparativeValidation:     \"\"\"\u4ed6\u306e\u5b9f\u88c5\u3068\u306e\u6bd4\u8f03\u691c\u8a3c\"\"\"</p> <pre><code>def __init__(self, custom_model, reference_model=None):\n    self.custom_model = custom_model\n    self.reference_model = reference_model\n\ndef compare_with_pytorch_transformer(self):\n    \"\"\"PyTorch\u306e\u6a19\u6e96Transformer\u3068\u306e\u6bd4\u8f03\"\"\"\n    print(\"\\n=== \u6bd4\u8f03\u691c\u8a3c: PyTorch\u6a19\u6e96\u5b9f\u88c5\u3068\u306e\u6bd4\u8f03 ===\")\n\n    d_model = 256\n    n_heads = 8\n    n_layers = 2\n\n    # PyTorch\u306e\u6a19\u6e96Transformer\n    pytorch_encoder = nn.TransformerEncoder(\n        nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=n_heads,\n            batch_first=True\n        ),\n        num_layers=n_layers\n    )\n\n    # \u540c\u3058\u91cd\u307f\u3067\u521d\u671f\u5316\uff08\u53ef\u80fd\u306a\u9650\u308a\uff09\n    # \u3053\u3053\u3067\u306f\u7c21\u7565\u5316\u306e\u305f\u3081\u65b0\u898f\u306e\u91cd\u307f\u3092\u4f7f\u7528\n\n    # \u30c6\u30b9\u30c8\u5165\u529b\n    batch_size = 2\n    seq_len = 10\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u4e21\u65b9\u306e\u30e2\u30c7\u30eb\u3067\u63a8\u8ad6\n    pytorch_encoder.eval()\n    self.custom_model.eval() if self.custom_model else None\n\n    with torch.no_grad():\n        pytorch_output = pytorch_encoder(x)\n\n        if self.custom_model:\n            # \u30ab\u30b9\u30bf\u30e0\u30e2\u30c7\u30eb\u304c\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306e\u307f\u306e\u5834\u5408\n            custom_output = self.custom_model.encoder(x) if hasattr(self.custom_model, 'encoder') else None\n\n            if custom_output is not None:\n                # \u51fa\u529b\u306e\u7d71\u8a08\u3092\u6bd4\u8f03\n                print(f\"  PyTorch\u51fa\u529b - \u5e73\u5747: {pytorch_output.mean():.4f}, \"\n                      f\"\u6a19\u6e96\u504f\u5dee: {pytorch_output.std():.4f}\")\n                print(f\"  \u30ab\u30b9\u30bf\u30e0\u51fa\u529b - \u5e73\u5747: {custom_output.mean():.4f}, \"\n                      f\"\u6a19\u6e96\u504f\u5dee: {custom_output.std():.4f}\")\n        else:\n            print(f\"  PyTorch\u51fa\u529b\u5f62\u72b6: {pytorch_output.shape}\")\n            print(f\"  \u5e73\u5747: {pytorch_output.mean():.4f}, \"\n                  f\"\u6a19\u6e96\u504f\u5dee: {pytorch_output.std():.4f}\")\n\ndef validate_attention_computation(self):\n    \"\"\"\u6ce8\u610f\u8a08\u7b97\u306e\u691c\u8a3c\"\"\"\n    print(\"\\n=== \u6bd4\u8f03\u691c\u8a3c: \u6ce8\u610f\u8a08\u7b97\u306e\u6b63\u78ba\u6027 ===\")\n\n    # \u624b\u52d5\u3067\u306e\u6ce8\u610f\u8a08\u7b97\n    d_model = 64\n    seq_len = 5\n\n    # \u30e9\u30f3\u30c0\u30e0\u306aQ, K, V\n    torch.manual_seed(42)\n    Q = torch.randn(1, seq_len, d_model)\n    K = torch.randn(1, seq_len, d_model)\n    V = torch.randn(1, seq_len, d_model)\n\n    # \u624b\u52d5\u8a08\u7b97\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n    attention_weights = F.softmax(scores, dim=-1)\n    output_manual = torch.matmul(attention_weights, V)\n\n    # nn.MultiheadAttention\u3067\u306e\u8a08\u7b97\uff08single head\uff09\n    attention = nn.MultiheadAttention(d_model, 1, batch_first=True)\n\n    # \u91cd\u307f\u3092\u8a2d\u5b9a\uff08\u6052\u7b49\u5909\u63db\uff09\n    with torch.no_grad():\n        attention.in_proj_weight.data = torch.eye(3 * d_model)\n        attention.in_proj_bias.data = torch.zeros(3 * d_model)\n        attention.out_proj.weight.data = torch.eye(d_model)\n        attention.out_proj.bias.data = torch.zeros(d_model)\n\n    # \u5165\u529b\u3092\u7d50\u5408\n    qkv = torch.cat([Q, K, V], dim=-1)\n    x = qkv[:, :, :d_model]  # Q\u306e\u90e8\u5206\u306e\u307f\uff08\u7c21\u7565\u5316\uff09\n\n    output_pytorch, weights_pytorch = attention(Q, K, V)\n\n    print(f\"  \u624b\u52d5\u8a08\u7b97\u3068PyTorch\u306e\u5dee:\")\n    print(f\"  \u6ce8\u610f\u91cd\u307f\u306e\u5dee: {(attention_weights - weights_pytorch).abs().max():.6f}\")\n    print(f\"  \u51fa\u529b\u306e\u5dee: {(output_manual - output_pytorch).abs().mean():.6f}\")\n</code></pre> <p>class ValidationSuite:     \"\"\"\u5b8c\u5168\u306a\u691c\u8a3c\u30b9\u30a4\u30fc\u30c8\"\"\"</p> <pre><code>def __init__(self, model_class):\n    self.model_class = model_class\n    self.test_results = {}\n\ndef run_all_tests(self):\n    \"\"\"\u3059\u3079\u3066\u306e\u30c6\u30b9\u30c8\u3092\u5b9f\u884c\"\"\"\n    print(\"=\" * 70)\n    print(\"Transformer\u691c\u8a3c\u30b9\u30a4\u30fc\u30c8\")\n    print(\"=\" * 70)\n\n    # 1. \u5358\u4f53\u30c6\u30b9\u30c8\n    print(\"\\n\u30101. \u5358\u4f53\u30c6\u30b9\u30c8\u3011\")\n    self._run_unit_tests()\n\n    # 2. \u7d71\u5408\u30c6\u30b9\u30c8\n    print(\"\\n\u30102. \u7d71\u5408\u30c6\u30b9\u30c8\u3011\")\n    self._run_integration_tests()\n\n    # 3. \u6027\u80fd\u30c6\u30b9\u30c8\n    print(\"\\n\u30103. \u6027\u80fd\u30c6\u30b9\u30c8\u3011\")\n    self._run_performance_tests()\n\n    # 4. \u6bd4\u8f03\u691c\u8a3c\n    print(\"\\n\u30104. \u6bd4\u8f03\u691c\u8a3c\u3011\")\n    self._run_comparative_tests()\n\n    # \u7d50\u679c\u30b5\u30de\u30ea\u30fc\n    self._print_summary()\n\ndef _run_unit_tests(self):\n    \"\"\"\u5358\u4f53\u30c6\u30b9\u30c8\u306e\u5b9f\u884c\"\"\"\n    # \u30c6\u30b9\u30c8\u30e9\u30f3\u30ca\u30fc\u306e\u4f5c\u6210\n    loader = unittest.TestLoader()\n    suite = unittest.TestSuite()\n\n    # \u30c6\u30b9\u30c8\u30af\u30e9\u30b9\u3092\u8ffd\u52a0\n    suite.addTests(loader.loadTestsFromTestCase(TestMultiHeadAttention))\n    suite.addTests(loader.loadTestsFromTestCase(TestPositionalEncoding))\n    suite.addTests(loader.loadTestsFromTestCase(TestTransformerBlock))\n\n    # \u30c6\u30b9\u30c8\u5b9f\u884c\n    runner = unittest.TextTestRunner(verbosity=0)\n    result = runner.run(suite)\n\n    self.test_results['unit_tests'] = {\n        'total': result.testsRun,\n        'failures': len(result.failures),\n        'errors': len(result.errors)\n    }\n\ndef _run_integration_tests(self):\n    \"\"\"\u7d71\u5408\u30c6\u30b9\u30c8\u306e\u5b9f\u884c\"\"\"\n    integration = IntegrationTests(self.model_class)\n\n    try:\n        integration.test_full_forward_pass()\n        integration.test_generation_consistency()\n        integration.test_attention_pattern_analysis()\n        self.test_results['integration_tests'] = 'PASS'\n    except Exception as e:\n        self.test_results['integration_tests'] = f'FAIL: {str(e)}'\n\ndef _run_performance_tests(self):\n    \"\"\"\u6027\u80fd\u30c6\u30b9\u30c8\u306e\u5b9f\u884c\"\"\"\n    # \u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u30c6\u30b9\u30c8\n    model = self.model_class(vocab_size=1000, d_model=128, n_layers=2)\n    benchmark = PerformanceBenchmark(model)\n\n    results = benchmark.benchmark_inference_speed()\n    benchmark.benchmark_memory_usage()\n\n    self.test_results['performance_tests'] = results\n\ndef _run_comparative_tests(self):\n    \"\"\"\u6bd4\u8f03\u30c6\u30b9\u30c8\u306e\u5b9f\u884c\"\"\"\n    model = self.model_class(vocab_size=1000, d_model=256, n_layers=2)\n    comparative = ComparativeValidation(model)\n\n    comparative.compare_with_pytorch_transformer()\n    comparative.validate_attention_computation()\n\n    self.test_results['comparative_tests'] = 'COMPLETED'\n\ndef _print_summary(self):\n    \"\"\"\u30c6\u30b9\u30c8\u7d50\u679c\u306e\u30b5\u30de\u30ea\u30fc\"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"\u30c6\u30b9\u30c8\u7d50\u679c\u30b5\u30de\u30ea\u30fc\")\n    print(\"=\" * 70)\n\n    # \u5358\u4f53\u30c6\u30b9\u30c8\u7d50\u679c\n    unit_results = self.test_results.get('unit_tests', {})\n    print(f\"\\n\u5358\u4f53\u30c6\u30b9\u30c8: {unit_results.get('total', 0)}\u500b\u306e\u30c6\u30b9\u30c8\")\n    print(f\"  \u6210\u529f: {unit_results.get('total', 0) - unit_results.get('failures', 0) - unit_results.get('errors', 0)}\")\n    print(f\"  \u5931\u6557: {unit_results.get('failures', 0)}\")\n    print(f\"  \u30a8\u30e9\u30fc: {unit_results.get('errors', 0)}\")\n\n    # \u7d71\u5408\u30c6\u30b9\u30c8\u7d50\u679c\n    print(f\"\\n\u7d71\u5408\u30c6\u30b9\u30c8: {self.test_results.get('integration_tests', 'N/A')}\")\n\n    # \u6027\u80fd\u30c6\u30b9\u30c8\u7d50\u679c\n    if 'performance_tests' in self.test_results:\n        print(\"\\n\u6027\u80fd\u30c6\u30b9\u30c8: \u5b8c\u4e86\")\n        # \u4ee3\u8868\u7684\u306a\u7d50\u679c\u3092\u8868\u793a\n        perf_results = self.test_results['performance_tests']\n        if 4 in perf_results and 100 in perf_results[4]:\n            time_ms = perf_results[4][100]['time_ms']\n            throughput = perf_results[4][100]['throughput']\n            print(f\"  \u4ee3\u8868\u4f8b (Batch=4, Seq=100): {time_ms:.2f}ms, {throughput:.1f} samples/sec\")\n\n    # \u6bd4\u8f03\u30c6\u30b9\u30c8\u7d50\u679c\n    print(f\"\\n\u6bd4\u8f03\u30c6\u30b9\u30c8: {self.test_results.get('comparative_tests', 'N/A')}\")\n\n    # \u7dcf\u5408\u8a55\u4fa1\n    print(\"\\n\" + \"=\" * 70)\n    all_passed = (\n        unit_results.get('failures', 1) == 0 and\n        unit_results.get('errors', 1) == 0 and\n        self.test_results.get('integration_tests') == 'PASS'\n    )\n\n    if all_passed:\n        print(\"\u2705 \u3059\u3079\u3066\u306e\u30c6\u30b9\u30c8\u306b\u5408\u683c\u3057\u307e\u3057\u305f\uff01\")\n    else:\n        print(\"\u274c \u4e00\u90e8\u306e\u30c6\u30b9\u30c8\u306b\u5931\u6557\u3057\u307e\u3057\u305f\u3002\")\n</code></pre>"},{"location":"part4/validation/#_4","title":"\u5b9f\u969b\u306e\u691c\u8a3c\u4f8b","text":"<p>def run_validation_example():     \"\"\"\u691c\u8a3c\u306e\u5b9f\u884c\u4f8b\"\"\"</p> <pre><code># \u30c0\u30df\u30fc\u306eTransformer\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\nclass DummyTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model, n_layers=4, n_heads=8):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=d_model,\n                nhead=n_heads,\n                batch_first=True\n            ),\n            num_layers=n_layers\n        )\n        self.output_projection = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.encoder(x)\n        x = self.output_projection(x)\n        return x\n\n    def generate(self, prompt, max_new_tokens, temperature):\n        # \u7c21\u7565\u5316\u3055\u308c\u305f\u751f\u6210\n        current = prompt\n        for _ in range(max_new_tokens):\n            output = self.forward(current)\n            next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n            current = torch.cat([current, next_token], dim=1)\n        return current\n\n# \u691c\u8a3c\u30b9\u30a4\u30fc\u30c8\u306e\u5b9f\u884c\nvalidation = ValidationSuite(DummyTransformer)\nvalidation.run_all_tests()\n</code></pre>"},{"location":"part4/validation/#_5","title":"\u30a8\u30e9\u30fc\u5206\u6790\u3068\u30c7\u30d0\u30c3\u30b0\u306e\u30d2\u30f3\u30c8","text":"<p>def debugging_tips():     \"\"\"\u30c7\u30d0\u30c3\u30b0\u306e\u30d2\u30f3\u30c8\"\"\"     print(\"\\n\" + \"=\" * 70)     print(\"\u4e00\u822c\u7684\u306a\u554f\u984c\u3068\u30c7\u30d0\u30c3\u30b0\u306e\u30d2\u30f3\u30c8\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code>tips = {\n    \"\u52fe\u914d\u6d88\u5931/\u7206\u767a\": [\n        \"\u5c64\u6b63\u898f\u5316\u304c\u6b63\u3057\u304f\u9069\u7528\u3055\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\",\n        \"\u6b8b\u5dee\u63a5\u7d9a\u304c\u6a5f\u80fd\u3057\u3066\u3044\u308b\u304b\u78ba\u8a8d\",\n        \"\u5b66\u7fd2\u7387\u304c\u9069\u5207\u304b\u78ba\u8a8d\",\n        \"\u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\u3092\u4f7f\u7528\"\n    ],\n\n    \"\u6ce8\u610f\u306e\u504f\u308a\": [\n        \"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u4fc2\u6570(1/\u221ad_k)\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\",\n        \"\u30de\u30b9\u30af\u304c\u6b63\u3057\u304f\u9069\u7528\u3055\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\",\n        \"\u521d\u671f\u5316\u65b9\u6cd5\u3092\u78ba\u8a8d\"\n    ],\n\n    \"\u751f\u6210\u54c1\u8cea\": [\n        \"\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8abf\u6574\",\n        \"Top-k/Top-p\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306e\u4f7f\u7528\",\n        \"\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u5b9f\u88c5\",\n        \"\u7e70\u308a\u8fd4\u3057\u30da\u30ca\u30eb\u30c6\u30a3\u306e\u8ffd\u52a0\"\n    ],\n\n    \"\u30e1\u30e2\u30ea\u4e0d\u8db3\": [\n        \"\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u524a\u6e1b\",\n        \"\u30b7\u30fc\u30b1\u30f3\u30b9\u9577\u306e\u5236\u9650\",\n        \"\u52fe\u914d\u7d2f\u7a4d\u306e\u4f7f\u7528\",\n        \"Mixed Precision Training\u306e\u4f7f\u7528\"\n    ]\n}\n\nfor problem, solutions in tips.items():\n    print(f\"{problem}:\")\n    for solution in solutions:\n        print(f\"  \u2022 {solution}\")\n    print()\n</code></pre> <p>if name == \"main\":     # \u691c\u8a3c\u4f8b\u306e\u5b9f\u884c     run_validation_example()</p> <pre><code># \u30c7\u30d0\u30c3\u30b0\u306e\u30d2\u30f3\u30c8\ndebugging_tips()\n</code></pre>"},{"location":"part5/gpt-architecture/","title":"GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3","text":""},{"location":"part5/gpt-architecture/#_1","title":"\u306f\u3058\u3081\u306b\uff1a\u751f\u6210\u306e\u9769\u547d","text":"<p>\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u306e\u51e6\u7406\u7cfb\u3067\u3001REPL\u3092\u5b9f\u88c5\u3057\u305f\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u3057\u305f\u30b3\u30fc\u30c9\u3092\u89e3\u6790\u3057\u3001\u8a55\u4fa1\u3057\u3001\u7d50\u679c\u3092\u8fd4\u3059\u3002\u305d\u3057\u3066\u91cd\u8981\u306a\u306e\u306f\u3001\u4ee5\u524d\u306e\u6587\u8108\u3092\u8a18\u61b6\u3057\u3066\u3044\u308b\u3053\u3068\u3067\u3059\u3002GPT\uff08Generative Pre-trained Transformer\uff09\u306f\u3001\u307e\u3055\u306b\u8a00\u8a9e\u306eREPL\u306e\u3088\u3046\u306a\u5b58\u5728\u3067\u3059\u3002</p> <p>GPT\u306f\u300c\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\u3059\u308b\u300d\u3068\u3044\u3046\u30b7\u30f3\u30d7\u30eb\u306a\u30bf\u30b9\u30af\u3092\u901a\u3058\u3066\u3001\u9a5a\u304f\u307b\u3069\u9ad8\u5ea6\u306a\u8a00\u8a9e\u7406\u89e3\u3068\u751f\u6210\u80fd\u529b\u3092\u7372\u5f97\u3057\u307e\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u8a73\u7d30\u3068\u3001\u306a\u305c\u305d\u308c\u304c\u6210\u529f\u3057\u305f\u306e\u304b\u3092\u63a2\u308a\u307e\u3059\u3002</p>"},{"location":"part5/gpt-architecture/#171-gpt","title":"17.1 GPT\u306e\u57fa\u672c\u69cb\u9020","text":""},{"location":"part5/gpt-architecture/#decoder-only","title":"Decoder-only\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import Optional, Tuple, List, Dict, Any import math from dataclasses import dataclass from matplotlib.patches import Rectangle, FancyBboxPatch, Circle import matplotlib.patches as mpatches</p> <p>class GPTArchitectureOverview:     \"\"\"GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6982\u8981\"\"\"</p> <pre><code>def explain_gpt_philosophy(self):\n    \"\"\"GPT\u306e\u8a2d\u8a08\u54f2\u5b66\u3092\u8aac\u660e\"\"\"\n    print(\"=== GPT\u306e\u8a2d\u8a08\u54f2\u5b66 ===\\n\")\n\n    print(\"1. \u30b7\u30f3\u30d7\u30eb\u3055\u306e\u8ffd\u6c42:\")\n    print(\"   - Decoder-only\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\")\n    print(\"   - \u81ea\u5df1\u56de\u5e30\u7684\u306a\u751f\u6210\")\n    print(\"   - \u7d71\u4e00\u3055\u308c\u305f\u30bf\u30b9\u30af\u5f62\u5f0f\\n\")\n\n    print(\"2. \u30b9\u30b1\u30fc\u30e9\u30d3\u30ea\u30c6\u30a3:\")\n    print(\"   - \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u5897\u52a0\u3067\u6027\u80fd\u5411\u4e0a\")\n    print(\"   - \u8a08\u7b97\u52b9\u7387\u306e\u826f\u3044\u8a2d\u8a08\")\n    print(\"   - \u4e26\u5217\u5316\u53ef\u80fd\u306a\u5b66\u7fd2\\n\")\n\n    print(\"3. \u6c4e\u7528\u6027:\")\n    print(\"   - \u3042\u3089\u3086\u308b\u8a00\u8a9e\u30bf\u30b9\u30af\u3092\u300c\u751f\u6210\u300d\u3068\u3057\u3066\u6271\u3046\")\n    print(\"   - Few-shot\u5b66\u7fd2\u80fd\u529b\")\n    print(\"   - \u30bc\u30ed\u30b7\u30e7\u30c3\u30c8\u6c4e\u5316\\n\")\n\n    # \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u56f3\n    self._visualize_gpt_architecture()\n\ndef _visualize_gpt_architecture(self):\n    \"\"\"GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u53ef\u8996\u5316\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n\n    # \u5de6\u5074\uff1a\u5168\u4f53\u69cb\u9020\n    ax1.set_title('GPT Architecture Overview', fontsize=14, weight='bold')\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(0, 14)\n\n    # \u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\n    components = [\n        {\"name\": \"Token Embeddings\", \"y\": 1, \"color\": \"lightgreen\"},\n        {\"name\": \"Position Embeddings\", \"y\": 2.5, \"color\": \"lightyellow\"},\n        {\"name\": \"Transformer Block 1\", \"y\": 4.5, \"color\": \"lightblue\"},\n        {\"name\": \"Transformer Block 2\", \"y\": 6, \"color\": \"lightblue\"},\n        {\"name\": \"...\", \"y\": 7.5, \"color\": \"white\"},\n        {\"name\": \"Transformer Block N\", \"y\": 9, \"color\": \"lightblue\"},\n        {\"name\": \"Layer Norm\", \"y\": 10.5, \"color\": \"lightgray\"},\n        {\"name\": \"Output Projection\", \"y\": 12, \"color\": \"lightcoral\"}\n    ]\n\n    for comp in components:\n        if comp[\"name\"] == \"...\":\n            ax1.text(5, comp[\"y\"], comp[\"name\"], ha='center', \n                    va='center', fontsize=16)\n        else:\n            rect = FancyBboxPatch((2, comp[\"y\"]-0.4), 6, 0.8,\n                                 boxstyle=\"round,pad=0.1\",\n                                 facecolor=comp[\"color\"],\n                                 edgecolor='black', linewidth=2)\n            ax1.add_patch(rect)\n            ax1.text(5, comp[\"y\"], comp[\"name\"], ha='center', \n                    va='center', fontsize=10, weight='bold')\n\n    # \u77e2\u5370\n    for i in range(len(components)-1):\n        if components[i][\"name\"] != \"...\":\n            ax1.arrow(5, components[i][\"y\"]+0.5, 0, 0.7,\n                     head_width=0.3, head_length=0.2,\n                     fc='black', ec='black')\n\n    ax1.axis('off')\n\n    # \u53f3\u5074\uff1aTransformer Block\u8a73\u7d30\n    ax2.set_title('Transformer Block Detail', fontsize=14, weight='bold')\n    ax2.set_xlim(0, 10)\n    ax2.set_ylim(0, 10)\n\n    # Transformer\u30d6\u30ed\u30c3\u30af\u306e\u8a73\u7d30\n    block_components = [\n        {\"name\": \"Input\", \"y\": 1, \"color\": \"white\"},\n        {\"name\": \"Multi-Head Attention\\n(Causal Mask)\", \"y\": 3, \"color\": \"lightcoral\"},\n        {\"name\": \"Add &amp; Norm\", \"y\": 4.5, \"color\": \"lightgray\"},\n        {\"name\": \"Feed Forward\", \"y\": 6, \"color\": \"lightblue\"},\n        {\"name\": \"Add &amp; Norm\", \"y\": 7.5, \"color\": \"lightgray\"},\n        {\"name\": \"Output\", \"y\": 9, \"color\": \"white\"}\n    ]\n\n    for comp in block_components:\n        if comp[\"color\"] == \"white\":\n            ax2.text(5, comp[\"y\"], comp[\"name\"], ha='center',\n                    va='center', fontsize=10, style='italic')\n        else:\n            rect = FancyBboxPatch((2, comp[\"y\"]-0.4), 6, 0.8,\n                                 boxstyle=\"round,pad=0.1\",\n                                 facecolor=comp[\"color\"],\n                                 edgecolor='black', linewidth=1.5)\n            ax2.add_patch(rect)\n            ax2.text(5, comp[\"y\"], comp[\"name\"], ha='center',\n                    va='center', fontsize=9)\n\n    # \u63a5\u7d9a\n    for i in range(len(block_components)-1):\n        ax2.arrow(5, block_components[i][\"y\"]+0.3, 0, \n                 block_components[i+1][\"y\"]-block_components[i][\"y\"]-0.6,\n                 head_width=0.2, head_length=0.15,\n                 fc='black', ec='black', alpha=0.7)\n\n    # \u6b8b\u5dee\u63a5\u7d9a\n    ax2.arrow(1.5, 2, 0, 2.2, head_width=0.15, head_length=0.1,\n             fc='blue', ec='blue', linestyle='--', linewidth=2)\n    ax2.arrow(1.5, 5.5, 0, 1.7, head_width=0.15, head_length=0.1,\n             fc='blue', ec='blue', linestyle='--', linewidth=2)\n\n    ax2.text(1, 3, 'Residual', rotation=90, va='center', color='blue')\n    ax2.text(1, 6.5, 'Residual', rotation=90, va='center', color='blue')\n\n    ax2.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>@dataclass class GPTConfig:     \"\"\"GPT\u306e\u8a2d\u5b9a\"\"\"     vocab_size: int = 50257      # GPT-2\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u30b5\u30a4\u30ba     n_positions: int = 1024      # \u6700\u5927\u30b7\u30fc\u30b1\u30f3\u30b9\u9577     n_embd: int = 768           # \u57cb\u3081\u8fbc\u307f\u6b21\u5143     n_layer: int = 12           # Transformer\u30d6\u30ed\u30c3\u30af\u6570     n_head: int = 12            # \u6ce8\u610f\u30d8\u30c3\u30c9\u6570     n_inner: int = None         # FFN\u306e\u96a0\u308c\u5c64\u30b5\u30a4\u30ba\uff08None = 4 * n_embd\uff09     activation: str = \"gelu\"     # \u6d3b\u6027\u5316\u95a2\u6570     dropout: float = 0.1        # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387     layer_norm_epsilon: float = 1e-5     initializer_range: float = 0.02</p> <pre><code>def __post_init__(self):\n    if self.n_inner is None:\n        self.n_inner = 4 * self.n_embd\n</code></pre> <p>class GPTAttention(nn.Module):     \"\"\"GPT\u306e\u6ce8\u610f\u6a5f\u69cb\uff08Causal Self-Attention\uff09\"\"\"</p> <pre><code>def __init__(self, config: GPTConfig):\n    super().__init__()\n    assert config.n_embd % config.n_head == 0\n\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n    self.dropout = config.dropout\n\n    # Q, K, V\u3092\u4e00\u5ea6\u306b\u8a08\u7b97\uff08\u52b9\u7387\u7684\uff09\n    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n\n    # \u51fa\u529b\u6295\u5f71\n    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n\n    # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n\n    # Causal\u30de\u30b9\u30af\u3092\u4e8b\u524d\u8a08\u7b97\n    self.register_buffer(\n        \"bias\",\n        torch.tril(torch.ones(config.n_positions, config.n_positions))\n        .view(1, 1, config.n_positions, config.n_positions)\n    )\n\ndef forward(self, x: torch.Tensor, \n            attention_mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    B, T, C = x.size()  # batch, sequence, embedding\n\n    # Q, K, V\u3092\u8a08\u7b97\n    qkv = self.c_attn(x)\n    q, k, v = qkv.split(self.n_embd, dim=2)\n\n    # \u30d8\u30c3\u30c9\u306b\u5206\u5272\n    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n    # Attention\u8a08\u7b97\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n\n    if attention_mask is not None:\n        att = att + attention_mask\n\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n\n    y = att @ v\n    y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n    # \u51fa\u529b\u6295\u5f71\n    y = self.resid_dropout(self.c_proj(y))\n\n    return y\n</code></pre> <p>class GPTMLP(nn.Module):     \"\"\"GPT\u306eFeed Forward Network\"\"\"</p> <pre><code>def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n    self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n    self.dropout = nn.Dropout(config.dropout)\n\n    # \u6d3b\u6027\u5316\u95a2\u6570\n    self.act = self._get_activation(config.activation)\n\ndef _get_activation(self, activation: str):\n    \"\"\"\u6d3b\u6027\u5316\u95a2\u6570\u3092\u53d6\u5f97\"\"\"\n    if activation == \"gelu\":\n        return nn.GELU()\n    elif activation == \"relu\":\n        return nn.ReLU()\n    elif activation == \"swish\":\n        return nn.SiLU()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    x = self.c_fc(x)\n    x = self.act(x)\n    x = self.c_proj(x)\n    x = self.dropout(x)\n    return x\n</code></pre> <p>class GPTBlock(nn.Module):     \"\"\"GPT\u306eTransformer\u30d6\u30ed\u30c3\u30af\"\"\"</p> <pre><code>def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n    self.attn = GPTAttention(config)\n    self.ln_2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n    self.mlp = GPTMLP(config)\n\ndef forward(self, x: torch.Tensor, \n            attention_mask: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    # Attention with residual\n    x = x + self.attn(self.ln_1(x), attention_mask)\n    # MLP with residual\n    x = x + self.mlp(self.ln_2(x))\n    return x\n</code></pre> <p>class GPTModel(nn.Module):     \"\"\"\u5b8c\u5168\u306aGPT\u30e2\u30c7\u30eb\"\"\"</p> <pre><code>def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.config = config\n\n    # \u57cb\u3081\u8fbc\u307f\u5c64\n    self.wte = nn.Embedding(config.vocab_size, config.n_embd)  # token embeddings\n    self.wpe = nn.Embedding(config.n_positions, config.n_embd)  # position embeddings\n    self.drop = nn.Dropout(config.dropout)\n\n    # Transformer\u30d6\u30ed\u30c3\u30af\n    self.h = nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)])\n\n    # \u6700\u7d42\u5c64\u6b63\u898f\u5316\n    self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n    # \u91cd\u307f\u306e\u521d\u671f\u5316\n    self.apply(self._init_weights)\n\ndef _init_weights(self, module):\n    \"\"\"\u91cd\u307f\u306e\u521d\u671f\u5316\"\"\"\n    if isinstance(module, (nn.Linear, nn.Embedding)):\n        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.LayerNorm):\n        module.bias.data.zero_()\n        module.weight.data.fill_(1.0)\n\ndef forward(self, input_ids: torch.Tensor,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    device = input_ids.device\n    B, T = input_ids.size()\n\n    # \u4f4d\u7f6eID\u306e\u751f\u6210\n    if position_ids is None:\n        position_ids = torch.arange(0, T, dtype=torch.long, device=device)\n        position_ids = position_ids.unsqueeze(0).expand(B, T)\n\n    # \u57cb\u3081\u8fbc\u307f\n    token_embeddings = self.wte(input_ids)\n    position_embeddings = self.wpe(position_ids)\n    hidden_states = self.drop(token_embeddings + position_embeddings)\n\n    # Attention mask\u306e\u6e96\u5099\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, None, None, :]\n        attention_mask = (1.0 - attention_mask) * -10000.0\n\n    # Transformer\u30d6\u30ed\u30c3\u30af\u3092\u901a\u904e\n    for block in self.h:\n        hidden_states = block(hidden_states, attention_mask)\n\n    # \u6700\u7d42\u5c64\u6b63\u898f\u5316\n    hidden_states = self.ln_f(hidden_states)\n\n    return hidden_states\n</code></pre> <p>class GPTLMHeadModel(nn.Module):     \"\"\"\u8a00\u8a9e\u30e2\u30c7\u30ea\u30f3\u30b0\u7528\u306eGPT\"\"\"</p> <pre><code>def __init__(self, config: GPTConfig):\n    super().__init__()\n    self.transformer = GPTModel(config)\n    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n    # \u91cd\u307f\u5171\u6709\uff08\u57cb\u3081\u8fbc\u307f\u3068\u51fa\u529b\u5c64\uff09\n    self.lm_head.weight = self.transformer.wte.weight\n\ndef forward(self, input_ids: torch.Tensor,\n            attention_mask: Optional[torch.Tensor] = None,\n            labels: Optional[torch.Tensor] = None) -&gt; Dict[str, torch.Tensor]:\n    # Transformer\n    hidden_states = self.transformer(input_ids, attention_mask)\n\n    # \u8a00\u8a9e\u30e2\u30c7\u30eb\u30d8\u30c3\u30c9\n    lm_logits = self.lm_head(hidden_states)\n\n    outputs = {\"logits\": lm_logits}\n\n    # \u640d\u5931\u8a08\u7b97\n    if labels is not None:\n        # \u30e9\u30d9\u30eb\u3092\u5de6\u306b\u30b7\u30d5\u30c8\n        shift_logits = lm_logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n\n        # \u640d\u5931\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(\n            shift_logits.view(-1, shift_logits.size(-1)),\n            shift_labels.view(-1)\n        )\n        outputs[\"loss\"] = loss\n\n    return outputs\n\n@torch.no_grad()\ndef generate(self, input_ids: torch.Tensor,\n            max_new_tokens: int = 50,\n            temperature: float = 1.0,\n            top_k: Optional[int] = None,\n            top_p: Optional[float] = None) -&gt; torch.Tensor:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u751f\u6210\"\"\"\n    self.eval()\n\n    for _ in range(max_new_tokens):\n        # \u6700\u5927\u9577\u306b\u5236\u9650\n        idx_cond = input_ids if input_ids.size(1) &lt;= self.transformer.config.n_positions else input_ids[:, -self.transformer.config.n_positions:]\n\n        # \u4e88\u6e2c\n        outputs = self.forward(idx_cond)\n        logits = outputs[\"logits\"][:, -1, :] / temperature\n\n        # Top-k\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits &lt; v[:, [-1]]] = float('-inf')\n\n        # Top-p\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\uff08Nucleus sampling\uff09\n        if top_p is not None:\n            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n            # \u7d2f\u7a4d\u78ba\u7387\u304ctop_p\u3092\u8d85\u3048\u308b\u4f4d\u7f6e\u3092\u898b\u3064\u3051\u308b\n            sorted_indices_to_remove = cumulative_probs &gt; top_p\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0\n\n            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n            logits.scatter_(1, indices_to_remove, float('-inf'))\n\n        # \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n\n        # \u8ffd\u52a0\n        input_ids = torch.cat((input_ids, idx_next), dim=1)\n\n    return input_ids\n</code></pre>"},{"location":"part5/gpt-architecture/#172-gpt","title":"17.2 GPT\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247","text":"<p>class ScalingLawsDemo:     \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\"\"\"</p> <pre><code>def explain_scaling_laws(self):\n    \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u306e\u8aac\u660e\"\"\"\n    print(\"=== GPT\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247 ===\\n\")\n\n    print(\"Kaplan\u3089\u306e\u767a\u898b\uff082020\uff09:\")\n    print(\"  Loss \u221d N^(-\u03b1) \u00d7 D^(-\u03b2) \u00d7 C^(-\u03b3)\")\n    print(\"  - N: \u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\")\n    print(\"  - D: \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30b5\u30a4\u30ba\")\n    print(\"  - C: \u8a08\u7b97\u91cf\\n\")\n\n    print(\"Chinchilla\u306e\u6cd5\u5247\uff082022\uff09:\")\n    print(\"  \u6700\u9069\u306a\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3068\u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u306e\u6bd4\u7387\")\n    print(\"  \u30c8\u30fc\u30af\u30f3\u6570 \u2248 20 \u00d7 \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\\n\")\n\n    # \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u306e\u53ef\u8996\u5316\n    self._visualize_scaling_laws()\n\ndef _visualize_scaling_laws(self):\n    \"\"\"\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\u3092\u53ef\u8996\u5316\"\"\"\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\n    # \u30e2\u30c7\u30eb\u30b5\u30a4\u30ba\u3068\u6027\u80fd\n    model_sizes = np.logspace(6, 11, 50)  # 1M to 100B parameters\n    alpha = 0.076  # \u5b9f\u9a13\u7684\u306b\u89b3\u6e2c\u3055\u308c\u305f\u5024\n    loss = 10 * model_sizes ** (-alpha)\n\n    ax1.loglog(model_sizes, loss, 'b-', linewidth=2)\n    ax1.set_xlabel('Model Parameters')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Model Size Scaling')\n    ax1.grid(True, alpha=0.3)\n\n    # \u5b9f\u969b\u306eGPT\u30e2\u30c7\u30eb\u3092\u30d7\u30ed\u30c3\u30c8\n    gpt_models = {\n        'GPT': 117e6,\n        'GPT-2': 1.5e9,\n        'GPT-3': 175e9,\n        'GPT-4': 1e12  # \u63a8\u5b9a\n    }\n\n    for name, size in gpt_models.items():\n        estimated_loss = 10 * size ** (-alpha)\n        ax1.scatter(size, estimated_loss, s=100, zorder=5)\n        ax1.annotate(name, (size, estimated_loss), \n                    xytext=(10, 10), textcoords='offset points')\n\n    # \u30c7\u30fc\u30bf\u30b5\u30a4\u30ba\u3068\u6027\u80fd\n    data_sizes = np.logspace(6, 12, 50)  # 1M to 1T tokens\n    beta = 0.095\n    loss_data = 8 * data_sizes ** (-beta)\n\n    ax2.loglog(data_sizes, loss_data, 'g-', linewidth=2)\n    ax2.set_xlabel('Dataset Size (tokens)')\n    ax2.set_ylabel('Loss')\n    ax2.set_title('Data Size Scaling')\n    ax2.grid(True, alpha=0.3)\n\n    # \u8a08\u7b97\u91cf\u3068\u6027\u80fd\n    compute = np.logspace(15, 25, 50)  # FLOPs\n    gamma = 0.050\n    loss_compute = 5 * compute ** (-gamma)\n\n    ax3.loglog(compute, loss_compute, 'r-', linewidth=2)\n    ax3.set_xlabel('Compute (FLOPs)')\n    ax3.set_ylabel('Loss')\n    ax3.set_title('Compute Scaling')\n    ax3.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # \u6700\u9069\u306a\u914d\u5206\u306e\u53ef\u8996\u5316\n    self._visualize_optimal_allocation()\n\ndef _visualize_optimal_allocation(self):\n    \"\"\"\u6700\u9069\u306a\u30ea\u30bd\u30fc\u30b9\u914d\u5206\u3092\u53ef\u8996\u5316\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u7bc4\u56f2\n    params = np.logspace(7, 12, 100)  # 10M to 1T\n\n    # Chinchilla\u306e\u63a8\u5968\n    optimal_tokens = 20 * params\n\n    # \u7570\u306a\u308b\u914d\u5206\u6226\u7565\n    strategies = {\n        'Chinchilla Optimal': optimal_tokens,\n        'Compute Optimal': 10 * params,  # \u3088\u308a\u5c11\u306a\u3044\u30c7\u30fc\u30bf\n        'Over-trained': 100 * params,    # \u3088\u308a\u591a\u3044\u30c7\u30fc\u30bf\n    }\n\n    for name, tokens in strategies.items():\n        ax.loglog(params, tokens, linewidth=2, label=name)\n\n    # \u5b9f\u969b\u306e\u30e2\u30c7\u30eb\n    real_models = [\n        ('GPT-3', 175e9, 300e9),\n        ('Chinchilla', 70e9, 1.4e12),\n        ('LLaMA', 7e9, 1e12),\n        ('LLaMA-2', 70e9, 2e12)\n    ]\n\n    for name, param, token in real_models:\n        ax.scatter(param, token, s=100, zorder=5)\n        ax.annotate(name, (param, token), \n                   xytext=(5, 5), textcoords='offset points',\n                   fontsize=9)\n\n    ax.set_xlabel('Model Parameters')\n    ax.set_ylabel('Training Tokens')\n    ax.set_title('Model Size vs Training Data')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # \u7b49\u8a08\u7b97\u91cf\u66f2\u7dda\n    compute_levels = [1e21, 1e22, 1e23, 1e24]\n    for c in compute_levels:\n        # \u7c21\u7565\u5316: compute \u221d params \u00d7 tokens\n        tokens_for_compute = c / params\n        mask = tokens_for_compute &lt; 1e15  # \u73fe\u5b9f\u7684\u306a\u7bc4\u56f2\n        ax.plot(params[mask], tokens_for_compute[mask], \n               'k--', alpha=0.3, linewidth=1)\n        ax.text(params[mask][-1], tokens_for_compute[mask][-1],\n               f'{c:.0e} FLOPs', fontsize=8, alpha=0.7)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part5/gpt-architecture/#173-gpt","title":"17.3 GPT\u306e\u5b66\u7fd2\u6280\u8853","text":"<p>class GPTTrainingTechniques:     \"\"\"GPT\u306e\u5b66\u7fd2\u6280\u8853\"\"\"</p> <pre><code>def demonstrate_training_techniques(self):\n    \"\"\"\u4e3b\u8981\u306a\u5b66\u7fd2\u6280\u8853\u3092\u30c7\u30e2\"\"\"\n    print(\"=== GPT\u306e\u5b66\u7fd2\u6280\u8853 ===\\n\")\n\n    # 1. Learning Rate Schedule\n    self._demonstrate_lr_schedule()\n\n    # 2. Gradient Clipping\n    print(\"\\n2. Gradient Clipping:\")\n    print(\"   - \u52fe\u914d\u7206\u767a\u3092\u9632\u3050\")\n    print(\"   - \u5178\u578b\u7684\u306a\u5024: 1.0\")\n    print(\"   - \u5b89\u5b9a\u3057\u305f\u5b66\u7fd2\u3092\u5b9f\u73fe\\n\")\n\n    # 3. Weight Decay\n    print(\"3. Weight Decay:\")\n    print(\"   - \u6b63\u5247\u5316\u52b9\u679c\")\n    print(\"   - \u57cb\u3081\u8fbc\u307f\u5c64\u3068LayerNorm\u306b\u306f\u9069\u7528\u3057\u306a\u3044\")\n    print(\"   - \u5178\u578b\u7684\u306a\u5024: 0.1\\n\")\n\n    # 4. Mixed Precision Training\n    print(\"4. Mixed Precision Training:\")\n    print(\"   - FP16\u3068FP32\u3092\u4f75\u7528\")\n    print(\"   - \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u3092\u524a\u6e1b\")\n    print(\"   - \u5b66\u7fd2\u901f\u5ea6\u3092\u5411\u4e0a\")\n\ndef _demonstrate_lr_schedule(self):\n    \"\"\"\u5b66\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u306e\u30c7\u30e2\"\"\"\n    print(\"1. Learning Rate Schedule:\")\n\n    # Cosine schedule with warmup\n    total_steps = 100000\n    warmup_steps = 10000\n    max_lr = 6e-4\n    min_lr = 6e-5\n\n    steps = np.arange(total_steps)\n    lr = np.zeros_like(steps, dtype=float)\n\n    # Warmup\n    lr[:warmup_steps] = max_lr * steps[:warmup_steps] / warmup_steps\n\n    # Cosine decay\n    progress = (steps[warmup_steps:] - warmup_steps) / (total_steps - warmup_steps)\n    lr[warmup_steps:] = min_lr + (max_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n\n    # \u53ef\u8996\u5316\n    plt.figure(figsize=(10, 6))\n    plt.plot(steps, lr, linewidth=2)\n    plt.axvline(x=warmup_steps, color='red', linestyle='--', \n               alpha=0.5, label='End of Warmup')\n    plt.xlabel('Training Steps')\n    plt.ylabel('Learning Rate')\n    plt.title('GPT Learning Rate Schedule (Cosine with Warmup)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    print(\"   - Linear warmup: \u5b66\u7fd2\u521d\u671f\u306e\u4e0d\u5b89\u5b9a\u6027\u3092\u56de\u907f\")\n    print(\"   - Cosine decay: \u30b9\u30e0\u30fc\u30ba\u306a\u5b66\u7fd2\u7387\u306e\u6e1b\u8870\")\n    print(\"   - \u6700\u7d42\u7684\u306b\u5c0f\u3055\u306a\u5b66\u7fd2\u7387\u3067\u5fae\u8abf\u6574\")\n</code></pre> <p>class GPTOptimizationTricks:     \"\"\"GPT\u306e\u6700\u9069\u5316\u30c8\u30ea\u30c3\u30af\"\"\"</p> <pre><code>def __init__(self):\n    self.config = GPTConfig(n_layer=12, n_head=12, n_embd=768)\n\ndef demonstrate_efficient_attention(self):\n    \"\"\"\u52b9\u7387\u7684\u306a\u6ce8\u610f\u6a5f\u69cb\u306e\u5b9f\u88c5\"\"\"\n    print(\"=== \u52b9\u7387\u7684\u306a\u6ce8\u610f\u6a5f\u69cb ===\\n\")\n\n    class FlashAttentionGPT(nn.Module):\n        \"\"\"Flash Attention\u98a8\u306e\u6700\u9069\u5316\"\"\"\n\n        def __init__(self, config: GPTConfig):\n            super().__init__()\n            self.n_head = config.n_head\n            self.n_embd = config.n_embd\n            self.dropout = config.dropout\n\n            # Fused QKV projection\n            self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n            self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n            B, T, C = x.shape\n\n            # \u52b9\u7387\u7684\u306aQKV\u8a08\u7b97\n            qkv = self.qkv_proj(x)\n            qkv = qkv.reshape(B, T, 3, self.n_head, C // self.n_head)\n            qkv = qkv.permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n            # Flash Attention\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\n            # \u5b9f\u969b\u306f\u30ab\u30b9\u30bf\u30e0CUDA\u30ab\u30fc\u30cd\u30eb\u3092\u4f7f\u7528\n            if T &lt;= 128:  # \u77ed\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306f\u901a\u5e38\u306e\u8a08\u7b97\n                att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n                att = att.masked_fill(\n                    torch.triu(torch.ones(T, T), diagonal=1).bool(), \n                    float('-inf')\n                )\n                att = F.softmax(att, dim=-1)\n                y = att @ v\n            else:\n                # \u30d6\u30ed\u30c3\u30af\u5358\u4f4d\u306e\u8a08\u7b97\uff08\u7c21\u7565\u7248\uff09\n                block_size = 64\n                y = torch.zeros_like(v)\n\n                for i in range(0, T, block_size):\n                    end_i = min(i + block_size, T)\n                    for j in range(0, i + block_size, block_size):\n                        end_j = min(j + block_size, T)\n\n                        if j &lt;= i:  # Causal mask\n                            q_block = q[:, :, i:end_i]\n                            k_block = k[:, :, j:end_j]\n                            v_block = v[:, :, j:end_j]\n\n                            att_block = (q_block @ k_block.transpose(-2, -1)) * \\\n                                      (1.0 / math.sqrt(k.size(-1)))\n\n                            # \u30d6\u30ed\u30c3\u30af\u5185\u306e\u30de\u30b9\u30af\n                            if i == j:\n                                block_mask = torch.triu(\n                                    torch.ones(end_i-i, end_j-j), \n                                    diagonal=1\n                                ).bool()\n                                att_block = att_block.masked_fill(\n                                    block_mask, float('-inf')\n                                )\n\n                            att_block = F.softmax(att_block, dim=-1)\n                            y[:, :, i:end_i] += att_block @ v_block\n\n            # \u51fa\u529b\n            y = y.transpose(1, 2).contiguous().view(B, T, C)\n            y = self.out_proj(y)\n\n            return y\n\n    print(\"Flash Attention\u306e\u5229\u70b9:\")\n    print(\"\u2713 \u30e1\u30e2\u30ea\u52b9\u7387: O(n) instead of O(n\u00b2)\")\n    print(\"\u2713 \u3088\u308a\u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u306e\u51e6\u7406\u304c\u53ef\u80fd\")\n    print(\"\u2713 \u30e1\u30e2\u30ea\u5e2f\u57df\u5e45\u306e\u6709\u52b9\u6d3b\u7528\")\n\n    # \u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u6bd4\u8f03\n    self._compare_attention_performance()\n\ndef _compare_attention_performance(self):\n    \"\"\"\u6ce8\u610f\u6a5f\u69cb\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u6bd4\u8f03\"\"\"\n    seq_lengths = [128, 256, 512, 1024, 2048]\n    standard_memory = []\n    optimized_memory = []\n\n    for seq_len in seq_lengths:\n        # \u6a19\u6e96\u7684\u306a\u6ce8\u610f\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\uff08\u6982\u7b97\uff09\n        # O(batch * heads * seq_len\u00b2)\n        std_mem = 32 * 12 * seq_len * seq_len * 4 / 1024**2  # MB\n        standard_memory.append(std_mem)\n\n        # \u6700\u9069\u5316\u3055\u308c\u305f\u6ce8\u610f\u306e\u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\n        # O(batch * heads * seq_len)\n        opt_mem = 32 * 12 * seq_len * 64 * 4 / 1024**2  # MB (block_size=64)\n        optimized_memory.append(opt_mem)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(seq_lengths, standard_memory, 'r-', marker='o', \n            label='Standard Attention', linewidth=2)\n    plt.plot(seq_lengths, optimized_memory, 'g-', marker='s', \n            label='Optimized Attention', linewidth=2)\n\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Memory Usage (MB)')\n    plt.title('Attention Memory Usage Comparison')\n    plt.legend()\n    plt.yscale('log')\n    plt.grid(True, alpha=0.3)\n    plt.show()\n</code></pre>"},{"location":"part5/gpt-architecture/#174-gpt","title":"17.4 GPT\u306e\u5fdc\u7528\u3068\u767a\u5c55","text":"<p>class GPTApplications:     \"\"\"GPT\u306e\u5fdc\u7528\u4f8b\"\"\"</p> <pre><code>def demonstrate_applications(self):\n    \"\"\"\u69d8\u3005\u306a\u5fdc\u7528\u3092\u5b9f\u6f14\"\"\"\n    print(\"=== GPT\u306e\u5fdc\u7528 ===\\n\")\n\n    # 1. Few-shot\u5b66\u7fd2\n    self._demonstrate_few_shot()\n\n    # 2. Chain-of-Thought\n    self._demonstrate_chain_of_thought()\n\n    # 3. Instruction Tuning\n    self._demonstrate_instruction_tuning()\n\ndef _demonstrate_few_shot(self):\n    \"\"\"Few-shot\u5b66\u7fd2\u306e\u30c7\u30e2\"\"\"\n    print(\"1. Few-shot Learning:\\n\")\n\n    few_shot_prompt = \"\"\"\n</code></pre> <p>Task: Sentiment Analysis</p> <p>Example 1: Text: \"This movie was fantastic! I loved every minute of it.\" Sentiment: Positive</p> <p>Example 2: Text: \"The service was terrible and the food was cold.\" Sentiment: Negative</p> <p>Example 3: Text: \"The weather is nice today.\" Sentiment: \"\"\"</p> <pre><code>    print(\"\u30d7\u30ed\u30f3\u30d7\u30c8:\")\n    print(few_shot_prompt)\n    print(\"\\nGPT\u306f\u6587\u8108\u304b\u3089\u5b66\u7fd2\u3057\u3066\u30bf\u30b9\u30af\u3092\u5b9f\u884c\")\n    print(\"\u671f\u5f85\u3055\u308c\u308b\u51fa\u529b: Neutral\\n\")\n\ndef _demonstrate_chain_of_thought(self):\n    \"\"\"Chain-of-Thought\u63a8\u8ad6\u306e\u30c7\u30e2\"\"\"\n    print(\"2. Chain-of-Thought Reasoning:\\n\")\n\n    cot_prompt = \"\"\"\n</code></pre> <p>Q: Jack has 5 apples. He buys 3 more apples and then gives 2 apples to his friend. How many apples does Jack have now?</p> <p>A: Let's think step by step: 1. Jack starts with 5 apples 2. He buys 3 more apples: 5 + 3 = 8 apples 3. He gives 2 apples to his friend: 8 - 2 = 6 apples Therefore, Jack has 6 apples now.</p> <p>Q: If a train travels at 60 mph for 2 hours, then at 40 mph for 1 hour, what is the total distance traveled?</p> <p>A: Let's think step by step:\"\"\"</p> <pre><code>    print(\"\u30d7\u30ed\u30f3\u30d7\u30c8:\")\n    print(cot_prompt)\n    print(\"\\nGPT\u306f\u6bb5\u968e\u7684\u306a\u63a8\u8ad6\u30d7\u30ed\u30bb\u30b9\u3092\u5b66\u7fd2\")\n\ndef _demonstrate_instruction_tuning(self):\n    \"\"\"Instruction Tuning\u306e\u30c7\u30e2\"\"\"\n    print(\"\\n3. Instruction Tuning:\\n\")\n\n    examples = [\n        {\n            \"instruction\": \"Translate the following English text to French:\",\n            \"input\": \"Hello, how are you?\",\n            \"output\": \"Bonjour, comment allez-vous?\"\n        },\n        {\n            \"instruction\": \"Summarize the following text in one sentence:\",\n            \"input\": \"The quick brown fox jumps over the lazy dog. This pangram contains all letters of the alphabet.\",\n            \"output\": \"This is a pangram that includes every letter of the alphabet.\"\n        },\n        {\n            \"instruction\": \"Convert the following number to binary:\",\n            \"input\": \"42\",\n            \"output\": \"101010\"\n        }\n    ]\n\n    print(\"Instruction-Response\u5f62\u5f0f\u3067\u306e\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0:\")\n    for ex in examples[:2]:\n        print(f\"\\nInstruction: {ex['instruction']}\")\n        print(f\"Input: {ex['input']}\")\n        print(f\"Expected Output: {ex['output']}\")\n</code></pre> <p>class GPTVariants:     \"\"\"GPT\u306e\u6d3e\u751f\u30e2\u30c7\u30eb\"\"\"</p> <pre><code>def explain_variants(self):\n    \"\"\"\u4e3b\u8981\u306a\u6d3e\u751f\u30e2\u30c7\u30eb\u3092\u8aac\u660e\"\"\"\n    print(\"=== GPT\u306e\u6d3e\u751f\u30e2\u30c7\u30eb ===\\n\")\n\n    variants = {\n        \"GPT-2\": {\n            \"params\": \"1.5B\",\n            \"context\": \"1024\",\n            \"\u7279\u5fb4\": \"Zero-shot\u6027\u80fd\u306e\u5b9f\u8a3c\"\n        },\n        \"GPT-3\": {\n            \"params\": \"175B\",\n            \"context\": \"2048\",\n            \"\u7279\u5fb4\": \"Few-shot\u5b66\u7fd2\u306e\u9769\u547d\"\n        },\n        \"GPT-4\": {\n            \"params\": \"~1T (\u63a8\u5b9a)\",\n            \"context\": \"8K-32K\",\n            \"\u7279\u5fb4\": \"\u30de\u30eb\u30c1\u30e2\u30fc\u30c0\u30eb\u3001\u9ad8\u5ea6\u306a\u63a8\u8ad6\"\n        },\n        \"GPT-Neo/GPT-J\": {\n            \"params\": \"2.7B/6B\",\n            \"context\": \"2048\",\n            \"\u7279\u5fb4\": \"\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u5b9f\u88c5\"\n        },\n        \"CodeGPT/Codex\": {\n            \"params\": \"12B\",\n            \"context\": \"4096\", \n            \"\u7279\u5fb4\": \"\u30b3\u30fc\u30c9\u751f\u6210\u306b\u7279\u5316\"\n        }\n    }\n\n    # \u6bd4\u8f03\u8868\u306e\u4f5c\u6210\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.axis('tight')\n    ax.axis('off')\n\n    # \u30c7\u30fc\u30bf\u306e\u6e96\u5099\n    headers = [\"Model\", \"Parameters\", \"Context Length\", \"Key Features\"]\n    cell_data = []\n\n    for model, info in variants.items():\n        cell_data.append([\n            model,\n            info[\"params\"],\n            info[\"context\"],\n            info[\"\u7279\u5fb4\"]\n        ])\n\n    # \u30c6\u30fc\u30d6\u30eb\u306e\u4f5c\u6210\n    table = ax.table(cellText=cell_data, colLabels=headers,\n                    cellLoc='left', loc='center',\n                    colWidths=[0.2, 0.2, 0.2, 0.4])\n\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1, 2)\n\n    # \u30b9\u30bf\u30a4\u30ea\u30f3\u30b0\n    for i in range(len(headers)):\n        table[(0, i)].set_facecolor('#4CAF50')\n        table[(0, i)].set_text_props(weight='bold', color='white')\n\n    plt.title('GPT Model Variants Comparison', fontsize=16, weight='bold', pad=20)\n    plt.show()\n</code></pre>"},{"location":"part5/gpt-architecture/#_2","title":"\u5b9f\u88c5\u4f8b\u3068\u30c7\u30e2","text":"<p>def run_gpt_demo():     \"\"\"GPT\u306e\u30c7\u30e2\u3092\u5b9f\u884c\"\"\"     print(\"=\" * 70)     print(\"GPT\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u30c7\u30e2\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># 1. \u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u6982\u8981\noverview = GPTArchitectureOverview()\noverview.explain_gpt_philosophy()\n\n# 2. \u5c0f\u3055\u306aGPT\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\nprint(\"\\n=== \u5c0f\u898f\u6a21GPT\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 ===\")\nconfig = GPTConfig(\n    vocab_size=1000,\n    n_positions=128,\n    n_embd=128,\n    n_layer=4,\n    n_head=4\n)\n\nmodel = GPTLMHeadModel(config)\n\n# \u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u306e\u8a08\u7b97\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\n\u30e2\u30c7\u30eb\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {total_params:,}\")\nprint(f\"\u30e2\u30c7\u30eb\u30b5\u30a4\u30ba: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n\n# 3. \u63a8\u8ad6\u30c7\u30e2\nprint(\"\\n=== \u63a8\u8ad6\u30c7\u30e2 ===\")\n\n# \u30c0\u30df\u30fc\u5165\u529b\nbatch_size = 2\nseq_len = 10\ninput_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n# \u9806\u4f1d\u64ad\nwith torch.no_grad():\n    outputs = model(input_ids)\n    print(f\"\u5165\u529b\u5f62\u72b6: {input_ids.shape}\")\n    print(f\"\u51fa\u529b\u30ed\u30b8\u30c3\u30c8\u5f62\u72b6: {outputs['logits'].shape}\")\n\n# \u751f\u6210\u30c7\u30e2\nprint(\"\\n\u751f\u6210\u4f8b:\")\nprompt = torch.tensor([[1, 2, 3]])  # \u30c0\u30df\u30fc\u30d7\u30ed\u30f3\u30d7\u30c8\ngenerated = model.generate(prompt, max_new_tokens=10, temperature=0.8)\nprint(f\"\u751f\u6210\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3ID: {generated.tolist()[0]}\")\n\n# 4. \u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u6cd5\u5247\nprint(\"\\n\")\nscaling_demo = ScalingLawsDemo()\nscaling_demo.explain_scaling_laws()\n\n# 5. \u5b66\u7fd2\u6280\u8853\nprint(\"\\n\")\ntraining_demo = GPTTrainingTechniques()\ntraining_demo.demonstrate_training_techniques()\n\n# 6. \u6700\u9069\u5316\nprint(\"\\n\")\noptimization_demo = GPTOptimizationTricks()\noptimization_demo.demonstrate_efficient_attention()\n\n# 7. \u5fdc\u7528\nprint(\"\\n\")\napplications = GPTApplications()\napplications.demonstrate_applications()\n\n# 8. \u6d3e\u751f\u30e2\u30c7\u30eb\nprint(\"\\n\")\nvariants = GPTVariants()\nvariants.explain_variants()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081\")\nprint(\"=\" * 70)\nprint(\"\\nGPT\u306e\u6210\u529f\u8981\u56e0:\")\nprint(\"\u2022 \u30b7\u30f3\u30d7\u30eb\u3067\u62e1\u5f35\u53ef\u80fd\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\")\nprint(\"\u2022 \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u306e\u4e8b\u524d\u5b66\u7fd2\")\nprint(\"\u2022 \u5275\u767a\u7684\u306a\u80fd\u529b\u306e\u7372\u5f97\")\nprint(\"\u2022 \u6c4e\u7528\u7684\u306a\u30bf\u30b9\u30af\u5f62\u5f0f\uff08\u6b21\u30c8\u30fc\u30af\u30f3\u4e88\u6e2c\uff09\")\nprint(\"\\nGPT\u306f\u8a00\u8a9e\u7406\u89e3\u3068\u751f\u6210\u306e\u65b0\u6642\u4ee3\u3092\u5207\u308a\u958b\u304d\u307e\u3057\u305f\u3002\")\n</code></pre> <p>if name == \"main\":     run_gpt_demo()</p>"},{"location":"part5/inference-techniques/","title":"\u63a8\u8ad6\u6280\u8853","text":""},{"location":"part5/inference-techniques/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u751f\u6210\u306e\u82b8\u8853","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u6700\u9069\u5316\u3092\u8003\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u540c\u3058\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u3082\u3001\u6700\u9069\u5316\u30ec\u30d9\u30eb\u3084\u30bf\u30fc\u30b2\u30c3\u30c8\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u3088\u3063\u3066\u5168\u304f\u7570\u306a\u308b\u30b3\u30fc\u30c9\u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002<code>-O0</code>\u3067\u306f\u8aad\u307f\u3084\u3059\u3044\u304c\u9045\u3044\u30b3\u30fc\u30c9\u3001<code>-O3</code>\u3067\u306f\u9ad8\u901f\u3060\u304c\u8907\u96d1\u306a\u30b3\u30fc\u30c9\u3002\u76ee\u7684\u306b\u5fdc\u3058\u3066\u9069\u5207\u306a\u6226\u7565\u3092\u9078\u3073\u307e\u3059\u3002</p> <p>\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u63a8\u8ad6\u3082\u540c\u3058\u3067\u3059\u3002\u540c\u3058\u30e2\u30c7\u30eb\u3067\u3082\u3001\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565\u3001\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u3001\u5236\u7d04\u6761\u4ef6\u306a\u3069\u306b\u3088\u3063\u3066\u5168\u304f\u7570\u306a\u308b\u51fa\u529b\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u3053\u306e\u7ae0\u3067\u306f\u3001\u9ad8\u54c1\u8cea\u306a\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u306e\u305f\u3081\u306e\u69d8\u3005\u306a\u63a8\u8ad6\u6280\u8853\u3092\u5b66\u3073\u307e\u3059\u3002</p>"},{"location":"part5/inference-techniques/#201","title":"20.1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565","text":""},{"location":"part5/inference-techniques/#_3","title":"\u78ba\u7387\u5206\u5e03\u304b\u3089\u306e\u8ce2\u3044\u9078\u629e","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import List, Dict, Tuple, Optional, Any, Union from dataclasses import dataclass import math from collections import defaultdict, Counter import time from tqdm import tqdm</p> <p>class SamplingStrategies:     \"\"\"\u69d8\u3005\u306a\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, vocab_size: int = 1000):\n    self.vocab_size = vocab_size\n\ndef explain_sampling_methods(self):\n    \"\"\"\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u624b\u6cd5\u306e\u8aac\u660e\"\"\"\n    print(\"=== \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565 ===\\n\")\n\n    methods = {\n        \"Greedy Decoding\": {\n            \"\u8aac\u660e\": \"\u5e38\u306b\u6700\u3082\u78ba\u7387\u306e\u9ad8\u3044\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\",\n            \"\u5229\u70b9\": \"\u6c7a\u5b9a\u7684\u3001\u9ad8\u901f\",\n            \"\u6b20\u70b9\": \"\u7e70\u308a\u8fd4\u3057\u3084\u3059\u3044\u3001\u591a\u69d8\u6027\u306a\u3057\",\n            \"\u7528\u9014\": \"\u4e8b\u5b9f\u7684\u306a\u56de\u7b54\u3001\u7ffb\u8a33\"\n        },\n\n        \"Temperature Sampling\": {\n            \"\u8aac\u660e\": \"\u6e29\u5ea6\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5206\u5e03\u3092\u8abf\u6574\",\n            \"\u5229\u70b9\": \"\u591a\u69d8\u6027\u306e\u5236\u5fa1\u53ef\u80fd\",\n            \"\u6b20\u70b9\": \"\u54c1\u8cea\u304c\u4e0d\u5b89\u5b9a\",\n            \"\u7528\u9014\": \"\u5275\u9020\u7684\u306a\u751f\u6210\"\n        },\n\n        \"Top-k Sampling\": {\n            \"\u8aac\u660e\": \"\u4e0a\u4f4dk\u500b\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\",\n            \"\u5229\u70b9\": \"\u4f4e\u78ba\u7387\u30c8\u30fc\u30af\u30f3\u3092\u9664\u5916\",\n            \"\u6b20\u70b9\": \"k\u306e\u9078\u629e\u304c\u96e3\u3057\u3044\",\n            \"\u7528\u9014\": \"\u30d0\u30e9\u30f3\u30b9\u306e\u53d6\u308c\u305f\u751f\u6210\"\n        },\n\n        \"Top-p (Nucleus) Sampling\": {\n            \"\u8aac\u660e\": \"\u7d2f\u7a4d\u78ba\u7387p\u4ee5\u4e0b\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\",\n            \"\u5229\u70b9\": \"\u52d5\u7684\u306a\u5019\u88dc\u6570\",\n            \"\u6b20\u70b9\": \"\u8a08\u7b97\u304c\u3084\u3084\u8907\u96d1\",\n            \"\u7528\u9014\": \"\u9ad8\u54c1\u8cea\u306a\u751f\u6210\"\n        },\n\n        \"Beam Search\": {\n            \"\u8aac\u660e\": \"\u8907\u6570\u306e\u5019\u88dc\u3092\u4e26\u5217\u63a2\u7d22\",\n            \"\u5229\u70b9\": \"\u5c40\u6240\u6700\u9069\u3092\u56de\u907f\",\n            \"\u6b20\u70b9\": \"\u8a08\u7b97\u30b3\u30b9\u30c8\u9ad8\u3001\u591a\u69d8\u6027\u4f4e\",\n            \"\u7528\u9014\": \"\u7ffb\u8a33\u3001\u8981\u7d04\"\n        }\n    }\n\n    for name, details in methods.items():\n        print(f\"{name}:\")\n        for key, value in details.items():\n            print(f\"  {key}: {value}\")\n        print()\n\n    # \u5404\u624b\u6cd5\u306e\u53ef\u8996\u5316\n    self._visualize_sampling_methods()\n\ndef _visualize_sampling_methods(self):\n    \"\"\"\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u624b\u6cd5\u3092\u53ef\u8996\u5316\"\"\"\n    # \u30c0\u30df\u30fc\u306e\u78ba\u7387\u5206\u5e03\n    torch.manual_seed(42)\n    logits = torch.randn(self.vocab_size) * 2\n    probs = F.softmax(logits, dim=0)\n\n    # \u30bd\u30fc\u30c8\n    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n\n    # 1. Original Distribution\n    ax = axes[0]\n    ax.bar(range(50), sorted_probs[:50].numpy(), color='lightblue')\n    ax.set_title('Original Probability Distribution')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n    ax.set_ylim(0, max(sorted_probs[:50]) * 1.1)\n\n    # 2. Greedy\n    ax = axes[1]\n    greedy_probs = torch.zeros_like(probs)\n    greedy_probs[sorted_indices[0]] = 1.0\n    ax.bar(range(50), greedy_probs[sorted_indices[:50]].numpy(), \n           color='green')\n    ax.set_title('Greedy Decoding')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n\n    # 3. Temperature\n    ax = axes[2]\n    for temp in [0.5, 1.0, 2.0]:\n        temp_logits = logits / temp\n        temp_probs = F.softmax(temp_logits, dim=0)\n        sorted_temp_probs = temp_probs[sorted_indices]\n        ax.plot(range(50), sorted_temp_probs[:50].numpy(), \n               label=f'T={temp}', linewidth=2)\n    ax.set_title('Temperature Sampling')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n    ax.legend()\n\n    # 4. Top-k\n    ax = axes[3]\n    k = 10\n    topk_probs = torch.zeros_like(probs)\n    topk_probs[sorted_indices[:k]] = sorted_probs[:k]\n    topk_probs = topk_probs / topk_probs.sum()  # \u518d\u6b63\u898f\u5316\n    ax.bar(range(50), topk_probs[sorted_indices[:50]].numpy(), \n           color='orange')\n    ax.set_title(f'Top-k Sampling (k={k})')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n    ax.axvline(x=k-0.5, color='red', linestyle='--', \n              label=f'k={k}')\n    ax.legend()\n\n    # 5. Top-p\n    ax = axes[4]\n    p = 0.9\n    cumsum = torch.cumsum(sorted_probs, dim=0)\n    cutoff_idx = (cumsum &lt;= p).sum().item()\n    topp_probs = torch.zeros_like(probs)\n    topp_probs[sorted_indices[:cutoff_idx]] = sorted_probs[:cutoff_idx]\n    topp_probs = topp_probs / topp_probs.sum()  # \u518d\u6b63\u898f\u5316\n    ax.bar(range(50), topp_probs[sorted_indices[:50]].numpy(), \n           color='purple')\n    ax.set_title(f'Top-p Sampling (p={p})')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n    ax.axvline(x=cutoff_idx-0.5, color='red', linestyle='--',\n              label=f'cutoff={cutoff_idx}')\n    ax.legend()\n\n    # 6. \u6bd4\u8f03\n    ax = axes[5]\n    ax.plot(range(20), sorted_probs[:20].numpy(), \n           'o-', label='Original', linewidth=2)\n\n    # \u5404\u624b\u6cd5\u306e\u6709\u52b9\u7bc4\u56f2\n    ax.axhspan(0, sorted_probs[0], alpha=0.3, color='green', \n              label='Greedy')\n    ax.axvspan(0, k-0.5, alpha=0.3, color='orange', \n              label=f'Top-k (k={k})')\n    ax.axvspan(0, cutoff_idx-0.5, alpha=0.3, color='purple', \n              label=f'Top-p (p={p})')\n\n    ax.set_title('Sampling Methods Comparison')\n    ax.set_xlabel('Token Rank')\n    ax.set_ylabel('Probability')\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class AdvancedSampling:     \"\"\"\u9ad8\u5ea6\u306a\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6280\u8853\"\"\"</p> <pre><code>def __init__(self, model: Optional[nn.Module] = None):\n    self.model = model\n\ndef top_k_top_p_filtering(self, logits: torch.Tensor, \n                          top_k: int = 0, top_p: float = 0.0,\n                          filter_value: float = -float('Inf')) -&gt; torch.Tensor:\n    \"\"\"Top-k\u3068Top-p\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u306e\u5b9f\u88c5\"\"\"\n    # Top-k\n    if top_k &gt; 0:\n        # \u4e0a\u4f4dk\u500b\u4ee5\u5916\u3092\u30d5\u30a3\u30eb\u30bf\n        indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    # Top-p\n    if top_p &gt; 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # \u7d2f\u7a4d\u78ba\u7387\u304cp\u3092\u8d85\u3048\u308b\u4f4d\u7f6e\u3092\u898b\u3064\u3051\u308b\n        sorted_indices_to_remove = cumulative_probs &gt; top_p\n\n        # \u6700\u521d\u306e\u30c8\u30fc\u30af\u30f3\u306f\u4fdd\u6301\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        # \u5143\u306e\u9806\u5e8f\u306b\u623b\u3057\u3066\u30d5\u30a3\u30eb\u30bf\n        indices_to_remove = sorted_indices_to_remove.scatter(\n            -1, sorted_indices, sorted_indices_to_remove\n        )\n        logits[indices_to_remove] = filter_value\n\n    return logits\n\ndef contrastive_search(self, input_ids: torch.Tensor,\n                      max_length: int = 50,\n                      alpha: float = 0.6,\n                      k: int = 5) -&gt; torch.Tensor:\n    \"\"\"Contrastive Search\u5b9f\u88c5\"\"\"\n    print(\"=== Contrastive Search ===\\n\")\n\n    print(\"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:\")\n    print(\"1. \u5404\u30b9\u30c6\u30c3\u30d7\u3067k\u500b\u306e\u5019\u88dc\u3092\u751f\u6210\")\n    print(\"2. \u30b9\u30b3\u30a2 = (1-\u03b1)\u00d7\u78ba\u7387 + \u03b1\u00d7\u591a\u69d8\u6027\")\n    print(\"3. \u6700\u9ad8\u30b9\u30b3\u30a2\u306e\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\\n\")\n\n    # \u7c21\u7565\u5316\u3057\u305f\u5b9f\u88c5\u4f8b\n    generated = input_ids.clone()\n\n    for _ in range(max_length - input_ids.size(1)):\n        # \u30e2\u30c7\u30eb\u4e88\u6e2c\uff08\u30c0\u30df\u30fc\uff09\n        if self.model:\n            outputs = self.model(generated)\n            logits = outputs.logits[:, -1, :]\n        else:\n            # \u30c0\u30df\u30fc\u306e\u30ed\u30b8\u30c3\u30c8\n            logits = torch.randn(1, 50000)\n\n        # Top-k\u5019\u88dc\n        top_k_probs, top_k_ids = torch.topk(\n            F.softmax(logits, dim=-1), k\n        )\n\n        # \u5404\u5019\u88dc\u306e\u591a\u69d8\u6027\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\n        diversity_scores = []\n\n        for candidate_id in top_k_ids[0]:\n            # \u65e2\u5b58\u30c8\u30fc\u30af\u30f3\u3068\u306e\u985e\u4f3c\u5ea6\uff08\u7c21\u7565\u7248\uff09\n            if generated.size(1) &gt; 1:\n                # \u5b9f\u969b\u306f\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u306e\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\n                similarity = torch.rand(1).item()\n            else:\n                similarity = 0\n\n            diversity = 1 - similarity\n            diversity_scores.append(diversity)\n\n        # \u7dcf\u5408\u30b9\u30b3\u30a2\n        diversity_scores = torch.tensor(diversity_scores)\n        scores = (1 - alpha) * top_k_probs[0] + alpha * diversity_scores\n\n        # \u6700\u9ad8\u30b9\u30b3\u30a2\u306e\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\n        best_idx = torch.argmax(scores)\n        next_token = top_k_ids[0, best_idx].unsqueeze(0).unsqueeze(0)\n\n        generated = torch.cat([generated, next_token], dim=1)\n\n    return generated\n\ndef typical_sampling(self, logits: torch.Tensor, \n                    typical_p: float = 0.95) -&gt; torch.Tensor:\n    \"\"\"Typical Sampling\u5b9f\u88c5\"\"\"\n    print(\"=== Typical Sampling ===\\n\")\n\n    print(\"\u6982\u5ff5: \u60c5\u5831\u7406\u8ad6\u306b\u57fa\u3065\u304f\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\")\n    print(\"\u300c\u5178\u578b\u7684\u300d\u306a\u30c8\u30fc\u30af\u30f3\u3092\u9078\u629e\\n\")\n\n    # \u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u8a08\u7b97\n    normalized = F.log_softmax(logits, dim=-1)\n    p = normalized.exp()\n    ent = -(normalized * p).sum(-1, keepdim=True)\n\n    # \u5404\u30c8\u30fc\u30af\u30f3\u306e\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6\u3092\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3067\u30b7\u30d5\u30c8\n    shifted_scores = normalized.abs() - ent\n\n    # \u5178\u578b\u6027\u3067\u30bd\u30fc\u30c8\n    sorted_scores, sorted_indices = torch.sort(shifted_scores)\n\n    # \u7d2f\u7a4d\u78ba\u7387\n    cumulative_probs = sorted_scores.exp().cumsum(dim=-1)\n\n    # typical_p\u4ee5\u4e0b\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4fdd\u6301\n    mask = cumulative_probs &lt;= typical_p\n\n    # \u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\n    filtered_logits = logits.clone()\n    filtered_logits[~mask] = -float('Inf')\n\n    return filtered_logits\n</code></pre>"},{"location":"part5/inference-techniques/#202","title":"20.2 \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u3068\u6d3e\u751f\u624b\u6cd5","text":"<p>class BeamSearchMethods:     \"\"\"\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u3068\u6d3e\u751f\u624b\u6cd5\"\"\"</p> <pre><code>def __init__(self, vocab_size: int = 1000):\n    self.vocab_size = vocab_size\n\ndef explain_beam_search(self):\n    \"\"\"\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u8aac\u660e\"\"\"\n    print(\"=== \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1 ===\\n\")\n\n    print(\"\u57fa\u672c\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:\")\n    print(\"1. \u30d3\u30fc\u30e0\u5e45k\u500b\u306e\u5019\u88dc\u3092\u4fdd\u6301\")\n    print(\"2. \u5404\u5019\u88dc\u304b\u3089\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u751f\u6210\")\n    print(\"3. \u5168\u5019\u88dc\u00d7\u8a9e\u5f59\u30b5\u30a4\u30ba\u304b\u3089\u4e0a\u4f4dk\u500b\u3092\u9078\u629e\")\n    print(\"4. \u7d42\u4e86\u6761\u4ef6\u307e\u3067\u7e70\u308a\u8fd4\u3057\\n\")\n\n    # \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u904e\u7a0b\u3092\u53ef\u8996\u5316\n    self._visualize_beam_search()\n\ndef _visualize_beam_search(self):\n    \"\"\"\u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u904e\u7a0b\u3092\u53ef\u8996\u5316\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 8))\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\n    beam_width = 3\n    vocab_size = 5  # \u7c21\u7565\u5316\n    steps = 4\n\n    # \u30ce\u30fc\u30c9\u306e\u4f4d\u7f6e\n    x_spacing = 2\n    y_spacing = 1.5\n\n    # \u4eee\u60f3\u7684\u306a\u30b9\u30b3\u30a2\n    np.random.seed(42)\n\n    # \u30d3\u30fc\u30e0\u3092\u8ffd\u8de1\n    beams = [[(0, 0, '&lt;start&gt;', 0)]]  # (x, y, token, score)\n\n    for step in range(1, steps):\n        new_beams = []\n\n        for beam_idx, (prev_x, prev_y, prev_token, prev_score) in enumerate(beams[-1][:beam_width]):\n            # \u5404\u30d3\u30fc\u30e0\u304b\u3089\u5c55\u958b\n            for v in range(min(vocab_size, beam_width + 1)):\n                x = step * x_spacing\n                y = prev_y + (v - vocab_size//2) * y_spacing * 0.3\n\n                score = prev_score - np.random.exponential(0.5)\n                token = f'T{step}{v}'\n\n                new_beams.append((x, y, token, score, prev_x, prev_y))\n\n        # \u30b9\u30b3\u30a2\u3067\u30bd\u30fc\u30c8\u3057\u3066\u4e0a\u4f4dbeam_width\u500b\u3092\u9078\u629e\n        new_beams.sort(key=lambda x: x[3], reverse=True)\n        selected_beams = new_beams[:beam_width]\n\n        # \u9078\u629e\u3055\u308c\u305f\u30d3\u30fc\u30e0\u3092\u63cf\u753b\n        for x, y, token, score, prev_x, prev_y in selected_beams:\n            # \u30ce\u30fc\u30c9\n            circle = plt.Circle((x, y), 0.3, color='lightblue', \n                              ec='darkblue', linewidth=2)\n            ax.add_patch(circle)\n            ax.text(x, y, token, ha='center', va='center', fontsize=8)\n\n            # \u30a8\u30c3\u30b8\n            ax.plot([prev_x, x], [prev_y, y], 'b-', alpha=0.5, linewidth=2)\n\n            # \u30b9\u30b3\u30a2\n            ax.text(x, y-0.5, f'{score:.2f}', ha='center', \n                   fontsize=6, color='red')\n\n        # \u9078\u629e\u3055\u308c\u306a\u304b\u3063\u305f\u30d3\u30fc\u30e0\u3092\u8584\u304f\u63cf\u753b\n        for x, y, token, score, prev_x, prev_y in new_beams[beam_width:]:\n            circle = plt.Circle((x, y), 0.2, color='lightgray', \n                              ec='gray', linewidth=1, alpha=0.3)\n            ax.add_patch(circle)\n            ax.plot([prev_x, x], [prev_y, y], 'gray', \n                   alpha=0.2, linewidth=1)\n\n        beams.append([(x, y, token, score) for x, y, token, score, _, _ in selected_beams])\n\n    # \u958b\u59cb\u30ce\u30fc\u30c9\n    start_circle = plt.Circle((0, 0), 0.3, color='lightgreen',\n                            ec='darkgreen', linewidth=2)\n    ax.add_patch(start_circle)\n    ax.text(0, 0, '&lt;start&gt;', ha='center', va='center', fontsize=8)\n\n    ax.set_xlim(-1, steps * x_spacing)\n    ax.set_ylim(-4, 4)\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax.set_title(f'Beam Search Process (beam_width={beam_width})', \n                fontsize=14, weight='bold')\n\n    # \u51e1\u4f8b\n    from matplotlib.patches import Patch\n    legend_elements = [\n        Patch(facecolor='lightblue', edgecolor='darkblue', \n              label='Selected beam'),\n        Patch(facecolor='lightgray', edgecolor='gray', \n              label='Pruned beam', alpha=0.3)\n    ]\n    ax.legend(handles=legend_elements, loc='upper right')\n\n    plt.tight_layout()\n    plt.show()\n\ndef diverse_beam_search(self, num_beams: int = 4, \n                       num_beam_groups: int = 2,\n                       diversity_penalty: float = 0.5):\n    \"\"\"Diverse Beam Search\u306e\u8aac\u660e\"\"\"\n    print(\"\\n=== Diverse Beam Search ===\\n\")\n\n    print(\"\u76ee\u7684: \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\u306e\u591a\u69d8\u6027\u3092\u5411\u4e0a\")\n    print(f\"\u8a2d\u5b9a: {num_beams}\u30d3\u30fc\u30e0, {num_beam_groups}\u30b0\u30eb\u30fc\u30d7\\n\")\n\n    print(\"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:\")\n    print(\"1. \u30d3\u30fc\u30e0\u3092\u30b0\u30eb\u30fc\u30d7\u306b\u5206\u5272\")\n    print(\"2. \u5404\u30b0\u30eb\u30fc\u30d7\u306f\u7570\u306a\u308b\u30da\u30ca\u30eb\u30c6\u30a3\u3067\u63a2\u7d22\")\n    print(\"3. \u30b0\u30eb\u30fc\u30d7\u9593\u306e\u591a\u69d8\u6027\u3092\u4fc3\u9032\\n\")\n\n    # \u4f8b\n    groups = []\n    beams_per_group = num_beams // num_beam_groups\n\n    for g in range(num_beam_groups):\n        print(f\"\u30b0\u30eb\u30fc\u30d7 {g+1}:\")\n        group_beams = []\n\n        for b in range(beams_per_group):\n            # \u4eee\u60f3\u7684\u306a\u5019\u88dc\n            candidate = f\"Beam_{g}_{b}: \" + \" \".join([\n                f\"token_{np.random.randint(100)}\" \n                for _ in range(5)\n            ])\n            group_beams.append(candidate)\n            print(f\"  {candidate}\")\n\n        groups.append(group_beams)\n\n    print(f\"\\n\u591a\u69d8\u6027\u30da\u30ca\u30eb\u30c6\u30a3: {diversity_penalty}\")\n    print(\"\u2192 \u30b0\u30eb\u30fc\u30d7\u9593\u3067\u7570\u306a\u308b\u6587\u3092\u751f\u6210\")\n</code></pre>"},{"location":"part5/inference-techniques/#203","title":"20.3 \u5236\u7d04\u4ed8\u304d\u751f\u6210","text":"<p>class ConstrainedGeneration:     \"\"\"\u5236\u7d04\u4ed8\u304d\u751f\u6210\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    self.constraints = []\n\ndef explain_constraints(self):\n    \"\"\"\u5236\u7d04\u306e\u7a2e\u985e\u3092\u8aac\u660e\"\"\"\n    print(\"=== \u5236\u7d04\u4ed8\u304d\u751f\u6210 ===\\n\")\n\n    constraints = {\n        \"\u9577\u3055\u5236\u7d04\": {\n            \"\u8aac\u660e\": \"\u6700\u5c0f/\u6700\u5927\u9577\u306e\u6307\u5b9a\",\n            \"\u4f8b\": \"min_length=10, max_length=50\",\n            \"\u5b9f\u88c5\": \"\u9577\u3055\u306b\u5fdc\u3058\u3066EOS\u30c8\u30fc\u30af\u30f3\u3092\u30de\u30b9\u30af\"\n        },\n\n        \"\u5185\u5bb9\u5236\u7d04\": {\n            \"\u8aac\u660e\": \"\u7279\u5b9a\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u542b\u3080/\u542b\u307e\u306a\u3044\",\n            \"\u4f8b\": \"must_include=['AI'], must_exclude=['\u5371\u967a']\",\n            \"\u5b9f\u88c5\": \"\u5236\u7d04\u3092\u6e80\u305f\u3059\u307e\u3067\u518d\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\"\n        },\n\n        \"\u6587\u6cd5\u5236\u7d04\": {\n            \"\u8aac\u660e\": \"\u6587\u6cd5\u7684\u306b\u6b63\u3057\u3044\u6587\u306e\u307f\u751f\u6210\",\n            \"\u4f8b\": \"JSON\u5f62\u5f0f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u30b3\u30fc\u30c9\",\n            \"\u5b9f\u88c5\": \"\u6587\u6cd5\u306b\u57fa\u3065\u304f\u30de\u30b9\u30ad\u30f3\u30b0\"\n        },\n\n        \"\u610f\u5473\u5236\u7d04\": {\n            \"\u8aac\u660e\": \"\u7279\u5b9a\u306e\u610f\u5473\u3084\u30c8\u30fc\u30f3\u3092\u7dad\u6301\",\n            \"\u4f8b\": \"\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u6587\u3001\u5c02\u9580\u7684\u306a\u6587\u4f53\",\n            \"\u5b9f\u88c5\": \"\u5206\u985e\u5668\u3067\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\"\n        }\n    }\n\n    for name, details in constraints.items():\n        print(f\"{name}:\")\n        for key, value in details.items():\n            print(f\"  {key}: {value}\")\n        print()\n\ndef implement_length_constraint(self, logits: torch.Tensor,\n                              current_length: int,\n                              min_length: int = 10,\n                              max_length: int = 50,\n                              eos_token_id: int = 2) -&gt; torch.Tensor:\n    \"\"\"\u9577\u3055\u5236\u7d04\u306e\u5b9f\u88c5\"\"\"\n    # \u6700\u5c0f\u9577\u672a\u6e80\u3067\u306fEOS\u3092\u7981\u6b62\n    if current_length &lt; min_length:\n        logits[..., eos_token_id] = -float('inf')\n\n    # \u6700\u5927\u9577\u306b\u9054\u3057\u305f\u3089EOS\u3092\u5f37\u5236\n    if current_length &gt;= max_length - 1:\n        mask = torch.ones_like(logits) * -float('inf')\n        mask[..., eos_token_id] = 0\n        logits = logits + mask\n\n    return logits\n\ndef implement_keyword_constraint(self, generated_text: str,\n                               must_include: List[str],\n                               must_exclude: List[str]) -&gt; bool:\n    \"\"\"\u30ad\u30fc\u30ef\u30fc\u30c9\u5236\u7d04\u306e\u30c1\u30a7\u30c3\u30af\"\"\"\n    # \u542b\u3080\u3079\u304d\u30ad\u30fc\u30ef\u30fc\u30c9\n    for keyword in must_include:\n        if keyword not in generated_text:\n            return False\n\n    # \u542b\u307e\u306a\u3044\u3079\u304d\u30ad\u30fc\u30ef\u30fc\u30c9\n    for keyword in must_exclude:\n        if keyword in generated_text:\n            return False\n\n    return True\n\ndef guided_generation_example(self):\n    \"\"\"\u30ac\u30a4\u30c9\u4ed8\u304d\u751f\u6210\u306e\u4f8b\"\"\"\n    print(\"\\n=== \u30ac\u30a4\u30c9\u4ed8\u304d\u751f\u6210\u306e\u4f8b ===\\n\")\n\n    # JSON\u751f\u6210\u306e\u4f8b\n    print(\"\u4f8b1: JSON\u5f62\u5f0f\u306e\u751f\u6210\")\n\n    json_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"number\"},\n            \"skills\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n        }\n    }\n\n    print(\"\u30b9\u30ad\u30fc\u30de:\")\n    print(json_schema)\n\n    print(\"\\n\u751f\u6210\u30d7\u30ed\u30bb\u30b9:\")\n    print(\"1. '{' \u2192 \u5fc5\u9808\")\n    print(\"2. '\\\"name\\\"' \u2192 \u30d7\u30ed\u30d1\u30c6\u30a3\u540d\u3092\u63d0\u6848\")\n    print(\"3. ':' \u2192 \u5fc5\u9808\")\n    print(\"4. '\\\"...\\\"' \u2192 \u6587\u5b57\u5217\u5024\u306e\u307f\u8a31\u53ef\")\n    print(\"5. ',' or '}' \u2192 \u6587\u6cd5\u306b\u5f93\u3063\u3066\u9078\u629e\")\n\n    # \u72b6\u614b\u9077\u79fb\u306e\u53ef\u8996\u5316\n    self._visualize_json_generation()\n\ndef _visualize_json_generation(self):\n    \"\"\"JSON\u751f\u6210\u306e\u72b6\u614b\u9077\u79fb\u3092\u53ef\u8996\u5316\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 8))\n\n    # \u72b6\u614b\n    states = {\n        \"start\": (2, 4),\n        \"{\": (4, 4),\n        \"key\": (6, 5),\n        \":\": (8, 5),\n        \"value\": (10, 5),\n        \",\": (10, 3),\n        \"}\": (12, 4)\n    }\n\n    # \u9077\u79fb\n    transitions = [\n        (\"start\", \"{\", \"\u5fc5\u9808\"),\n        (\"{\", \"key\", \"\u30d7\u30ed\u30d1\u30c6\u30a3\u540d\"),\n        (\"key\", \":\", \"\u5fc5\u9808\"),\n        (\":\", \"value\", \"\u578b\u306b\u5fdc\u3058\u3066\"),\n        (\"value\", \",\", \"\u7d9a\u304d\u3042\u308a\"),\n        (\"value\", \"}\", \"\u7d42\u4e86\"),\n        (\",\", \"key\", \"\u6b21\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\")\n    ]\n\n    # \u72b6\u614b\u3092\u63cf\u753b\n    for state, (x, y) in states.items():\n        if state == \"start\":\n            color = 'lightgreen'\n        elif state == \"}\":\n            color = 'lightcoral'\n        else:\n            color = 'lightblue'\n\n        circle = plt.Circle((x, y), 0.6, color=color,\n                          ec='black', linewidth=2)\n        ax.add_patch(circle)\n        ax.text(x, y, state, ha='center', va='center',\n               fontsize=10, weight='bold')\n\n    # \u9077\u79fb\u3092\u63cf\u753b\n    for from_state, to_state, label in transitions:\n        from_pos = states[from_state]\n        to_pos = states[to_state]\n\n        # \u77e2\u5370\n        if from_state == \",\" and to_state == \"key\":\n            # \u66f2\u7dda\u77e2\u5370\n            ax.annotate('', xy=to_pos, xytext=from_pos,\n                      arrowprops=dict(arrowstyle='-&gt;', lw=2,\n                                    connectionstyle=\"arc3,rad=-.5\"))\n        else:\n            ax.annotate('', xy=to_pos, xytext=from_pos,\n                      arrowprops=dict(arrowstyle='-&gt;', lw=2))\n\n        # \u30e9\u30d9\u30eb\n        mid_x = (from_pos[0] + to_pos[0]) / 2\n        mid_y = (from_pos[1] + to_pos[1]) / 2\n\n        if from_state == \",\" and to_state == \"key\":\n            mid_y -= 0.5\n\n        ax.text(mid_x, mid_y + 0.3, label, ha='center',\n               fontsize=8, style='italic',\n               bbox=dict(boxstyle=\"round,pad=0.3\",\n                       facecolor='white', alpha=0.8))\n\n    ax.set_xlim(0, 14)\n    ax.set_ylim(1, 7)\n    ax.axis('off')\n    ax.set_title('JSON Generation State Machine', \n                fontsize=14, weight='bold')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part5/inference-techniques/#204","title":"20.4 \u52b9\u7387\u7684\u306a\u63a8\u8ad6","text":"<p>class EfficientInference:     \"\"\"\u52b9\u7387\u7684\u306a\u63a8\u8ad6\u6280\u8853\"\"\"</p> <pre><code>def explain_optimization_techniques(self):\n    \"\"\"\u6700\u9069\u5316\u6280\u8853\u306e\u8aac\u660e\"\"\"\n    print(\"=== \u63a8\u8ad6\u306e\u6700\u9069\u5316\u6280\u8853 ===\\n\")\n\n    techniques = {\n        \"KV Cache\": {\n            \"\u8aac\u660e\": \"Key-Value\u306e\u518d\u8a08\u7b97\u3092\u56de\u907f\",\n            \"\u524a\u6e1b\": \"\u8a08\u7b97\u91cf\u3092O(n\u00b2)\u2192O(n)\u306b\",\n            \"\u30e1\u30e2\u30ea\": \"O(n)\u306e\u8ffd\u52a0\u30e1\u30e2\u30ea\u5fc5\u8981\"\n        },\n\n        \"Flash Decoding\": {\n            \"\u8aac\u660e\": \"\u52b9\u7387\u7684\u306a\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u8a08\u7b97\",\n            \"\u524a\u6e1b\": \"\u30e1\u30e2\u30ea\u5e2f\u57df\u5e45\u306e\u6700\u9069\u5316\",\n            \"\u30e1\u30e2\u30ea\": \"\u5927\u5e45\u306a\u30e1\u30e2\u30ea\u524a\u6e1b\"\n        },\n\n        \"Speculative Decoding\": {\n            \"\u8aac\u660e\": \"\u5c0f\u30e2\u30c7\u30eb\u3067\u5019\u88dc\u751f\u6210\u3001\u5927\u30e2\u30c7\u30eb\u3067\u691c\u8a3c\",\n            \"\u524a\u6e1b\": \"\u30ec\u30a4\u30c6\u30f3\u30b7\u30922-3\u500d\u9ad8\u901f\u5316\",\n            \"\u30e1\u30e2\u30ea\": \"\u8ffd\u52a0\u30e2\u30c7\u30eb\u306e\u30e1\u30e2\u30ea\u5fc5\u8981\"\n        },\n\n        \"Continuous Batching\": {\n            \"\u8aac\u660e\": \"\u52d5\u7684\u306a\u30d0\u30c3\u30c1\u51e6\u7406\",\n            \"\u524a\u6e1b\": \"GPU\u306e\u4f7f\u7528\u7387\u5411\u4e0a\",\n            \"\u30e1\u30e2\u30ea\": \"\u52b9\u7387\u7684\u306a\u30e1\u30e2\u30ea\u7ba1\u7406\"\n        },\n\n        \"Quantization\": {\n            \"\u8aac\u660e\": \"\u4f4e\u7cbe\u5ea6\u3067\u306e\u63a8\u8ad6\",\n            \"\u524a\u6e1b\": \"\u30e1\u30e2\u30ea\u3068\u8a08\u7b97\u91cf\u3092\u524a\u6e1b\",\n            \"\u30e1\u30e2\u30ea\": \"2-4\u500d\u306e\u30e1\u30e2\u30ea\u524a\u6e1b\"\n        }\n    }\n\n    for name, details in techniques.items():\n        print(f\"{name}:\")\n        for key, value in details.items():\n            print(f\"  {key}: {value}\")\n        print()\n\n    # \u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u6bd4\u8f03\n    self._compare_performance()\n\ndef _compare_performance(self):\n    \"\"\"\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u6bd4\u8f03\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # \u30ec\u30a4\u30c6\u30f3\u30b7\u6bd4\u8f03\n    methods = ['Baseline', 'KV Cache', 'Flash', 'Speculative', 'All']\n    latencies = [100, 60, 40, 35, 20]  # ms\n    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n\n    bars = ax1.bar(methods, latencies, color=colors)\n    ax1.set_ylabel('Latency (ms)')\n    ax1.set_title('Inference Latency Comparison')\n\n    # \u5024\u3092\u8868\u793a\n    for bar, latency in zip(bars, latencies):\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f'{latency}ms', ha='center', va='bottom')\n\n    # \u30b9\u30eb\u30fc\u30d7\u30c3\u30c8\u6bd4\u8f03\n    batch_sizes = [1, 4, 16, 64, 256]\n    baseline_throughput = [10, 35, 120, 400, 800]\n    optimized_throughput = [15, 60, 220, 800, 2000]\n\n    ax2.plot(batch_sizes, baseline_throughput, 'ro-', \n            label='Baseline', linewidth=2, markersize=8)\n    ax2.plot(batch_sizes, optimized_throughput, 'go-', \n            label='Optimized', linewidth=2, markersize=8)\n\n    ax2.set_xlabel('Batch Size')\n    ax2.set_ylabel('Throughput (tokens/sec)')\n    ax2.set_title('Throughput Scaling')\n    ax2.set_xscale('log')\n    ax2.set_yscale('log')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\ndef implement_kv_cache(self):\n    \"\"\"KV\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u5b9f\u88c5\u4f8b\"\"\"\n    print(\"\\n=== KV Cache\u306e\u5b9f\u88c5 ===\\n\")\n\n    code = '''\n</code></pre> <p>class KVCache:     \"\"\"Key-Value\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, batch_size: int, max_seq_len: int,\n             n_heads: int, head_dim: int, n_layers: int):\n    self.batch_size = batch_size\n    self.max_seq_len = max_seq_len\n    self.n_heads = n_heads\n    self.head_dim = head_dim\n    self.n_layers = n_layers\n\n    # \u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u521d\u671f\u5316\n    cache_shape = (batch_size, n_heads, max_seq_len, head_dim)\n    self.k_cache = [torch.zeros(cache_shape) for _ in range(n_layers)]\n    self.v_cache = [torch.zeros(cache_shape) for _ in range(n_layers)]\n    self.cache_len = 0\n\ndef update(self, layer_idx: int, k: torch.Tensor, v: torch.Tensor):\n    \"\"\"\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u66f4\u65b0\"\"\"\n    seq_len = k.size(2)\n\n    # \u65b0\u3057\u3044K, V\u3092\u30ad\u30e3\u30c3\u30b7\u30e5\u306b\u8ffd\u52a0\n    self.k_cache[layer_idx][:, :, self.cache_len:self.cache_len+seq_len] = k\n    self.v_cache[layer_idx][:, :, self.cache_len:self.cache_len+seq_len] = v\n\n    # \u904e\u53bb\u306eK, V\u3068\u7d50\u5408\n    k_full = self.k_cache[layer_idx][:, :, :self.cache_len+seq_len]\n    v_full = self.v_cache[layer_idx][:, :, :self.cache_len+seq_len]\n\n    self.cache_len += seq_len\n\n    return k_full, v_full\n\ndef clear(self):\n    \"\"\"\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u30af\u30ea\u30a2\"\"\"\n    for k, v in zip(self.k_cache, self.v_cache):\n        k.zero_()\n        v.zero_()\n    self.cache_len = 0\n</code></pre> <p>'''</p> <pre><code>    print(code)\n\n    print(\"\\n\u52b9\u679c:\")\n    print(\"\u2022 \u5404\u30c8\u30fc\u30af\u30f3\u751f\u6210\u6642\u306e\u8a08\u7b97\u91cf: O(n\u00b2) \u2192 O(n)\")\n    print(\"\u2022 \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf: O(n\u00d7d\u00d7layers\u00d7heads)\")\n    print(\"\u2022 \u9577\u3044\u30b7\u30fc\u30b1\u30f3\u30b9\u307b\u3069\u52b9\u679c\u5927\")\n</code></pre> <p>class SpeculativeDecoding:     \"\"\"\u6295\u6a5f\u7684\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\"\"</p> <pre><code>def explain_algorithm(self):\n    \"\"\"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8aac\u660e\"\"\"\n    print(\"=== Speculative Decoding ===\\n\")\n\n    print(\"\u6982\u5ff5: \u5c0f\u3055\u306a\u30e2\u30c7\u30eb\u3067\u300c\u63a8\u6e2c\u300d\u3001\u5927\u304d\u306a\u30e2\u30c7\u30eb\u3067\u300c\u691c\u8a3c\u300d\\n\")\n\n    print(\"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0:\")\n    print(\"1. \u30c9\u30e9\u30d5\u30c8\u30e2\u30c7\u30eb\uff08\u5c0f\uff09\u304cK\u500b\u306e\u30c8\u30fc\u30af\u30f3\u3092\u751f\u6210\")\n    print(\"2. \u30bf\u30fc\u30b2\u30c3\u30c8\u30e2\u30c7\u30eb\uff08\u5927\uff09\u304c\u4e00\u5ea6\u306b\u691c\u8a3c\")\n    print(\"3. \u4e00\u81f4\u3059\u308b\u90e8\u5206\u307e\u3067\u63a1\u7528\")\n    print(\"4. \u4e0d\u4e00\u81f4\u70b9\u304b\u3089\u518d\u958b\\n\")\n\n    # \u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316\n    self._visualize_speculative_process()\n\ndef _visualize_speculative_process(self):\n    \"\"\"\u6295\u6a5f\u7684\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u53ef\u8996\u5316\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n    # \u30c9\u30e9\u30d5\u30c8\u30e2\u30c7\u30eb\u306e\u751f\u6210\n    ax = axes[0]\n    ax.set_title('Draft Model Generation', fontsize=12)\n\n    draft_tokens = ['The', 'cat', 'is', 'sleeping', 'on', 'the', 'mat']\n    draft_probs = [0.9, 0.85, 0.8, 0.7, 0.6, 0.5, 0.4]\n\n    x_pos = 0\n    for token, prob in zip(draft_tokens, draft_probs):\n        width = 1.5\n        color = plt.cm.Greens(prob)\n        rect = plt.Rectangle((x_pos, 0), width, 1,\n                           facecolor=color, edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(x_pos + width/2, 0.5, token, ha='center', va='center')\n        ax.text(x_pos + width/2, 1.2, f'{prob:.2f}', ha='center',\n               fontsize=8, color='darkgreen')\n        x_pos += width + 0.1\n\n    ax.set_xlim(-0.5, x_pos)\n    ax.set_ylim(-0.5, 2)\n    ax.axis('off')\n    ax.text(x_pos/2, -0.3, 'Fast generation (low latency)',\n           ha='center', style='italic')\n\n    # \u30bf\u30fc\u30b2\u30c3\u30c8\u30e2\u30c7\u30eb\u306e\u691c\u8a3c\n    ax = axes[1]\n    ax.set_title('Target Model Verification', fontsize=12)\n\n    # \u691c\u8a3c\u7d50\u679c\uff085\u756a\u76ee\u3067\u4e0d\u4e00\u81f4\uff09\n    verification = [True, True, True, True, False, False, False]\n\n    x_pos = 0\n    for i, (token, verified) in enumerate(zip(draft_tokens, verification)):\n        width = 1.5\n        if verified:\n            color = 'lightgreen'\n            edge_color = 'green'\n            edge_width = 3\n        else:\n            color = 'lightcoral'\n            edge_color = 'red'\n            edge_width = 1\n\n        rect = plt.Rectangle((x_pos, 0), width, 1,\n                           facecolor=color, edgecolor=edge_color,\n                           linewidth=edge_width)\n        ax.add_patch(rect)\n        ax.text(x_pos + width/2, 0.5, token, ha='center', va='center')\n\n        if verified:\n            ax.text(x_pos + width/2, 1.2, '\u2713', ha='center',\n                   fontsize=16, color='green')\n        else:\n            ax.text(x_pos + width/2, 1.2, '\u2717', ha='center',\n                   fontsize=16, color='red')\n\n        x_pos += width + 0.1\n\n    # \u63a1\u7528\u7bc4\u56f2\u3092\u793a\u3059\n    accept_width = 4 * (1.5 + 0.1) - 0.1\n    accept_rect = plt.Rectangle((-0.3, -0.3), accept_width + 0.6, 1.6,\n                              facecolor='none', edgecolor='green',\n                              linewidth=3, linestyle='--')\n    ax.add_patch(accept_rect)\n    ax.text(accept_width/2, -0.6, 'Accepted tokens',\n           ha='center', color='green', weight='bold')\n\n    ax.set_xlim(-0.5, x_pos)\n    ax.set_ylim(-1, 2)\n    ax.axis('off')\n    ax.text(x_pos/2, -0.9, 'Parallel verification (high throughput)',\n           ha='center', style='italic')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\u7d50\u679c: 4\u30c8\u30fc\u30af\u30f3\u304c\u4e00\u5ea6\u306b\u78ba\u5b9a\uff084\u500d\u306e\u9ad8\u901f\u5316\uff09\")\n</code></pre>"},{"location":"part5/inference-techniques/#_4","title":"\u30c7\u30e2\u3068\u307e\u3068\u3081","text":"<p>def run_inference_demo():     \"\"\"\u63a8\u8ad6\u6280\u8853\u306e\u30c7\u30e2\u3092\u5b9f\u884c\"\"\"     print(\"=\" * 70)     print(\"\u63a8\u8ad6\u6280\u8853\u306e\u8a73\u89e3\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># 1. \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6226\u7565\nsampling = SamplingStrategies()\nsampling.explain_sampling_methods()\n\n# 2. \u9ad8\u5ea6\u306a\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\nprint(\"\\n\")\nadvanced = AdvancedSampling()\n\n# Top-k/Top-p\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u306e\u4f8b\nlogits = torch.randn(100)\nfiltered = advanced.top_k_top_p_filtering(logits, top_k=10, top_p=0.9)\nprint(\"Top-k/Top-p\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u9069\u7528\")\n\n# Contrastive Search\ndummy_input = torch.tensor([[1, 2, 3]])\nadvanced.contrastive_search(dummy_input, max_length=10)\n\n# Typical Sampling\nadvanced.typical_sampling(logits, typical_p=0.95)\n\n# 3. \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1\nprint(\"\\n\")\nbeam = BeamSearchMethods()\nbeam.explain_beam_search()\nbeam.diverse_beam_search()\n\n# 4. \u5236\u7d04\u4ed8\u304d\u751f\u6210\nprint(\"\\n\")\nconstrained = ConstrainedGeneration()\nconstrained.explain_constraints()\nconstrained.guided_generation_example()\n\n# 5. \u52b9\u7387\u7684\u306a\u63a8\u8ad6\nprint(\"\\n\")\nefficient = EfficientInference()\nefficient.explain_optimization_techniques()\nefficient.implement_kv_cache()\n\n# 6. \u6295\u6a5f\u7684\u30c7\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\nprint(\"\\n\")\nspeculative = SpeculativeDecoding()\nspeculative.explain_algorithm()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081\")\nprint(\"=\" * 70)\nprint(\"\\n\u63a8\u8ad6\u6280\u8853\u306e\u8981\u70b9:\")\nprint(\"\u2022 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0: \u54c1\u8cea\u3068\u591a\u69d8\u6027\u306e\u30d0\u30e9\u30f3\u30b9\")\nprint(\"\u2022 \u30d3\u30fc\u30e0\u30b5\u30fc\u30c1: \u9ad8\u54c1\u8cea\u3060\u304c\u8a08\u7b97\u30b3\u30b9\u30c8\u9ad8\")\nprint(\"\u2022 \u5236\u7d04\u4ed8\u304d\u751f\u6210: \u30bf\u30b9\u30af\u7279\u6709\u306e\u8981\u6c42\u306b\u5bfe\u5fdc\")\nprint(\"\u2022 \u6700\u9069\u5316\u6280\u8853: \u5b9f\u7528\u7684\u306a\u901f\u5ea6\u3092\u5b9f\u73fe\")\nprint(\"\\n\u9069\u5207\u306a\u63a8\u8ad6\u6280\u8853\u306e\u9078\u629e\u306b\u3088\u308a\u3001\")\nprint(\"\u30e2\u30c7\u30eb\u306e\u6f5c\u5728\u80fd\u529b\u3092\u6700\u5927\u9650\u306b\u5f15\u304d\u51fa\u305b\u307e\u3059\u3002\")\n</code></pre> <p>if name == \"main\":     run_inference_demo()</p>"},{"location":"part5/pretraining-finetuning/","title":"\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0","text":""},{"location":"part5/pretraining-finetuning/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u77e5\u8b58\u306e\u8ee2\u79fb","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u958b\u767a\u3067\u3001\u65e2\u5b58\u306e\u30d1\u30fc\u30b5\u30fc\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3084\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3092\u8003\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u30bc\u30ed\u304b\u3089\u4f5c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u6c4e\u7528\u7684\u306a\u57fa\u76e4\u306e\u4e0a\u306b\u7279\u5b9a\u306e\u8a00\u8a9e\u4ed5\u69d8\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002\u6df1\u5c64\u5b66\u7fd2\u306b\u304a\u3051\u308b\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3082\u540c\u3058\u8003\u3048\u65b9\u3067\u3059\u3002</p> <p>\u5927\u898f\u6a21\u306a\u6c4e\u7528\u30b3\u30fc\u30d1\u30b9\u3067\u57fa\u790e\u7684\u306a\u8a00\u8a9e\u7406\u89e3\u3092\u5b66\u7fd2\u3057\uff08\u4e8b\u524d\u5b66\u7fd2\uff09\u3001\u305d\u306e\u5f8c\u7279\u5b9a\u306e\u30bf\u30b9\u30af\u306b\u9069\u5fdc\u3055\u305b\u308b\uff08\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\uff09\u3002\u3053\u306e\u4e8c\u6bb5\u968e\u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u304c\u3001\u73fe\u4ee3\u306eNLP\u306e\u6210\u529f\u306e\u9375\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"part5/pretraining-finetuning/#181","title":"18.1 \u4e8b\u524d\u5b66\u7fd2\u306e\u4ed5\u7d44\u307f","text":""},{"location":"part5/pretraining-finetuning/#_3","title":"\u81ea\u5df1\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u5a01\u529b","text":"<p>```python import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import List, Dict, Tuple, Optional, Any from dataclasses import dataclass import random from torch.utils.data import Dataset, DataLoader from collections import defaultdict import time from tqdm import tqdm</p> <p>class PretrainingObjectives:     \"\"\"\u4e8b\u524d\u5b66\u7fd2\u306e\u76ee\u7684\u95a2\u6570\"\"\"</p> <pre><code>def explain_objectives(self):\n    \"\"\"\u4e3b\u8981\u306a\u4e8b\u524d\u5b66\u7fd2\u76ee\u7684\u3092\u8aac\u660e\"\"\"\n    print(\"=== \u4e8b\u524d\u5b66\u7fd2\u306e\u76ee\u7684\u95a2\u6570 ===\\n\")\n\n    objectives = {\n        \"Causal Language Modeling (CLM)\": {\n            \"\u8aac\u660e\": \"\u6b21\u306e\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\uff08GPT\u7cfb\uff09\",\n            \"\u4f8b\": \"The cat sat on the [PREDICT]\",\n            \"\u5229\u70b9\": \"\u81ea\u7136\u306a\u751f\u6210\u80fd\u529b\",\n            \"\u6b20\u70b9\": \"\u5358\u65b9\u5411\u306e\u6587\u8108\u306e\u307f\"\n        },\n\n        \"Masked Language Modeling (MLM)\": {\n            \"\u8aac\u660e\": \"\u30de\u30b9\u30af\u3055\u308c\u305f\u30c8\u30fc\u30af\u30f3\u3092\u4e88\u6e2c\uff08BERT\u7cfb\uff09\",\n            \"\u4f8b\": \"The [MASK] sat on the mat\",\n            \"\u5229\u70b9\": \"\u53cc\u65b9\u5411\u306e\u6587\u8108\u3092\u6d3b\u7528\",\n            \"\u6b20\u70b9\": \"\u751f\u6210\u30bf\u30b9\u30af\u306b\u4e0d\u5411\u304d\"\n        },\n\n        \"Permutation Language Modeling\": {\n            \"\u8aac\u660e\": \"\u30e9\u30f3\u30c0\u30e0\u306a\u9806\u5e8f\u3067\u4e88\u6e2c\uff08XLNet\uff09\",\n            \"\u4f8b\": \"\u9806\u5217: [2,4,1,3] \u3067\u4e88\u6e2c\",\n            \"\u5229\u70b9\": \"\u53cc\u65b9\u5411\u6027\u3068\u81ea\u5df1\u56de\u5e30\u306e\u4e21\u7acb\",\n            \"\u6b20\u70b9\": \"\u5b9f\u88c5\u304c\u8907\u96d1\"\n        },\n\n        \"Denoising Objectives\": {\n            \"\u8aac\u660e\": \"\u7834\u640d\u3057\u305f\u30c6\u30ad\u30b9\u30c8\u3092\u5fa9\u5143\uff08T5, BART\uff09\",\n            \"\u4f8b\": \"The &lt;X&gt; on &lt;Y&gt; mat \u2192 The cat sat on the mat\",\n            \"\u5229\u70b9\": \"\u67d4\u8edf\u306a\u4e8b\u524d\u5b66\u7fd2\",\n            \"\u6b20\u70b9\": \"\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\"\n        }\n    }\n\n    for name, details in objectives.items():\n        print(f\"{name}:\")\n        for key, value in details.items():\n            print(f\"  {key}: {value}\")\n        print()\n\n    # \u53ef\u8996\u5316\n    self._visualize_objectives()\n\ndef _visualize_objectives(self):\n    \"\"\"\u76ee\u7684\u95a2\u6570\u3092\u53ef\u8996\u5316\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # 1. Causal LM\n    ax = axes[0, 0]\n    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n    positions = range(len(tokens))\n\n    # \u30c8\u30fc\u30af\u30f3\u3092\u8868\u793a\n    for i, token in enumerate(tokens):\n        rect = plt.Rectangle((i, 0), 1, 1, facecolor='lightblue', \n                           edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(i+0.5, 0.5, token, ha='center', va='center')\n\n    # \u4e88\u6e2c\u306e\u77e2\u5370\n    for i in range(len(tokens)-1):\n        ax.arrow(i+0.5, 1.2, 0.5, 0, head_width=0.1, \n                head_length=0.1, fc='red', ec='red')\n        ax.text(i+1, 1.5, \"\u2192\", ha='center', color='red')\n\n    ax.set_xlim(-0.5, len(tokens))\n    ax.set_ylim(-0.5, 2)\n    ax.set_title('Causal Language Modeling', fontsize=12)\n    ax.axis('off')\n\n    # 2. Masked LM\n    ax = axes[0, 1]\n    masked_tokens = [\"The\", \"[MASK]\", \"sat\", \"on\", \"[MASK]\", \"mat\"]\n\n    for i, token in enumerate(masked_tokens):\n        if token == \"[MASK]\":\n            color = 'lightcoral'\n        else:\n            color = 'lightblue'\n        rect = plt.Rectangle((i, 0), 1, 1, facecolor=color,\n                           edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(i+0.5, 0.5, token, ha='center', va='center',\n               fontsize=8 if token == \"[MASK]\" else 10)\n\n    # \u53cc\u65b9\u5411\u306e\u77e2\u5370\n    for mask_pos in [1, 4]:\n        for j in range(len(masked_tokens)):\n            if j != mask_pos:\n                ax.arrow(j+0.5, -0.3, \n                        (mask_pos-j)*0.8, 0, \n                        head_width=0.05, head_length=0.05,\n                        fc='blue', ec='blue', alpha=0.3)\n\n    ax.set_xlim(-0.5, len(masked_tokens))\n    ax.set_ylim(-0.5, 1.5)\n    ax.set_title('Masked Language Modeling', fontsize=12)\n    ax.axis('off')\n\n    # 3. Permutation LM\n    ax = axes[1, 0]\n    perm_order = [2, 0, 3, 1, 4]  # \u4f8b\n    perm_tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n\n    # \u5143\u306e\u9806\u5e8f\n    for i, token in enumerate(perm_tokens):\n        rect = plt.Rectangle((i, 1), 1, 0.5, facecolor='lightgreen',\n                           edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(i+0.5, 1.25, token, ha='center', va='center', fontsize=9)\n\n    # \u4e88\u6e2c\u9806\u5e8f\n    for pred_idx, orig_idx in enumerate(perm_order):\n        rect = plt.Rectangle((pred_idx, 0), 1, 0.5, facecolor='lightyellow',\n                           edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(pred_idx+0.5, 0.25, perm_tokens[orig_idx], \n               ha='center', va='center', fontsize=9)\n\n        # \u77e2\u5370\n        ax.arrow(orig_idx+0.5, 0.9, \n                (pred_idx-orig_idx)*0.9, -0.35,\n                head_width=0.05, head_length=0.05,\n                fc='purple', ec='purple', alpha=0.5)\n\n    ax.set_xlim(-0.5, len(perm_tokens))\n    ax.set_ylim(-0.2, 2)\n    ax.set_title('Permutation Language Modeling', fontsize=12)\n    ax.text(2.5, -0.1, 'Prediction Order', ha='center', fontsize=8)\n    ax.text(2.5, 1.7, 'Original Order', ha='center', fontsize=8)\n    ax.axis('off')\n\n    # 4. Denoising\n    ax = axes[1, 1]\n    corrupted = [\"The\", \"&lt;X&gt;\", \"on\", \"&lt;Y&gt;\"]\n    original = [\"The\", \"cat sat\", \"on\", \"the mat\"]\n\n    # \u7834\u640d\u7248\n    x_pos = 0\n    for token in corrupted:\n        width = 2 if token in [\"&lt;X&gt;\", \"&lt;Y&gt;\"] else 1\n        rect = plt.Rectangle((x_pos, 1), width, 0.5, \n                           facecolor='lightcoral' if \"&lt;\" in token else 'lightblue',\n                           edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(x_pos+width/2, 1.25, token, ha='center', va='center')\n        x_pos += width\n\n    # \u77e2\u5370\n    ax.arrow(3, 0.9, 0, -0.3, head_width=0.3, head_length=0.1,\n            fc='green', ec='green', linewidth=2)\n\n    # \u5fa9\u5143\u7248\n    x_pos = 0\n    for token in original:\n        width = len(token.split()) * 0.8\n        rect = plt.Rectangle((x_pos, 0), width, 0.5,\n                           facecolor='lightgreen', edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(x_pos+width/2, 0.25, token, ha='center', va='center',\n               fontsize=8)\n        x_pos += width\n\n    ax.set_xlim(-0.5, 6)\n    ax.set_ylim(-0.2, 2)\n    ax.set_title('Denoising Objective', fontsize=12)\n    ax.text(3, 0.65, 'Reconstruct', ha='center', fontsize=8, color='green')\n    ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class PretrainingDataset(Dataset):     \"\"\"\u4e8b\u524d\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\"\"\"</p> <pre><code>def __init__(self, texts: List[str], tokenizer, max_length: int = 512,\n             objective: str = \"clm\"):\n    self.texts = texts\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n    self.objective = objective\n\ndef __len__(self):\n    return len(self.texts)\n\ndef __getitem__(self, idx):\n    text = self.texts[idx]\n\n    # \u30c8\u30fc\u30af\u30f3\u5316\n    tokens = self.tokenizer.encode(text, max_length=self.max_length,\n                                 truncation=True)\n\n    if self.objective == \"clm\":\n        # Causal LM: input\u3068label\u306f\u540c\u3058\uff08\u30b7\u30d5\u30c8\u306f\u5f8c\u3067\uff09\n        return {\n            \"input_ids\": torch.tensor(tokens),\n            \"labels\": torch.tensor(tokens)\n        }\n\n    elif self.objective == \"mlm\":\n        # Masked LM\n        masked_tokens, labels = self._mask_tokens(tokens)\n        return {\n            \"input_ids\": torch.tensor(masked_tokens),\n            \"labels\": torch.tensor(labels)\n        }\n\n    elif self.objective == \"denoising\":\n        # Denoising\n        corrupted, original = self._corrupt_tokens(tokens)\n        return {\n            \"input_ids\": torch.tensor(corrupted),\n            \"labels\": torch.tensor(original)\n        }\n\ndef _mask_tokens(self, tokens: List[int], mask_prob: float = 0.15):\n    \"\"\"\u30c8\u30fc\u30af\u30f3\u3092\u30de\u30b9\u30af\"\"\"\n    masked_tokens = tokens.copy()\n    labels = [-100] * len(tokens)  # -100 = ignore in loss\n\n    # \u30de\u30b9\u30af\u4f4d\u7f6e\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\n    mask_indices = np.random.binomial(1, mask_prob, size=len(tokens)).astype(bool)\n\n    for i, mask in enumerate(mask_indices):\n        if mask:\n            labels[i] = tokens[i]\n\n            # 80%: [MASK]\u30c8\u30fc\u30af\u30f3\u306b\u7f6e\u63db\n            if random.random() &lt; 0.8:\n                masked_tokens[i] = self.tokenizer.mask_token_id\n            # 10%: \u30e9\u30f3\u30c0\u30e0\u306a\u30c8\u30fc\u30af\u30f3\u306b\u7f6e\u63db\n            elif random.random() &lt; 0.5:\n                masked_tokens[i] = random.randint(0, len(self.tokenizer) - 1)\n            # 10%: \u305d\u306e\u307e\u307e\n\n    return masked_tokens, labels\n\ndef _corrupt_tokens(self, tokens: List[int]):\n    \"\"\"\u30c8\u30fc\u30af\u30f3\u3092\u7834\u640d\u3055\u305b\u308b\uff08T5\u30b9\u30bf\u30a4\u30eb\uff09\"\"\"\n    # \u7c21\u7565\u7248\uff1a\u9023\u7d9a\u3059\u308b\u30c8\u30fc\u30af\u30f3\u3092\u30de\u30b9\u30af\n    corrupted = []\n    original = tokens.copy()\n\n    i = 0\n    while i &lt; len(tokens):\n        if random.random() &lt; 0.15:  # 15%\u306e\u78ba\u7387\u3067\u30b9\u30d1\u30f3\u3092\u30de\u30b9\u30af\n            span_length = np.random.poisson(3)  # \u5e73\u5747\u95773\n            span_length = min(span_length, len(tokens) - i)\n\n            # \u30b9\u30d1\u30f3\u3092\u7279\u6b8a\u30c8\u30fc\u30af\u30f3\u3067\u7f6e\u63db\n            corrupted.append(self.tokenizer.mask_token_id)\n            i += span_length\n        else:\n            corrupted.append(tokens[i])\n            i += 1\n\n    return corrupted, original\n</code></pre> <p>class PretrainingPipeline:     \"\"\"\u4e8b\u524d\u5b66\u7fd2\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\"\"\"</p> <pre><code>def __init__(self, model: nn.Module, config: Dict[str, Any]):\n    self.model = model\n    self.config = config\n    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    self.model.to(self.device)\n\n    # \u6700\u9069\u5316\n    self.optimizer = self._create_optimizer()\n    self.scheduler = self._create_scheduler()\n\n    # \u7d71\u8a08\n    self.stats = defaultdict(list)\n\ndef _create_optimizer(self):\n    \"\"\"\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u4f5c\u6210\"\"\"\n    # Weight decay\u3092\u9069\u7528\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u3057\u306a\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5206\u96e2\n    decay_params = []\n    no_decay_params = []\n\n    for name, param in self.model.named_parameters():\n        if \"bias\" in name or \"norm\" in name or \"embedding\" in name:\n            no_decay_params.append(param)\n        else:\n            decay_params.append(param)\n\n    optimizer_grouped_parameters = [\n        {\"params\": decay_params, \"weight_decay\": self.config[\"weight_decay\"]},\n        {\"params\": no_decay_params, \"weight_decay\": 0.0}\n    ]\n\n    return torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=self.config[\"learning_rate\"],\n        betas=(0.9, 0.999),\n        eps=1e-8\n    )\n\ndef _create_scheduler(self):\n    \"\"\"\u5b66\u7fd2\u7387\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u30fc\u306e\u4f5c\u6210\"\"\"\n    # Linear warmup + Cosine decay\n    def lr_lambda(step):\n        if step &lt; self.config[\"warmup_steps\"]:\n            return step / self.config[\"warmup_steps\"]\n        else:\n            progress = (step - self.config[\"warmup_steps\"]) / \\\n                      (self.config[\"total_steps\"] - self.config[\"warmup_steps\"])\n            return 0.5 * (1 + np.cos(np.pi * progress))\n\n    return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n\ndef train(self, train_dataloader: DataLoader, \n          val_dataloader: Optional[DataLoader] = None):\n    \"\"\"\u4e8b\u524d\u5b66\u7fd2\u306e\u5b9f\u884c\"\"\"\n    print(\"=== \u4e8b\u524d\u5b66\u7fd2\u958b\u59cb ===\\n\")\n\n    global_step = 0\n    best_val_loss = float('inf')\n\n    for epoch in range(self.config[\"num_epochs\"]):\n        print(f\"\\nEpoch {epoch + 1}/{self.config['num_epochs']}\")\n\n        # \u8a13\u7df4\n        train_loss = self._train_epoch(train_dataloader, global_step)\n        self.stats[\"train_loss\"].append(train_loss)\n\n        # \u691c\u8a3c\n        if val_dataloader is not None:\n            val_loss = self._validate(val_dataloader)\n            self.stats[\"val_loss\"].append(val_loss)\n\n            print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n            # \u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u4fdd\u5b58\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                self._save_checkpoint(epoch, val_loss)\n        else:\n            print(f\"Train Loss: {train_loss:.4f}\")\n\n        global_step += len(train_dataloader)\n\n    # \u5b66\u7fd2\u66f2\u7dda\u306e\u30d7\u30ed\u30c3\u30c8\n    self._plot_training_curves()\n\ndef _train_epoch(self, dataloader: DataLoader, global_step: int):\n    \"\"\"1\u30a8\u30dd\u30c3\u30af\u306e\u8a13\u7df4\"\"\"\n    self.model.train()\n    total_loss = 0\n\n    progress_bar = tqdm(dataloader, desc=\"Training\")\n    for batch in progress_bar:\n        # \u30c7\u30d0\u30a4\u30b9\u306b\u8ee2\u9001\n        input_ids = batch[\"input_ids\"].to(self.device)\n        labels = batch[\"labels\"].to(self.device)\n\n        # \u9806\u4f1d\u64ad\n        outputs = self.model(input_ids, labels=labels)\n        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\n        # \u9006\u4f1d\u64ad\n        loss.backward()\n\n        # \u52fe\u914d\u30af\u30ea\u30c3\u30d4\u30f3\u30b0\n        torch.nn.utils.clip_grad_norm_(\n            self.model.parameters(), \n            self.config[\"max_grad_norm\"]\n        )\n\n        # \u6700\u9069\u5316\u30b9\u30c6\u30c3\u30d7\n        self.optimizer.step()\n        self.scheduler.step()\n        self.optimizer.zero_grad()\n\n        # \u7d71\u8a08\u66f4\u65b0\n        total_loss += loss.item()\n\n        # \u9032\u6357\u8868\u793a\n        progress_bar.set_postfix({\n            \"loss\": loss.item(),\n            \"lr\": self.optimizer.param_groups[0][\"lr\"]\n        })\n\n        # \u5b9a\u671f\u7684\u306a\u30ed\u30b0\n        if (global_step + 1) % self.config[\"log_interval\"] == 0:\n            self.stats[\"step_loss\"].append(loss.item())\n            self.stats[\"learning_rate\"].append(\n                self.optimizer.param_groups[0][\"lr\"]\n            )\n\n        global_step += 1\n\n    return total_loss / len(dataloader)\n\ndef _validate(self, dataloader: DataLoader):\n    \"\"\"\u691c\u8a3c\"\"\"\n    self.model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validation\"):\n            input_ids = batch[\"input_ids\"].to(self.device)\n            labels = batch[\"labels\"].to(self.device)\n\n            outputs = self.model(input_ids, labels=labels)\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\n            total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\ndef _save_checkpoint(self, epoch: int, val_loss: float):\n    \"\"\"\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\"\"\"\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": self.model.state_dict(),\n        \"optimizer_state_dict\": self.optimizer.state_dict(),\n        \"scheduler_state_dict\": self.scheduler.state_dict(),\n        \"val_loss\": val_loss,\n        \"config\": self.config\n    }\n\n    torch.save(checkpoint, f\"checkpoint_epoch_{epoch}.pt\")\n    print(f\"Checkpoint saved: checkpoint_epoch_{epoch}.pt\")\n\ndef _plot_training_curves(self):\n    \"\"\"\u5b66\u7fd2\u66f2\u7dda\u306e\u30d7\u30ed\u30c3\u30c8\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # \u640d\u5931\n    epochs = range(1, len(self.stats[\"train_loss\"]) + 1)\n    ax1.plot(epochs, self.stats[\"train_loss\"], 'b-', label='Train Loss')\n    if self.stats[\"val_loss\"]:\n        ax1.plot(epochs, self.stats[\"val_loss\"], 'r-', label='Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Validation Loss')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # \u5b66\u7fd2\u7387\n    if self.stats[\"learning_rate\"]:\n        steps = range(len(self.stats[\"learning_rate\"]))\n        ax2.plot(steps, self.stats[\"learning_rate\"], 'g-')\n        ax2.set_xlabel('Step')\n        ax2.set_ylabel('Learning Rate')\n        ax2.set_title('Learning Rate Schedule')\n        ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part5/pretraining-finetuning/#182","title":"18.2 \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565","text":"<p>class FineTuningStrategies:     \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565\"\"\"</p> <pre><code>def explain_strategies(self):\n    \"\"\"\u4e3b\u8981\u306a\u6226\u7565\u3092\u8aac\u660e\"\"\"\n    print(\"=== \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565 ===\\n\")\n\n    strategies = {\n        \"Full Fine-tuning\": {\n            \"\u8aac\u660e\": \"\u5168\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\",\n            \"\u5229\u70b9\": \"\u6700\u5927\u306e\u8868\u73fe\u529b\",\n            \"\u6b20\u70b9\": \"\u8a08\u7b97\u30b3\u30b9\u30c8\u304c\u9ad8\u3044\u3001\u904e\u5b66\u7fd2\u306e\u30ea\u30b9\u30af\",\n            \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\": \"100%\"\n        },\n\n        \"LoRA (Low-Rank Adaptation)\": {\n            \"\u8aac\u660e\": \"\u4f4e\u30e9\u30f3\u30af\u884c\u5217\u3067\u306e\u9069\u5fdc\",\n            \"\u5229\u70b9\": \"\u30d1\u30e9\u30e1\u30fc\u30bf\u52b9\u7387\u7684\u3001\u8907\u6570\u30bf\u30b9\u30af\u306e\u540c\u6642\u5bfe\u5fdc\",\n            \"\u6b20\u70b9\": \"\u82e5\u5e72\u306e\u6027\u80fd\u4f4e\u4e0b\u306e\u53ef\u80fd\u6027\",\n            \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\": \"~0.1%\"\n        },\n\n        \"Prefix Tuning\": {\n            \"\u8aac\u660e\": \"\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u30d9\u30af\u30c8\u30eb\u306e\u5b66\u7fd2\",\n            \"\u5229\u70b9\": \"\u5143\u306e\u30e2\u30c7\u30eb\u3092\u5909\u66f4\u3057\u306a\u3044\",\n            \"\u6b20\u70b9\": \"\u9577\u3044\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u304c\u5fc5\u8981\",\n            \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\": \"~0.1%\"\n        },\n\n        \"Adapter Layers\": {\n            \"\u8aac\u660e\": \"\u5c0f\u3055\u306a\u30a2\u30c0\u30d7\u30bf\u30fc\u5c64\u3092\u633f\u5165\",\n            \"\u5229\u70b9\": \"\u30e2\u30b8\u30e5\u30e9\u30fc\u3001\u30bf\u30b9\u30af\u7279\u5316\",\n            \"\u6b20\u70b9\": \"\u63a8\u8ad6\u6642\u306e\u30aa\u30fc\u30d0\u30fc\u30d8\u30c3\u30c9\",\n            \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\": \"~1-5%\"\n        },\n\n        \"BitFit\": {\n            \"\u8aac\u660e\": \"\u30d0\u30a4\u30a2\u30b9\u9805\u306e\u307f\u3092\u66f4\u65b0\",\n            \"\u5229\u70b9\": \"\u6975\u3081\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u52b9\u7387\u7684\",\n            \"\u6b20\u70b9\": \"\u8868\u73fe\u529b\u304c\u9650\u5b9a\u7684\",\n            \"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\": \"~0.05%\"\n        }\n    }\n\n    # \u6bd4\u8f03\u8868\u793a\n    self._visualize_strategies(strategies)\n\n    # \u5b9f\u88c5\u4f8b\n    self._implement_lora()\n\ndef _visualize_strategies(self, strategies: Dict[str, Dict[str, str]]):\n    \"\"\"\u6226\u7565\u3092\u53ef\u8996\u5316\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 8))\n\n    # \u30c7\u30fc\u30bf\u6e96\u5099\n    names = list(strategies.keys())\n    param_percentages = []\n\n    for name, info in strategies.items():\n        # \u30d1\u30fc\u30bb\u30f3\u30c6\u30fc\u30b8\u3092\u62bd\u51fa\n        param_str = info[\"\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\"]\n        if \"~\" in param_str:\n            param_str = param_str.replace(\"~\", \"\")\n        if \"-\" in param_str:\n            # \u7bc4\u56f2\u306e\u5834\u5408\u306f\u5e73\u5747\u3092\u53d6\u308b\n            parts = param_str.replace(\"%\", \"\").split(\"-\")\n            param_percentages.append(np.mean([float(p) for p in parts]))\n        else:\n            param_percentages.append(float(param_str.replace(\"%\", \"\")))\n\n    # \u68d2\u30b0\u30e9\u30d5\n    colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n    bars = ax.bar(names, param_percentages, color=colors)\n\n    # \u30ed\u30b0\u30b9\u30b1\u30fc\u30eb\n    ax.set_yscale('log')\n    ax.set_ylabel('Trainable Parameters (%)', fontsize=12)\n    ax.set_title('Parameter Efficiency of Fine-tuning Strategies', fontsize=14)\n\n    # \u5024\u3092\u8868\u793a\n    for bar, pct in zip(bars, param_percentages):\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n               f'{pct:.2f}%', ha='center', va='bottom')\n\n    # \u56de\u8ee2\n    plt.xticks(rotation=45, ha='right')\n    plt.grid(True, alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.show()\n\ndef _implement_lora(self):\n    \"\"\"LoRA\u306e\u5b9f\u88c5\u4f8b\"\"\"\n    print(\"\\n=== LoRA\u5b9f\u88c5\u4f8b ===\\n\")\n\n    class LoRALayer(nn.Module):\n        \"\"\"LoRA\u30ec\u30a4\u30e4\u30fc\"\"\"\n\n        def __init__(self, in_features: int, out_features: int, \n                    rank: int = 16, alpha: float = 16.0):\n            super().__init__()\n            self.rank = rank\n            self.alpha = alpha\n            self.scaling = alpha / rank\n\n            # \u4f4e\u30e9\u30f3\u30af\u884c\u5217\n            self.lora_A = nn.Parameter(torch.randn(in_features, rank))\n            self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n\n            # \u521d\u671f\u5316\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n\n        def forward(self, x: torch.Tensor, original_weight: torch.Tensor):\n            # \u5143\u306e\u91cd\u307f + LoRA\n            lora_weight = (self.lora_A @ self.lora_B) * self.scaling\n            return F.linear(x, original_weight + lora_weight.T)\n\n    print(\"LoRA\u306e\u7279\u5fb4:\")\n    print(\"\u2022 W = W\u2080 + BA (\u4f4e\u30e9\u30f3\u30af\u5206\u89e3)\")\n    print(\"\u2022 rank &lt;&lt; min(in_features, out_features)\")\n    print(\"\u2022 \u63a8\u8ad6\u6642\u306b\u91cd\u307f\u3092\u30de\u30fc\u30b8\u53ef\u80fd\")\n    print(\"\u2022 \u8907\u6570\u306eLoRA\u3092\u5207\u308a\u66ff\u3048\u53ef\u80fd\")\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u524a\u6e1b\u306e\u8a08\u7b97\n    in_features, out_features = 768, 768\n    rank = 16\n\n    original_params = in_features * out_features\n    lora_params = (in_features * rank) + (rank * out_features)\n    reduction = 100 * (1 - lora_params / original_params)\n\n    print(f\"\\n\u4f8b: {in_features}\u00d7{out_features}\u306e\u884c\u5217\")\n    print(f\"  \u5143\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {original_params:,}\")\n    print(f\"  LoRA\u30d1\u30e9\u30e1\u30fc\u30bf\u6570: {lora_params:,}\")\n    print(f\"  \u524a\u6e1b\u7387: {reduction:.1f}%\")\n</code></pre> <p>class TaskSpecificFineTuning:     \"\"\"\u30bf\u30b9\u30af\u7279\u5316\u306e\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\"\"\"</p> <pre><code>def __init__(self, base_model: nn.Module):\n    self.base_model = base_model\n\ndef create_classification_head(self, num_classes: int):\n    \"\"\"\u5206\u985e\u30d8\u30c3\u30c9\u306e\u4f5c\u6210\"\"\"\n\n    class ClassificationModel(nn.Module):\n        def __init__(self, base_model, num_classes, hidden_size=768):\n            super().__init__()\n            self.base_model = base_model\n            self.classifier = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Linear(hidden_size, hidden_size),\n                nn.Tanh(),\n                nn.Dropout(0.1),\n                nn.Linear(hidden_size, num_classes)\n            )\n\n        def forward(self, input_ids, attention_mask=None):\n            # \u30d9\u30fc\u30b9\u30e2\u30c7\u30eb\u306e\u51fa\u529b\n            outputs = self.base_model(input_ids, attention_mask=attention_mask)\n\n            # [CLS]\u30c8\u30fc\u30af\u30f3\u307e\u305f\u306f\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\n            if hasattr(outputs, 'last_hidden_state'):\n                hidden_states = outputs.last_hidden_state\n            else:\n                hidden_states = outputs\n\n            # \u30d7\u30fc\u30ea\u30f3\u30b0\uff08\u6700\u521d\u306e\u30c8\u30fc\u30af\u30f3\uff09\n            pooled_output = hidden_states[:, 0]\n\n            # \u5206\u985e\n            logits = self.classifier(pooled_output)\n\n            return logits\n\n    return ClassificationModel(self.base_model, num_classes)\n\ndef create_generation_head(self, max_length: int = 512):\n    \"\"\"\u751f\u6210\u30d8\u30c3\u30c9\u306e\u4f5c\u6210\"\"\"\n\n    class GenerationModel(nn.Module):\n        def __init__(self, base_model, max_length):\n            super().__init__()\n            self.base_model = base_model\n            self.max_length = max_length\n\n        def forward(self, input_ids, labels=None):\n            outputs = self.base_model(input_ids, labels=labels)\n            return outputs\n\n        @torch.no_grad()\n        def generate(self, input_ids, max_new_tokens=50, **kwargs):\n            return self.base_model.generate(\n                input_ids, \n                max_new_tokens=max_new_tokens,\n                **kwargs\n            )\n\n    return GenerationModel(self.base_model, max_length)\n</code></pre> <p>class FineTuningDataCollator:     \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7528\u30c7\u30fc\u30bf\u30b3\u30ec\u30fc\u30bf\u30fc\"\"\"</p> <pre><code>def __init__(self, tokenizer, max_length: int = 512):\n    self.tokenizer = tokenizer\n    self.max_length = max_length\n\ndef __call__(self, examples: List[Dict[str, Any]]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\u30d0\u30c3\u30c1\u306e\u4f5c\u6210\"\"\"\n    # \u30bf\u30b9\u30af\u306b\u5fdc\u3058\u305f\u51e6\u7406\n    if \"labels\" in examples[0]:\n        # \u5206\u985e\u30bf\u30b9\u30af\n        return self._collate_classification(examples)\n    elif \"target_text\" in examples[0]:\n        # \u751f\u6210\u30bf\u30b9\u30af\n        return self._collate_generation(examples)\n    else:\n        # \u30c7\u30d5\u30a9\u30eb\u30c8\n        return self._collate_default(examples)\n\ndef _collate_classification(self, examples):\n    \"\"\"\u5206\u985e\u30bf\u30b9\u30af\u7528\"\"\"\n    texts = [ex[\"text\"] for ex in examples]\n    labels = [ex[\"labels\"] for ex in examples]\n\n    # \u30c8\u30fc\u30af\u30f3\u5316\n    encoding = self.tokenizer(\n        texts,\n        truncation=True,\n        padding=True,\n        max_length=self.max_length,\n        return_tensors=\"pt\"\n    )\n\n    encoding[\"labels\"] = torch.tensor(labels)\n    return encoding\n\ndef _collate_generation(self, examples):\n    \"\"\"\u751f\u6210\u30bf\u30b9\u30af\u7528\"\"\"\n    inputs = [ex[\"input_text\"] for ex in examples]\n    targets = [ex[\"target_text\"] for ex in examples]\n\n    # \u5165\u529b\u3068\u51fa\u529b\u3092\u7d50\u5408\n    model_inputs = self.tokenizer(\n        inputs,\n        truncation=True,\n        padding=True,\n        max_length=self.max_length,\n        return_tensors=\"pt\"\n    )\n\n    # \u30e9\u30d9\u30eb\u306e\u6e96\u5099\n    with self.tokenizer.as_target_tokenizer():\n        labels = self.tokenizer(\n            targets,\n            truncation=True,\n            padding=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ndef _collate_default(self, examples):\n    \"\"\"\u30c7\u30d5\u30a9\u30eb\u30c8\u51e6\u7406\"\"\"\n    return {key: torch.stack([ex[key] for ex in examples]) \n            for key in examples[0].keys()}\n</code></pre>"},{"location":"part5/pretraining-finetuning/#183","title":"18.3 \u5b9f\u8df5\u7684\u306a\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0","text":"<p>class PracticalFineTuning:     \"\"\"\u5b9f\u8df5\u7684\u306a\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u4f8b\"\"\"</p> <pre><code>def __init__(self):\n    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef sentiment_analysis_example(self):\n    \"\"\"\u611f\u60c5\u5206\u6790\u306e\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u4f8b\"\"\"\n    print(\"=== \u611f\u60c5\u5206\u6790\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0 ===\\n\")\n\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f8b\n    train_data = [\n        {\"text\": \"This movie was fantastic!\", \"label\": 1},  # Positive\n        {\"text\": \"Terrible experience, would not recommend.\", \"label\": 0},  # Negative\n        {\"text\": \"The food was amazing and the service excellent.\", \"label\": 1},\n        {\"text\": \"Waste of time and money.\", \"label\": 0},\n        {\"text\": \"Best purchase I've ever made!\", \"label\": 1},\n        {\"text\": \"Completely disappointed with the quality.\", \"label\": 0}\n    ]\n\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30af\u30e9\u30b9\n    class SentimentDataset(Dataset):\n        def __init__(self, data, tokenizer, max_length=128):\n            self.data = data\n            self.tokenizer = tokenizer\n            self.max_length = max_length\n\n        def __len__(self):\n            return len(self.data)\n\n        def __getitem__(self, idx):\n            item = self.data[idx]\n            encoding = self.tokenizer(\n                item[\"text\"],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n\n            return {\n                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n                \"labels\": torch.tensor(item[\"label\"])\n            }\n\n    print(\"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f8b:\")\n    for item in train_data[:3]:\n        print(f\"  Text: '{item['text']}'\")\n        print(f\"  Label: {item['label']} ({'Positive' if item['label'] == 1 else 'Negative'})\\n\")\n\n    # \u5b66\u7fd2\u8a2d\u5b9a\n    print(\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u8a2d\u5b9a:\")\n    print(\"  Learning Rate: 2e-5\")\n    print(\"  Batch Size: 16\")\n    print(\"  Epochs: 3\")\n    print(\"  Warmup Steps: 100\")\n\n    # \u7d50\u679c\u306e\u53ef\u8996\u5316\n    self._visualize_finetuning_results()\n\ndef _visualize_finetuning_results(self):\n    \"\"\"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u7d50\u679c\u306e\u53ef\u8996\u5316\"\"\"\n    # \u30c0\u30df\u30fc\u306e\u5b66\u7fd2\u66f2\u7dda\n    epochs = np.arange(1, 4)\n    train_loss = [0.693, 0.245, 0.089]\n    val_loss = [0.672, 0.298, 0.156]\n    train_acc = [0.52, 0.91, 0.98]\n    val_acc = [0.55, 0.87, 0.92]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    # \u640d\u5931\n    ax1.plot(epochs, train_loss, 'b-o', label='Train Loss')\n    ax1.plot(epochs, val_loss, 'r-o', label='Val Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training and Validation Loss')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # \u7cbe\u5ea6\n    ax2.plot(epochs, train_acc, 'b-o', label='Train Accuracy')\n    ax2.plot(epochs, val_acc, 'r-o', label='Val Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Training and Validation Accuracy')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    ax2.set_ylim(0, 1)\n\n    plt.tight_layout()\n    plt.show()\n\ndef instruction_tuning_example(self):\n    \"\"\"Instruction Tuning\u306e\u4f8b\"\"\"\n    print(\"\\n=== Instruction Tuning ===\\n\")\n\n    instruction_examples = [\n        {\n            \"instruction\": \"Translate the following English text to French:\",\n            \"input\": \"Hello, how are you today?\",\n            \"output\": \"Bonjour, comment allez-vous aujourd'hui?\"\n        },\n        {\n            \"instruction\": \"Summarize the following text in one sentence:\",\n            \"input\": \"The quick brown fox jumps over the lazy dog. This pangram sentence contains every letter of the English alphabet at least once.\",\n            \"output\": \"This is a pangram that includes all 26 letters of the English alphabet.\"\n        },\n        {\n            \"instruction\": \"Write a Python function that calculates the factorial of a number:\",\n            \"input\": \"5\",\n            \"output\": \"def factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    return n * factorial(n - 1)\\n\\nresult = factorial(5)  # Returns 120\"\n        }\n    ]\n\n    print(\"Instruction Tuning\u5f62\u5f0f:\")\n    for i, example in enumerate(instruction_examples[:2]):\n        print(f\"\\n\u4f8b {i+1}:\")\n        print(f\"Instruction: {example['instruction']}\")\n        print(f\"Input: {example['input']}\")\n        print(f\"Output: {example['output']}\")\n\n    # \u30d7\u30ed\u30f3\u30d7\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\n    print(\"\\n\\n\u30d7\u30ed\u30f3\u30d7\u30c8\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8:\")\n    template = \"\"\"### Instruction:\n</code></pre> <p>{instruction}</p>"},{"location":"part5/pretraining-finetuning/#input","title":"Input:","text":"<p>{input}</p>"},{"location":"part5/pretraining-finetuning/#response","title":"Response:","text":"<p>{output}\"\"\"</p> <pre><code>    print(template)\n\n    print(\"\\n\u52b9\u679c:\")\n    print(\"\u2713 \u660e\u78ba\u306a\u6307\u793a\u306b\u5f93\u3046\u80fd\u529b\u306e\u5411\u4e0a\")\n    print(\"\u2713 \u30bc\u30ed\u30b7\u30e7\u30c3\u30c8\u6c4e\u5316\u306e\u6539\u5584\")\n    print(\"\u2713 \u3088\u308a\u81ea\u7136\u306a\u5bfe\u8a71\u304c\u53ef\u80fd\")\n</code></pre>"},{"location":"part5/pretraining-finetuning/#184","title":"18.4 \u52b9\u7387\u7684\u306a\u5b66\u7fd2\u624b\u6cd5","text":"<p>class EfficientTrainingMethods:     \"\"\"\u52b9\u7387\u7684\u306a\u5b66\u7fd2\u624b\u6cd5\"\"\"</p> <pre><code>def demonstrate_mixed_precision(self):\n    \"\"\"Mixed Precision Training\u306e\u30c7\u30e2\"\"\"\n    print(\"=== Mixed Precision Training ===\\n\")\n\n    print(\"\u901a\u5e38\u306e\u5b66\u7fd2 (FP32):\")\n    print(\"  \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf: 100%\")\n    print(\"  \u8a08\u7b97\u901f\u5ea6: 1.0x\")\n    print(\"  \u6570\u5024\u7cbe\u5ea6: \u9ad8\u3044\\n\")\n\n    print(\"Mixed Precision (FP16 + FP32):\")\n    print(\"  \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf: ~50%\")\n    print(\"  \u8a08\u7b97\u901f\u5ea6: 2-3x\")\n    print(\"  \u6570\u5024\u7cbe\u5ea6: \u52d5\u7684\u30ed\u30b9\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3067\u7dad\u6301\\n\")\n\n    # \u5b9f\u88c5\u4f8b\n    print(\"PyTorch\u5b9f\u88c5\u4f8b:\")\n    print(\"\"\"\n</code></pre> <p>from torch.cuda.amp import autocast, GradScaler</p> <p>scaler = GradScaler()</p> <p>for batch in dataloader:     optimizer.zero_grad()</p> <pre><code># \u81ea\u52d5\u6df7\u5408\u7cbe\u5ea6\nwith autocast():\n    outputs = model(batch['input_ids'])\n    loss = criterion(outputs, batch['labels'])\n\n# \u30b9\u30b1\u30fc\u30eb\u3055\u308c\u305f\u9006\u4f1d\u64ad\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre> <p>\"\"\")</p> <pre><code>def demonstrate_gradient_accumulation(self):\n    \"\"\"\u52fe\u914d\u7d2f\u7a4d\u306e\u30c7\u30e2\"\"\"\n    print(\"\\n=== Gradient Accumulation ===\\n\")\n\n    print(\"\u52b9\u679c\u7684\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u5897\u52a0:\")\n    print(\"  \u5b9f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba: 8\")\n    print(\"  \u7d2f\u7a4d\u30b9\u30c6\u30c3\u30d7: 4\")\n    print(\"  \u52b9\u679c\u7684\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba: 32\\n\")\n\n    # \u30e1\u30e2\u30ea\u4f7f\u7528\u91cf\u306e\u6bd4\u8f03\n    batch_sizes = [8, 16, 32, 64, 128]\n    memory_usage = [2, 4, 8, 16, 32]  # GB\n    effective_batch_with_accumulation = [32, 64, 128, 256, 512]\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    x = np.arange(len(batch_sizes))\n    width = 0.35\n\n    bars1 = ax.bar(x - width/2, memory_usage, width, \n                   label='Direct (OOM risk)', color='red', alpha=0.7)\n    bars2 = ax.bar(x + width/2, [2] * len(batch_sizes), width,\n                   label='With Gradient Accumulation', color='green', alpha=0.7)\n\n    # \u52b9\u679c\u7684\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u8868\u793a\n    for i, (bar, eff_batch) in enumerate(zip(bars2, effective_batch_with_accumulation)):\n        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n               f'Eff: {eff_batch}', ha='center', va='bottom', fontsize=8)\n\n    ax.set_xlabel('Actual Batch Size')\n    ax.set_ylabel('Memory Usage (GB)')\n    ax.set_title('Memory Usage: Direct vs Gradient Accumulation')\n    ax.set_xticks(x)\n    ax.set_xticklabels(batch_sizes)\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n\n    # GPU\u30e1\u30e2\u30ea\u5236\u9650\u30e9\u30a4\u30f3\n    ax.axhline(y=16, color='orange', linestyle='--', \n              label='GPU Memory Limit (16GB)')\n\n    plt.tight_layout()\n    plt.show()\n\ndef demonstrate_data_parallelism(self):\n    \"\"\"\u30c7\u30fc\u30bf\u4e26\u5217\u306e\u30c7\u30e2\"\"\"\n    print(\"\\n=== Data Parallelism ===\\n\")\n\n    strategies = {\n        \"Single GPU\": {\n            \"GPUs\": 1,\n            \"Batch/GPU\": 8,\n            \"Total Batch\": 8,\n            \"Speed\": \"1x\"\n        },\n        \"Data Parallel\": {\n            \"GPUs\": 4,\n            \"Batch/GPU\": 8,\n            \"Total Batch\": 32,\n            \"Speed\": \"~3.8x\"\n        },\n        \"Distributed Data Parallel\": {\n            \"GPUs\": 4,\n            \"Batch/GPU\": 8,\n            \"Total Batch\": 32,\n            \"Speed\": \"~3.95x\"\n        },\n        \"FSDP (Fully Sharded)\": {\n            \"GPUs\": 4,\n            \"Batch/GPU\": 16,\n            \"Total Batch\": 64,\n            \"Speed\": \"~3.9x\"\n        }\n    }\n\n    print(\"\u4e26\u5217\u5316\u6226\u7565\u306e\u6bd4\u8f03:\\n\")\n    for name, details in strategies.items():\n        print(f\"{name}:\")\n        for key, value in details.items():\n            print(f\"  {key}: {value}\")\n        print()\n</code></pre>"},{"location":"part5/pretraining-finetuning/#_4","title":"\u5b9f\u884c\u3068\u30c7\u30e2","text":"<p>def run_pretraining_finetuning_demo():     \"\"\"\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u30c7\u30e2\"\"\"     print(\"=\" * 70)     print(\"\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u8a73\u89e3\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># 1. \u4e8b\u524d\u5b66\u7fd2\u76ee\u7684\u95a2\u6570\nobjectives = PretrainingObjectives()\nobjectives.explain_objectives()\n\n# 2. \u4e8b\u524d\u5b66\u7fd2\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\uff08\u6982\u8981\u306e\u307f\uff09\nprint(\"\\n=== \u4e8b\u524d\u5b66\u7fd2\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u4f8b ===\")\nconfig = {\n    \"learning_rate\": 6e-4,\n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 10000,\n    \"total_steps\": 1000000,\n    \"num_epochs\": 1,\n    \"log_interval\": 100,\n    \"max_grad_norm\": 1.0\n}\n\nprint(\"\u5178\u578b\u7684\u306a\u8a2d\u5b9a:\")\nfor key, value in config.items():\n    print(f\"  {key}: {value}\")\n\n# 3. \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u6226\u7565\nprint(\"\\n\")\nstrategies = FineTuningStrategies()\nstrategies.explain_strategies()\n\n# 4. \u5b9f\u8df5\u4f8b\nprint(\"\\n\")\npractical = PracticalFineTuning()\npractical.sentiment_analysis_example()\npractical.instruction_tuning_example()\n\n# 5. \u52b9\u7387\u7684\u306a\u5b66\u7fd2\nprint(\"\\n\")\nefficient = EfficientTrainingMethods()\nefficient.demonstrate_mixed_precision()\nefficient.demonstrate_gradient_accumulation()\nefficient.demonstrate_data_parallelism()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081\")\nprint(\"=\" * 70)\nprint(\"\\n\u4e8b\u524d\u5b66\u7fd2\u3068\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u30dd\u30a4\u30f3\u30c8:\")\nprint(\"\u2022 \u4e8b\u524d\u5b66\u7fd2: \u5927\u898f\u6a21\u30c7\u30fc\u30bf\u3067\u6c4e\u7528\u7684\u306a\u8a00\u8a9e\u7406\u89e3\u3092\u7372\u5f97\")\nprint(\"\u2022 \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0: \u7279\u5b9a\u30bf\u30b9\u30af\u3078\u306e\u52b9\u7387\u7684\u306a\u9069\u5fdc\")\nprint(\"\u2022 \u30d1\u30e9\u30e1\u30fc\u30bf\u52b9\u7387\u7684\u624b\u6cd5: \u5c11\u306a\u3044\u30ea\u30bd\u30fc\u30b9\u3067\u9ad8\u6027\u80fd\u3092\u5b9f\u73fe\")\nprint(\"\u2022 \u6700\u9069\u5316\u6280\u8853: Mixed Precision\u3001\u52fe\u914d\u7d2f\u7a4d\u306a\u3069\u3067\u52b9\u7387\u5316\")\nprint(\"\\n\u3053\u308c\u3089\u306e\u6280\u8853\u306b\u3088\u308a\u3001\u9650\u3089\u308c\u305f\u30ea\u30bd\u30fc\u30b9\u3067\u3082\")\nprint(\"\u9ad8\u6027\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u958b\u767a\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3057\u305f\u3002\")\n</code></pre> <p>if name == \"main\":     run_pretraining_finetuning_demo()</p>"},{"location":"part5/tokenizer-details/","title":"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u8a73\u7d30","text":""},{"location":"part5/tokenizer-details/#_2","title":"\u306f\u3058\u3081\u306b\uff1a\u8a00\u8a9e\u306e\u539f\u5b50","text":"<p>\u30b3\u30f3\u30d1\u30a4\u30e9\u306e\u5b57\u53e5\u89e3\u6790\u5668\uff08\u30ec\u30ad\u30b5\u30fc\uff09\u3092\u4f5c\u3063\u305f\u3053\u3068\u3092\u601d\u3044\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u610f\u5473\u306e\u3042\u308b\u6700\u5c0f\u5358\u4f4d\uff08\u30c8\u30fc\u30af\u30f3\uff09\u306b\u5206\u5272\u3059\u308b\u4f5c\u696d\u3067\u3059\u3002<code>if (x &gt; 10) { return true; }</code> \u3092 <code>IF</code>, <code>LPAREN</code>, <code>IDENT(x)</code>, <code>GT</code>, <code>NUMBER(10)</code>, ... \u306b\u5206\u89e3\u3057\u307e\u3059\u3002</p> <p>\u81ea\u7136\u8a00\u8a9e\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u3082\u540c\u3058\u5f79\u5272\u3092\u679c\u305f\u3057\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u9055\u3063\u3066\u660e\u78ba\u306a\u898f\u5247\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u5358\u8a9e\uff1f\u6587\u5b57\uff1f\u305d\u308c\u3068\u3082\u4f55\u304b\u5225\u306e\u5358\u4f4d\uff1f\u3053\u306e\u7ae0\u3067\u306f\u3001\u73fe\u4ee3\u306e\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u304c\u3069\u306e\u3088\u3046\u306b\u8a00\u8a9e\u3092\u300c\u539f\u5b50\u300d\u306b\u5206\u89e3\u3057\u3001\u306a\u305c\u305d\u308c\u304c\u91cd\u8981\u306a\u306e\u304b\u3092\u63a2\u308a\u307e\u3059\u3002</p>"},{"location":"part5/tokenizer-details/#191","title":"19.1 \u30c8\u30fc\u30af\u30f3\u5316\u306e\u57fa\u790e","text":""},{"location":"part5/tokenizer-details/#_3","title":"\u306a\u305c\u30c8\u30fc\u30af\u30f3\u5316\u304c\u91cd\u8981\u304b","text":"<p>```python import numpy as np import matplotlib.pyplot as plt import seaborn as sns from typing import List, Dict, Tuple, Optional, Set from collections import Counter, defaultdict import regex as re  # Unicode\u30b5\u30dd\u30fc\u30c8\u304c\u512a\u308c\u3066\u3044\u308b import json from dataclasses import dataclass import heapq from tqdm import tqdm</p> <p>class TokenizationBasics:     \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u306e\u57fa\u790e\u6982\u5ff5\"\"\"</p> <pre><code>def explain_tokenization_challenges(self):\n    \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u306e\u8ab2\u984c\u3092\u8aac\u660e\"\"\"\n    print(\"=== \u30c8\u30fc\u30af\u30f3\u5316\u306e\u8ab2\u984c ===\\n\")\n\n    # \u69d8\u3005\u306a\u8a00\u8a9e\u3067\u306e\u4f8b\n    examples = {\n        \"\u82f1\u8a9e\": {\n            \"text\": \"I don't think it's working.\",\n            \"word_tokens\": [\"I\", \"don't\", \"think\", \"it's\", \"working\", \".\"],\n            \"char_tokens\": list(\"I don't think it's working.\"),\n            \"challenges\": \"\u7e2e\u7d04\u5f62\uff08don't, it's\uff09\u306e\u6271\u3044\"\n        },\n\n        \"\u65e5\u672c\u8a9e\": {\n            \"text\": \"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\",\n            \"word_tokens\": [\"\u79c1\", \"\u306f\", \"\u732b\", \"\u304c\", \"\u597d\u304d\", \"\u3067\u3059\", \"\u3002\"],\n            \"char_tokens\": list(\"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\"),\n            \"challenges\": \"\u5358\u8a9e\u5883\u754c\u304c\u4e0d\u660e\u78ba\"\n        },\n\n        \"\u30c9\u30a4\u30c4\u8a9e\": {\n            \"text\": \"Donaudampfschifffahrtsgesellschaft\",\n            \"word_tokens\": [\"Donaudampfschifffahrtsgesellschaft\"],\n            \"char_tokens\": list(\"Donaudampfschifffahrtsgesellschaft\"),\n            \"challenges\": \"\u8907\u5408\u8a9e\u304c\u975e\u5e38\u306b\u9577\u3044\"\n        },\n\n        \"\u4e2d\u56fd\u8a9e\": {\n            \"text\": \"\u6211\u559c\u6b22\u5403\u82f9\u679c\",\n            \"word_tokens\": [\"\u6211\", \"\u559c\u6b22\", \"\u5403\", \"\u82f9\u679c\"],\n            \"char_tokens\": list(\"\u6211\u559c\u6b22\u5403\u82f9\u679c\"),\n            \"challenges\": \"\u30b9\u30da\u30fc\u30b9\u304c\u306a\u3044\"\n        }\n    }\n\n    for lang, info in examples.items():\n        print(f\"{lang}:\")\n        print(f\"  \u30c6\u30ad\u30b9\u30c8: {info['text']}\")\n        print(f\"  \u5358\u8a9e\u30c8\u30fc\u30af\u30f3: {info['word_tokens'][:5]}...\")\n        print(f\"  \u6587\u5b57\u30c8\u30fc\u30af\u30f3: {info['char_tokens'][:10]}...\")\n        print(f\"  \u8ab2\u984c: {info['challenges']}\\n\")\n\n    # \u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\n    self._compare_tokenization_methods()\n\ndef _compare_tokenization_methods(self):\n    \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03\"\"\"\n    print(\"=== \u30c8\u30fc\u30af\u30f3\u5316\u624b\u6cd5\u306e\u6bd4\u8f03 ===\\n\")\n\n    methods = {\n        \"Word-level\": {\n            \"vocab_size\": \"50,000-200,000\",\n            \"OOV_handling\": \"Poor\",\n            \"morphology\": \"No\",\n            \"efficiency\": \"Low\"\n        },\n        \"Character-level\": {\n            \"vocab_size\": \"100-1,000\",\n            \"OOV_handling\": \"Perfect\",\n            \"morphology\": \"Implicit\",\n            \"efficiency\": \"Very Low\"\n        },\n        \"Subword (BPE/WordPiece)\": {\n            \"vocab_size\": \"10,000-100,000\",\n            \"OOV_handling\": \"Good\",\n            \"morphology\": \"Partial\",\n            \"efficiency\": \"High\"\n        },\n        \"SentencePiece\": {\n            \"vocab_size\": \"10,000-100,000\",\n            \"OOV_handling\": \"Good\",\n            \"morphology\": \"Partial\",\n            \"efficiency\": \"High\"\n        }\n    }\n\n    # \u8868\u5f62\u5f0f\u3067\u8868\u793a\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.axis('tight')\n    ax.axis('off')\n\n    # \u30d8\u30c3\u30c0\u30fc\n    headers = [\"Method\", \"Vocab Size\", \"OOV Handling\", \"Morphology\", \"Efficiency\"]\n    cell_data = []\n\n    for method, props in methods.items():\n        row = [method] + list(props.values())\n        cell_data.append(row)\n\n    # \u30c6\u30fc\u30d6\u30eb\u4f5c\u6210\n    table = ax.table(cellText=cell_data, colLabels=headers,\n                    cellLoc='center', loc='center',\n                    colWidths=[0.25, 0.2, 0.15, 0.15, 0.15])\n\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1, 2)\n\n    # \u30b9\u30bf\u30a4\u30ea\u30f3\u30b0\n    for i in range(len(headers)):\n        table[(0, i)].set_facecolor('#4CAF50')\n        table[(0, i)].set_text_props(weight='bold', color='white')\n\n    # \u8272\u5206\u3051\n    colors = ['#ffebee', '#e8f5e9', '#fff3e0', '#e3f2fd']\n    for i, color in enumerate(colors):\n        for j in range(len(headers)):\n            table[(i+1, j)].set_facecolor(color)\n\n    plt.title('Comparison of Tokenization Methods', fontsize=14, weight='bold', pad=20)\n    plt.show()\n</code></pre>"},{"location":"part5/tokenizer-details/#192-byte-pair-encoding-bpe","title":"19.2 Byte Pair Encoding (BPE)","text":"<p>class BytePairEncoding:     \"\"\"BPE\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    self.vocab = {}\n    self.merges = []\n\ndef train(self, texts: List[str], vocab_size: int = 1000):\n    \"\"\"BPE\u306e\u5b66\u7fd2\"\"\"\n    print(\"=== BPE\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9 ===\\n\")\n\n    # \u521d\u671f\u8a9e\u5f59\uff08\u6587\u5b57\u30ec\u30d9\u30eb\uff09\n    word_freqs = defaultdict(int)\n    for text in texts:\n        words = text.split()\n        for word in words:\n            word_freqs[' '.join(list(word) + ['&lt;/w&gt;'])] += 1\n\n    # \u521d\u671f\u8a9e\u5f59\u3092\u4f5c\u6210\n    self.vocab = {}\n    for word, freq in word_freqs.items():\n        for char in word.split():\n            if char not in self.vocab:\n                self.vocab[char] = len(self.vocab)\n\n    print(f\"\u521d\u671f\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n    print(f\"\u521d\u671f\u8a9e\u5f59\u4f8b: {list(self.vocab.keys())[:10]}\\n\")\n\n    # \u30de\u30fc\u30b8\u64cd\u4f5c\n    num_merges = vocab_size - len(self.vocab)\n\n    for i in tqdm(range(num_merges), desc=\"Learning merges\"):\n        # \u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\n        pair_freqs = self._get_pair_frequencies(word_freqs)\n\n        if not pair_freqs:\n            break\n\n        # \u6700\u983b\u51fa\u30da\u30a2\u3092\u9078\u629e\n        best_pair = max(pair_freqs, key=pair_freqs.get)\n        self.merges.append(best_pair)\n\n        # \u8a9e\u5f59\u3092\u66f4\u65b0\n        new_token = ''.join(best_pair)\n        self.vocab[new_token] = len(self.vocab)\n\n        # \u30b3\u30fc\u30d1\u30b9\u3092\u66f4\u65b0\n        word_freqs = self._merge_pair(word_freqs, best_pair)\n\n        # \u9032\u6357\u8868\u793a\n        if (i + 1) % 100 == 0:\n            print(f\"\u30de\u30fc\u30b8 {i+1}: {best_pair} \u2192 {new_token}\")\n\n    print(f\"\\n\u6700\u7d42\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n\n    # \u5b66\u7fd2\u7d50\u679c\u306e\u53ef\u8996\u5316\n    self._visualize_vocabulary()\n\ndef _get_pair_frequencies(self, word_freqs: Dict[str, int]) -&gt; Dict[Tuple[str, str], int]:\n    \"\"\"\u30da\u30a2\u306e\u983b\u5ea6\u3092\u8a08\u7b97\"\"\"\n    pair_freqs = defaultdict(int)\n\n    for word, freq in word_freqs.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pair_freqs[(symbols[i], symbols[i + 1])] += freq\n\n    return pair_freqs\n\ndef _merge_pair(self, word_freqs: Dict[str, int], \n                pair: Tuple[str, str]) -&gt; Dict[str, int]:\n    \"\"\"\u30da\u30a2\u3092\u30de\u30fc\u30b8\"\"\"\n    new_word_freqs = {}\n    bigram = ' '.join(pair)\n    replacement = ''.join(pair)\n\n    for word, freq in word_freqs.items():\n        new_word = word.replace(bigram, replacement)\n        new_word_freqs[new_word] = freq\n\n    return new_word_freqs\n\ndef encode(self, text: str) -&gt; List[int]:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\"\"\"\n    words = text.split()\n    tokens = []\n\n    for word in words:\n        # \u5358\u8a9e\u3092\u6587\u5b57\u306b\u5206\u5272\n        word_tokens = list(word) + ['&lt;/w&gt;']\n\n        # \u30de\u30fc\u30b8\u3092\u9069\u7528\n        for merge in self.merges:\n            i = 0\n            while i &lt; len(word_tokens) - 1:\n                if (word_tokens[i], word_tokens[i + 1]) == merge:\n                    word_tokens = word_tokens[:i] + [''.join(merge)] + word_tokens[i + 2:]\n                else:\n                    i += 1\n\n        # \u30c8\u30fc\u30af\u30f3ID\u306b\u5909\u63db\n        for token in word_tokens:\n            if token in self.vocab:\n                tokens.append(self.vocab[token])\n            else:\n                # Unknown token\n                tokens.append(self.vocab.get('&lt;unk&gt;', 0))\n\n    return tokens\n\ndef decode(self, token_ids: List[int]) -&gt; str:\n    \"\"\"\u30c8\u30fc\u30af\u30f3ID\u3092\u30c7\u30b3\u30fc\u30c9\"\"\"\n    # \u9006\u5f15\u304d\u8f9e\u66f8\n    id_to_token = {v: k for k, v in self.vocab.items()}\n\n    tokens = [id_to_token.get(id, '&lt;unk&gt;') for id in token_ids]\n    text = ' '.join(tokens).replace('&lt;/w&gt; ', ' ').replace('&lt;/w&gt;', '')\n\n    return text\n\ndef _visualize_vocabulary(self):\n    \"\"\"\u8a9e\u5f59\u306e\u53ef\u8996\u5316\"\"\"\n    # \u30c8\u30fc\u30af\u30f3\u9577\u306e\u5206\u5e03\n    token_lengths = [len(token.replace('&lt;/w&gt;', '')) for token in self.vocab.keys()]\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(token_lengths, bins=range(1, max(token_lengths) + 2), \n            alpha=0.7, edgecolor='black')\n    plt.xlabel('Token Length (characters)')\n    plt.ylabel('Count')\n    plt.title('Distribution of Token Lengths in BPE Vocabulary')\n    plt.grid(True, alpha=0.3)\n\n    # \u7d71\u8a08\u60c5\u5831\n    avg_length = np.mean(token_lengths)\n    plt.axvline(avg_length, color='red', linestyle='--', \n               label=f'Average: {avg_length:.2f}')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class BPEDemo:     \"\"\"BPE\u306e\u30c7\u30e2\u30f3\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\"\"\"</p> <pre><code>def demonstrate_bpe_process(self):\n    \"\"\"BPE\u30d7\u30ed\u30bb\u30b9\u306e\u30c7\u30e2\"\"\"\n    print(\"=== BPE\u30d7\u30ed\u30bb\u30b9\u306e\u53ef\u8996\u5316 ===\\n\")\n\n    # \u30b5\u30f3\u30d7\u30eb\u30c6\u30ad\u30b9\u30c8\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog sat on the log\",\n        \"cats and dogs are pets\"\n    ]\n\n    # \u521d\u671f\u72b6\u614b\n    words = []\n    for text in corpus:\n        words.extend(text.split())\n\n    # \u5358\u8a9e\u3092\u6587\u5b57\u306b\u5206\u5272\n    word_splits = {}\n    for word in set(words):\n        word_splits[word] = list(word) + ['&lt;/w&gt;']\n\n    print(\"\u521d\u671f\u72b6\u614b\uff08\u6587\u5b57\u5206\u5272\uff09:\")\n    for word, splits in list(word_splits.items())[:5]:\n        print(f\"  {word}: {' '.join(splits)}\")\n\n    # \u30de\u30fc\u30b8\u30d7\u30ed\u30bb\u30b9\u306e\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\n    merges = [\n        ('t', 'h'),      # th\n        ('th', 'e'),     # the\n        ('a', 't'),      # at\n        ('s', 'at'),     # sat\n        ('o', 'n'),      # on\n    ]\n\n    print(\"\\n\\n\u30de\u30fc\u30b8\u30d7\u30ed\u30bb\u30b9:\")\n    current_splits = word_splits.copy()\n\n    for i, (a, b) in enumerate(merges):\n        print(f\"\\n\u30b9\u30c6\u30c3\u30d7 {i+1}: '{a}' + '{b}' \u2192 '{a+b}'\")\n\n        # \u30de\u30fc\u30b8\u3092\u9069\u7528\n        for word, splits in current_splits.items():\n            new_splits = []\n            j = 0\n            while j &lt; len(splits):\n                if j &lt; len(splits) - 1 and splits[j] == a and splits[j+1] == b:\n                    new_splits.append(a + b)\n                    j += 2\n                else:\n                    new_splits.append(splits[j])\n                    j += 1\n            current_splits[word] = new_splits\n\n        # \u5909\u66f4\u3055\u308c\u305f\u5358\u8a9e\u3092\u8868\u793a\n        for word, splits in list(current_splits.items())[:3]:\n            print(f\"    {word}: {' '.join(splits)}\")\n\n    # \u6700\u7d42\u7684\u306a\u30c8\u30fc\u30af\u30f3\u5316\n    self._visualize_tokenization_result(current_splits)\n\ndef _visualize_tokenization_result(self, word_splits: Dict[str, List[str]]):\n    \"\"\"\u30c8\u30fc\u30af\u30f3\u5316\u7d50\u679c\u306e\u53ef\u8996\u5316\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # \u30b5\u30f3\u30d7\u30eb\u6587\n    sentence = \"the cat sat on the mat\"\n    words = sentence.split()\n\n    y_pos = 0.5\n    x_pos = 0\n\n    colors = plt.cm.Set3(np.linspace(0, 1, 20))\n    color_idx = 0\n\n    for word in words:\n        tokens = word_splits.get(word, list(word) + ['&lt;/w&gt;'])\n\n        for token in tokens:\n            if token == '&lt;/w&gt;':\n                # \u5358\u8a9e\u5883\u754c\u30de\u30fc\u30ab\u30fc\n                width = 0.3\n                rect = plt.Rectangle((x_pos, y_pos), width, 0.3,\n                                   facecolor='lightgray', \n                                   edgecolor='black', linewidth=1)\n                ax.add_patch(rect)\n                ax.text(x_pos + width/2, y_pos + 0.15, '&lt;/w&gt;',\n                       ha='center', va='center', fontsize=8)\n            else:\n                # \u901a\u5e38\u306e\u30c8\u30fc\u30af\u30f3\n                width = len(token) * 0.15\n                rect = plt.Rectangle((x_pos, y_pos), width, 0.3,\n                                   facecolor=colors[color_idx % len(colors)],\n                                   edgecolor='black', linewidth=1)\n                ax.add_patch(rect)\n                ax.text(x_pos + width/2, y_pos + 0.15, token,\n                       ha='center', va='center', fontsize=10)\n                color_idx += 1\n\n            x_pos += width + 0.05\n\n        x_pos += 0.2  # \u5358\u8a9e\u9593\u306e\u30b9\u30da\u30fc\u30b9\n\n    ax.set_xlim(-0.1, x_pos)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.set_title('BPE Tokenization Result', fontsize=14, weight='bold')\n\n    # \u5143\u306e\u6587\u3092\u8868\u793a\n    ax.text(x_pos/2, 0.9, f'Original: \"{sentence}\"', \n           ha='center', va='center', fontsize=12, style='italic')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part5/tokenizer-details/#193-wordpiecesentencepiece","title":"19.3 WordPiece\u3068SentencePiece","text":"<p>class WordPieceTokenizer:     \"\"\"WordPiece\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self):\n    self.vocab = {}\n    self.unk_token = '[UNK]'\n    self.max_input_chars_per_word = 100\n\ndef train(self, texts: List[str], vocab_size: int = 1000):\n    \"\"\"WordPiece\u306e\u5b66\u7fd2\uff08\u7c21\u7565\u7248\uff09\"\"\"\n    print(\"=== WordPiece\u5b66\u7fd2 ===\\n\")\n\n    # \u521d\u671f\u8a9e\u5f59\u306e\u69cb\u7bc9\n    char_counts = Counter()\n    word_counts = Counter()\n\n    for text in texts:\n        words = text.lower().split()\n        for word in words:\n            word_counts[word] += 1\n            for char in word:\n                char_counts[char] += 1\n\n    # \u57fa\u672c\u8a9e\u5f59\n    self.vocab = {\n        '[PAD]': 0,\n        '[UNK]': 1,\n        '[CLS]': 2,\n        '[SEP]': 3,\n        '[MASK]': 4\n    }\n\n    # \u6587\u5b57\u3092\u8ffd\u52a0\n    for char, count in char_counts.most_common():\n        if len(self.vocab) &lt; 100:  # \u6700\u521d\u306e100\u306f\u6587\u5b57\u7528\n            self.vocab[char] = len(self.vocab)\n\n    # WordPiece\u306e\u8ffd\u52a0\n    print(f\"\u521d\u671f\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n\n    # \u30b5\u30d6\u30ef\u30fc\u30c9\u5019\u88dc\u306e\u751f\u6210\u3068\u8a55\u4fa1\n    while len(self.vocab) &lt; vocab_size:\n        # \u5019\u88dc\u3092\u751f\u6210\n        candidates = self._generate_candidates(word_counts)\n\n        if not candidates:\n            break\n\n        # \u30b9\u30b3\u30a2\u3092\u8a08\u7b97\uff08\u7c21\u7565\u7248\uff09\n        best_candidate = max(candidates, \n                           key=lambda x: self._score_candidate(x, word_counts))\n\n        # \u8a9e\u5f59\u306b\u8ffd\u52a0\n        if best_candidate.startswith('##'):\n            self.vocab[best_candidate] = len(self.vocab)\n        else:\n            self.vocab['##' + best_candidate] = len(self.vocab)\n\n        # \u9032\u6357\n        if len(self.vocab) % 100 == 0:\n            print(f\"\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n\n    print(f\"\\n\u6700\u7d42\u8a9e\u5f59\u30b5\u30a4\u30ba: {len(self.vocab)}\")\n\n    # WordPiece\u306e\u7279\u5fb4\u3092\u53ef\u8996\u5316\n    self._visualize_wordpiece_features()\n\ndef _generate_candidates(self, word_counts: Counter) -&gt; List[str]:\n    \"\"\"\u30b5\u30d6\u30ef\u30fc\u30c9\u5019\u88dc\u3092\u751f\u6210\"\"\"\n    candidates = set()\n\n    for word in word_counts:\n        for i in range(len(word)):\n            for j in range(i + 1, min(i + 10, len(word) + 1)):\n                subword = word[i:j]\n                if len(subword) &gt; 1:\n                    candidates.add(subword)\n\n    return list(candidates)\n\ndef _score_candidate(self, candidate: str, word_counts: Counter) -&gt; float:\n    \"\"\"\u5019\u88dc\u306e\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\"\"\"\n    score = 0\n    for word, count in word_counts.items():\n        if candidate in word:\n            score += count\n    return score\n\ndef tokenize(self, text: str) -&gt; List[str]:\n    \"\"\"\u30c6\u30ad\u30b9\u30c8\u3092\u30c8\u30fc\u30af\u30f3\u5316\"\"\"\n    output_tokens = []\n\n    for word in text.lower().split():\n        if len(word) &gt; self.max_input_chars_per_word:\n            output_tokens.append(self.unk_token)\n            continue\n\n        is_bad = False\n        sub_tokens = []\n        start = 0\n\n        while start &lt; len(word):\n            end = len(word)\n            cur_substr = None\n\n            while start &lt; end:\n                substr = word[start:end]\n                if start &gt; 0:\n                    substr = '##' + substr\n\n                if substr in self.vocab:\n                    cur_substr = substr\n                    break\n\n                end -= 1\n\n            if cur_substr is None:\n                is_bad = True\n                break\n\n            sub_tokens.append(cur_substr)\n            start = end\n\n        if is_bad:\n            output_tokens.append(self.unk_token)\n        else:\n            output_tokens.extend(sub_tokens)\n\n    return output_tokens\n\ndef _visualize_wordpiece_features(self):\n    \"\"\"WordPiece\u306e\u7279\u5fb4\u3092\u53ef\u8996\u5316\"\"\"\n    # ##\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u306e\u7d71\u8a08\n    prefix_tokens = [token for token in self.vocab.keys() \n                    if token.startswith('##')]\n\n    print(f\"\\n##\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\u4ed8\u304d\u30c8\u30fc\u30af\u30f3: {len(prefix_tokens)}\")\n    print(f\"\u4f8b: {prefix_tokens[:10]}\")\n\n    # \u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b\n    examples = [\n        \"playing\",\n        \"unbelievable\",\n        \"internationalization\"\n    ]\n\n    print(\"\\n\\n\u30c8\u30fc\u30af\u30f3\u5316\u306e\u4f8b:\")\n    for word in examples:\n        tokens = self.tokenize(word)\n        print(f\"  {word} \u2192 {tokens}\")\n</code></pre> <p>class SentencePieceDemo:     \"\"\"SentencePiece\u306e\u30c7\u30e2\"\"\"</p> <pre><code>def explain_sentencepiece(self):\n    \"\"\"SentencePiece\u306e\u8aac\u660e\"\"\"\n    print(\"=== SentencePiece ===\\n\")\n\n    print(\"\u7279\u5fb4:\")\n    print(\"1. \u8a00\u8a9e\u72ec\u7acb:\")\n    print(\"   - \u524d\u51e6\u7406\u4e0d\u8981\uff08\u30c8\u30fc\u30af\u30f3\u5316\u306a\u3057\uff09\")\n    print(\"   - \u751f\u306e\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u76f4\u63a5\u5b66\u7fd2\")\n    print(\"   - \u30b9\u30da\u30fc\u30b9\u3082\u901a\u5e38\u306e\u6587\u5b57\u3068\u3057\u3066\u6271\u3046\\n\")\n\n    print(\"2. \u53ef\u9006\u7684:\")\n    print(\"   - \u30c7\u30c8\u30fc\u30af\u30f3\u5316\u3067\u5143\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u5b8c\u5168\u5fa9\u5143\")\n    print(\"   - \u60c5\u5831\u306e\u640d\u5931\u306a\u3057\\n\")\n\n    print(\"3. \u30b5\u30d6\u30ef\u30fc\u30c9\u306e\u6b63\u898f\u5316:\")\n    print(\"   - \u78ba\u7387\u7684\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\")\n    print(\"   - \u8907\u6570\u306e\u5206\u5272\u5019\u88dc\u304b\u3089\u9078\u629e\\n\")\n\n    # \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6bd4\u8f03\n    self._compare_subword_algorithms()\n\ndef _compare_subword_algorithms(self):\n    \"\"\"\u30b5\u30d6\u30ef\u30fc\u30c9\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6bd4\u8f03\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n\n    # BPE\n    ax = axes[0]\n    ax.set_title('BPE', fontsize=12, weight='bold')\n\n    # BPE\u306e\u30de\u30fc\u30b8\u30d7\u30ed\u30bb\u30b9\n    bpe_steps = [\n        \"c a t s &lt;/w&gt;\",\n        \"ca t s &lt;/w&gt;\",\n        \"ca ts &lt;/w&gt;\",\n        \"cats &lt;/w&gt;\"\n    ]\n\n    for i, step in enumerate(bpe_steps):\n        y = 0.8 - i * 0.2\n        ax.text(0.5, y, step, ha='center', va='center',\n               fontsize=10, family='monospace',\n               bbox=dict(boxstyle=\"round,pad=0.3\", \n                       facecolor='lightblue', alpha=0.7))\n\n        if i &lt; len(bpe_steps) - 1:\n            ax.arrow(0.5, y - 0.05, 0, -0.1, \n                    head_width=0.05, head_length=0.02,\n                    fc='black', ec='black')\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.text(0.5, 0.95, 'Bottom-up Merging', ha='center', \n           fontsize=10, style='italic')\n\n    # WordPiece\n    ax = axes[1]\n    ax.set_title('WordPiece', fontsize=12, weight='bold')\n\n    # WordPiece\u306e\u5206\u5272\n    word = \"playing\"\n    tokens = [\"play\", \"##ing\"]\n\n    # \u5358\u8a9e\u5168\u4f53\n    rect = plt.Rectangle((0.2, 0.5), 0.6, 0.2,\n                       facecolor='lightgreen', edgecolor='black')\n    ax.add_patch(rect)\n    ax.text(0.5, 0.6, word, ha='center', va='center', fontsize=12)\n\n    # \u5206\u5272\u5f8c\n    x_pos = 0.2\n    for token in tokens:\n        width = 0.25\n        rect = plt.Rectangle((x_pos, 0.2), width, 0.15,\n                           facecolor='lightyellow', edgecolor='black')\n        ax.add_patch(rect)\n        ax.text(x_pos + width/2, 0.275, token, \n               ha='center', va='center', fontsize=10)\n        x_pos += width + 0.1\n\n    # \u77e2\u5370\n    ax.arrow(0.5, 0.48, 0, -0.1, head_width=0.05, head_length=0.02,\n            fc='black', ec='black')\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.text(0.5, 0.95, 'Likelihood Maximization', ha='center',\n           fontsize=10, style='italic')\n\n    # SentencePiece\n    ax = axes[2]\n    ax.set_title('SentencePiece', fontsize=12, weight='bold')\n\n    # \u751f\u30c6\u30ad\u30b9\u30c8\n    text = \"\u2581the\u2581cat\"\n    ax.text(0.5, 0.7, text, ha='center', va='center',\n           fontsize=12, family='monospace',\n           bbox=dict(boxstyle=\"round,pad=0.3\", \n                   facecolor='lightcoral', alpha=0.7))\n\n    # \u8907\u6570\u306e\u5206\u5272\u5019\u88dc\n    candidates = [\n        [\"\u2581the\", \"\u2581cat\"],\n        [\"\u2581th\", \"e\", \"\u2581cat\"],\n        [\"\u2581the\", \"\u2581c\", \"at\"]\n    ]\n\n    y_start = 0.4\n    for i, cand in enumerate(candidates):\n        y = y_start - i * 0.15\n        text = ' '.join(cand)\n        ax.text(0.5, y, text, ha='center', va='center',\n               fontsize=9, family='monospace',\n               bbox=dict(boxstyle=\"round,pad=0.2\",\n                       facecolor='lightyellow', alpha=0.5))\n\n    ax.text(0.8, 0.25, 'Sample', ha='center', fontsize=8,\n           style='italic', color='red')\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n    ax.text(0.5, 0.95, 'Unigram LM / BPE', ha='center',\n           fontsize=10, style='italic')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"part5/tokenizer-details/#194","title":"19.4 \u73fe\u4ee3\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc","text":"<p>class ModernTokenizers:     \"\"\"\u73fe\u4ee3\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5\u3068\u6bd4\u8f03\"\"\"</p> <pre><code>def compare_modern_tokenizers(self):\n    \"\"\"\u73fe\u4ee3\u7684\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6bd4\u8f03\"\"\"\n    print(\"=== \u73fe\u4ee3\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc ===\\n\")\n\n    tokenizers = {\n        \"GPT-2/GPT-3\": {\n            \"type\": \"BPE\",\n            \"vocab_size\": \"50,257\",\n            \"special\": \"Byte-level BPE\",\n            \"features\": \"\u30b9\u30da\u30fc\u30b9\u51e6\u7406\u306e\u6539\u5584\"\n        },\n\n        \"BERT\": {\n            \"type\": \"WordPiece\",\n            \"vocab_size\": \"30,522\",\n            \"special\": \"##\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\",\n            \"features\": \"\u4e8b\u524d\u30c8\u30fc\u30af\u30f3\u5316\u5fc5\u8981\"\n        },\n\n        \"T5/mT5\": {\n            \"type\": \"SentencePiece\",\n            \"vocab_size\": \"32,000\",\n            \"special\": \"\u2581(\u30b9\u30da\u30fc\u30b9\u30de\u30fc\u30ab\u30fc)\",\n            \"features\": \"\u8a00\u8a9e\u72ec\u7acb\"\n        },\n\n        \"LLaMA\": {\n            \"type\": \"SentencePiece (BPE)\",\n            \"vocab_size\": \"32,000\",\n            \"special\": \"Byte fallback\",\n            \"features\": \"\u672a\u77e5\u6587\u5b57\u306e\u51e6\u7406\"\n        },\n\n        \"ChatGPT\": {\n            \"type\": \"cl100k_base (tiktoken)\",\n            \"vocab_size\": \"100,277\",\n            \"special\": \"\u6539\u826f\u3055\u308c\u305fBPE\",\n            \"features\": \"\u52b9\u7387\u7684\u306a\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\"\n        }\n    }\n\n    # \u6bd4\u8f03\u8868\u793a\n    self._visualize_tokenizer_comparison(tokenizers)\n\n    # \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u52b9\u7387\u306e\u6bd4\u8f03\n    self._compare_encoding_efficiency()\n\ndef _visualize_tokenizer_comparison(self, tokenizers: Dict[str, Dict[str, str]]):\n    \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u6bd4\u8f03\u3092\u53ef\u8996\u5316\"\"\"\n    # \u8a9e\u5f59\u30b5\u30a4\u30ba\u306e\u6bd4\u8f03\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n    # \u8a9e\u5f59\u30b5\u30a4\u30ba\n    names = list(tokenizers.keys())\n    vocab_sizes = []\n    for name, info in tokenizers.items():\n        size_str = info[\"vocab_size\"].replace(\",\", \"\")\n        vocab_sizes.append(int(size_str))\n\n    colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n    bars = ax1.bar(names, vocab_sizes, color=colors)\n\n    # \u5024\u3092\u8868\u793a\n    for bar, size in zip(bars, vocab_sizes):\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f'{size:,}', ha='center', va='bottom')\n\n    ax1.set_ylabel('Vocabulary Size')\n    ax1.set_title('Vocabulary Sizes of Modern Tokenizers')\n    ax1.tick_params(axis='x', rotation=45)\n\n    # \u30bf\u30a4\u30d7\u5225\u5206\u985e\n    type_counts = Counter(info[\"type\"].split()[0] for info in tokenizers.values())\n\n    ax2.pie(type_counts.values(), labels=type_counts.keys(),\n           autopct='%1.1f%%', startangle=90)\n    ax2.set_title('Distribution of Tokenizer Types')\n\n    plt.tight_layout()\n    plt.show()\n\ndef _compare_encoding_efficiency(self):\n    \"\"\"\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u52b9\u7387\u306e\u6bd4\u8f03\"\"\"\n    print(\"\\n=== \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u52b9\u7387\u306e\u6bd4\u8f03 ===\")\n\n    # \u30b5\u30f3\u30d7\u30eb\u30c6\u30ad\u30b9\u30c8\n    samples = {\n        \"English\": \"The quick brown fox jumps over the lazy dog.\",\n        \"Code\": \"def factorial(n): return 1 if n &lt;= 1 else n * factorial(n-1)\",\n        \"Mixed\": \"Hello\u4e16\u754c! \ud83c\udf0d This is a test \u2192 \u03bbx.x+1\",\n        \"URL\": \"https://github.com/openai/gpt-3/blob/main/model.py\"\n    }\n\n    # \u4eee\u60f3\u7684\u306a\u30c8\u30fc\u30af\u30f3\u6570\uff08\u5b9f\u969b\u306e\u6bd4\u7387\u306b\u57fa\u3065\u304f\uff09\n    tokenizer_efficiency = {\n        \"GPT-2\": {\"English\": 11, \"Code\": 24, \"Mixed\": 18, \"URL\": 35},\n        \"BERT\": {\"English\": 12, \"Code\": 28, \"Mixed\": 22, \"URL\": 40},\n        \"T5\": {\"English\": 10, \"Code\": 25, \"Mixed\": 16, \"URL\": 38},\n        \"ChatGPT\": {\"English\": 9, \"Code\": 20, \"Mixed\": 14, \"URL\": 25}\n    }\n\n    # \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u8868\u793a\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    tokenizers = list(tokenizer_efficiency.keys())\n    text_types = list(samples.keys())\n\n    efficiency_matrix = np.array([\n        [tokenizer_efficiency[tok][txt] for txt in text_types]\n        for tok in tokenizers\n    ])\n\n    im = ax.imshow(efficiency_matrix, cmap='RdYlGn_r', aspect='auto')\n\n    # \u30e9\u30d9\u30eb\n    ax.set_xticks(np.arange(len(text_types)))\n    ax.set_yticks(np.arange(len(tokenizers)))\n    ax.set_xticklabels(text_types)\n    ax.set_yticklabels(tokenizers)\n\n    # \u5024\u3092\u8868\u793a\n    for i in range(len(tokenizers)):\n        for j in range(len(text_types)):\n            text = ax.text(j, i, efficiency_matrix[i, j],\n                         ha=\"center\", va=\"center\", color=\"black\")\n\n    ax.set_title('Token Count Comparison (Lower is Better)')\n    plt.colorbar(im, ax=ax, label='Number of Tokens')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>class TokenizerImplementationTips:     \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u5b9f\u88c5\u306e\u30d2\u30f3\u30c8\"\"\"</p> <pre><code>def share_best_practices(self):\n    \"\"\"\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u306e\u5171\u6709\"\"\"\n    print(\"=== \u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u5b9f\u88c5\u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9 ===\\n\")\n\n    practices = {\n        \"1. \u524d\u51e6\u7406\": [\n            \"Unicode\u6b63\u898f\u5316\uff08NFKC\uff09\",\n            \"\u7a7a\u767d\u6587\u5b57\u306e\u7d71\u4e00\",\n            \"\u7279\u6b8a\u6587\u5b57\u306e\u30a8\u30b9\u30b1\u30fc\u30d7\",\n            \"\u5927\u6587\u5b57\u5c0f\u6587\u5b57\u306e\u6271\u3044\u3092\u6c7a\u5b9a\"\n        ],\n\n        \"2. \u7279\u6b8a\u30c8\u30fc\u30af\u30f3\": [\n            \"[PAD], [UNK], [CLS], [SEP]\u306e\u8ffd\u52a0\",\n            \"\u30bf\u30b9\u30af\u56fa\u6709\u30c8\u30fc\u30af\u30f3\u306e\u8a2d\u8a08\",\n            \"\u4e88\u7d04\u9818\u57df\u306e\u78ba\u4fdd\",\n            \"\u30c8\u30fc\u30af\u30f3ID\u306e\u56fa\u5b9a\u5316\"\n        ],\n\n        \"3. \u52b9\u7387\u5316\": [\n            \"\u30c8\u30e9\u30a4\u6728\u3067\u306e\u9ad8\u901f\u691c\u7d22\",\n            \"\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u6d3b\u7528\",\n            \"\u30d0\u30c3\u30c1\u51e6\u7406\u306e\u5b9f\u88c5\",\n            \"\u4e26\u5217\u5316\u53ef\u80fd\u306a\u8a2d\u8a08\"\n        ],\n\n        \"4. \u5805\u7262\u6027\": [\n            \"\u672a\u77e5\u6587\u5b57\u306e\u9069\u5207\u306a\u51e6\u7406\",\n            \"\u6700\u5927\u9577\u306e\u5236\u9650\",\n            \"\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\",\n            \"\u30c7\u30d0\u30c3\u30b0\u60c5\u5831\u306e\u51fa\u529b\"\n        ]\n    }\n\n    for category, items in practices.items():\n        print(f\"{category}:\")\n        for item in items:\n            print(f\"  \u2022 {item}\")\n        print()\n\n    # \u5b9f\u88c5\u4f8b\n    self._show_implementation_example()\n\ndef _show_implementation_example(self):\n    \"\"\"\u5b9f\u88c5\u4f8b\u3092\u8868\u793a\"\"\"\n    print(\"\\n=== \u52b9\u7387\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5\u4f8b ===\\n\")\n\n    code = '''\n</code></pre> <p>class EfficientTokenizer:     \"\"\"\u52b9\u7387\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u5b9f\u88c5\"\"\"</p> <pre><code>def __init__(self, vocab: Dict[str, int]):\n    self.vocab = vocab\n    self.trie = self._build_trie(vocab)\n    self.cache = {}\n\ndef _build_trie(self, vocab: Dict[str, int]) -&gt; Dict:\n    \"\"\"\u30c8\u30e9\u30a4\u6728\u306e\u69cb\u7bc9\"\"\"\n    trie = {}\n    for token, token_id in vocab.items():\n        node = trie\n        for char in token:\n            if char not in node:\n                node[char] = {}\n            node = node[char]\n        node['&lt;END&gt;'] = token_id\n    return trie\n\ndef encode(self, text: str) -&gt; List[int]:\n    \"\"\"\u9ad8\u901f\u30a8\u30f3\u30b3\u30fc\u30c9\"\"\"\n    # \u30ad\u30e3\u30c3\u30b7\u30e5\u30c1\u30a7\u30c3\u30af\n    if text in self.cache:\n        return self.cache[text]\n\n    tokens = []\n    i = 0\n\n    while i &lt; len(text):\n        # \u6700\u9577\u4e00\u81f4\n        node = self.trie\n        longest_token_id = None\n        longest_end = i\n\n        for j in range(i, len(text)):\n            if text[j] not in node:\n                break\n            node = node[text[j]]\n            if '&lt;END&gt;' in node:\n                longest_token_id = node['&lt;END&gt;']\n                longest_end = j + 1\n\n        if longest_token_id is not None:\n            tokens.append(longest_token_id)\n            i = longest_end\n        else:\n            # Unknown token\n            tokens.append(self.vocab.get('&lt;UNK&gt;', 0))\n            i += 1\n\n    # \u30ad\u30e3\u30c3\u30b7\u30e5\u306b\u4fdd\u5b58\n    self.cache[text] = tokens\n    return tokens\n</code></pre> <p>'''</p> <pre><code>    print(code)\n</code></pre>"},{"location":"part5/tokenizer-details/#_4","title":"\u5b9f\u884c\u3068\u30c7\u30e2","text":"<p>def run_tokenizer_demo():     \"\"\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u30c7\u30e2\u3092\u5b9f\u884c\"\"\"     print(\"=\" * 70)     print(\"\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u8a73\u7d30\")     print(\"=\" * 70 + \"\\n\")</p> <pre><code># 1. \u57fa\u790e\u6982\u5ff5\nbasics = TokenizationBasics()\nbasics.explain_tokenization_challenges()\n\n# 2. BPE\nprint(\"\\n\")\nbpe = BytePairEncoding()\n\n# \u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30d1\u30b9\u3067BPE\u3092\u5b66\u7fd2\nsample_corpus = [\n    \"the cat sat on the mat\",\n    \"the dog sat on the log\", \n    \"cats and dogs are pets\",\n    \"the quick brown fox jumps\"\n]\n\nbpe.train(sample_corpus, vocab_size=50)\n\n# BPE\u306e\u30c7\u30e2\nprint(\"\\n\")\nbpe_demo = BPEDemo()\nbpe_demo.demonstrate_bpe_process()\n\n# 3. WordPiece\nprint(\"\\n\")\nwp = WordPieceTokenizer()\nwp.train(sample_corpus, vocab_size=100)\n\n# 4. SentencePiece\nprint(\"\\n\")\nsp_demo = SentencePieceDemo()\nsp_demo.explain_sentencepiece()\n\n# 5. \u73fe\u4ee3\u7684\u306a\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\nprint(\"\\n\")\nmodern = ModernTokenizers()\nmodern.compare_modern_tokenizers()\n\n# 6. \u5b9f\u88c5\u306e\u30d2\u30f3\u30c8\nprint(\"\\n\")\ntips = TokenizerImplementationTips()\ntips.share_best_practices()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u307e\u3068\u3081\")\nprint(\"=\" * 70)\nprint(\"\\n\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306e\u8981\u70b9:\")\nprint(\"\u2022 \u8a00\u8a9e\u306e\u591a\u69d8\u6027\u306b\u5bfe\u5fdc\u3059\u308b\u67d4\u8edf\u6027\")\nprint(\"\u2022 \u8a08\u7b97\u52b9\u7387\u3068\u8868\u73fe\u529b\u306e\u30d0\u30e9\u30f3\u30b9\")\nprint(\"\u2022 \u30b5\u30d6\u30ef\u30fc\u30c9\u5206\u5272\u306b\u3088\u308b\u672a\u77e5\u8a9e\u5bfe\u5fdc\")\nprint(\"\u2022 \u30bf\u30b9\u30af\u3068\u30e2\u30c7\u30eb\u306b\u9069\u3057\u305f\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u9078\u629e\")\nprint(\"\\n\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u306f\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u300c\u76ee\u300d\u3067\u3042\u308a\u3001\")\nprint(\"\u305d\u306e\u8a2d\u8a08\u304c\u30e2\u30c7\u30eb\u306e\u6027\u80fd\u306b\u5927\u304d\u304f\u5f71\u97ff\u3057\u307e\u3059\u3002\")\n</code></pre> <p>if name == \"main\":     run_tokenizer_demo()</p>"}]}